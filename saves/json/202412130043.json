[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v1",
                "updated": "2024-12-11T18:03:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21324v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21324v3",
                "updated": "2024-12-11T12:03:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    3,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-31T04:16:20Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    4,
                    16,
                    20,
                    2,
                    213,
                    0
                ],
                "title": "Pushing the Limits of In-Network Caching for Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pushing the Limits of In-Network Caching for Key-Value Stores"
                },
                "summary": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OrbitCache, a new in-network caching architecture that can cache\nvariable-length items to balance a wide range of key-value workloads. Unlike\nexisting works, OrbitCache does not cache hot items in the switch memory.\nInstead, we make hot items revisit the switch data plane continuously by\nexploiting packet recirculation. Our approach keeps cached key-value pairs in\nthe switch data plane while freeing them from item size limitations caused by\nhardware constraints. We implement an OrbitCache prototype on an Intel Tofino\nswitch. Our experimental results show that OrbitCache can balance highly skewed\nworkloads and is robust to various system conditions."
                },
                "authors": [
                    {
                        "name": "Gyuyeong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gyuyeong Kim"
                },
                "author": "Gyuyeong Kim",
                "arxiv_comment": "To be appeared in USENIX NSDI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21324v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21324v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12952v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12952v2",
                "updated": "2024-12-10T22:53:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    22,
                    53,
                    16,
                    1,
                    345,
                    0
                ],
                "published": "2024-03-19T17:54:34Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    17,
                    54,
                    34,
                    1,
                    79,
                    0
                ],
                "title": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Just Shift It: Test-Time Prototype Shifting for Zero-Shot Generalization\n  with Vision-Language Models"
                },
                "summary": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in vision-language models (VLMs) have propelled the field of\ncomputer vision, particularly in the zero-shot learning setting. Despite their\npromise, the effectiveness of these models often diminishes due to domain\nshifts in test environments. To address this, we introduce the Test-Time\nPrototype Shifting (TPS) framework, a pioneering approach designed to adapt\nVLMs to test datasets using unlabeled test inputs. Our method is based on the\nnotion of modulating per-class prototypes in the shared embedding space. By\npre-computing and caching prototypes generated with the pre-trained text\nencoder, TPS not only facilitates optimization-free prototype reuse for\nsubsequent predictions but also enables seamless integration with current\nadvancements in prompt engineering. At test-time, TPS dynamically learns shift\nvectors for each prototype based solely on the given test sample, effectively\nbridging the domain gap and enhancing classification accuracy. A notable aspect\nof our framework is its significantly reduced memory and computational demands\nwhen compared to conventional text-prompt tuning methods. Extensive evaluations\nacross 15 image classification datasets involving natural distribution shifts\nand cross-dataset generalization, as well as in context-dependent visual\nreasoning, demonstrate TPS's superior performance, achieving state-of-the-art\nresults while reducing resource requirements."
                },
                "authors": [
                    {
                        "name": "Elaine Sui"
                    },
                    {
                        "name": "Xiaohan Wang"
                    },
                    {
                        "name": "Serena Yeung-Levy"
                    }
                ],
                "author_detail": {
                    "name": "Serena Yeung-Levy"
                },
                "author": "Serena Yeung-Levy",
                "arxiv_comment": "Accepted at WACV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.12952v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12952v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07772v1",
                "updated": "2024-12-10T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    50,
                    1,
                    345,
                    0
                ],
                "title": "From Slow Bidirectional to Fast Causal Video Generators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Slow Bidirectional to Fast Causal Video Generators"
                },
                "summary": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current video diffusion models achieve impressive generation quality but\nstruggle in interactive applications due to bidirectional attention\ndependencies. The generation of a single frame requires the model to process\nthe entire sequence, including the future. We address this limitation by\nadapting a pretrained bidirectional diffusion transformer to a causal\ntransformer that generates frames on-the-fly. To further reduce latency, we\nextend distribution matching distillation (DMD) to videos, distilling 50-step\ndiffusion model into a 4-step generator. To enable stable and high-quality\ndistillation, we introduce a student initialization scheme based on teacher's\nODE trajectories, as well as an asymmetric distillation strategy that\nsupervises a causal student model with a bidirectional teacher. This approach\neffectively mitigates error accumulation in autoregressive generation, allowing\nlong-duration video synthesis despite training on short clips. Our model\nsupports fast streaming generation of high quality videos at 9.4 FPS on a\nsingle GPU thanks to KV caching. Our approach also enables streaming\nvideo-to-video translation, image-to-video, and dynamic prompting in a\nzero-shot manner. We will release the code based on an open-source model in the\nfuture."
                },
                "authors": [
                    {
                        "name": "Tianwei Yin"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Richard Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Fredo Durand"
                    },
                    {
                        "name": "Eli Shechtman"
                    },
                    {
                        "name": "Xun Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xun Huang"
                },
                "author": "Xun Huang",
                "arxiv_comment": "Project Page: https://causvid.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v1",
                "updated": "2024-12-10T18:50:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian PÃ¶ppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07720v1",
                "updated": "2024-12-10T18:13:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "published": "2024-12-10T18:13:20Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    13,
                    20,
                    1,
                    345,
                    0
                ],
                "title": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ACDiT: Interpolating Autoregressive Conditional Modeling and Diffusion\n  Transformer"
                },
                "summary": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The recent surge of interest in comprehensive multimodal models has\nnecessitated the unification of diverse modalities. However, the unification\nsuffers from disparate methodologies. Continuous visual generation necessitates\nthe full-sequence diffusion-based approach, despite its divergence from the\nautoregressive modeling in the text domain. We posit that autoregressive\nmodeling, i.e., predicting the future based on past deterministic experience,\nremains crucial in developing both a visual generation model and a potential\nunified multimodal model. In this paper, we explore an interpolation between\nthe autoregressive modeling and full-parameters diffusion to model visual\ninformation. At its core, we present ACDiT, an Autoregressive blockwise\nConditional Diffusion Transformer, where the block size of diffusion, i.e., the\nsize of autoregressive units, can be flexibly adjusted to interpolate between\ntoken-wise autoregression and full-sequence diffusion. ACDiT is easy to\nimplement, as simple as creating a Skip-Causal Attention Mask (SCAM) during\ntraining. During inference, the process iterates between diffusion denoising\nand autoregressive decoding that can make full use of KV-Cache. We verify the\neffectiveness of ACDiT on image and video generation tasks. We also demonstrate\nthat benefitted from autoregressive modeling, ACDiT can be seamlessly used in\nvisual understanding tasks despite being trained on the diffusion objective.\nThe analysis of the trade-off between autoregressive modeling and diffusion\ndemonstrates the potential of ACDiT to be used in long-horizon visual\ngeneration tasks. These strengths make it promising as the backbone of future\nunified models."
                },
                "authors": [
                    {
                        "name": "Jinyi Hu"
                    },
                    {
                        "name": "Shengding Hu"
                    },
                    {
                        "name": "Yuxuan Song"
                    },
                    {
                        "name": "Yufei Huang"
                    },
                    {
                        "name": "Mingxuan Wang"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Wei-Ying Ma"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14485v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14485v4",
                "updated": "2024-12-10T12:45:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    10,
                    12,
                    45,
                    31,
                    1,
                    345,
                    0
                ],
                "published": "2024-09-22T15:13:31Z",
                "published_parsed": [
                    2024,
                    9,
                    22,
                    15,
                    13,
                    31,
                    6,
                    266,
                    0
                ],
                "title": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-XL: Extra-Long Vision Language Model for Hour-Scale Video\n  Understanding"
                },
                "summary": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long video understanding poses a significant challenge for current\nMulti-modal Large Language Models (MLLMs). Notably, the MLLMs are constrained\nby their limited context lengths and the substantial costs while processing\nlong videos. Although several existing methods attempt to reduce visual tokens,\ntheir strategies encounter severe bottleneck, restricting MLLMs' ability to\nperceive fine-grained visual details. In this work, we propose Video-XL, a\nnovel approach that leverages MLLMs' inherent key-value (KV) sparsification\ncapacity to condense the visual input. Specifically, we introduce a new special\ntoken, the Visual Summarization Token (VST), for each interval of the video,\nwhich summarizes the visual information within the interval as its associated\nKV. The VST module is trained by instruction fine-tuning, where two optimizing\nstrategies are offered. 1.Curriculum learning, where VST learns to make small\n(easy) and large compression (hard) progressively. 2. Composite data curation,\nwhich integrates single-image, multi-image, and synthetic data to overcome the\nscarcity of long-video instruction data. The compression quality is further\nimproved by dynamic compression, which customizes compression granularity based\non the information density of different video intervals. Video-XL's\neffectiveness is verified from three aspects. First, it achieves a superior\nlong-video understanding capability, outperforming state-of-the-art models of\ncomparable sizes across multiple popular benchmarks. Second, it effectively\npreserves video information, with minimal compression loss even at 16x\ncompression ratio. Third, it realizes outstanding cost-effectiveness, enabling\nhigh-quality processing of thousands of frames on a single A100 GPU."
                },
                "authors": [
                    {
                        "name": "Yan Shu"
                    },
                    {
                        "name": "Zheng Liu"
                    },
                    {
                        "name": "Peitian Zhang"
                    },
                    {
                        "name": "Minghao Qin"
                    },
                    {
                        "name": "Junjie Zhou"
                    },
                    {
                        "name": "Zhengyang Liang"
                    },
                    {
                        "name": "Tiejun Huang"
                    },
                    {
                        "name": "Bo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Bo Zhao"
                },
                "author": "Bo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14485v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14485v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.05276v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.05276v3",
                "updated": "2024-12-09T01:44:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    44,
                    10,
                    0,
                    344,
                    0
                ],
                "published": "2024-11-08T02:21:19Z",
                "published_parsed": [
                    2024,
                    11,
                    8,
                    2,
                    21,
                    19,
                    4,
                    313,
                    0
                ],
                "title": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT Semantic Cache: Reducing LLM Costs and Latency via Semantic\n  Embedding Caching"
                },
                "summary": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, have revolutionized artificial\nintelligence by enabling nuanced understanding and generation of human-like\ntext across a wide range of applications. However, the high computational and\nfinancial costs associated with frequent API calls to these models present a\nsubstantial bottleneck, especially for applications like customer service\nchatbots that handle repetitive queries. In this paper, we introduce GPT\nSemantic Cache, a method that leverages semantic caching of query embeddings in\nin-memory storage (Redis). By storing embeddings of user queries, our approach\nefficiently identifies semantically similar questions, allowing for the\nretrieval of pre-generated responses without redundant API calls to the LLM.\nThis technique achieves a notable reduction in operational costs while\nsignificantly enhancing response times, making it a robust solution for\noptimizing LLM-powered applications. Our experiments demonstrate that GPT\nSemantic Cache reduces API calls by up to 68.8% across various query\ncategories, with cache hit rates ranging from 61.6% to 68.8%. Additionally, the\nsystem achieves high accuracy, with positive hit rates exceeding 97%,\nconfirming the reliability of cached responses. This technique not only reduces\noperational costs, but also improves response times, enhancing the efficiency\nof LLM-powered applications."
                },
                "authors": [
                    {
                        "name": "Sajal Regmi"
                    },
                    {
                        "name": "Chetan Phakami Pun"
                    }
                ],
                "author_detail": {
                    "name": "Chetan Phakami Pun"
                },
                "author": "Chetan Phakami Pun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.05276v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.05276v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.01844v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.01844v3",
                "updated": "2024-12-09T01:39:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    9,
                    1,
                    39,
                    15,
                    0,
                    344,
                    0
                ],
                "published": "2024-05-03T04:27:32Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    4,
                    27,
                    32,
                    4,
                    124,
                    0
                ],
                "title": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Privacy-Preserving Caching at Network Edge: Classification,\n  Solutions, and Challenges"
                },
                "summary": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching content at the edge network is a popular and effective technique\nwidely deployed to alleviate the burden of network backhaul, shorten service\ndelay and improve service quality. However, there has been some controversy\nover privacy violations in caching content at the edge network. On the one\nhand, the multi-access open edge network provides an ideal entrance or\ninterface for external attackers to obtain private data from edge caches by\nextracting sensitive information. On the other hand, privacy can be infringed\non by curious edge caching providers through caching trace analysis targeting\nthe achievement of better caching performance or higher profits. Therefore, an\nin-depth understanding of privacy issues in edge caching networks is vital and\nindispensable for creating a privacy-preserving caching service at the edge\nnetwork. In this article, we are among the first to fill this gap by examining\nprivacy-preserving techniques for caching content at the edge network. Firstly,\nwe provide an introduction to the background of privacy-preserving edge caching\n(PPEC). Next, we summarize the key privacy issues and present a taxonomy for\ncaching at the edge network from the perspective of private information.\nAdditionally, we conduct a retrospective review of the state-of-the-art\ncountermeasures against privacy leakage from content caching at the edge\nnetwork. Finally, we conclude the survey and envision challenges for future\nresearch."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    },
                    {
                        "name": "Shazia Riaz"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Linchang Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Linchang Xiao"
                },
                "author": "Linchang Xiao",
                "arxiv_doi": "10.1145/3706630",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3706630",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.01844v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.01844v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05896v1",
                "updated": "2024-12-08T11:32:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T11:32:08Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    32,
                    8,
                    6,
                    343,
                    0
                ],
                "title": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XKV: Personalized KV Cache Memory Reduction for Long-Context LLM\n  Inference"
                },
                "summary": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently the generative Large Language Model (LLM) has achieved remarkable\nsuccess in numerous applications. Notably its inference generates output tokens\none-by-one, leading to many redundant computations. The widely-used KV-Cache\nframework makes a compromise between time and space complexities. However,\ncaching data generates the increasingly growing memory demand, that can quickly\nexhaust the limited memory capacity of the modern accelerator like GPUs,\nparticularly in long-context inference tasks. Existing studies reduce memory\nconsumption by evicting some of cached data that have less important impact on\ninference accuracy. But the benefit in practice is far from ideal due to the\nstatic cache allocation across different LLM network layers. This paper\nobserves that the layer-specific cached data have very different impacts on\naccuracy. We quantify this difference, and give experimental and theoretical\nvalidation. We accordingly make a formal analysis and shows that customizing\nthe cache size for each layer in a personalized manner can yield a significant\nmemory reduction, while still providing comparable accuracy. We simulate the\ncache allocation as a combinatorial optimization problem and give a global\noptimal solution. In particular, we devise a mini- and sampling-based inference\nover a lightweight variant of the LLM model, so as to quickly capture the\ndifference and then feed it into the personalized algorithms. Extensive\nexperiments on real-world datasets demonstrate that our proposals can reduce KV\ncache memory consumption by 61.6% on average, improve computational efficiency\nby 2.1x and then increase the throughput by up to 5.5x."
                },
                "authors": [
                    {
                        "name": "Weizhuo Li"
                    },
                    {
                        "name": "Zhigang Wang"
                    },
                    {
                        "name": "Yu Gu"
                    },
                    {
                        "name": "Ge Yu"
                    }
                ],
                "author_detail": {
                    "name": "Ge Yu"
                },
                "author": "Ge Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05831v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05831v1",
                "updated": "2024-12-08T06:37:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "published": "2024-12-08T06:37:27Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    6,
                    37,
                    27,
                    6,
                    343,
                    0
                ],
                "title": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semi-Supervised Contrastive Learning for Controllable Video-to-Music\n  Retrieval"
                },
                "summary": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content creators often use music to enhance their videos, from soundtracks in\nmovies to background music in video blogs and social media content. However,\nidentifying the best music for a video can be a difficult and time-consuming\ntask. To address this challenge, we propose a novel framework for automatically\nretrieving a matching music clip for a given video, and vice versa. Our\napproach leverages annotated music labels, as well as the inherent artistic\ncorrespondence between visual and music elements. Distinct from previous\ncross-modal music retrieval works, our method combines both self-supervised and\nsupervised training objectives. We use self-supervised and label-supervised\ncontrastive learning to train a joint embedding space between music and video.\nWe show the effectiveness of our approach by using music genre labels for the\nsupervised training component, and our framework can be generalized to other\nmusic annotations (e.g., emotion, instrument, etc.). Furthermore, our method\nenables fine-grained control over how much the retrieval process focuses on\nself-supervised vs. label information at inference time. We evaluate the\nlearned embeddings through a variety of video-to-music and music-to-video\nretrieval tasks. Our experiments show that the proposed approach successfully\ncombines self-supervised and supervised objectives and is effective for\ncontrollable music-video retrieval."
                },
                "authors": [
                    {
                        "name": "Shanti Stewart"
                    },
                    {
                        "name": "Gouthaman KV"
                    },
                    {
                        "name": "Lie Lu"
                    },
                    {
                        "name": "Andrea Fanelli"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Fanelli"
                },
                "author": "Andrea Fanelli",
                "arxiv_comment": "4 pages + 1 reference page, 2 figures, 2 tables. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05831v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05831v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05704v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05704v1",
                "updated": "2024-12-07T17:22:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T17:22:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    17,
                    22,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultrafast lattice and electron dynamics induced in a PbSe crystal by an\n  intense terahertz pulse"
                },
                "summary": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We have studied the ultrafast optical response of a PbSe crystal to an\nintense picosecond terahertz pulse with a peak electric field strength of up to\n$\\sim$ 500 kV/cm. The reflectivity anisotropy signal contains oscillations at\nthe fundamental frequency of the resonant infrared-active phonon mode as well\nas its second, third, and fourth harmonics. The effect is ascribed to coherent\nanharmonic phonons resonantly excited by the strong terahertz field. Pump\nterahertz pulses also induce an almost instantaneous Kerr effect and a\nlong-lived optical anisotropy of the crystal with a characteristic decay time\nof $\\gtrsim$ 100 ps. We consider lattice distortion and phonon-assisted side\nvalley population as possible origins of this metastable state."
                },
                "authors": [
                    {
                        "name": "A. A. Melnikov"
                    },
                    {
                        "name": "Yu. G. Selivanov"
                    },
                    {
                        "name": "D. G. Poydashev"
                    },
                    {
                        "name": "S. V. Chekalin"
                    }
                ],
                "author_detail": {
                    "name": "S. V. Chekalin"
                },
                "author": "S. V. Chekalin",
                "arxiv_comment": "7 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05704v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05704v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05693v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05693v1",
                "updated": "2024-12-07T16:41:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-07T16:41:54Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    41,
                    54,
                    5,
                    342,
                    0
                ],
                "title": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Batch-Max: Higher LLM Throughput using Larger Batch Sizes and KV Cache\n  Compression"
                },
                "summary": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several works have developed eviction policies to remove key-value (KV) pairs\nfrom the KV cache for more efficient inference. The focus has been on\ncompressing the KV cache after the input prompt has been processed for faster\ntoken generation. In settings with limited GPU memory, and when the input\ncontext is longer than the generation length, we show that by also compressing\nthe KV cache during the input processing phase, larger batch sizes can be used\nresulting in significantly higher throughput while still maintaining the\noriginal model's accuracy."
                },
                "authors": [
                    {
                        "name": "Michael R. Metel"
                    },
                    {
                        "name": "Boxing Chen"
                    },
                    {
                        "name": "Mehdi Rezagholizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Mehdi Rezagholizadeh"
                },
                "author": "Mehdi Rezagholizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05693v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05693v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06567v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06567v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-06-03T13:28:43Z",
                "published_parsed": [
                    2024,
                    6,
                    3,
                    13,
                    28,
                    43,
                    0,
                    155,
                    0
                ],
                "title": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DHA: Learning Decoupled-Head Attention from Transformer Checkpoints via\n  Adaptive Heads Fusion"
                },
                "summary": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with billions of parameters demonstrate\nimpressive performance. However, the widely used Multi-Head Attention (MHA) in\nLLMs incurs substantial computational and memory costs during inference. While\nsome efforts have optimized attention mechanisms by pruning heads or sharing\nparameters among heads, these methods often lead to performance degradation or\nnecessitate substantial continued pre-training costs to restore performance.\nBased on the analysis of attention redundancy, we design a Decoupled-Head\nAttention (DHA) mechanism. DHA adaptively configures group sharing for key\nheads and value heads across various layers, achieving a better balance between\nperformance and efficiency. Inspired by the observation of clustering similar\nheads, we propose to progressively transform the MHA checkpoint into the DHA\nmodel through linear fusion of similar head parameters step by step, retaining\nthe parametric knowledge of the MHA checkpoint. We construct DHA models by\ntransforming various scales of MHA checkpoints given target head budgets. Our\nexperiments show that DHA remarkably requires a mere 0.25\\% of the original\nmodel's pre-training budgets to achieve 97.6\\% of performance while saving 75\\%\nof KV cache. Compared to Group-Query Attention (GQA), DHA achieves a 5$\\times$\ntraining acceleration, a maximum of 13.93\\% performance improvement under\n0.01\\% pre-training budget, and 4\\% relative improvement under 0.05\\%\npre-training budget."
                },
                "authors": [
                    {
                        "name": "Yilong Chen"
                    },
                    {
                        "name": "Linhao Zhang"
                    },
                    {
                        "name": "Junyuan Shang"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Tingwen Liu"
                    },
                    {
                        "name": "Shuohuan Wang"
                    },
                    {
                        "name": "Yu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yu Sun"
                },
                "author": "Yu Sun",
                "arxiv_comment": "Accepted at NeurIPS 2024 10 pages, 9 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06567v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06567v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03409v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03409v2",
                "updated": "2024-12-07T13:23:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    13,
                    23,
                    39,
                    5,
                    342,
                    0
                ],
                "published": "2024-12-04T15:48:59Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    15,
                    48,
                    59,
                    2,
                    339,
                    0
                ],
                "title": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixKV: Adaptive Prefix KV Cache is What Vision Instruction-Following\n  Models Need for Efficient Generation"
                },
                "summary": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large vision-language models (LVLMs) have rapidly gained popularity\nfor their strong generation and reasoning capabilities given diverse multimodal\ninputs. However, these models incur significant computational and memory\noverhead during inference, which greatly hinders the efficient deployment in\npractical scenarios. The extensive key-value (KV) cache, necessitated by the\nlengthy input and output sequences, notably contributes to the high inference\ncost. Based on this, recent works have investigated ways to reduce the KV cache\nsize for higher efficiency. Although effective, they generally overlook the\ndistinct importance distributions of KV vectors across layers and maintain the\nsame cache size for each layer during the next token prediction. This results\nin the significant contextual information loss for certain layers, leading to\nnotable performance decline. To address this, we present PrefixKV. It reframes\nthe challenge of determining KV cache sizes for all layers into the task of\nsearching for the optimal global prefix configuration. With an adaptive\nlayer-wise KV retention recipe based on binary search, the maximum contextual\ninformation can thus be preserved in each layer, facilitating the generation.\nExtensive experiments demonstrate that our method achieves the state-of-the-art\nperformance compared with others. It exhibits superior inference efficiency and\ngeneration quality trade-offs, showing promising potential for practical\napplications. Code is available at \\url{https://github.com/THU-MIG/PrefixKV}."
                },
                "authors": [
                    {
                        "name": "Ao Wang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Jianchao Tan"
                    },
                    {
                        "name": "Kefeng Zhang"
                    },
                    {
                        "name": "Xunliang Cai"
                    },
                    {
                        "name": "Zijia Lin"
                    },
                    {
                        "name": "Jungong Han"
                    },
                    {
                        "name": "Guiguang Ding"
                    }
                ],
                "author_detail": {
                    "name": "Guiguang Ding"
                },
                "author": "Guiguang Ding",
                "arxiv_comment": "12 pages, 5 figures;",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03409v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03409v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v2",
                "updated": "2024-12-07T04:08:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    7,
                    4,
                    8,
                    56,
                    5,
                    342,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05392v1",
                "updated": "2024-12-06T19:35:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T19:35:52Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    19,
                    35,
                    52,
                    4,
                    341,
                    0
                ],
                "title": "Effect of electric field on excitons in wide quantum wells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effect of electric field on excitons in wide quantum wells"
                },
                "summary": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A microscopic model of a heterostructure with a quantum well (QW) is proposed\nto study the exciton behavior in an external electric field. The effect of an\nelectric field ranging from 0 to 6 kV/cm applied to the GaAs/AlGaAs QW\nstructure in the growth direction is studied for several QWs of various widths\nup to 100 nm. The three-dimensional Schr\\\"odinger equation (SE) of exciton is\nnumerically solved using the finite difference method. Wave functions and\nenergies for several states of the heavy-hole and light-hole excitons are\ncalculated. Dependencies of the exciton state energy, the binding energy, the\nradiative broadening, and the static dipole moment on the applied electric\nfields are determined. The threshold of exciton dissociation for the 100-nm QW\nis also determined. In addition, we found the electric-field-induced shift of\nthe center of mass of the heavy-hole and light-hole exciton in the QWs.\nFinally, we have modeled reflection spectra of heterostructures with the\nGaAs/AlGaAs QWs in the electric field using the calculated energies and\nradiative broadenings of excitons."
                },
                "authors": [
                    {
                        "name": "Shiming Zheng"
                    },
                    {
                        "name": "E. S. Khramtsov"
                    },
                    {
                        "name": "I. V. Ignatiev"
                    }
                ],
                "author_detail": {
                    "name": "I. V. Ignatiev"
                },
                "author": "I. V. Ignatiev",
                "arxiv_comment": "12 pages, 8 figures, to be published in Physical Review B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05228v1",
                "updated": "2024-12-06T17:58:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T17:58:57Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    58,
                    57,
                    4,
                    341,
                    0
                ],
                "title": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MC3: Memory Contention based Covert Channel Communication on Shared DRAM\n  System-on-Chips"
                },
                "summary": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Shared-memory system-on-chips (SM-SoC) are ubiquitously employed by a\nwide-range of mobile computing platforms, including edge/IoT devices,\nautonomous systems and smartphones. In SM-SoCs, system-wide shared physical\nmemory enables a convenient and financially-feasible way to make data\naccessible by dozens of processing units (PUs), such as CPU cores and domain\nspecific accelerators. In this study, we investigate vulnerabilities that stem\nfrom the shared use of physical memory in such systems. Due to the diverse\ncomputational characteristics of the PUs they embed, SM-SoCs often do not\nemploy a shared last level cache (LLC). While the literature proposes covert\nchannel attacks for shared memory systems, high-throughput communication is\ncurrently possible by either relying on an LLC or privileged/physical access to\nthe shared memory subsystem.\n  In this study, we introduce a new memory-contention based covert\ncommunication attack, MC3, which specifically targets the shared system memory\nin mobile SoCs. Different from existing attacks, our approach achieves high\nthroughput communication between applications running on CPU and GPU without\nthe need for an LLC or elevated access to the system. We extensively explore\nthe effectiveness of our methodology by demonstrating the trade-off between the\nchannel transmission rate and the robustness of the communication. We\ndemonstrate the utility of MC3 on NVIDIA Orin AGX, Orin NX, and Orin Nano up to\na transmit rate of 6.4 kbps with less than 1% error rate."
                },
                "authors": [
                    {
                        "name": "Ismet Dagli"
                    },
                    {
                        "name": "James Crea"
                    },
                    {
                        "name": "Soner Seckiner"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "SelÃ§uk KÃ¶se"
                    },
                    {
                        "name": "Mehmet E. Belviranli"
                    }
                ],
                "author_detail": {
                    "name": "Mehmet E. Belviranli"
                },
                "author": "Mehmet E. Belviranli",
                "arxiv_comment": "This paper is accepted to 2025 Design, Automation Test in Europe\n  Conference Exhibition (DATE)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02031v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02031v2",
                "updated": "2024-12-06T11:47:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    47,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-07-02T07:59:08Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    7,
                    59,
                    8,
                    1,
                    184,
                    0
                ],
                "title": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftDiffusion: Efficient Diffusion Model Serving with Add-on Modules"
                },
                "summary": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) generation using diffusion models has become a\nblockbuster service in today's AI cloud. A production T2I service typically\ninvolves a serving workflow where a base diffusion model is augmented with\nvarious \"add-on\" modules, notably ControlNet and LoRA, to enhance image\ngeneration control. Compared to serving the base model alone, these add-on\nmodules introduce significant loading and computational overhead, resulting in\nincreased latency. In this paper, we present SwiftDiffusion, a system that\nefficiently serves a T2I workflow through a holistic approach. SwiftDiffusion\ndecouples ControNet from the base model and deploys it as a separate,\nindependently scaled service on dedicated GPUs, enabling ControlNet caching,\nparallelization, and sharing. To mitigate the high loading overhead of LoRA\nserving, SwiftDiffusion employs a bounded asynchronous LoRA loading (BAL)\ntechnique, allowing LoRA loading to overlap with the initial base model\nexecution by up to k steps without compromising image quality. Furthermore,\nSwiftDiffusion optimizes base model execution with a novel latent parallelism\ntechnique. Collectively, these designs enable SwiftDiffusion to outperform the\nstate-of-the-art T2I serving systems, achieving up to 7.8x latency reduction\nand 1.6x throughput improvement in serving SDXL models on H800 GPUs, without\nsacrificing image quality."
                },
                "authors": [
                    {
                        "name": "Suyi Li"
                    },
                    {
                        "name": "Lingyun Yang"
                    },
                    {
                        "name": "Xiaoxiao Jiang"
                    },
                    {
                        "name": "Hanfeng Lu"
                    },
                    {
                        "name": "Dakai An"
                    },
                    {
                        "name": "Zhipeng Di"
                    },
                    {
                        "name": "Weiyi Lu"
                    },
                    {
                        "name": "Jiawei Chen"
                    },
                    {
                        "name": "Kan Liu"
                    },
                    {
                        "name": "Yinghao Yu"
                    },
                    {
                        "name": "Tao Lan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Liping Zhang"
                    },
                    {
                        "name": "Wei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Wang"
                },
                "author": "Wei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02031v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02031v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04757v1",
                "updated": "2024-12-06T03:46:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T03:46:06Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    3,
                    46,
                    6,
                    4,
                    341,
                    0
                ],
                "title": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ltri-LLM: Streaming Long Context Inference for LLMs with Training-Free\n  Dynamic Triangular Attention Pattern"
                },
                "summary": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The quadratic computational complexity of the attention mechanism in current\nLarge Language Models (LLMs) renders inference with long contexts prohibitively\nexpensive. To address this challenge, various approaches aim to retain critical\nportions of the context to optimally approximate Full Attention (FA) through\nKey-Value (KV) compression or Sparse Attention (SA), enabling the processing of\nvirtually unlimited text lengths in a streaming manner. However, these methods\nstruggle to achieve performance levels comparable to FA, particularly in\nretrieval tasks. In this paper, our analysis of attention head patterns reveals\nthat LLMs' attention distributions show strong local correlations, naturally\nreflecting a chunking mechanism for input context. We propose Ltri-LLM\nframework, which divides KVs into spans, stores them in an offline index, and\nretrieves the relevant KVs into memory for various queries. Experimental\nresults on popular long text benchmarks show that Ltri-LLM can achieve\nperformance close to FA while maintaining efficient, streaming-based inference."
                },
                "authors": [
                    {
                        "name": "Hongyin Tang"
                    },
                    {
                        "name": "Di Xiu"
                    },
                    {
                        "name": "Lanrui Wang"
                    },
                    {
                        "name": "Xiurui Geng"
                    },
                    {
                        "name": "Jingang Wang"
                    },
                    {
                        "name": "Xunliang Cai"
                    }
                ],
                "author_detail": {
                    "name": "Xunliang Cai"
                },
                "author": "Xunliang Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04698v1",
                "updated": "2024-12-06T01:20:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "published": "2024-12-06T01:20:47Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    1,
                    20,
                    47,
                    4,
                    341,
                    0
                ],
                "title": "One-Hop Sub-Query Result Caches for Graph Database Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One-Hop Sub-Query Result Caches for Graph Database Systems"
                },
                "summary": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel one-hop sub-query result cache for processing\ngraph read transactions, gR-Txs, in a graph database system. The one-hop\nnavigation is from a vertex using either its in-coming or out-going edges with\nselection predicates that filter edges and vertices. Its cache entry identifies\na unique one-hop sub-query (key) and its result set consisting of immutable\nvertex ids (value). When processing a gR-Tx, the query processor identifies its\nsequence of individual one-hop sub-queries and looks up their results in the\ncache. A cache hit fetches less data from the storage manager and eliminates\nthe requirement to process the one-hop sub-query. A cache miss populates the\ncache asynchronously and in a transactional manner, maintaining the separation\nof read and write paths of our transactional storage manager. A graph read and\nwrite transaction, gRW-Tx, identifies the impacted cache entries and either\ndeletes or updates them. Our implementation of the cache is inside the graph\nquery processing engine and transparent to a user application. We evaluate the\ncache using our eCommerce production workload and with rules that re-write\ngraph queries to maximize the performance enhancements observed with the cache.\nObtained results show the cache enhances 95th and 99th percentile of query\nresponse times by at least 2x and 1.63x, respectively. When combined with query\nre-writing, the enhancements are at least 2.33x and 4.48x, respectively. An\ninteresting result is the significant performance enhancement observed by the\nindirect beneficiaries of the cache, gRW-Txs and gR-Txs that do not reference\none-hop sub-queries. The cache frees system resources to expedite their\nprocessing significantly."
                },
                "authors": [
                    {
                        "name": "Hieu Nguyen"
                    },
                    {
                        "name": "Jun Li"
                    },
                    {
                        "name": "Shahram Ghandeharizadeh"
                    }
                ],
                "author_detail": {
                    "name": "Shahram Ghandeharizadeh"
                },
                "author": "Shahram Ghandeharizadeh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04652v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04652v1",
                "updated": "2024-12-05T22:47:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:47:17Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    47,
                    17,
                    3,
                    340,
                    0
                ],
                "title": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cross-Self KV Cache Pruning for Efficient Vision-Language Inference"
                },
                "summary": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache pruning has emerged as a promising technique for reducing memory and\ncomputation costs in long-context auto-regressive generation. Existing methods\nfor vision-language models (VLMs) typically rely on self-attention scores from\nlarge language models (LLMs) to identify and prune irrelevant tokens. However,\nthese approaches overlook the inherent distributional discrepancies between\nmodalities, often leading to inaccurate token importance estimation and the\nover-pruning of critical visual tokens. To address this, we propose decomposing\nattention scores into intra-modality attention (within the same modality) and\ninter-modality attention (across modalities), enabling more precise KV cache\npruning by independently managing these distinct attention types. Additionally,\nwe introduce an n-softmax function to counteract distribution shifts caused by\npruning, preserving the original smoothness of attention scores and ensuring\nstable performance. Our final training-free method,\n\\textbf{C}ross-\\textbf{S}elf \\textbf{P}runing (CSP), achieves competitive\nperformance compared to models with full KV caches while significantly\noutperforming previous pruning methods. Extensive evaluations on MileBench, a\nbenchmark encompassing 29 multimodal datasets, demonstrate CSP's effectiveness,\nachieving up to a 41\\% performance improvement on challenging tasks like\nconversational embodied dialogue while reducing the KV cache budget by 13.6\\%.\nThe code is available at https://github.com/TerryPei/CSP"
                },
                "authors": [
                    {
                        "name": "Xiaohuan Pei"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Chang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chang Xu"
                },
                "author": "Chang Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04652v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04652v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04634v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04634v1",
                "updated": "2024-12-05T22:06:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T22:06:23Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    22,
                    6,
                    23,
                    3,
                    340,
                    0
                ],
                "title": "Neural Two-Level Monte Carlo Real-Time Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Two-Level Monte Carlo Real-Time Rendering"
                },
                "summary": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce an efficient Two-Level Monte Carlo (subset of Multi-Level Monte\nCarlo, MLMC) estimator for real-time rendering of scenes with global\nillumination. Using MLMC we split the shading integral into two parts: the\nradiance cache integral and the residual error integral that compensates for\nthe bias of the first one. For the first part, we developed the Neural Incident\nRadiance Cache (NIRC) leveraging the power of fully-fused tiny neural networks\nas a building block, which is trained on the fly. The cache is designed to\nprovide a fast and reasonable approximation of the incident radiance: an\nevaluation takes 2-25x less compute time than a path tracing sample. This\nenables us to estimate the radiance cache integral with a high number of\nsamples and by this achieve faster convergence. For the residual error\nintegral, we compute the difference between the NIRC predictions and the\nunbiased path tracing simulation. Our method makes no assumptions about the\ngeometry, materials, or lighting of a scene and has only few intuitive\nhyper-parameters. We provide a comprehensive comparative analysis in different\nexperimental scenarios. Since the algorithm is trained in an on-line fashion,\nit demonstrates significant noise level reduction even for dynamic scenes and\ncan easily be combined with other importance sampling schemes and noise\nreduction techniques."
                },
                "authors": [
                    {
                        "name": "Mikhail Dereviannykh"
                    },
                    {
                        "name": "Dmitrii Klepikov"
                    },
                    {
                        "name": "Johannes Hanika"
                    },
                    {
                        "name": "Carsten Dachsbacher"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Dachsbacher"
                },
                "author": "Carsten Dachsbacher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04634v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04634v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04449v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04449v1",
                "updated": "2024-12-05T18:58:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-05T18:58:03Z",
                "published_parsed": [
                    2024,
                    12,
                    5,
                    18,
                    58,
                    3,
                    3,
                    340,
                    0
                ],
                "title": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "p-MoD: Building Mixture-of-Depths MLLMs via Progressive Ratio Decay"
                },
                "summary": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable performance of multimodal large language models\n(MLLMs) across diverse tasks, the substantial training and inference costs\nimpede their advancement. The majority of computation stems from the\noverwhelming volume of vision tokens processed by the transformer decoder. In\nthis paper, we propose to build efficient MLLMs by leveraging the\nMixture-of-Depths (MoD) mechanism, where each transformer decoder layer selects\nessential vision tokens to process while skipping redundant ones. However,\nintegrating MoD into MLLMs is non-trivial. To address the challenges of\ntraining and inference stability as well as limited training data, we adapt the\nMoD module with two novel designs: tanh-gated weight normalization (TanhNorm)\nand symmetric token reweighting (STRing). Moreover, we observe that vision\ntokens exhibit higher redundancy in deeper layer and thus design a progressive\nratio decay (PRD) strategy, which gradually reduces the token retention ratio\nlayer by layer, employing a shifted cosine schedule. This crucial design fully\nunleashes the potential of MoD, significantly boosting the efficiency and\nperformance of our models. To validate the effectiveness of our approach, we\nconduct extensive experiments with two baseline models across 14 benchmarks.\nOur model, p-MoD, matches or even surpasses the performance of the baseline\nmodels, with only 55.6% TFLOPs and 53.8% KV cache storage during inference, and\n77.7% GPU hours during training."
                },
                "authors": [
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Desen Meng"
                    },
                    {
                        "name": "Ji Qi"
                    },
                    {
                        "name": "Zhenpeng Huang"
                    },
                    {
                        "name": "Tao Wu"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "Technical Report; Code released at https://github.com/MCG-NJU/p-MoD",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04449v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04449v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.03960v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.03960v2",
                "updated": "2024-12-05T14:56:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    14,
                    56,
                    56,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-04T22:45:26Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    22,
                    45,
                    26,
                    4,
                    278,
                    0
                ],
                "title": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwiftKV: Fast Prefill-Optimized Inference with Knowledge-Preserving\n  Model Transformation"
                },
                "summary": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM inference for popular enterprise use cases, such as summarization, RAG,\nand code-generation, typically observes orders of magnitude longer prompt\nlengths than generation lengths. This characteristic leads to high cost of\nprefill and increased response latency. In this paper, we present SwiftKV, a\nnovel model transformation and distillation procedure specifically designed to\nreduce the time and cost of processing prompt tokens while preserving high\nquality of generated tokens. SwiftKV combines three key mechanisms: i)\nSingleInputKV, which prefills later layers' KV cache using a much earlier\nlayer's output, allowing prompt tokens to skip much of the model computation,\nii) AcrossKV, which merges the KV caches of neighboring layers to reduce the\nmemory footprint and support larger batch size for higher throughput, and iii)\na knowledge-preserving distillation procedure that can adapt existing LLMs for\nSwiftKV with minimal accuracy impact and low compute and data requirement. For\nLlama-3.1-8B and 70B, SwiftKV reduces the compute requirement of prefill by 50%\nand the memory requirement of the KV cache by 62.5% while incurring minimum\nquality degradation across a wide range of tasks. In the end-to-end inference\nserving using an optimized vLLM implementation, SwiftKV realizes up to 2x\nhigher aggregate throughput and 60% lower time per output token. It can achieve\na staggering 560 TFlops/GPU of normalized inference throughput, which\ntranslates to 16K tokens/s for Llama-3.1-70B in 16-bit precision on 4x H100\nGPUs. Our training, inference, and model implementations are open-sourced and\ncan be found through\nhttps://huggingface.co/collections/Snowflake/swiftkv-models-674f7d7474eb789e185d31cb."
                },
                "authors": [
                    {
                        "name": "Aurick Qiao"
                    },
                    {
                        "name": "Zhewei Yao"
                    },
                    {
                        "name": "Samyam Rajbhandari"
                    },
                    {
                        "name": "Yuxiong He"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiong He"
                },
                "author": "Yuxiong He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.03960v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.03960v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19574v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19574v2",
                "updated": "2024-12-05T12:19:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    12,
                    19,
                    38,
                    3,
                    340,
                    0
                ],
                "published": "2024-11-29T09:42:38Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    9,
                    42,
                    38,
                    4,
                    334,
                    0
                ],
                "title": "KV Shifting Attention Enhances Language Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV Shifting Attention Enhances Language Modeling"
                },
                "summary": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current large language models are mainly based on decode-only structure\ntransformers, which have great in-context learning (ICL) capabilities. It is\ngenerally believed that the important foundation of its ICL capability is the\ninduction heads mechanism, which requires at least two layers attention. In\norder to more efficiently implement the ability of the model's induction, we\nrevisit the induction heads mechanism and proposed a KV shifting attention. We\ntheoretically prove that the KV shifting attention reducing the model's\nrequirements for the depth and width of the induction heads mechanism. Our\nexperimental results demonstrate that KV shifting attention is beneficial to\nlearning induction heads and language modeling, which lead to better\nperformance or faster convergence from toy models to the pre-training models\nwith more than 10 B parameters."
                },
                "authors": [
                    {
                        "name": "Mingyu Xu"
                    },
                    {
                        "name": "Wei Cheng"
                    },
                    {
                        "name": "Bingning Wang"
                    },
                    {
                        "name": "Weipeng Chen"
                    }
                ],
                "author_detail": {
                    "name": "Weipeng Chen"
                },
                "author": "Weipeng Chen",
                "arxiv_comment": "22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19574v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19574v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01485v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01485v2",
                "updated": "2024-12-05T06:52:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    6,
                    52,
                    42,
                    3,
                    340,
                    0
                ],
                "published": "2024-10-02T12:35:53Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    12,
                    35,
                    53,
                    2,
                    276,
                    0
                ],
                "title": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Little Goes a Long Way: Efficient Long Context Training and Inference\n  with Partial Contexts"
                },
                "summary": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training and serving long-context large language models (LLMs) incurs\nsubstantial overhead. To address this, two critical steps are often required: a\npretrained LLM typically undergoes a separate stage for context length\nextension by training on long-context data, followed by architectural\nmodifications to reduce the overhead of KV cache during serving. This paper\nargues that integrating length extension with a GPU-friendly KV cache reduction\narchitecture not only reduces training overhead during length extension, but\nalso achieves better long-context performance. This leads to our proposed\nLongGen, which finetunes a pretrained LLM into an efficient architecture during\nlength extension. LongGen builds on three key insights: (1) Sparse attention\npatterns, such as window attention (attending to recent tokens), attention sink\n(initial ones), and blockwise sparse attention (strided token blocks) are\nwell-suited for building efficient long-context models, primarily due to their\nGPU-friendly memory access patterns, enabling efficiency gains not just\ntheoretically but in practice as well. (2) It is essential for the model to\nhave direct access to all tokens. A hybrid architecture with 1/3 full attention\nlayers and 2/3 efficient ones achieves a balanced trade-off between efficiency\nand long-context performance. (3) Lightweight training on 5B long-context data\nis sufficient to extend the hybrid model's context length from 4K to 128K.\n  We evaluate LongGen on both Llama-2 7B and Llama-2 70B, demonstrating its\neffectiveness across different scales. During training with 128K-long contexts,\nLongGen achieves 1.55x training speedup and reduces wall-clock time by 36%,\ncompared to a full-attention baseline. During inference, LongGen reduces KV\ncache memory by 62%, achieving 1.67x prefilling speedup and 1.41x decoding\nspeedup."
                },
                "authors": [
                    {
                        "name": "Suyu Ge"
                    },
                    {
                        "name": "Xihui Lin"
                    },
                    {
                        "name": "Yunan Zhang"
                    },
                    {
                        "name": "Jiawei Han"
                    },
                    {
                        "name": "Hao Peng"
                    }
                ],
                "author_detail": {
                    "name": "Hao Peng"
                },
                "author": "Hao Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01485v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01485v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v3",
                "updated": "2024-12-05T04:29:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    4,
                    29,
                    49,
                    3,
                    340,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "01. AI"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Albert Wang"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Ethan Dai"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zirui Zhang"
                },
                "author": "Zirui Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2305.01516v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2305.01516v2",
                "updated": "2024-12-05T01:50:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    5,
                    1,
                    50,
                    27,
                    3,
                    340,
                    0
                ],
                "published": "2023-05-02T15:27:16Z",
                "published_parsed": [
                    2023,
                    5,
                    2,
                    15,
                    27,
                    16,
                    1,
                    122,
                    0
                ],
                "title": "F2: Designing a Key-Value Store for Large Skewed Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F2: Designing a Key-Value Store for Large Skewed Workloads"
                },
                "summary": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many real-world workloads present a challenging set of requirements: point\noperations requiring high throughput, working sets much larger than main\nmemory, and natural skew in key access patterns for both reads and writes. We\nfind that modern key-value designs are either optimized for memory-efficiency,\nsacrificing high-performance (LSM-tree designs), or achieve high-performance,\nsaturating modern NVMe SSD bandwidth, at the cost of substantial memory\nresources or high disk wear (CPU-optimized designs). Unfortunately these\ndesigns are not able to handle meet the challenging demands of such\nlarger-than-memory, skewed workloads.\n  To this end, we present F2, a new key-value store that bridges this gap by\ncombining the strengths of both approaches. F2 adopts a tiered, record-oriented\narchitecture inspired by LSM-trees to effectively separate hot from cold\nrecords, while incorporating concurrent latch-free mechanisms from\nCPU-optimized engines to maximize performance on modern NVMe SSDs. To realize\nthis design, we tackle key challenges and introduce several innovations,\nincluding new latch-free algorithms for multi-threaded log compaction and user\noperations (e.g., RMWs), as well as new components: a two-level hash index to\nreduce indexing overhead for cold records and a read-cache for serving read-hot\ndata.\n  Detailed experimental results show that F2 matches or outperforms existing\nsolutions, achieving on average better throughput on memory-constrained\nenvironments compared to state-of-the-art systems like RocksDB (11.75x),\nSplinterDB (4.52x), KVell (10.56x), LeanStore (2.04x), and FASTER (2.38x). F2\nalso maintains its high performance across varying workload skewness levels and\nmemory budgets, while achieving low disk write amplification."
                },
                "authors": [
                    {
                        "name": "Konstantinos Kanellis"
                    },
                    {
                        "name": "Badrish Chandramouli"
                    },
                    {
                        "name": "Shivaram Venkataraman"
                    }
                ],
                "author_detail": {
                    "name": "Shivaram Venkataraman"
                },
                "author": "Shivaram Venkataraman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2305.01516v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2305.01516v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19379v2",
                "updated": "2024-12-04T18:40:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    18,
                    40,
                    24,
                    2,
                    339,
                    0
                ],
                "published": "2024-11-28T21:10:20Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    21,
                    10,
                    20,
                    3,
                    333,
                    0
                ],
                "title": "Marconi: Prefix Caching for the Era of Hybrid LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Marconi: Prefix Caching for the Era of Hybrid LLMs"
                },
                "summary": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid models that combine the language modeling capabilities of Attention\nlayers with the efficiency of Recurrent layers (e.g., State Space Models) have\ngained traction in practically supporting long contexts in Large Language Model\nserving. Yet, the unique properties of these models complicate the usage of\ncomplementary efficiency optimizations such as prefix caching that skip\nredundant computations across requests. Most notably, their use of in-place\nstate updates for recurrent layers precludes rolling back cache entries for\npartial sequence overlaps, and instead mandates only exact-match cache hits;\nthe effect is a deluge of (large) cache entries per sequence, most of which\nyield minimal reuse opportunities. We present Marconi, the first system that\nsupports efficient prefix caching with Hybrid LLMs. Key to Marconi are its\nnovel admission and eviction policies that more judiciously assess potential\ncache entries based not only on recency, but also on (1) forecasts of their\nreuse likelihood across a taxonomy of different hit scenarios, and (2) the\ncompute savings that hits deliver relative to memory footprints. Across diverse\nworkloads and Hybrid models, Marconi achieves up to 34.4$\\times$ higher token\nhit rates (71.1% or 617 ms lower TTFT) compared to state-of-the-art prefix\ncaching systems."
                },
                "authors": [
                    {
                        "name": "Rui Pan"
                    },
                    {
                        "name": "Zhuang Wang"
                    },
                    {
                        "name": "Zhen Jia"
                    },
                    {
                        "name": "Can Karakus"
                    },
                    {
                        "name": "Luca Zancato"
                    },
                    {
                        "name": "Tri Dao"
                    },
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ravi Netravali"
                    }
                ],
                "author_detail": {
                    "name": "Ravi Netravali"
                },
                "author": "Ravi Netravali",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03361v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03361v1",
                "updated": "2024-12-04T14:47:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T14:47:42Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    14,
                    47,
                    42,
                    2,
                    339,
                    0
                ],
                "title": "Measurement of electron beam induced sample heating in SEM experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measurement of electron beam induced sample heating in SEM experiments"
                },
                "summary": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scanning Electron Microscopy (SEM) experiments provide detailed insights into\nmaterial microstructures, enabling high-resolution imaging as well as\ncrystallographic analysis through advanced techniques like Electron Backscatter\nDiffraction (EBSD). However, the interaction of the high-energy electron beam\nwith the material can lead to localized heating, which may significantly impact\nspecimen integrity, especially in applications requiring prolonged beam\nexposure, for instance when mapping the crystal structure using EBSD. This\nstudy examines electron-beam-induced heating effects on a model metal sample\n(iron), directly measuring the locally deposited electron beam energy with a\nMEMS-based heating device and validating these measurements through\nsimulations, including Monte Carlo and Finite Element methods. The analysis\nfocuses on the effects of various experimental parameters such as acceleration\nvoltage (from 5 to 30 kV), beam current (from 0.17 nA to 22 nA), dwell time\n(from 1$\\mu$s to 1ms) and sample tilt (0{\\deg} to 70{\\deg}). The findings\nreveal that local sample temperatures can increase by up to 70 {\\deg}C during\nEBSD experiments, primarily affected by the choice in beam current and\nacceleration voltage, with beam current having the most significant impact."
                },
                "authors": [
                    {
                        "name": "Christina Koenig"
                    },
                    {
                        "name": "Alice Bastos da Silva Fanta"
                    },
                    {
                        "name": "Joerg R. Jinschek"
                    }
                ],
                "author_detail": {
                    "name": "Joerg R. Jinschek"
                },
                "author": "Joerg R. Jinschek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03361v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03361v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03213v1",
                "updated": "2024-12-04T10:58:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T10:58:27Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    10,
                    58,
                    27,
                    2,
                    339,
                    0
                ],
                "title": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ClusterKV: Manipulating LLM KV Cache in Semantic Space for Recallable\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely deployed in a variety of\napplications, and the context length is rapidly increasing to handle tasks such\nas long-document QA and complex logical reasoning. However, long context poses\nsignificant challenges for inference efficiency, including high memory costs of\nkey-value (KV) cache and increased latency due to extensive memory accesses.\nRecent works have proposed compressing KV cache to approximate computation, but\nthese methods either evict tokens permanently, never recalling them for later\ninference, or recall previous tokens at the granularity of pages divided by\ntextual positions. Both approaches degrade the model accuracy and output\nquality. To achieve efficient and accurate recallable KV cache compression, we\nintroduce ClusterKV, which recalls tokens at the granularity of semantic\nclusters. We design and implement efficient algorithms and systems for\nclustering, selection, indexing and caching. Experiment results show that\nClusterKV attains negligible accuracy loss across various tasks with 32k\ncontext lengths, using only a 1k to 2k KV cache budget, and achieves up to a\n2$\\times$ speedup in latency and a 2.5$\\times$ improvement in decoding\nthroughput. Compared to SoTA recallable KV compression methods, ClusterKV\ndemonstrates higher model accuracy and output quality, while maintaining or\nexceeding inference efficiency."
                },
                "authors": [
                    {
                        "name": "Guangda Liu"
                    },
                    {
                        "name": "Chengwei Li"
                    },
                    {
                        "name": "Jieru Zhao"
                    },
                    {
                        "name": "Chenqi Zhang"
                    },
                    {
                        "name": "Minyi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Minyi Guo"
                },
                "author": "Minyi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03131v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03131v1",
                "updated": "2024-12-04T08:51:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T08:51:23Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    8,
                    51,
                    23,
                    2,
                    339,
                    0
                ],
                "title": "Unifying KV Cache Compression for Large Language Models with LeanKV",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying KV Cache Compression for Large Language Models with LeanKV"
                },
                "summary": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate exceptional performance but incur\nhigh serving costs due to substantial memory demands, with the key-value (KV)\ncache being a primary bottleneck. Existing KV cache compression methods,\nincluding quantization and pruning, struggle with limitations such as uniform\ntreatment of keys and values and static memory allocation across attention\nheads. To address these challenges, we introduce LeanKV, a unified KV cache\ncompression framework that enhances LLM serving efficiency without compromising\naccuracy through three innovations: (1) Hetero-KV quantization, which stores\nkeys at a higher precision than values to reflect their greater impact on\nattention computations; (2) per-head dynamic sparsity, which allocates memory\nbased on token importance per head and per request; and (3) unified KV\ncompression, integrating mixed-precision quantization and selective pruning to\nenable a smooth tradeoff between model accuracy and memory efficiency. To\nefficiently support these techniques, LeanKV introduces systems optimizations\nincluding unified paging and on-GPU parallel memory management. Implemented on\nvLLM, LeanKV compresses the KV cache by $3.0\\times$ to $5.0\\times$ without\naccuracy loss and up to $11.0\\times$ with under 5% accuracy loss, enhancing\nthroughput by $1.9\\times$ to $2.5\\times$, and up to $6.9\\times$."
                },
                "authors": [
                    {
                        "name": "Yanqi Zhang"
                    },
                    {
                        "name": "Yuwei Hu"
                    },
                    {
                        "name": "Runyuan Zhao"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03131v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03131v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2303.08066v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2303.08066v2",
                "updated": "2024-12-04T05:32:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    5,
                    32,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2023-02-06T13:46:08Z",
                "published_parsed": [
                    2023,
                    2,
                    6,
                    13,
                    46,
                    8,
                    0,
                    37,
                    0
                ],
                "title": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PASCAL: A Learning-aided Cooperative Bandwidth Control Policy for\n  Hierarchical Storage Systems"
                },
                "summary": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nowadays, the Hierarchical Storage System (HSS) is considered as an ideal\nmodel to meet the cost-performance demand. The data migration between storing\ntiers of HSS is the way to achieve the cost-performance goal. The bandwidth\ncontrol is to limit the maximum amount of data migration. Most of previous\nresearch about HSS focus on studying the data migration policy instead of\nbandwidth control. However, the recent research about cache and networking\noptimization suggest that the bandwidth control has significant impact on the\nsystem performance. Few previous work achieves a satisfactory bandwidth control\nin HSS since it is hard to control bandwidth for so many data migration tasks\nsimultaneously. In this paper, we first give a stochastic programming model to\nformalize the bandwidth control problem in HSS. Then we propose a\nlearning-aided bandwidth control policy for HSS, named \\Pascal{}, which learns\nto control the bandwidth of different data migration task in an cooperative\nway. We implement \\Pascal{} on a commercial HSS and compare it with three\nstrong baselines over a group of workloads. Our evaluation on the physical\nsystem shows that \\Pascal{} can effectively decrease 1.95X the tail latency and\ngreatly improve throughput stability (2X $\\downarrow$ throughput jitter), and\nmeanwhile keep the throughput at a relatively high level."
                },
                "authors": [
                    {
                        "name": "Xijun Li"
                    },
                    {
                        "name": "Yunfan Zhou"
                    },
                    {
                        "name": "Ji Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ji Zhang"
                },
                "author": "Ji Zhang",
                "arxiv_comment": "for modifying part of contents",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2303.08066v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2303.08066v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03023v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03023v1",
                "updated": "2024-12-04T04:29:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "published": "2024-12-04T04:29:12Z",
                "published_parsed": [
                    2024,
                    12,
                    4,
                    4,
                    29,
                    12,
                    2,
                    339,
                    0
                ],
                "title": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Multi-Functional Web Tool for Comprehensive Threat Detection Through\n  IP Address Analysis"
                },
                "summary": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the advances in digitalisation have also adversely\ncontributed to the significant rise in cybercrimes. Hence, building the threat\nintelligence to shield against rising cybercrimes has become a fundamental\nrequisite. Internet Protocol (IP) addresses play a crucial role in the threat\nintelligence and prevention of cyber crimes. However, we have noticed the lack\nof one-stop, free, and open-source tools that can analyse IP addresses. Hence,\nthis work introduces a comprehensive web tool for advanced IP address\ncharacterisation. Our tool offers a wide range of features, including\ngeolocation, blocklist check, VPN detection, proxy detection, bot detection,\nTor detection, port scan, and accurate domain statistics that include the\ndetails about the name servers and registrar information. In addition, our tool\ncalculates a confidence score based on a weighted sum of publicly accessible\nonline results from different reliable sources to give users a dependable\nmeasure of accuracy. Further, to improve performance, our tool also\nincorporates a local database for caching the results, to enable fast content\nretrieval with minimal external Web API calls. Our tool supports domain names\nand IPv4 addresses, making it a multi-functional and powerful IP analyser tool\nfor threat intelligence. Our tool is available at www.ipanalyzer.in"
                },
                "authors": [
                    {
                        "name": "Cebajel Tanan"
                    },
                    {
                        "name": "Sameer G. Kulkarni"
                    },
                    {
                        "name": "Tamal Das"
                    },
                    {
                        "name": "Manjesh K. Hanawal"
                    }
                ],
                "author_detail": {
                    "name": "Manjesh K. Hanawal"
                },
                "author": "Manjesh K. Hanawal",
                "arxiv_comment": "Presented at ICIE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03023v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03023v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.12622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.12622v2",
                "updated": "2024-12-03T22:48:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    48,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2023-10-19T10:02:52Z",
                "published_parsed": [
                    2023,
                    10,
                    19,
                    10,
                    2,
                    52,
                    3,
                    292,
                    0
                ],
                "title": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cRVR: A Stackelberg Game Approach for Joint Privacy-Aware Video\n  Requesting and Edge Caching"
                },
                "summary": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As users conveniently stream their favorite online videos, video request\nrecords are automatically stored by video content providers, which have a high\nchance of privacy leakage. Unfortunately, most existing privacy-enhancing\napproaches are not applicable for protecting user privacy in video requests,\nbecause they cannot be easily altered or distorted by users and must be visible\nfor content providers to stream correct videos. To preserve request privacy in\nonline video services, it is possible to request additional videos that are\nirrelevant to users' interests so that content providers cannot precisely infer\nusers' interest information. However, a naive redundant requesting approach\nwould significantly degrade the performance of edge caches and increase\nbandwidth overhead. In this paper, we are among the first to propose a\nCache-Friendly Redundant Video Requesting (cRVR) algorithm for User Devices\n(UDs) and its corresponding caching algorithm for the Edge Cache (EC), which\ncan effectively mitigate the problem of request privacy leakage with minimal\nimpact on the EC's performance. To tackle the problem, we first develop a\nStackelberg game to analyze the dedicated interaction between UDs and EC, and\nobtain their optimal strategies to maximize their respective utility. For UDs,\nthe utility function is a combination of both video playback utility and\nprivacy protection utility. We prove the existence and uniqueness of the\nequilibrium of the Stackelberg game. Extensive experiments are conducted with\nreal traces to demonstrate that cRVR can effectively protect video request\nprivacy by reducing up to 59.03\\% of privacy disclosure compared to baseline\nalgorithms. Meanwhile, the caching performance of EC is only slightly affected."
                },
                "authors": [
                    {
                        "name": "Xianzhi Zhang"
                    },
                    {
                        "name": "Linchang Xiao"
                    },
                    {
                        "name": "Yipeng Zhou"
                    },
                    {
                        "name": "Miao Hu"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Quan Z. Sheng"
                    }
                ],
                "author_detail": {
                    "name": "Quan Z. Sheng"
                },
                "author": "Quan Z. Sheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.12622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.12622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02867v1",
                "updated": "2024-12-03T22:02:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T22:02:42Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    22,
                    2,
                    42,
                    1,
                    338,
                    0
                ],
                "title": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GoldFish: Serverless Actors with Short-Term Memory State for the\n  Edge-Cloud Continuum"
                },
                "summary": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless Computing is a computing paradigm that provides efficient\ninfrastructure management and elastic scalability. Serverless functions scale\nup or down based on demand, which means that functions are not directly\naddressable and rely on platform-managed invocation. Serverless stateless\nnature requires functions to leverage external services, such as object storage\nand KVS, to exchange data. Serverless actors have emerged as a solution to\nthese issues. However, the state-of-the-art serverless lifecycle and\nevent-trigger invocation force actors to leverage remote services to manage\ntheir state and exchange data, which impacts the performance and incurs\nadditional costs and dependency on third-party services.\n  To address these issues, in this paper, we introduce a novel serverless\nlifecycle model that allows short-term stateful actors, enabling actors to\nmaintain their state between executions. Additionally, we propose a novel\nserverless Invocation Model that enables serverless actors to influence the\nprocessing of future messages. We present GoldFish, a lightweight WebAssembly\nshort-term stateful serverless actor platform that provides a novel serverless\nactor lifecycle and invocation model. GoldFish leverages WebAssembly to provide\nthe actors with lightweight sandbox isolation, making them suitable for the\nEdge-Cloud Continuum, where computational resources are limited. Experimental\nresults show that GoldFish optimizes the data exchange latency by up to 92% and\nincreases the throughput by up to 10x compared to OpenFaaS and Spin."
                },
                "authors": [
                    {
                        "name": "Cynthia Marcelino"
                    },
                    {
                        "name": "Jack Shahhoud"
                    },
                    {
                        "name": "Stefan Nastic"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Nastic"
                },
                "author": "Stefan Nastic",
                "arxiv_doi": "10.1145/3703790.3703797",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3703790.3703797",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.02867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "14th International Conference on the Internet of Things (IoT 2024),\n  November 19--22, 2024, Oulu, Finland",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00876v2",
                "updated": "2024-12-03T16:12:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    16,
                    12,
                    9,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-01T16:32:31Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    16,
                    32,
                    31,
                    6,
                    336,
                    0
                ],
                "title": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic-LLaVA: Efficient Multimodal Large Language Models via Dynamic\n  Vision-language Context Sparsification"
                },
                "summary": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) have achieved remarkable success in\nvision understanding, reasoning, and interaction. However, the inference\ncomputation and memory increase progressively with the generation of output\ntokens during decoding, directly affecting the efficacy of MLLMs. Existing\nmethods attempt to reduce the vision context redundancy to achieve efficient\nMLLMs. Unfortunately, the efficiency benefits of the vision context reduction\nin the prefill stage gradually diminish during the decoding stage. To address\nthis problem, we proposed a dynamic vision-language context sparsification\nframework Dynamic-LLaVA, which dynamically reduces the redundancy of vision\ncontext in the prefill stage and decreases the memory and computation overhead\nof the generated language context during decoding. Dynamic-LLaVA designs a\ntailored sparsification inference scheme for different inference modes, i.e.,\nprefill, decoding with and without KV cache, to achieve efficient inference of\nMLLMs. In practice, Dynamic-LLaVA can reduce computation consumption by\n$\\sim$75\\% in the prefill stage. Meanwhile, throughout the entire generation\nprocess of MLLMs, Dynamic-LLaVA reduces the $\\sim$50\\% computation consumption\nunder decoding without KV cache, while saving $\\sim$50\\% GPU memory overhead\nwhen decoding with KV cache, due to the vision-language context sparsification.\nExtensive experiments also demonstrate that Dynamic-LLaVA achieves efficient\ninference for MLLMs with negligible understanding and generation ability\ndegradation or even performance gains compared to the full-context inference\nbaselines. Code is available at https://github.com/Osilly/dynamic_llava ."
                },
                "authors": [
                    {
                        "name": "Wenxuan Huang"
                    },
                    {
                        "name": "Zijie Zhai"
                    },
                    {
                        "name": "Yunhang Shen"
                    },
                    {
                        "name": "Shaoshen Cao"
                    },
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Xiangfeng Xu"
                    },
                    {
                        "name": "Zheyu Ye"
                    },
                    {
                        "name": "Shaohui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Shaohui Lin"
                },
                "author": "Shaohui Lin",
                "arxiv_comment": "Code is available at https://github.com/Osilly/dynamic_llava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17897v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17897v3",
                "updated": "2024-12-03T12:36:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    12,
                    36,
                    19,
                    1,
                    338,
                    0
                ],
                "published": "2024-10-23T14:15:07Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    14,
                    15,
                    7,
                    2,
                    297,
                    0
                ],
                "title": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Value Residual Learning For Alleviating Attention Concentration In\n  Transformers"
                },
                "summary": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers can capture long-range dependencies using self-attention,\nallowing tokens to attend to all others directly. However, stacking multiple\nattention layers leads to attention concentration. One natural way to address\nthis issue is to use cross-layer attention, allowing information from earlier\nlayers to be directly accessible to later layers. However, this approach is\ncomputationally expensive. To address this problem, we propose Transformer with\nresidual value (ResFormer) which approximates cross-layer attention through\nadding a residual connection from the values of the the first layer to all\nsubsequent layers. Based on this method, one variant is the Transformer with\nsingle layer value (SVFormer), where all layers share the same value embedding\nfrom first layer. Comprehensive empirical evidence demonstrates ResFormer\nachieves equivalent validation loss with 10.4% fewer model parameters and 13.6%\nless training data compared to Transformer, while maintaining similar memory\nusage and computational cost. Besides, SVFormer reduces KV cache size by nearly\nhalf with only a small performance penalty and can be integrated with other\nKV-efficient methods, yielding further reductions in KV cache, with performance\ninfluenced by sequence length and cumulative learning rate. Further\nvisualization results suggest that Resformer and SVFormer alleviate attention\nconcentration in deeper layers through avoiding value-state drains and enhance\nrepresentation across most layers."
                },
                "authors": [
                    {
                        "name": "Zhanchao Zhou"
                    },
                    {
                        "name": "Tianyi Wu"
                    },
                    {
                        "name": "Zhiyun Jiang"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    }
                ],
                "author_detail": {
                    "name": "Zhenzhong Lan"
                },
                "author": "Zhenzhong Lan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17897v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17897v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v1",
                "updated": "2024-12-03T08:29:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing context window size in Large Language Models (LLMs), such as\nthe GPT and LLaMA series, has improved their ability to tackle complex,\nlong-text tasks, but at the cost of inference efficiency, particularly\nregarding memory and computational complexity. Existing methods, including\nselective token retention and window-based attention, improve efficiency but\nrisk discarding important tokens needed for future text generation. In this\npaper, we propose an approach that enhances LLM efficiency without token loss\nby reducing the memory and computational load of less important tokens, rather\nthan discarding them.We address two challenges: 1) investigating the\ndistribution of important tokens in the context, discovering recent tokens are\nmore important than distant tokens in context, and 2) optimizing resources for\ndistant tokens by sharing attention scores across layers. The experiments show\nthat our method saves $35\\%$ KV cache without compromising the performance."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02122v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02122v1",
                "updated": "2024-12-03T03:20:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "published": "2024-12-03T03:20:40Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    3,
                    20,
                    40,
                    1,
                    338,
                    0
                ],
                "title": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Sequential Recommender Systems with Online and In-store User\n  Behavior"
                },
                "summary": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online e-commerce platforms have been extending in-store shopping, which\nallows users to keep the canonical online browsing and checkout experience\nwhile exploring in-store shopping. However, the growing transition between\nonline and in-store becomes a challenge to sequential recommender systems for\nfuture online interaction prediction due to the lack of holistic modeling of\nhybrid user behaviors (online and in-store). The challenges are twofold. First,\ncombining online and in-store user behavior data into a single data schema and\nsupporting multiple stages in the model life cycle (pre-training, training,\ninference, etc.) organically needs a new data pipeline design. Second, online\nrecommender systems, which solely rely on online user behavior sequences, must\nbe redesigned to support online and in-store user data as input under the\nsequential modeling setting. To overcome the first challenge, we propose a\nhybrid, omnichannel data pipeline to compile online and in-store user behavior\ndata by caching information from diverse data sources. Later, we introduce a\nmodel-agnostic encoder module to the sequential recommender system to interpret\nthe user in-store transaction and augment the modeling capacity for better\nonline interaction prediction given the hybrid user behavior."
                },
                "authors": [
                    {
                        "name": "Luyi Ma"
                    },
                    {
                        "name": "Aashika Padmanabhan"
                    },
                    {
                        "name": "Anjana Ganesh"
                    },
                    {
                        "name": "Shengwei Tang"
                    },
                    {
                        "name": "Jiao Chen"
                    },
                    {
                        "name": "Xiaohan Li"
                    },
                    {
                        "name": "Lalitesh Morishetti"
                    },
                    {
                        "name": "Kaushiki Nag"
                    },
                    {
                        "name": "Malay Patel"
                    },
                    {
                        "name": "Jason Cho"
                    },
                    {
                        "name": "Sushant Kumar"
                    },
                    {
                        "name": "Kannan Achan"
                    }
                ],
                "author_detail": {
                    "name": "Kannan Achan"
                },
                "author": "Kannan Achan",
                "arxiv_comment": "6 pages, IEEE BigData 2024 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02122v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02122v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01959v1",
                "updated": "2024-12-02T20:39:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T20:39:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    20,
                    39,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Development and Application of a Decentralized Domain Name Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Development and Application of a Decentralized Domain Name Service"
                },
                "summary": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The current Domain Name System (DNS), as a core infrastructure of the\ninternet, exhibits several shortcomings: its centralized architecture leads to\ncensorship risks and single points of failure, making domain name resolution\nvulnerable to attacks. The lack of encryption in the resolution process exposes\nit to DNS hijacking and cache poisoning attacks. Additionally, the high\noperational costs limit participation and innovation among small to\nmedium-sized users. To address these issues, this paper proposes a\nDecentralized Domain Name Service (DDNS) based on blockchain (Phicoin) and\ndistributed storage (IPFS). By leveraging the immutability of blockchain and\nthe content verification of IPFS, the system achieves decentralized storage and\ndistribution of domain name records, eliminating the centralized dependencies\nof traditional DNS. With a block time of 15 seconds, the system supports rapid\nbroadcasting of domain name updates, significantly improving resolution\nefficiency. The DDNS aims to serve as a complement or backup to the existing\nDNS system, providing a pollution-resistant, censorship-resistant,\nhigh-performance, and low-cost domain name resolution solution, offering a new\ntechnical path for the security and stability of the internet."
                },
                "authors": [
                    {
                        "name": "Guang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Guang Yang"
                },
                "author": "Guang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01827v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01827v1",
                "updated": "2024-12-02T18:59:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T18:59:53Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    18,
                    59,
                    53,
                    0,
                    337,
                    0
                ],
                "title": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RandAR: Decoder-only Autoregressive Visual Generation in Random Orders"
                },
                "summary": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce RandAR, a decoder-only visual autoregressive (AR) model capable\nof generating images in arbitrary token orders. Unlike previous decoder-only AR\nmodels that rely on a predefined generation order, RandAR removes this\ninductive bias, unlocking new capabilities in decoder-only generation. Our\nessential design enables random order by inserting a \"position instruction\ntoken\" before each image token to be predicted, representing the spatial\nlocation of the next image token. Trained on randomly permuted token sequences\n-- a more challenging task than fixed-order generation, RandAR achieves\ncomparable performance to its conventional raster-order counterpart. More\nimportantly, decoder-only transformers trained from random orders acquire new\ncapabilities. For the efficiency bottleneck of AR models, RandAR adopts\nparallel decoding with KV-Cache at inference time, enjoying 2.5x acceleration\nwithout sacrificing generation quality. Additionally, RandAR supports\ninpainting, outpainting and resolution extrapolation in a zero-shot manner. We\nhope RandAR inspires new directions for decoder-only visual generation models\nand broadens their applications across diverse scenarios. Our project page is\nat https://rand-ar.github.io/."
                },
                "authors": [
                    {
                        "name": "Ziqi Pang"
                    },
                    {
                        "name": "Tianyuan Zhang"
                    },
                    {
                        "name": "Fujun Luan"
                    },
                    {
                        "name": "Yunze Man"
                    },
                    {
                        "name": "Hao Tan"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "William T. Freeman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu-Xiong Wang"
                },
                "author": "Yu-Xiong Wang",
                "arxiv_comment": "Project page: https://rand-ar.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01827v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01827v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01659v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01659v1",
                "updated": "2024-12-02T16:10:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T16:10:26Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    16,
                    10,
                    26,
                    0,
                    337,
                    0
                ],
                "title": "Local and Regional Contributions to Tropospheric Ozone Concentrations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local and Regional Contributions to Tropospheric Ozone Concentrations"
                },
                "summary": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Wasatch Front in Utah, USA, is currently a non-attainment area for ozone\naccording to the Environmental Protection Agency's (EPA) National Ambient Air\nQuality Standards (NAAQS). Nitrogen oxides ($\\mathrm{NO_x = NO_2 + NO}$) and\nvolatile organic compounds (VOCs), in the presence of sunlight, lead to ozone\nformation in the troposphere. When the rate of oxidant production, defined as\nthe sum of $\\mathrm{O_3}$ and $\\mathrm{NO_2}$, is faster than the rate of\n$\\mathrm{NO_x}$ production, a region is said to be $\\mathrm{NO_x}$limited, and\nozone formation will be limited by the concentration of $\\mathrm{NO_x}$ species\nin the region. The inverse of this situation makes the region VOC-limited.\nKnowing whether a region is $\\mathrm{NO_x}$-limited or VOC-limited can aid in\ngenerating effective mitigation strategies. Understanding the background or\nregional contributions to ozone in a region, whether from the transport of\nprecursors or of ozone, provides information about the lower limit for ozone\nconcentrations that a region can achieve through regulation of local\nprecursors. In this paper, measured oxidant and $\\mathrm{NO_x}$ concentrations\nare analyzed from 14 counties in the state of Utah to calculate the regional\nand local contributions to ozone for each region. This analysis is used to\ndetermine the nature of the atmosphere in each county by identifying whether\nthe region is VOC or $\\mathrm{NO_x}$-limited. Furthermore, this analysis is\nperformed for each county for the years 2012 and 2022 to assess changes in the\noxidative nature and quantify regional and local contributions to ozone over a\n10-year period. All studied counties--except for Washington County--in Utah\nwere found to be VOC-limited in 2012. This shifted in 2022, with most counties\nbeing either in a transitional state or $\\mathrm{NO_x}$-limited. Local\ncontributions to ozone increased in two major counties, Cache and Salt Lake\nCounties, but decreased in Carbon, Davis, Duchesne, Uinta, Utah, Washington,\nand Weber Counties. Generally, the regional contributions to oxidant\nconcentrations decreased across the state. A summertime spike in both regional\nand local contributions to oxidants was observed. Smoke from wildfires was\nfound to increase regional contributions to oxidants and shift the local regime\nto be more $\\mathrm{NO_x}$-limited."
                },
                "authors": [
                    {
                        "name": "Callum E. Flowerday"
                    },
                    {
                        "name": "Ryan Thalman"
                    },
                    {
                        "name": "Jaron C. Hansen"
                    }
                ],
                "author_detail": {
                    "name": "Jaron C. Hansen"
                },
                "author": "Jaron C. Hansen",
                "arxiv_doi": "10.3390/atmos14081262",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3390/atmos14081262",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.01659v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01659v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Atmosphere 2023, 14, 1262",
                "arxiv_primary_category": {
                    "term": "physics.ao-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01415v1",
                "updated": "2024-12-02T11:57:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:57:03Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    57,
                    3,
                    0,
                    337,
                    0
                ],
                "title": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Excitation of quasi-monochromotic waves by a high-voltage pulse in a\n  ferrite coaxial line with the periodic structure"
                },
                "summary": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experimental data and results of numerical simulations are presented,\nconcerning excitation of narrowband gigahertz-range wave trains in coaxial\nguiding structures that are partially filled with ferromagnetic material and\nmay involve periodically arranged metal inserts. The experiments performed\nconfirm the possibility of exciting weakly damped electromagnetic waves by\nfeeding high voltage, unilateral electromagnetic pulses of short duration into\nthe line. The coax line was of outer diameter 50.5 mm, filled with an isotropic\ndielectric (relative dielectric constant {\\epsilon} = 2.25) and a set of\nferrite rings with {\\epsilon}=16 and saturated-state {\\mu} about 4 to 5. With a\npeak voltage of the primary pulse close to 160 kV and a magnetizing field of\n17.5 kA/m, the parameters of the waves excited reached magnitudes as: frequency\n1.89 GHz to 2.1 GHz; bandwidth 16%; VHF power at the output about 20 MW."
                },
                "authors": [
                    {
                        "name": "A. B. Batrakov"
                    },
                    {
                        "name": "S. Yu. Karelin"
                    },
                    {
                        "name": "O. M. Lebedenko"
                    },
                    {
                        "name": "V. S. Mukhin"
                    },
                    {
                        "name": "I. N. Onishchenko"
                    },
                    {
                        "name": "O. L. Rak"
                    },
                    {
                        "name": "V. G. Sinitsin"
                    },
                    {
                        "name": "M. V. Volovenko"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Volovenko"
                },
                "author": "M. V. Volovenko",
                "arxiv_comment": "4 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.06892v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.06892v2",
                "updated": "2024-12-02T11:24:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    24,
                    20,
                    0,
                    337,
                    0
                ],
                "published": "2024-03-11T16:48:25Z",
                "published_parsed": [
                    2024,
                    3,
                    11,
                    16,
                    48,
                    25,
                    0,
                    71,
                    0
                ],
                "title": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time Transformer-based Open-Vocabulary Detection with Efficient\n  Fusion Head"
                },
                "summary": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end transformer-based detectors (DETRs) have shown exceptional\nperformance in both closed-set and open-vocabulary object detection (OVD) tasks\nthrough the integration of language modalities. However, their demanding\ncomputational requirements have hindered their practical application in\nreal-time object detection (OD) scenarios. In this paper, we scrutinize the\nlimitations of two leading models in the OVDEval benchmark, OmDet and\nGrounding-DINO, and introduce OmDet-Turbo. This novel transformer-based\nreal-time OVD model features an innovative Efficient Fusion Head (EFH) module\ndesigned to alleviate the bottlenecks observed in OmDet and Grounding-DINO.\nNotably, OmDet-Turbo-Base achieves a 100.2 frames per second (FPS) with\nTensorRT and language cache techniques applied. Notably, in zero-shot scenarios\non COCO and LVIS datasets, OmDet-Turbo achieves performance levels nearly on\npar with current state-of-the-art supervised models. Furthermore, it\nestablishes new state-of-the-art benchmarks on ODinW and OVDEval, boasting an\nAP of 30.1 and an NMS-AP of 26.86, respectively. The practicality of\nOmDet-Turbo in industrial applications is underscored by its exceptional\nperformance on benchmark datasets and superior inference speed, positioning it\nas a compelling choice for real-time object detection tasks. Code:\n\\url{https://github.com/om-ai-lab/OmDet}"
                },
                "authors": [
                    {
                        "name": "Tiancheng Zhao"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Xuan He"
                    },
                    {
                        "name": "Lu Zhang"
                    },
                    {
                        "name": "Kyusong Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kyusong Lee"
                },
                "author": "Kyusong Lee",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.06892v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.06892v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01380v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01380v1",
                "updated": "2024-12-02T11:07:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T11:07:51Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    11,
                    7,
                    51,
                    0,
                    337,
                    0
                ],
                "title": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference using Dynamic Input Pruning and Cache-Aware\n  Masking"
                },
                "summary": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While mobile devices provide ever more compute power, improvements in DRAM\nbandwidth are much slower. This is unfortunate for large language model (LLM)\ntoken generation, which is heavily memory-bound. Previous work has proposed to\nleverage natural dynamic activation sparsity in ReLU-activated LLMs to reduce\neffective DRAM bandwidth per token. However, more recent LLMs use SwiGLU\ninstead of ReLU, which result in little inherent sparsity. While SwiGLU\nactivations can be pruned based on magnitude, the resulting sparsity patterns\nare difficult to predict, rendering previous approaches ineffective. To\ncircumvent this issue, our work introduces Dynamic Input Pruning (DIP): a\npredictor-free dynamic sparsification approach, which preserves accuracy with\nminimal fine-tuning. DIP can further use lightweight LoRA adapters to regain\nsome performance lost during sparsification. Lastly, we describe a novel\ncache-aware masking strategy, which considers the cache state and activation\nmagnitude to further increase cache hit rate, improving LLM token rate on\nmobile devices. DIP outperforms other methods in terms of accuracy, memory and\nthroughput trade-offs across simulated hardware settings. On Phi-3-Medium, DIP\nachieves a 46% reduction in memory and 40% increase in throughput with $<$ 0.1\nloss in perplexity."
                },
                "authors": [
                    {
                        "name": "Marco Federici"
                    },
                    {
                        "name": "Davide Belli"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Amir Jalalirad"
                    },
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Bence Major"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    }
                ],
                "author_detail": {
                    "name": "Paul Whatmough"
                },
                "author": "Paul Whatmough",
                "arxiv_comment": "Main Text: 10 pages, 11 figures. Appendix: 3 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01380v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01380v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01195v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01195v1",
                "updated": "2024-12-02T06:57:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "published": "2024-12-02T06:57:46Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    57,
                    46,
                    0,
                    337,
                    0
                ],
                "title": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Training for Deep Speaker Embedding Learning in Speaker\n  Verification"
                },
                "summary": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent speaker verification (SV) systems have shown a trend toward adopting\ndeeper speaker embedding extractors. Although deeper and larger neural networks\ncan significantly improve performance, their substantial memory requirements\nhinder training on consumer GPUs. In this paper, we explore a memory-efficient\ntraining strategy for deep speaker embedding learning in resource-constrained\nscenarios. Firstly, we conduct a systematic analysis of GPU memory allocation\nduring SV system training. Empirical observations show that activations and\noptimizer states are the main sources of memory consumption. For activations,\nwe design two types of reversible neural networks which eliminate the need to\nstore intermediate activations during back-propagation, thereby significantly\nreducing memory usage without performance loss. For optimizer states, we\nintroduce a dynamic quantization approach that replaces the original 32-bit\nfloating-point values with a dynamic tree-based 8-bit data type. Experimental\nresults on VoxCeleb demonstrate that the reversible variants of ResNets and\nDF-ResNets can perform training without the need to cache activations in GPU\nmemory. In addition, the 8-bit versions of SGD and Adam save 75% of memory\ncosts while maintaining performance compared to their 32-bit counterparts.\nFinally, a detailed comparison of memory usage and performance indicates that\nour proposed models achieve up to 16.2x memory savings, with nearly identical\nparameters and performance compared to the vanilla systems. In contrast to the\nprevious need for multiple high-end GPUs such as the A100, we can effectively\ntrain deep speaker embedding extractors with just one or two consumer-level\n2080Ti GPUs."
                },
                "authors": [
                    {
                        "name": "Bei Liu"
                    },
                    {
                        "name": "Yanmin Qian"
                    }
                ],
                "author_detail": {
                    "name": "Yanmin Qian"
                },
                "author": "Yanmin Qian",
                "arxiv_comment": "Submitted to IEEE/ACM Transactions on Audio, Speech, and Language\n  Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01195v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01195v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04762v2",
                "updated": "2024-12-02T06:30:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    2,
                    6,
                    30,
                    4,
                    0,
                    337,
                    0
                ],
                "published": "2024-11-07T14:59:44Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    59,
                    44,
                    3,
                    312,
                    0
                ],
                "title": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "JC5A: Service Delay Minimization for Aerial MEC-assisted Industrial\n  Cyber-Physical Systems"
                },
                "summary": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of the sixth generation (6G) and industrial Internet of Things\n(IIoT), an industrial cyber-physical system (ICPS) drives the proliferation of\nsensor devices and computing-intensive tasks. To address the limited resources\nof IIoT sensor devices, unmanned aerial vehicle (UAV)-assisted mobile edge\ncomputing (MEC) has emerged as a promising solution, providing flexible and\ncost-effective services in close proximity of IIoT sensor devices (ISDs).\nHowever, leveraging aerial MEC to meet the delay-sensitive and\ncomputation-intensive requirements of the ISDs could face several challenges,\nincluding the limited communication, computation and caching (3C) resources,\nstringent offloading requirements for 3C services, and constrained on-board\nenergy of UAVs. To address these issues, we first present a collaborative\naerial MEC-assisted ICPS architecture by incorporating the computing\ncapabilities of the macro base station (MBS) and UAVs. We then formulate a\nservice delay minimization optimization problem (SDMOP). Since the SDMOP is\nproved to be an NP-hard problem, we propose a joint computation offloading,\ncaching, communication resource allocation, computation resource allocation,\nand UAV trajectory control approach (JC5A). Specifically, JC5A consists of a\nblock successive upper bound minimization method of multipliers (BSUMM) for\ncomputation offloading and service caching, a convex optimization-based method\nfor communication and computation resource allocation, and a successive convex\napproximation (SCA)-based method for UAV trajectory control. Moreover, we\ntheoretically prove the convergence and polynomial complexity of JC5A.\nSimulation results demonstrate that the proposed approach can achieve superior\nsystem performance compared to the benchmark approaches and algorithms."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Jiaxu Wu"
                    },
                    {
                        "name": "Zemin Sun"
                    },
                    {
                        "name": "Long He"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Abbas Jamalipour"
                    },
                    {
                        "name": "Shiwen Mao"
                    }
                ],
                "author_detail": {
                    "name": "Shiwen Mao"
                },
                "author": "Shiwen Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00977v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00977v1",
                "updated": "2024-12-01T21:47:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T21:47:35Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    21,
                    47,
                    35,
                    6,
                    336,
                    0
                ],
                "title": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Power Allocation in Uplink NOMA with Simultaneous Cache-Enabled\n  D2D Communications"
                },
                "summary": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-orthogonal multiple access (NOMA) is widely viewed as a potential\ncandidate for providing enhanced multiple access in future mobile networks by\neliminating the orthogonal distribution of radio resources amongst the users.\nNevertheless, the performance of NOMA can be significantly improved by\ncombining it with other sophisticated technologies such as wireless data\ncaching and device-to-device (D2D) communications. In this letter, we propose a\nnovel cellular system model which integrates uplink NOMA with cache based\ndevice-to-device (D2D) communications. The proposed system would enable a\ncellular user to upload data file to base station while simultaneously\nexchanging useful cache content with another nearby user. We maximize the\nsystem sum rate by deriving closed form solutions for optimal power allocation.\nSimulation results demonstrate the superior performance of our proposed model\nover other potential combinations of uplink NOMA and D2D communications."
                },
                "authors": [
                    {
                        "name": "Aditya Powari"
                    },
                    {
                        "name": "Daniel K. C. So"
                    }
                ],
                "author_detail": {
                    "name": "Daniel K. C. So"
                },
                "author": "Daniel K. C. So",
                "arxiv_comment": "5 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00977v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00977v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00857v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00857v1",
                "updated": "2024-12-01T15:45:26Z",
                "updated_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "published": "2024-12-01T15:45:26Z",
                "published_parsed": [
                    2024,
                    12,
                    1,
                    15,
                    45,
                    26,
                    6,
                    336,
                    0
                ],
                "title": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advanced Video Inpainting Using Optical Flow-Guided Efficient Diffusion"
                },
                "summary": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, diffusion-based methods have achieved great improvements in the\nvideo inpainting task. However, these methods still face many challenges, such\nas maintaining temporal consistency and the time-consuming issue. This paper\nproposes an advanced video inpainting framework using optical Flow-guided\nEfficient Diffusion, called FloED. Specifically, FloED employs a dual-branch\narchitecture, where a flow branch first restores corrupted flow and a\nmulti-scale flow adapter provides motion guidance to the main inpainting\nbranch. Additionally, a training-free latent interpolation method is proposed\nto accelerate the multi-step denoising process using flow warping. Further\nintroducing a flow attention cache mechanism, FLoED efficiently reduces the\ncomputational cost brought by incorporating optical flow. Comprehensive\nexperiments in both background restoration and object removal tasks demonstrate\nthat FloED outperforms state-of-the-art methods from the perspective of both\nperformance and efficiency."
                },
                "authors": [
                    {
                        "name": "Bohai Gu"
                    },
                    {
                        "name": "Hao Luo"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Peiran Dong"
                    }
                ],
                "author_detail": {
                    "name": "Peiran Dong"
                },
                "author": "Peiran Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00857v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00857v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02532v3",
                "updated": "2024-11-30T21:33:59Z",
                "updated_parsed": [
                    2024,
                    11,
                    30,
                    21,
                    33,
                    59,
                    5,
                    335,
                    0
                ],
                "published": "2024-06-04T17:53:36Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    17,
                    53,
                    36,
                    1,
                    156,
                    0
                ],
                "title": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecExec: Massively Parallel Speculative Decoding for Interactive LLM\n  Inference on Consumer Devices"
                },
                "summary": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models gain widespread adoption, running them efficiently\nbecomes crucial. Recent works on LLM inference use speculative decoding to\nachieve extreme speedups. However, most of these works implicitly design their\nalgorithms for high-end datacenter hardware. In this work, we ask the opposite\nquestion: how fast can we run LLMs on consumer machines? Consumer GPUs can no\nlonger fit the largest available models (50B+ parameters) and must offload them\nto RAM or SSD. When running with offloaded parameters, the inference engine can\nprocess batches of hundreds or thousands of tokens at the same time as just one\ntoken, making it a natural fit for speculative decoding. We propose SpecExec\n(Speculative Execution), a simple parallel decoding method that can generate up\nto 20 tokens per target model iteration for popular LLM families. It utilizes\nthe high spikiness of the token probabilities distribution in modern LLMs and a\nhigh degree of alignment between model output probabilities. SpecExec takes the\nmost probable tokens continuation from the draft model to build a \"cache\" tree\nfor the target model, which then gets validated in a single pass. Using\nSpecExec, we demonstrate inference of 50B+ parameter LLMs on consumer GPUs with\nRAM offloading at 4-6 tokens per second with 4-bit quantization or 2-3 tokens\nper second with 16-bit weights."
                },
                "authors": [
                    {
                        "name": "Ruslan Svirschevski"
                    },
                    {
                        "name": "Avner May"
                    },
                    {
                        "name": "Zhuoming Chen"
                    },
                    {
                        "name": "Beidi Chen"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Max Ryabinin"
                    }
                ],
                "author_detail": {
                    "name": "Max Ryabinin"
                },
                "author": "Max Ryabinin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00209v1",
                "updated": "2024-11-29T19:14:45Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T19:14:45Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    14,
                    45,
                    4,
                    334,
                    0
                ],
                "title": "Digital Twin in Industries: A Comprehensive Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital Twin in Industries: A Comprehensive Survey"
                },
                "summary": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Industrial networks are undergoing rapid transformation driven by the\nconvergence of emerging technologies that are revolutionizing conventional\nworkflows, enhancing operational efficiency, and fundamentally redefining the\nindustrial landscape across diverse sectors. Amidst this revolution, Digital\nTwin (DT) emerges as a transformative innovation that seamlessly integrates\nreal-world systems with their virtual counterparts, bridging the physical and\ndigital realms. In this article, we present a comprehensive survey of the\nemerging DT-enabled services and applications across industries, beginning with\nan overview of DT fundamentals and its components to a discussion of key\nenabling technologies for DT. Different from literature works, we investigate\nand analyze the capabilities of DT across a wide range of industrial services,\nincluding data sharing, data offloading, integrated sensing and communication,\ncontent caching, resource allocation, wireless networking, and metaverse. In\nparticular, we present an in-depth technical discussion of the roles of DT in\nindustrial applications across various domains, including manufacturing,\nhealthcare, transportation, energy, agriculture, space, oil and gas, as well as\nrobotics. Throughout the technical analysis, we delve into real-time data\ncommunications between physical and virtual platforms to enable industrial DT\nnetworking. Subsequently, we extensively explore and analyze a wide range of\nmajor privacy and security issues in DT-based industry. Taxonomy tables and the\nkey research findings from the survey are also given, emphasizing important\ninsights into the significance of DT in industries. Finally, we point out\nfuture research directions to spur further research in this promising area."
                },
                "authors": [
                    {
                        "name": "Md Bokhtiar Al Zami"
                    },
                    {
                        "name": "Shaba Shaon"
                    },
                    {
                        "name": "Vu Khanh Quy"
                    },
                    {
                        "name": "Dinh C. Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Dinh C. Nguyen"
                },
                "author": "Dinh C. Nguyen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19730v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19730v1",
                "updated": "2024-11-29T14:23:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T14:23:25Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    14,
                    23,
                    25,
                    4,
                    334,
                    0
                ],
                "title": "Ten Ways in which Virtual Reality Differs from Video Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ten Ways in which Virtual Reality Differs from Video Streaming"
                },
                "summary": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) applications have a number of unique characteristics\nthat set them apart from traditional video streaming. These characteristics\nhave major implications on the design of VR rendering, adaptation, prefetching,\ncaching, and transport mechanisms. This paper contrasts VR to video streaming,\nstored 2D video streaming in particular, and discusses how to rethink system\nand network support for VR."
                },
                "authors": [
                    {
                        "name": "Gustavo de Veciana"
                    },
                    {
                        "name": "Sonia Fahmy"
                    },
                    {
                        "name": "George Kesidis"
                    },
                    {
                        "name": "Voicu Popescu"
                    }
                ],
                "author_detail": {
                    "name": "Voicu Popescu"
                },
                "author": "Voicu Popescu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19730v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19730v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01852v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01852v1",
                "updated": "2024-11-29T10:21:12Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T10:21:12Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    10,
                    21,
                    12,
                    4,
                    334,
                    0
                ],
                "title": "Communication efficient application of sequences of planar rotations to\n  a matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication efficient application of sequences of planar rotations to\n  a matrix"
                },
                "summary": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an efficient algorithm for the application of sequences of planar\nrotations to a matrix. Applying such sequences efficiently is important in many\nnumerical linear algebra algorithms for eigenvalues. Our algorithm is novel in\nthree main ways. First, we introduce a new kernel that is optimized for\nregister reuse in a novel way. Second, we introduce a blocking and packing\nscheme that improves the cache efficiency of the algorithm. Finally, we\nthoroughly analyze the memory operations of the algorithm which leads to\nimportant theoretical insights and makes it easier to select good parameters.\nNumerical experiments show that our algorithm outperforms the state-of-the-art\nand achieves a flop rate close to the theoretical peak on modern hardware."
                },
                "authors": [
                    {
                        "name": "Thijs Steel"
                    },
                    {
                        "name": "Julien Langou"
                    }
                ],
                "author_detail": {
                    "name": "Julien Langou"
                },
                "author": "Julien Langou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01852v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01852v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65F15, 65Y05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v3",
                "updated": "2024-11-29T08:48:01Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    48,
                    1,
                    4,
                    334,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel KÃ¼pper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024). Publication by IEEE still pending",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.07533v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18191v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18191v2",
                "updated": "2024-11-29T08:33:49Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    8,
                    33,
                    49,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-27T10:14:38Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    10,
                    14,
                    38,
                    2,
                    332,
                    0
                ],
                "title": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InputSnatch: Stealing Input in LLM Services via Timing Side-Channel\n  Attacks"
                },
                "summary": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) possess extensive knowledge and\nquestion-answering capabilities, having been widely deployed in\nprivacy-sensitive domains like finance and medical consultation. During LLM\ninferences, cache-sharing methods are commonly employed to enhance efficiency\nby reusing cached states or responses for the same or similar inference\nrequests. However, we identify that these cache mechanisms pose a risk of\nprivate input leakage, as the caching can result in observable variations in\nresponse times, making them a strong candidate for a timing-based attack hint.\n  In this study, we propose a novel timing-based side-channel attack to execute\ninput theft in LLMs inference. The cache-based attack faces the challenge of\nconstructing candidate inputs in a large search space to hit and steal cached\nuser queries. To address these challenges, we propose two primary components.\nThe input constructor employs machine learning techniques and LLM-based\napproaches for vocabulary correlation learning while implementing optimized\nsearch mechanisms for generalized input construction. The time analyzer\nimplements statistical time fitting with outlier elimination to identify cache\nhit patterns, continuously providing feedback to refine the constructor's\nsearch strategy. We conduct experiments across two cache mechanisms and the\nresults demonstrate that our approach consistently attains high attack success\nrates in various applications. Our work highlights the security vulnerabilities\nassociated with performance optimizations, underscoring the necessity of\nprioritizing privacy and security alongside enhancements in LLM inference."
                },
                "authors": [
                    {
                        "name": "Xinyao Zheng"
                    },
                    {
                        "name": "Husheng Han"
                    },
                    {
                        "name": "Shangyi Shi"
                    },
                    {
                        "name": "Qiyan Fang"
                    },
                    {
                        "name": "Zidong Du"
                    },
                    {
                        "name": "Xing Hu"
                    },
                    {
                        "name": "Qi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Qi Guo"
                },
                "author": "Qi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18191v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18191v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v1",
                "updated": "2024-11-29T05:57:37Z",
                "updated_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many LLM tasks are performed in large batches or even offline, and the\nperformance indictor for which is throughput. These tasks usually show the\ncharacteristic of prefix sharing, where different prompt input can partially\nshow the common prefix. However, the existing LLM inference engines tend to\noptimize the streaming requests and show limitations of supporting the large\nbatched tasks with the prefix sharing characteristic. The existing solutions\nuse the LRU-based cache to reuse the KV context of common prefix. The KV\ncontext that is about to be reused may prematurely be evicted with the implicit\ncache management. Even if not evicted, the lifetime of the shared KV context is\nextended since requests sharing the same context are not scheduled together,\nresulting in larger memory usage. These streaming oriented systems schedule the\nrequests in the first-come-first-serve or similar order. As a result, the\nrequests with larger ratio of decoding steps may be scheduled too late to be\nable to mix with the prefill chunks to increase the hardware utilization.\nBesides, the token and request number based batching can limit the size of\ntoken-batch, which keeps the GPU from saturating for the iterations dominated\nby decoding tokens. We propose BatchLLM to address the above problems. BatchLLM\nexplicitly identifies the common prefixes globally. The requests sharing the\nsame prefix will be scheduled together to reuse the KV context the best, which\nalso shrinks the lifetime of common KV memory. BatchLLM reorders the requests\nand schedules the requests with larger ratio of decoding first to better mix\nthe decoding tokens with the latter prefill chunks and applies memory-centric\ntoken batching to enlarge the token-batch sizes, which helps to increase the\nGPU utilization. Extensive evaluation shows that BatchLLM outperforms vLLM by\n1.1x to 2x on a set of microbenchmarks and two typical industry workloads."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19248v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19248v1",
                "updated": "2024-11-28T16:35:22Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T16:35:22Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    16,
                    35,
                    22,
                    3,
                    333,
                    0
                ],
                "title": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reflecting Intelligent Surfaces-Assisted Multiple-Antenna Coded Caching"
                },
                "summary": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surface (RIS) has been treated as a core technique\nin improving wireless propagation environments for the next generation wireless\ncommunication systems. This paper proposes a new coded caching problem,\nreferred to as Reconfigurable Intelligent Surface (RIS)-assisted\nmultiple-antenna coded caching, which is composed of a server with multiple\nantennas and some single-antenna cache-aided users. Different from the existing\nmulti-antenna coded caching problems, we introduce a passive RIS (with limited\nnumber of units) into the systems to further increase the multicast gain (i.e.,\ndegrees of freedom (DoF)) in the transmission, which is done by using\nRIS-assisted interference nulling. That is, by using RIS, we can `erase' any\npath between one transmission antenna and one receive antenna. We first propose\na new RIS-assisted interference nulling approach to search for the phase-shift\ncoefficients of RIS for the sake of interference nulling, which converges\nfaster than the state-of-the-art algorithm. After erasing some paths in each\ntime slot, the delivery can be divided into several non-overlapping groups\nincluding transmission antennas and users, where in each group the transmission\nantennas serve the contained users without suffering interference from the\ntransmissions by other groups. The division of groups for the sake of\nmaximizing the DoF could be formulated into a combinatorial optimization\nproblem. We propose a grouping algorithm which can find the optimal solution\nwith low complexity, and the corresponding coded caching scheme achieving this\nDoF."
                },
                "authors": [
                    {
                        "name": "Xiaofan Niu"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Robert Caiming Qiu"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "The short version of this paper was presented in 2024 IEEE\n  Information Theory Workshop, Nov. 24-28, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19248v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19248v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12468v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12468v2",
                "updated": "2024-11-28T14:42:54Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    14,
                    42,
                    54,
                    3,
                    333,
                    0
                ],
                "published": "2024-04-18T19:04:33Z",
                "published_parsed": [
                    2024,
                    4,
                    18,
                    19,
                    4,
                    33,
                    3,
                    109,
                    0
                ],
                "title": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fresh Caching of Dynamic Contents using Restless Multi-armed Bandits"
                },
                "summary": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a dynamic content caching problem wherein the contents get\nupdated at a central server, and local copies of a subset of contents are\ncached at a local cache associated with a Base station (BS). When a content\nrequest arrives, based on whether the content is in the local cache, the BS can\ndecide whether to fetch the content from the central server or serve the cached\nversion from the local cache. Fetching a content incurs a fixed fetching cost,\nand serving the cached version incurs an ageing cost proportional to the\nage-of-version (AoV) of the content. The BS has only partial information\nregarding AoVs of the contents. We formulate an optimal content fetching and\ncaching problem to minimize the average cost subject to cache capacity\nconstraints. The problem suffers from the curse of dimensionality and is\nprovably hard to solve. We formulate this problem as a continuous time restless\nmulti-armed bandit process (RMAB), where a single content problem of the\ncorresponding RMAB is a partially observable Markov decision process. We\nreformulate the single content problem as a semi-Markov decision process, prove\nindexability, and provide a Whittle index based solution to this problem.\nFinally, we compare the performance with recent work and show that our proposed\npolicy is optimal via simulations."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12468v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12468v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19108v1",
                "updated": "2024-11-28T12:50:05Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-28T12:50:05Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    12,
                    50,
                    5,
                    3,
                    333,
                    0
                ],
                "title": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model"
                },
                "summary": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As a fundamental backbone for video generation, diffusion models are\nchallenged by low inference speed due to the sequential nature of denoising.\nPrevious methods speed up the models by caching and reusing model outputs at\nuniformly selected timesteps. However, such a strategy neglects the fact that\ndifferences among model outputs are not uniform across timesteps, which hinders\nselecting the appropriate model outputs to cache, leading to a poor balance\nbetween inference efficiency and visual quality. In this study, we introduce\nTimestep Embedding Aware Cache (TeaCache), a training-free caching approach\nthat estimates and leverages the fluctuating differences among model outputs\nacross timesteps. Rather than directly using the time-consuming model outputs,\nTeaCache focuses on model inputs, which have a strong correlation with the\nmodeloutputs while incurring negligible computational cost. TeaCache first\nmodulates the noisy inputs using the timestep embeddings to ensure their\ndifferences better approximating those of model outputs. TeaCache then\nintroduces a rescaling strategy to refine the estimated differences and\nutilizes them to indicate output caching. Experiments show that TeaCache\nachieves up to 4.41x acceleration over Open-Sora-Plan with negligible (-0.07%\nVbench score) degradation of visual quality."
                },
                "authors": [
                    {
                        "name": "Feng Liu"
                    },
                    {
                        "name": "Shiwei Zhang"
                    },
                    {
                        "name": "Xiaofeng Wang"
                    },
                    {
                        "name": "Yujie Wei"
                    },
                    {
                        "name": "Haonan Qiu"
                    },
                    {
                        "name": "Yuzhong Zhao"
                    },
                    {
                        "name": "Yingya Zhang"
                    },
                    {
                        "name": "Qixiang Ye"
                    },
                    {
                        "name": "Fang Wan"
                    }
                ],
                "author_detail": {
                    "name": "Fang Wan"
                },
                "author": "Fang Wan",
                "arxiv_comment": "Project: https://liewfeng.github.io/TeaCache",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18077v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18077v2",
                "updated": "2024-11-28T02:01:50Z",
                "updated_parsed": [
                    2024,
                    11,
                    28,
                    2,
                    1,
                    50,
                    3,
                    333,
                    0
                ],
                "published": "2024-11-27T06:10:49Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    6,
                    10,
                    49,
                    2,
                    332,
                    0
                ],
                "title": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MiniKV: Pushing the Limits of LLM Inference via 2-Bit\n  Layer-Discriminative KV Cache"
                },
                "summary": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How to efficiently serve LLMs in practice has become exceptionally\nchallenging due to their prohibitive memory and computation requirements. In\nthis study, we investigate optimizing the KV cache, whose memory footprint\nposes a critical bottleneck in LLM inference, especially when dealing with long\ncontext tasks. To tackle the challenge, we introduce MiniKV, a KV cache\noptimization method that simultaneously preserves long context task accuracy\nwhile significantly reducing KV cache size via a novel 2-bit\nlayer-discriminative KV cache. More importantly, we develop specialized CUDA\nkernels to make MiniKV compatible with FlashAttention. Experiments on a wide\nrange of long context tasks show that MiniKV effectively achieves 86% KV cache\ncompression ratio while recovering over 98.5% of accuracy, outperforming\nstate-of-the-art methods while achieving excellent measured system performance\nimprovements."
                },
                "authors": [
                    {
                        "name": "Akshat Sharma"
                    },
                    {
                        "name": "Hangliang Ding"
                    },
                    {
                        "name": "Jianping Li"
                    },
                    {
                        "name": "Neel Dani"
                    },
                    {
                        "name": "Minjia Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Minjia Zhang"
                },
                "author": "Minjia Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18077v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18077v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00099v1",
                "updated": "2024-11-27T18:59:48Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:59:48Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    59,
                    48,
                    2,
                    332,
                    0
                ],
                "title": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cache-Conditional Experts for Efficient Mobile Device\n  Inference"
                },
                "summary": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Experts (MoE) LLMs have recently gained attention for their\nability to enhance performance by selectively engaging specialized subnetworks\nor \"experts\" for each input. However, deploying MoEs on memory-constrained\ndevices remains challenging, particularly when generating tokens sequentially\nwith a batch size of one, as opposed to typical high-throughput settings\ninvolving long sequences or large batches. In this work, we optimize MoE on\nmemory-constrained devices where only a subset of expert weights fit in DRAM.\nWe introduce a novel cache-aware routing strategy that leverages expert reuse\nduring token generation to improve cache locality. We evaluate our approach on\nlanguage modeling, MMLU, and GSM8K benchmarks and present on-device results\ndemonstrating 2$\\times$ speedups on mobile devices, offering a flexible,\ntraining-free solution to extend MoE's applicability across real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Andrii Skliar"
                    },
                    {
                        "name": "Ties van Rozendaal"
                    },
                    {
                        "name": "Romain Lepert"
                    },
                    {
                        "name": "Todor Boinovski"
                    },
                    {
                        "name": "Mart van Baalen"
                    },
                    {
                        "name": "Markus Nagel"
                    },
                    {
                        "name": "Paul Whatmough"
                    },
                    {
                        "name": "Babak Ehteshami Bejnordi"
                    }
                ],
                "author_detail": {
                    "name": "Babak Ehteshami Bejnordi"
                },
                "author": "Babak Ehteshami Bejnordi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18566v1",
                "updated": "2024-11-27T18:09:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T18:09:29Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    9,
                    29,
                    2,
                    332,
                    0
                ],
                "title": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS: Open Acquisition System for IEPE Sensors -- Upgraded,\n  Refined, and Overhauled Software"
                },
                "summary": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OASIS-UROS continues the previously published Open Acquisition System for\nIEPE Sensors (OASIS). While still building on the ESP32 microcontroller, this\nversion improves the overall performance by switching to an SD card caching\nsystem and upgrading the analog-digital converter to an AD7606C-18, which has a\nhigher resolution, provides eight channels, oversampling, and\nsoftware-adjustable voltage ranges. Also improved is the IEPE front-end and\npower supply, as well as the firmware of the acquisition system, which can now\nachieve a sample rate of up to 36 kHz while sampling all eight channels. This\npaper documents the hardware and software of OASIS-UROS and provides all\nmaterials required to reproduce the open acquisition system. Lastly, the system\nwas validated against commercial hardware and software in an experimental modal\nanalysis context. This showed that the system performs close to the commercial\none in some aspects with respect to the utilized test case. While OASIS-UROS\ncannot match the full performance of the commercial system, the developed\nsystem can be a viable alternative for students, people in academia, or smaller\ncompanies that have a constrained budget or require complete insight as well as\nadaptability of the hardware and software."
                },
                "authors": [
                    {
                        "name": "Oliver Maximilian Zobel"
                    },
                    {
                        "name": "Johannes Maierhofer"
                    },
                    {
                        "name": "Andreas KÃ¶stler"
                    },
                    {
                        "name": "Daniel J. Rixen"
                    }
                ],
                "author_detail": {
                    "name": "Daniel J. Rixen"
                },
                "author": "Daniel J. Rixen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.08895v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.08895v4",
                "updated": "2024-11-27T18:05:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    18,
                    5,
                    57,
                    2,
                    332,
                    0
                ],
                "published": "2024-01-17T00:36:58Z",
                "published_parsed": [
                    2024,
                    1,
                    17,
                    0,
                    36,
                    58,
                    2,
                    17,
                    0
                ],
                "title": "cedar: Optimized and Unified Machine Learning Input Data Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cedar: Optimized and Unified Machine Learning Input Data Pipelines"
                },
                "summary": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The input data pipeline is an essential component of each machine learning\n(ML) training job. It is responsible for reading massive amounts of training\ndata, processing batches of samples using complex transformations, and loading\nthem onto training nodes at low latency and high throughput. Performant input\ndata systems are becoming increasingly critical, driven by skyrocketing data\nvolumes and training throughput demands. Unfortunately, current input data\nsystems cannot fully leverage key performance optimizations, resulting in\nhugely inefficient infrastructures that require significant resources - or\nworse - underutilize expensive accelerators.\n  To address these demands, we present cedar, an optimized and unified\nprogramming framework for ML input data pipelines. cedar allows users to define\ninput data pipelines using composable operators that support arbitrary ML\nframeworks and libraries. cedar introduces an extensible optimizer that\nsystematically applies a complex combination of optimizations (e.g.,\noffloading, caching, prefetching, fusion, and reordering). It orchestrates\nprocessing across a customizable set of local and distributed compute resources\nin order to improve processing performance and efficiency, all without user\ninput. Across eight pipelines, cedar improves performance by up to 1.87x to\n10.65x compared to state-of-the-art input data systems."
                },
                "authors": [
                    {
                        "name": "Mark Zhao"
                    },
                    {
                        "name": "Emanuel Adamiak"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    }
                ],
                "author_detail": {
                    "name": "Christos Kozyrakis"
                },
                "author": "Christos Kozyrakis",
                "arxiv_comment": "Published in PVLDB Volume 18, Issue 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.08895v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.08895v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18424v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18424v1",
                "updated": "2024-11-27T15:07:28Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-27T15:07:28Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    15,
                    7,
                    28,
                    2,
                    332,
                    0
                ],
                "title": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FastSwitch: Optimizing Context Switching Efficiency in Fairness-aware\n  Large Language Model Serving"
                },
                "summary": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving numerous users and requests concurrently requires good fairness in\nLarge Language Models (LLMs) serving system. This ensures that, at the same\ncost, the system can meet the Service Level Objectives (SLOs) of more users ,\nsuch as time to first token (TTFT) and time between tokens (TBT), rather than\nallowing a few users to experience performance far exceeding the SLOs. To\nachieve better fairness, the preemption-based scheduling policy dynamically\nadjusts the priority of each request to maintain balance during runtime.\nHowever, existing systems tend to overly prioritize throughput, overlooking the\noverhead caused by preemption-induced context switching, which is crucial for\nmaintaining fairness through priority adjustments. In this work, we identify\nthree main challenges that result in this overhead. 1) Inadequate I/O\nutilization. 2) GPU idleness. 3) Unnecessary I/O transmission during multi-turn\nconversations. Our key insight is that the block-based KV cache memory policy\nin existing systems, while achieving near-zero memory waste, leads to\ndiscontinuity and insufficient granularity in the KV cache memory. To respond,\nwe introduce FastSwitch, a fairness-aware serving system that not only aligns\nwith existing KV cache memory allocation policy but also mitigates context\nswitching overhead. Our evaluation shows that FastSwitch outperforms the\nstate-of-the-art LLM serving system vLLM with speedups of 1.4-11.2x across\ndifferent tail TTFT and TBT."
                },
                "authors": [
                    {
                        "name": "Ao Shen"
                    },
                    {
                        "name": "Zhiyao Li"
                    },
                    {
                        "name": "Mingyu Gao"
                    }
                ],
                "author_detail": {
                    "name": "Mingyu Gao"
                },
                "author": "Mingyu Gao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18424v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18424v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17616v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17616v2",
                "updated": "2024-11-27T14:43:46Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    14,
                    43,
                    46,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T17:28:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Accelerating Vision Diffusion Transformers with Skip Branches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Vision Diffusion Transformers with Skip Branches"
                },
                "summary": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiT), an emerging image and video generation model\narchitecture, has demonstrated great potential because of its high generation\nquality and scalability properties. Despite the impressive performance, its\npractical deployment is constrained by computational complexity and redundancy\nin the sequential denoising process. While feature caching across timesteps has\nproven effective in accelerating diffusion models, its application to DiT is\nlimited by fundamental architectural differences from U-Net-based approaches.\nThrough empirical analysis of DiT feature dynamics, we identify that\nsignificant feature variation between DiT blocks presents a key challenge for\nfeature reusability. To address this, we convert standard DiT into Skip-DiT\nwith skip branches to enhance feature smoothness. Further, we introduce\nSkip-Cache which utilizes the skip branches to cache DiT features across\ntimesteps at the inference time. We validated effectiveness of our proposal on\ndifferent DiT backbones for video and image generation, showcasing skip\nbranches to help preserve generation quality and achieve higher speedup.\nExperimental results indicate that Skip-DiT achieves a 1.5x speedup almost for\nfree and a 2.2x speedup with only a minor reduction in quantitative metrics.\nCode is available at https://github.com/OpenSparseLLMs/Skip-DiT.git."
                },
                "authors": [
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Xinyu Zhao"
                    },
                    {
                        "name": "Yucheng Zhou"
                    },
                    {
                        "name": "Tianlong Chen"
                    },
                    {
                        "name": "Yu Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Yu Cheng"
                },
                "author": "Yu Cheng",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17616v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17616v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17459v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17459v2",
                "updated": "2024-11-27T08:21:47Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    8,
                    21,
                    47,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-26T14:23:53Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    14,
                    23,
                    53,
                    1,
                    331,
                    0
                ],
                "title": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent\n  Video Diffusion Model"
                },
                "summary": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video Variational Autoencoder (VAE) encodes videos into a low-dimensional\nlatent space, becoming a key component of most Latent Video Diffusion Models\n(LVDMs) to reduce model training costs. However, as the resolution and duration\nof generated videos increase, the encoding cost of Video VAEs becomes a\nlimiting bottleneck in training LVDMs. Moreover, the block-wise inference\nmethod adopted by most LVDMs can lead to discontinuities of latent space when\nprocessing long-duration videos. The key to addressing the computational\nbottleneck lies in decomposing videos into distinct components and efficiently\nencoding the critical information. Wavelet transform can decompose videos into\nmultiple frequency-domain components and improve the efficiency significantly,\nwe thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages\nmulti-level wavelet transform to facilitate low-frequency energy flow into\nlatent representation. Furthermore, we introduce a method called Causal Cache,\nwhich maintains the integrity of latent space during block-wise inference.\nCompared to state-of-the-art video VAEs, WF-VAE demonstrates superior\nperformance in both PSNR and LPIPS metrics, achieving 2x higher throughput and\n4x lower memory consumption while maintaining competitive reconstruction\nquality. Our code and models are available at\nhttps://github.com/PKU-YuanGroup/WF-VAE."
                },
                "authors": [
                    {
                        "name": "Zongjian Li"
                    },
                    {
                        "name": "Bin Lin"
                    },
                    {
                        "name": "Yang Ye"
                    },
                    {
                        "name": "Liuhan Chen"
                    },
                    {
                        "name": "Xinhua Cheng"
                    },
                    {
                        "name": "Shenghai Yuan"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "arxiv_comment": "8 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17459v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17459v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15785v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15785v2",
                "updated": "2024-11-27T03:07:20Z",
                "updated_parsed": [
                    2024,
                    11,
                    27,
                    3,
                    7,
                    20,
                    2,
                    332,
                    0
                ],
                "published": "2024-11-24T11:30:00Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    11,
                    30,
                    0,
                    6,
                    329,
                    0
                ],
                "title": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Method for Building Large Language Models with Predefined KV Cache\n  Capacity"
                },
                "summary": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel approach, the Bounded-Cache Transformer (BCT),\nfor building large language models with a predefined Key-Value (KV) cache\ncapacity. The BCT addresses the excessive memory consumption issue in\ntraditional KV caches by implementing a bounded-length KV cache, which is\nparticularly suitable for the attention layers in Transformer decode-only\narchitectures. By dynamically updating the key-value vector sequences, the BCT\nachieves efficient inference within limited cache capacity, significantly\nreducing memory usage while maintaining model performance and system\nthroughput. Experimental results demonstrate that the BCT significantly reduces\nmemory usage while maintaining the model's inference quality, offering a new\nsolution for efficient inference in large language models."
                },
                "authors": [
                    {
                        "name": "Zhonghua Yi"
                    },
                    {
                        "name": "Ge Niu"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Wei Tang"
                    },
                    {
                        "name": "Liqiu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Liqiu Zhang"
                },
                "author": "Liqiu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15785v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15785v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17685v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17685v1",
                "updated": "2024-11-26T18:52:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:52:06Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    52,
                    6,
                    1,
                    331,
                    0
                ],
                "title": "Attamba: Attending To Multi-Token States",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attamba: Attending To Multi-Token States"
                },
                "summary": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When predicting the next token in a sequence, vanilla transformers compute\nattention over all previous tokens, resulting in quadratic scaling of compute\nwith sequence length. State-space models compress the entire sequence of tokens\ninto a fixed-dimensional representation to improve efficiency, while other\narchitectures achieve sub-quadratic complexity via low-rank projections or\nsparse attention patterns over the sequence. In this paper, we introduce\nAttamba, a novel architecture that uses state-space models to compress chunks\nof tokens and applies attention on these compressed key-value representations.\nWe find that replacing key and value projections in a transformer with SSMs can\nimprove model quality and enable flexible token chunking, resulting in 24%\nimproved perplexity with transformer of similar KV-Cache and attention\nfootprint, and ~4 times smaller KV-Cache and Attention FLOPs for 5% perplexity\ntrade-off. Attamba can perform attention on chunked-sequences of variable\nlength, enabling a smooth transition between quadratic and linear scaling,\noffering adaptable efficiency gains."
                },
                "authors": [
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Safeen Huda"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed S. Abdelfattah"
                },
                "author": "Mohamed S. Abdelfattah",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17685v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17685v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17800v1",
                "updated": "2024-11-26T18:42:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T18:42:42Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    18,
                    42,
                    42,
                    1,
                    331,
                    0
                ],
                "title": "STAR: Synthesis of Tailored Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STAR: Synthesis of Tailored Architectures"
                },
                "summary": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative improvement of model architectures is fundamental to deep learning:\nTransformers first enabled scaling, and recent advances in model hybridization\nhave pushed the quality-efficiency frontier. However, optimizing architectures\nremains challenging and expensive. Current automated or manual approaches fall\nshort, largely due to limited progress in the design of search spaces and due\nto the simplicity of resulting patterns and heuristics. In this work, we\npropose a new approach for the synthesis of tailored architectures (STAR). Our\napproach combines a novel search space based on the theory of linear\ninput-varying systems, supporting a hierarchical numerical encoding into\narchitecture genomes. STAR genomes are automatically refined and recombined\nwith gradient-free, evolutionary algorithms to optimize for multiple model\nquality and efficiency metrics. Using STAR, we optimize large populations of\nnew architectures, leveraging diverse computational units and interconnection\npatterns, improving over highly-optimized Transformers and striped hybrid\nmodels on the frontier of quality, parameter size, and inference cache for\nautoregressive language modeling."
                },
                "authors": [
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Rom Parnichkun"
                    },
                    {
                        "name": "Alexander Amini"
                    },
                    {
                        "name": "Stefano Massaroli"
                    },
                    {
                        "name": "Michael Poli"
                    }
                ],
                "author_detail": {
                    "name": "Michael Poli"
                },
                "author": "Michael Poli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15651v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15651v3",
                "updated": "2024-11-26T17:28:06Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    17,
                    28,
                    6,
                    1,
                    331,
                    0
                ],
                "published": "2024-03-22T23:47:19Z",
                "published_parsed": [
                    2024,
                    3,
                    22,
                    23,
                    47,
                    19,
                    4,
                    82,
                    0
                ],
                "title": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GaNI: Global and Near Field Illumination Aware Neural Inverse Rendering"
                },
                "summary": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we present GaNI, a Global and Near-field Illumination-aware\nneural inverse rendering technique that can reconstruct geometry, albedo, and\nroughness parameters from images of a scene captured with co-located light and\ncamera. Existing inverse rendering techniques with co-located light-camera\nfocus on single objects only, without modeling global illumination and\nnear-field lighting more prominent in scenes with multiple objects. We\nintroduce a system that solves this problem in two stages; we first reconstruct\nthe geometry powered by neural volumetric rendering NeuS, followed by inverse\nneural radiosity that uses the previously predicted geometry to estimate albedo\nand roughness. However, such a naive combination fails and we propose multiple\ntechnical contributions that enable this two-stage approach. We observe that\nNeuS fails to handle near-field illumination and strong specular reflections\nfrom the flashlight in a scene. We propose to implicitly model the effects of\nnear-field illumination and introduce a surface angle loss function to handle\nspecular reflections. Similarly, we observe that invNeRad assumes constant\nillumination throughout the capture and cannot handle moving flashlights during\ncapture. We propose a light position-aware radiance cache network and\nadditional smoothness priors on roughness to reconstruct reflectance.\nExperimental evaluation on synthetic and real data shows that our method\noutperforms the existing co-located light-camera-based inverse rendering\ntechniques. Our approach produces significantly better reflectance and slightly\nbetter geometry than capture strategies that do not require a dark room."
                },
                "authors": [
                    {
                        "name": "Jiaye Wu"
                    },
                    {
                        "name": "Saeed Hadadan"
                    },
                    {
                        "name": "Geng Lin"
                    },
                    {
                        "name": "Matthias Zwicker"
                    },
                    {
                        "name": "David Jacobs"
                    },
                    {
                        "name": "Roni Sengupta"
                    }
                ],
                "author_detail": {
                    "name": "Roni Sengupta"
                },
                "author": "Roni Sengupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15651v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15651v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17559v1",
                "updated": "2024-11-26T16:21:10Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T16:21:10Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    16,
                    21,
                    10,
                    1,
                    331,
                    0
                ],
                "title": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Degrees of Freedom of Cache-Aided Interference Channels Assisted by\n  Active Intelligent Reflecting Surfaces"
                },
                "summary": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper studies cache-aided wireless networks in the presence of active\nintelligent reflecting surfaces (IRS) from an information-theoretic\nperspective. Specifically, we explore interference management in a cache-aided\nwireless network assisted by an active IRS, to enhance the achievable degrees\nof freedom (DoF). To this end, we jointly design the content placement,\ndelivery phase, and phase shifts of the IRS and propose a one-shot achievable\nscheme. Our scheme exploits transmitters' cooperation, cache contents (as side\ninformation), interference alignment, and IRS capabilities, adapting to the\nnetwork's parameters. We derive the achievable one-shot sum-DoF for different\nsizes of cache memories, network configurations, and numbers of IRS elements.\nOur results highlight the potential of deploying an IRS in cache-aided wireless\ncommunication systems, underscoring the enhancement of achievable DoF for\nvarious parameter regimes, particularly when the sizes of the caches\n(especially at the transmitters) are inadequate. Notably, we show that access\nto an IRS with a sufficient number of elements enables the achievement of the\nmaximum possible DoF for various parameter regimes of interest."
                },
                "authors": [
                    {
                        "name": "Abolfazl Changizi"
                    },
                    {
                        "name": "Ali H. Abdollahi Bafghi"
                    },
                    {
                        "name": "Masoumeh Nasiri-Kenari"
                    }
                ],
                "author_detail": {
                    "name": "Masoumeh Nasiri-Kenari"
                },
                "author": "Masoumeh Nasiri-Kenari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17786v1",
                "updated": "2024-11-26T15:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T15:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    15,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DreamCache: Finetuning-Free Lightweight Personalized Image Generation\n  via Feature Caching"
                },
                "summary": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized image generation requires text-to-image generative models that\ncapture the core features of a reference subject to allow for controlled\ngeneration across different contexts. Existing methods face challenges due to\ncomplex training requirements, high inference costs, limited flexibility, or a\ncombination of these issues. In this paper, we introduce DreamCache, a scalable\napproach for efficient and high-quality personalized image generation. By\ncaching a small number of reference image features from a subset of layers and\na single timestep of the pretrained diffusion denoiser, DreamCache enables\ndynamic modulation of the generated image features through lightweight, trained\nconditioning adapters. DreamCache achieves state-of-the-art image and text\nalignment, utilizing an order of magnitude fewer extra parameters, and is both\nmore computationally effective and versatile than existing models."
                },
                "authors": [
                    {
                        "name": "Emanuele Aiello"
                    },
                    {
                        "name": "Umberto Michieli"
                    },
                    {
                        "name": "Diego Valsesia"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Enrico Magli"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Magli"
                },
                "author": "Enrico Magli",
                "arxiv_comment": "16 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17116v1",
                "updated": "2024-11-26T05:10:04Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T05:10:04Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    5,
                    10,
                    4,
                    1,
                    331,
                    0
                ],
                "title": "Star Attention: Efficient LLM Inference over Long Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Star Attention: Efficient LLM Inference over Long Sequences"
                },
                "summary": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference with Transformer-based Large Language Models (LLMs) on long\nsequences is both costly and slow due to the quadratic complexity of the\nself-attention mechanism. We introduce Star Attention, a two-phase block-sparse\napproximation that improves computational efficiency by sharding attention\nacross multiple hosts while minimizing communication overhead. In the first\nphase, the context is processed using blockwise-local attention across hosts,\nin parallel. In the second phase, query and response tokens attend to all prior\ncached tokens through sequence-global attention. Star Attention integrates\nseamlessly with most Transformer-based LLMs trained with global attention,\nreducing memory requirements and inference time by up to 11x while preserving\n95-100% of accuracy."
                },
                "authors": [
                    {
                        "name": "Shantanu Acharya"
                    },
                    {
                        "name": "Fei Jia"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Code: https://github.com/NVIDIA/Star-Attention",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17089v1",
                "updated": "2024-11-26T04:03:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "published": "2024-11-26T04:03:14Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    4,
                    3,
                    14,
                    1,
                    331,
                    0
                ],
                "title": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM Inference with I/O-Aware Partial KV Cache Recomputation"
                },
                "summary": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference for Large Language Models (LLMs) is computationally demanding. To\nreduce the cost of auto-regressive decoding, Key-Value (KV) caching is used to\nstore intermediate activations, enabling GPUs to perform only the incremental\ncomputation required for each new token. This approach significantly lowers the\ncomputational overhead for token generation. However, the memory required for\nKV caching grows rapidly, often exceeding the capacity of GPU memory. A\ncost-effective alternative is to offload KV cache to CPU memory, which\nalleviates GPU memory pressure but shifts the bottleneck to the limited\nbandwidth of the PCIe connection between the CPU and GPU. Existing methods\nattempt to address these issues by overlapping GPU computation with I/O or\nemploying CPU-GPU heterogeneous execution, but they are hindered by excessive\ndata movement and dependence on CPU capabilities. In this paper, we introduce\nan efficient CPU-GPU I/O-aware LLM inference method that avoids transferring\nthe entire KV cache from CPU to GPU by recomputing partial KV cache from\nactivations while concurrently transferring the remaining KV cache via PCIe\nbus. This approach overlaps GPU recomputation with data transfer to minimize\nidle GPU time and maximize inference performance. Our method is fully automated\nby integrating a profiler module that utilizes input characteristics and system\nhardware information, a scheduler module to optimize the distribution of\ncomputation and communication workloads, and a runtime module to efficiently\nexecute the derived execution plan. Experimental results show that our method\nachieves up to 35.8% lower latency and 46.2% higher throughput during decoding\ncompared to state-of-the-art approaches."
                },
                "authors": [
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Murali Annavaram"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavaram"
                },
                "author": "Murali Annavaram",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16375v1",
                "updated": "2024-11-25T13:33:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "published": "2024-11-25T13:33:41Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    13,
                    33,
                    41,
                    0,
                    330,
                    0
                ],
                "title": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ca2-VDM: Efficient Autoregressive Video Diffusion Model with Causal\n  Generation and Cache Sharing"
                },
                "summary": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advance of diffusion models, today's video generation has achieved\nimpressive quality. To extend the generation length and facilitate real-world\napplications, a majority of video diffusion models (VDMs) generate videos in an\nautoregressive manner, i.e., generating subsequent clips conditioned on the\nlast frame(s) of the previous clip. However, existing autoregressive VDMs are\nhighly inefficient and redundant: The model must re-compute all the conditional\nframes that are overlapped between adjacent clips. This issue is exacerbated\nwhen the conditional frames are extended autoregressively to provide the model\nwith long-term context. In such cases, the computational demands increase\nsignificantly (i.e., with a quadratic complexity w.r.t. the autoregression\nstep). In this paper, we propose Ca2-VDM, an efficient autoregressive VDM with\nCausal generation and Cache sharing. For causal generation, it introduces\nunidirectional feature computation, which ensures that the cache of conditional\nframes can be precomputed in previous autoregression steps and reused in every\nsubsequent step, eliminating redundant computations. For cache sharing, it\nshares the cache across all denoising steps to avoid the huge cache storage\ncost. Extensive experiments demonstrated that our Ca2-VDM achieves\nstate-of-the-art quantitative and qualitative video generation results and\nsignificantly improves the generation speed. Code is available at\nhttps://github.com/Dawn-LX/CausalCache-VDM"
                },
                "authors": [
                    {
                        "name": "Kaifeng Gao"
                    },
                    {
                        "name": "Jiaxin Shi"
                    },
                    {
                        "name": "Hanwang Zhang"
                    },
                    {
                        "name": "Chunping Wang"
                    },
                    {
                        "name": "Jun Xiao"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Technical Report. Code is available at\n  https://github.com/Dawn-LX/CausalCache-VDM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19315v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19315v2",
                "updated": "2024-11-25T12:14:33Z",
                "updated_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    14,
                    33,
                    0,
                    330,
                    0
                ],
                "published": "2024-09-28T11:00:11Z",
                "published_parsed": [
                    2024,
                    9,
                    28,
                    11,
                    0,
                    11,
                    5,
                    272,
                    0
                ],
                "title": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analog In-Memory Computing Attention Mechanism for Fast and\n  Energy-Efficient Large Language Models"
                },
                "summary": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer networks, driven by self-attention, are central to Large Language\nModels. In generative Transformers, self-attention uses cache memory to store\ntoken projections, avoiding recomputation at each time step. However,\nGPU-stored projections must be loaded into SRAM for each new generation step,\ncausing latency and energy bottlenecks.\n  We present a custom self-attention in-memory computing architecture based on\nemerging charge-based memories called gain cells, which can be efficiently\nwritten to store new tokens during sequence generation and enable parallel\nanalog dot-product computation required for self-attention. However, the analog\ngain cell circuits introduce non-idealities and constraints preventing the\ndirect mapping of pre-trained models. To circumvent this problem, we design an\ninitialization algorithm achieving text processing performance comparable to\nGPT-2 without training from scratch. Our architecture respectively reduces\nattention latency and energy consumption by up to two and five orders of\nmagnitude compared to GPUs, marking a significant step toward ultra-fast,\nlow-power generative Transformers."
                },
                "authors": [
                    {
                        "name": "Nathan Leroux"
                    },
                    {
                        "name": "Paul-Philipp Manea"
                    },
                    {
                        "name": "Chirag Sudarshan"
                    },
                    {
                        "name": "Jan Finkbeiner"
                    },
                    {
                        "name": "Sebastian Siegel"
                    },
                    {
                        "name": "John Paul Strachan"
                    },
                    {
                        "name": "Emre Neftci"
                    }
                ],
                "author_detail": {
                    "name": "Emre Neftci"
                },
                "author": "Emre Neftci",
                "arxiv_comment": "25 pages, 6 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19315v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19315v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.11469v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.11469v2",
                "updated": "2024-11-24T21:57:29Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    21,
                    57,
                    29,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-18T11:12:57Z",
                "published_parsed": [
                    2024,
                    11,
                    18,
                    11,
                    12,
                    57,
                    0,
                    323,
                    0
                ],
                "title": "Deegen: A JIT-Capable VM Generator for Dynamic Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deegen: A JIT-Capable VM Generator for Dynamic Languages"
                },
                "summary": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building a high-performance JIT-capable VM for a dynamic language has\ntraditionally required a tremendous amount of time, money, and expertise. We\npresent Deegen, a meta-compiler that allows users to generate a\nhigh-performance JIT-capable VM for their own language at an engineering cost\nsimilar to writing a simple interpreter. Deegen takes in the execution\nsemantics of the bytecodes implemented as C++ functions, and automatically\ngenerates a two-tier VM execution engine with a state-of-the-art interpreter, a\nstate-of-the-art baseline JIT, and the tier-switching logic that connects them\ninto a self-adaptive system.\n  We are the first to demonstrate the automatic generation of a JIT compiler,\nand the automatic generation of an interpreter that outperforms the state of\nthe art. Our performance comes from a long list of optimizations supported by\nDeegen, including bytecode specialization and quickening, register pinning, tag\nregister optimization, call inline caching, generic inline caching, JIT\npolymorphic IC, JIT IC inline slab, type-check removal and strength reduction,\ntype-based slow-path extraction and outlining, JIT hot-cold code splitting, and\nJIT OSR-entry. These optimizations are either employed automatically, or guided\nby the language implementer through intuitive APIs. As a result, the\ndisassembly of the Deegen-generated interpreter, baseline JIT, and the\ngenerated JIT code rivals the assembly code hand-written by experts in\nstate-of-the-art VMs.\n  We implement LuaJIT Remake (LJR), a standard-compliant Lua 5.1 VM, using\nDeegen. Across 44 benchmarks, LJR's interpreter is on average 179% faster than\nthe official PUC Lua interpreter, and 31% faster than LuaJIT's interpreter.\nLJR's baseline JIT has negligible startup delay, and its execution performance\nis on average 360% faster than PUC Lua and only 33% slower (but faster on 13/44\nbenchmarks) than LuaJIT's optimizing JIT."
                },
                "authors": [
                    {
                        "name": "Haoran Xu"
                    },
                    {
                        "name": "Fredrik Kjolstad"
                    }
                ],
                "author_detail": {
                    "name": "Fredrik Kjolstad"
                },
                "author": "Fredrik Kjolstad",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.11469v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.11469v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17741v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17741v1",
                "updated": "2024-11-24T16:20:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T16:20:57Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    16,
                    20,
                    57,
                    6,
                    329,
                    0
                ],
                "title": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Chameleon: Adaptive Caching and Scheduling for Many-Adapter LLM\n  Inference Environments"
                },
                "summary": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread adoption of LLMs has driven an exponential rise in their\ndeployment, imposing substantial demands on inference clusters. These clusters\nmust handle numerous concurrent queries for different LLM downstream tasks. To\nhandle multi-task settings with vast LLM parameter counts, methods like\nLow-Rank Adaptation (LoRA) enable task-specific fine-tuning while sharing most\nof the base LLM model across tasks. Hence, they allow concurrent task serving\nwith minimal memory requirements. However, existing LLM serving systems face\ninefficiencies: they overlook workload heterogeneity, impose high link\nbandwidth from frequent adapter loading, and suffer from head-of-line blocking\nin their schedulers. To address these challenges, we present Chameleon, a novel\nLLM serving system optimized for many adapter environments, that relies on two\ncore ideas: adapter caching and adapter-aware scheduling. First, Chameleon\ncaches popular adapters in GPU memory, minimizing the adapter loading times.\nImportantly, it uses the otherwise idle GPU memory, avoiding extra memory\ncosts. Second, Chameleon uses a non-preemptive multi-queue scheduling to\nefficiently account for workload heterogeneity. In this way, Chameleon\nsimultaneously prevents head of line blocking and starvation. We implement\nChameleon on top of a state-of-the-art LLM serving platform and evaluate it\nwith real-world production traces and open-source LLMs. Under high loads,\nChameleon reduces P99 and P50 TTFT latency by 80.7% and 48.1%, respectively,\nwhile improving throughput by 1.5x compared to state-of-the-art baselines."
                },
                "authors": [
                    {
                        "name": "Nikoleta Iliakopoulou"
                    },
                    {
                        "name": "Jovan Stojkovic"
                    },
                    {
                        "name": "Chloe Alverti"
                    },
                    {
                        "name": "Tianyin Xu"
                    },
                    {
                        "name": "Hubertus Franke"
                    },
                    {
                        "name": "Josep Torrellas"
                    }
                ],
                "author_detail": {
                    "name": "Josep Torrellas"
                },
                "author": "Josep Torrellas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17741v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17741v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.0; D.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15735v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15735v1",
                "updated": "2024-11-24T06:43:38Z",
                "updated_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "published": "2024-11-24T06:43:38Z",
                "published_parsed": [
                    2024,
                    11,
                    24,
                    6,
                    43,
                    38,
                    6,
                    329,
                    0
                ],
                "title": "Test-time Alignment-Enhanced Adapter for Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Alignment-Enhanced Adapter for Vision-Language Models"
                },
                "summary": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation with pre-trained vision-language models (VLMs) has\nattracted increasing attention for tackling the issue of distribution shift\nduring the test phase. While prior methods have shown effectiveness in\naddressing distribution shift by adjusting classification logits, they are not\noptimal due to keeping text features unchanged. To address this issue, we\nintroduce a new approach called Test-time Alignment-Enhanced Adapter (TAEA),\nwhich trains an adapter with test samples to adjust text features during the\ntest phase. We can enhance the text-to-image alignment prediction by utilizing\nan adapter to adapt text features. Furthermore, we also propose to adopt the\nnegative cache from TDA as enhancement module, which further improves the\nperformance of TAEA. Our approach outperforms the state-of-the-art TTA method\nof pre-trained VLMs by an average of 0.75% on the out-of-distribution benchmark\nand 2.5% on the cross-domain benchmark, with an acceptable training time. Code\nwill be available at https://github.com/BaoshunWq/clip-TAEA."
                },
                "authors": [
                    {
                        "name": "Baoshun Tong"
                    },
                    {
                        "name": "Kaiyu Song"
                    },
                    {
                        "name": "Hanjiang Lai"
                    }
                ],
                "author_detail": {
                    "name": "Hanjiang Lai"
                },
                "author": "Hanjiang Lai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15735v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15735v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09688v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09688v2",
                "updated": "2024-11-23T22:11:42Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    22,
                    11,
                    42,
                    5,
                    328,
                    0
                ],
                "published": "2024-11-14T18:54:19Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    18,
                    54,
                    19,
                    3,
                    319,
                    0
                ],
                "title": "Squeezed Attention: Accelerating Long Context Length LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Squeezed Attention: Accelerating Long Context Length LLM Inference"
                },
                "summary": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emerging Large Language Model (LLM) applications require long input prompts\nto perform complex downstream tasks like document analysis and code generation.\nFor these long context length applications, the length of the input prompt\nposes a significant challenge in terms of inference efficiency since the\ninference costs increase linearly with sequence length. However, for many of\nthese applications, much of the context in the prompt is fixed across different\nuser inputs, thereby providing the opportunity to perform offline optimizations\nto process user inputs quickly, as they are received. In this work, we propose\nSqueezed Attention as a mechanism to accelerate LLM applications where a large\nportion of the input prompt is fixed. We first leverage K-means clustering\noffline to group the keys for the fixed context based on semantic similarity\nand represent each cluster with a single centroid value. During inference, we\ncompare query tokens from the user input with the centroids to predict which of\nthe keys from the fixed context are semantically relevant and need to be loaded\nduring inference. We then compute exact attention using only these important\nkeys from the fixed context, thereby reducing bandwidth and computational\ncosts. We also extend our method to use a hierarchical centroid lookup to\nidentify important keys, which can reduce the complexity of attention from\nlinear to logarithmic with respect to the context length. We implement\noptimized Triton kernels for centroid comparison and sparse FlashAttention with\nimportant keys, achieving more than 4x speedups during both the prefill and\ngeneration phases for long-context inference. Furthermore, we have extensively\nevaluated our method on various long-context benchmarks including LongBench,\nwhere it achieves a 3x reduction in KV cache budget without accuracy loss and\nup to an 8x reduction with <0.5 point accuracy gap for various models."
                },
                "authors": [
                    {
                        "name": "Coleman Hooper"
                    },
                    {
                        "name": "Sehoon Kim"
                    },
                    {
                        "name": "Hiva Mohammadzadeh"
                    },
                    {
                        "name": "Monishwaran Maheswaran"
                    },
                    {
                        "name": "June Paik"
                    },
                    {
                        "name": "Michael W. Mahoney"
                    },
                    {
                        "name": "Kurt Keutzer"
                    },
                    {
                        "name": "Amir Gholami"
                    }
                ],
                "author_detail": {
                    "name": "Amir Gholami"
                },
                "author": "Amir Gholami",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09688v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09688v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.05396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.05396v3",
                "updated": "2024-11-23T10:42:11Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    10,
                    42,
                    11,
                    5,
                    328,
                    0
                ],
                "published": "2024-02-08T04:16:35Z",
                "published_parsed": [
                    2024,
                    2,
                    8,
                    4,
                    16,
                    35,
                    3,
                    39,
                    0
                ],
                "title": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASER: Temporal Adaptive Sampling for Fast and Accurate Dynamic Graph\n  Representation Learning"
                },
                "summary": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Temporal Graph Neural Networks (TGNNs) have demonstrated\nstate-of-the-art performance in various high-impact applications, including\nfraud detection and content recommendation. Despite the success of TGNNs, they\nare prone to the prevalent noise found in real-world dynamic graphs like\ntime-deprecated links and skewed interaction distribution. The noise causes two\ncritical issues that significantly compromise the accuracy of TGNNs: (1) models\nare supervised by inferior interactions, and (2) noisy input induces high\nvariance in the aggregated messages. However, current TGNN denoising techniques\ndo not consider the diverse and dynamic noise pattern of each node. In\naddition, they also suffer from the excessive mini-batch generation overheads\ncaused by traversing more neighbors. We believe the remedy for fast and\naccurate TGNNs lies in temporal adaptive sampling. In this work, we propose\nTASER, the first adaptive sampling method for TGNNs optimized for accuracy,\nefficiency, and scalability. TASER adapts its mini-batch selection based on\ntraining dynamics and temporal neighbor selection based on the contextual,\nstructural, and temporal properties of past interactions. To alleviate the\nbottleneck in mini-batch generation, TASER implements a pure GPU-based temporal\nneighbor finder and a dedicated GPU feature cache. We evaluate the performance\nof TASER using two state-of-the-art backbone TGNNs. On five popular datasets,\nTASER outperforms the corresponding baselines by an average of 2.3% in Mean\nReciprocal Rank (MRR) while achieving an average of 5.1x speedup in training\ntime."
                },
                "authors": [
                    {
                        "name": "Gangda Deng"
                    },
                    {
                        "name": "Hongkuan Zhou"
                    },
                    {
                        "name": "Hanqing Zeng"
                    },
                    {
                        "name": "Yinglong Xia"
                    },
                    {
                        "name": "Christopher Leung"
                    },
                    {
                        "name": "Jianbo Li"
                    },
                    {
                        "name": "Rajgopal Kannan"
                    },
                    {
                        "name": "Viktor Prasanna"
                    }
                ],
                "author_detail": {
                    "name": "Viktor Prasanna"
                },
                "author": "Viktor Prasanna",
                "arxiv_comment": "IPDPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.05396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.05396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02109v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02109v2",
                "updated": "2024-11-23T01:44:00Z",
                "updated_parsed": [
                    2024,
                    11,
                    23,
                    1,
                    44,
                    0,
                    5,
                    328,
                    0
                ],
                "published": "2024-07-02T09:51:56Z",
                "published_parsed": [
                    2024,
                    7,
                    2,
                    9,
                    51,
                    56,
                    1,
                    184,
                    0
                ],
                "title": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HRSAM: Efficient Interactive Segmentation in High-Resolution Images"
                },
                "summary": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Segment Anything Model (SAM) has advanced interactive segmentation but is\nlimited by the high computational cost on high-resolution images. This requires\ndownsampling to meet GPU constraints, sacrificing the fine-grained details\nneeded for high-precision interactive segmentation. To address SAM's\nlimitations, we focus on visual length extrapolation and propose a lightweight\nmodel named HRSAM. The extrapolation enables HRSAM trained on low resolutions\nto generalize to high resolutions. We begin by finding the link between the\nextrapolation and attention scores, which leads us to base HRSAM on Swin\nattention. We then introduce the Flexible Local Attention (FLA) framework,\nusing CUDA-optimized Efficient Memory Attention to accelerate HRSAM. Within\nFLA, we implement Flash Swin attention, achieving over a 35% speedup compared\nto traditional Swin attention, and propose a KV-only padding mechanism to\nenhance extrapolation. We also develop the Cycle-scan module that uses State\nSpace models to efficiently expand HRSAM's receptive field. We further develop\nthe HRSAM++ within FLA by adding an anchor map, providing multi-scale data\naugmentation for the extrapolation and a larger receptive field at slight\ncomputational cost. Experiments show that, under standard training, HRSAMs\nsurpass the previous SOTA with only 38% of the latency. With SAM-distillation,\nthe extrapolation enables HRSAMs to outperform the teacher model at lower\nlatency. Further finetuning achieves performance significantly exceeding the\nprevious SOTA."
                },
                "authors": [
                    {
                        "name": "You Huang"
                    },
                    {
                        "name": "Wenbin Lai"
                    },
                    {
                        "name": "Jiayi Ji"
                    },
                    {
                        "name": "Liujuan Cao"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02109v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02109v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15322v1",
                "updated": "2024-11-22T19:30:40Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T19:30:40Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    19,
                    30,
                    40,
                    4,
                    327,
                    0
                ],
                "title": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep Learning-Based Automatic Delineation of Liver Domes in kV Triggered\n  Images for Online Breath-hold Reproducibility Verification of Liver\n  Stereotactic Body Radiation Therapy"
                },
                "summary": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereotactic Body Radiation Therapy (SBRT) can be a precise, minimally\ninvasive treatment method for liver cancer and liver metastases. However, the\neffectiveness of SBRT relies on the accurate delivery of the dose to the tumor\nwhile sparing healthy tissue. Challenges persist in ensuring breath-hold\nreproducibility, with current methods often requiring manual verification of\nliver dome positions from kV-triggered images. To address this, we propose a\nproof-of-principle study of a deep learning-based pipeline to automatically\ndelineate the liver dome from kV-planar images. From 24 patients who received\nSBRT for liver cancer or metastasis inside liver, 711 KV-triggered images\nacquired for online breath-hold verification were included in the current\nstudy. We developed a pipeline comprising a trained U-Net for automatic liver\ndome region segmentation from the triggered images followed by extraction of\nthe liver dome via thresholding, edge detection, and morphological operations.\nThe performance and generalizability of the pipeline was evaluated using 2-fold\ncross validation. The training of the U-Net model for liver region segmentation\ntook under 30 minutes and the automatic delineation of a liver dome for any\ntriggered image took less than one second. The RMSE and rate of detection for\nFold1 with 366 images was (6.4 +/- 1.6) mm and 91.7%, respectively. For Fold2\nwith 345 images, the RMSE and rate of detection was (7.7 +/- 2.3) mm and 76.3%\nrespectively."
                },
                "authors": [
                    {
                        "name": "Sugandima Weragoda"
                    },
                    {
                        "name": "Ping Xia"
                    },
                    {
                        "name": "Kevin Stephans"
                    },
                    {
                        "name": "Neil Woody"
                    },
                    {
                        "name": "Michael Martens"
                    },
                    {
                        "name": "Robert Brown"
                    },
                    {
                        "name": "Bingqi Guo"
                    }
                ],
                "author_detail": {
                    "name": "Bingqi Guo"
                },
                "author": "Bingqi Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.med-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.med-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v1",
                "updated": "2024-11-22T18:06:14Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15024v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15024v1",
                "updated": "2024-11-22T15:55:19Z",
                "updated_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "published": "2024-11-22T15:55:19Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    15,
                    55,
                    19,
                    4,
                    327,
                    0
                ],
                "title": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DyCoke: Dynamic Compression of Tokens for Fast Video Large Language\n  Models"
                },
                "summary": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video large language models (VLLMs) have significantly advanced recently in\nprocessing complex video content, yet their inference efficiency remains\nconstrained because of the high computational cost stemming from the thousands\nof visual tokens generated from the video inputs. We empirically observe that,\nunlike single image inputs, VLLMs typically attend visual tokens from different\nframes at different decoding iterations, making a one-shot pruning strategy\nprone to removing important tokens by mistake. Motivated by this, we present\nDyCoke, a training-free token compression method to optimize token\nrepresentation and accelerate VLLMs. DyCoke incorporates a plug-and-play\ntemporal compression module to minimize temporal redundancy by merging\nredundant tokens across frames, and applies dynamic KV cache reduction to prune\nspatially redundant tokens selectively. It ensures high-quality inference by\ndynamically retaining the critical tokens at each decoding step. Extensive\nexperimental results demonstrate that DyCoke can outperform the prior SoTA\ncounterparts, achieving 1.5X inference speedup, 1.4X memory reduction against\nthe baseline VLLM, while still improving the performance, with no training."
                },
                "authors": [
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Can Qin"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Yang Sui"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15024v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15024v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04032v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04032v5",
                "updated": "2024-11-21T05:55:43Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    55,
                    43,
                    3,
                    326,
                    0
                ],
                "published": "2024-02-06T14:26:22Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    14,
                    26,
                    22,
                    1,
                    37,
                    0
                ],
                "title": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProactivePIM: Accelerating Weight-Sharing Embedding Layer with PIM for\n  Scalable Recommendation System"
                },
                "summary": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The model size growth of personalized recommendation systems poses new\nchallenges for inference. Weight-sharing algorithms have been proposed for size\nreduction, but they increase memory access. Recent advancements in\nprocessing-in-memory (PIM) enhanced the model throughput by exploiting memory\nparallelism, but such algorithms introduce massive CPU-PIM communication into\nprior PIM systems. We propose ProactivePIM, a PIM system for weight-sharing\nrecommendation system acceleration. ProactivePIM integrates a cache within the\nPIM with a prefetching scheme to leverage a unique locality of the algorithm\nand eliminate communication overhead through a subtable mapping strategy.\nProactivePIM achieves a 4.8x speedup compared to prior works."
                },
                "authors": [
                    {
                        "name": "Youngsuk Kim"
                    },
                    {
                        "name": "Junghwan Lim"
                    },
                    {
                        "name": "Hyuk-Jae Lee"
                    },
                    {
                        "name": "Chae Eun Rhee"
                    }
                ],
                "author_detail": {
                    "name": "Chae Eun Rhee"
                },
                "author": "Chae Eun Rhee",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04032v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04032v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13854v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13854v1",
                "updated": "2024-11-21T05:26:57Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T05:26:57Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    5,
                    26,
                    57,
                    3,
                    326,
                    0
                ],
                "title": "Static Reuse Profile Estimation for Array Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Static Reuse Profile Estimation for Array Applications"
                },
                "summary": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reuse distance analysis is a widely recognized method for application\ncharacterization that illustrates cache locality. Although there are various\ntechniques to calculate the reuse profile from dynamic memory traces, it is\nboth time and space-consuming due to the requirement to collect dynamic memory\ntraces at runtime. In contrast, static analysis reuse profile estimation is a\npromisingly faster approach since it is calculated at compile time without\nrunning the program or collecting memory traces. This work presents a static\nanalysis technique to estimate the reuse profile of loop-based programs. For an\ninput program, we generate a basic block-level control flow graph and the\nexecution count by analyzing the LLVM IR of the program. We present the memory\naccesses of the application kernel in a compact bracketed format and use a\nrecursive algorithm to predict the reuse distance histogram. We deploy a\nseparate predictor that unrolls the loop(s) for smaller bounds and generates a\ntemporary reuse distance profile for those small cases. Using these smaller\nprofiles, the reuse profile is extrapolated for the actual loop bound(s). We\nuse this reuse profile to predict the cache hit rate. Results show that our\nmodel can predict cache hit rates with an average accuracy of 95% relative to\nthe dynamic reuse profile methods."
                },
                "authors": [
                    {
                        "name": "Abdur Razzak"
                    },
                    {
                        "name": "Atanu Barai"
                    },
                    {
                        "name": "Nandakishore Santhi"
                    },
                    {
                        "name": "Abdel-Hameed A. Badawy"
                    }
                ],
                "author_detail": {
                    "name": "Abdel-Hameed A. Badawy"
                },
                "author": "Abdel-Hameed A. Badawy",
                "arxiv_comment": "Accepted in The International Symposium on Memory Systems (MEMSYS\n  24), September 30 to October 03, 2024, Washington, DC, USA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13854v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13854v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.02243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.02243v3",
                "updated": "2024-11-21T04:12:53Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    4,
                    12,
                    53,
                    3,
                    326,
                    0
                ],
                "published": "2023-06-04T03:06:37Z",
                "published_parsed": [
                    2023,
                    6,
                    4,
                    3,
                    6,
                    37,
                    6,
                    155,
                    0
                ],
                "title": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Enhanced Visual Prompt Learning for Few-shot Classification"
                },
                "summary": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Contrastive Language-Image Pretraining (CLIP) model has been widely used\nin various downstream vision tasks. The few-shot learning paradigm has been\nwidely adopted to augment its capacity for these tasks. However, current\nparadigms may struggle with fine-grained classification, such as satellite\nimage recognition, due to widening domain gaps. To address this limitation, we\npropose retrieval-enhanced visual prompt learning (RePrompt), which introduces\nretrieval mechanisms to cache and reuse the knowledge of downstream tasks.\nRePrompt constructs a retrieval database from either training examples or\nexternal data if available, and uses a retrieval mechanism to enhance multiple\nstages of a simple prompt learning baseline, thus narrowing the domain gap.\nDuring inference, our enhanced model can reference similar samples brought by\nretrieval to make more accurate predictions. A detailed analysis reveals that\nretrieval helps to improve the distribution of late features, thus, improving\ngeneralization for downstream tasks. Reprompt attains state-of-the-art\nperformance on a wide range of vision datasets, including 11 image datasets, 3\nvideo datasets, 1 multi-view dataset, and 4 domain generalization benchmarks."
                },
                "authors": [
                    {
                        "name": "Jintao Rong"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Linlin Ou"
                    },
                    {
                        "name": "Tianxiao Chen"
                    },
                    {
                        "name": "Xinyi Yu"
                    },
                    {
                        "name": "Yifan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yifan Liu"
                },
                "author": "Yifan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.02243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.02243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13820v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13820v1",
                "updated": "2024-11-21T03:52:41Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T03:52:41Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    52,
                    41,
                    3,
                    326,
                    0
                ],
                "title": "InstCache: A Predictive Cache for LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "InstCache: A Predictive Cache for LLM Serving"
                },
                "summary": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models are revolutionizing every aspect of human life.\nHowever, the unprecedented power comes at the cost of significant computing\nintensity, suggesting long latency and large energy footprint. Key-Value Cache\nand Semantic Cache have been proposed as a solution to the above problem, but\nboth suffer from limited scalability due to significant memory cost for each\ntoken or instruction embeddings. Motivated by the observations that most\ninstructions are short, repetitive and predictable by LLMs, we propose to\npredict user-instructions by an instruction-aligned LLM and store them in a\npredictive cache, so-called InstCache. We introduce an instruction\npre-population algorithm based on the negative log likelihood of instructions,\ndetermining the cache size with regard to the hit rate. The proposed InstCache\nis efficiently implemented as a hash table with minimal lookup latency for\ndeployment. Experimental results show that InstCache can achieve up to 51.34%\nhit rate on LMSys dataset, which corresponds to a 2x speedup, at a memory cost\nof only 4.5GB."
                },
                "authors": [
                    {
                        "name": "Longwei Zou"
                    },
                    {
                        "name": "Tingfeng Liu"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Jiangang Kong"
                    },
                    {
                        "name": "Yangdong Deng"
                    }
                ],
                "author_detail": {
                    "name": "Yangdong Deng"
                },
                "author": "Yangdong Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13820v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13820v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.22649v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.22649v2",
                "updated": "2024-11-21T03:34:44Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    3,
                    34,
                    44,
                    3,
                    326,
                    0
                ],
                "published": "2024-10-30T02:36:55Z",
                "published_parsed": [
                    2024,
                    10,
                    30,
                    2,
                    36,
                    55,
                    2,
                    304,
                    0
                ],
                "title": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WaveRoRA: Wavelet Rotary Route Attention for Multivariate Time Series\n  Forecasting"
                },
                "summary": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, Transformer-based models (Transformers) have achieved\nsignificant success in multivariate time series forecasting (MTSF). However,\nprevious works focus on extracting features either from the time domain or the\nfrequency domain, which inadequately captures the trends and periodic\ncharacteristics. To address this issue, we propose a wavelet learning framework\nto model complex temporal dependencies of the time series data. The wavelet\ndomain integrates both time and frequency information, allowing for the\nanalysis of local characteristics of signals at different scales. Additionally,\nthe Softmax self-attention mechanism used by Transformers has quadratic\ncomplexity, which leads to excessive computational costs when capturing\nlong-term dependencies. Therefore, we propose a novel attention mechanism:\nRotary Route Attention (RoRA). Unlike Softmax attention, RoRA utilizes rotary\nposition embeddings to inject relative positional information to sequence\ntokens and introduces a small number of routing tokens $r$ to aggregate\ninformation from the $KV$ matrices and redistribute it to the $Q$ matrix,\noffering linear complexity. We further propose WaveRoRA, which leverages RoRA\nto capture inter-series dependencies in the wavelet domain. We conduct\nextensive experiments on eight real-world datasets. The results indicate that\nWaveRoRA outperforms existing state-of-the-art models while maintaining lower\ncomputational costs. Our code is available at\nhttps://github.com/Leopold2333/WaveRoRA."
                },
                "authors": [
                    {
                        "name": "Aobo Liang"
                    },
                    {
                        "name": "Yan Sun"
                    },
                    {
                        "name": "Nadra Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Nadra Guizani"
                },
                "author": "Nadra Guizani",
                "arxiv_comment": "Model architecture changed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.22649v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.22649v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13786v1",
                "updated": "2024-11-21T02:15:52Z",
                "updated_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "published": "2024-11-21T02:15:52Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    2,
                    15,
                    52,
                    3,
                    326,
                    0
                ],
                "title": "Adaptable Embeddings Network (AEN)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptable Embeddings Network (AEN)"
                },
                "summary": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern day Language Models see extensive use in text classification, yet this\ncomes at significant computational cost. Compute-effective classification\nmodels are needed for low-resource environments, most notably on edge devices.\nWe introduce Adaptable Embeddings Networks (AEN), a novel dual-encoder\narchitecture using Kernel Density Estimation (KDE). This architecture allows\nfor runtime adaptation of classification criteria without retraining and is\nnon-autoregressive. Through thorough synthetic data experimentation, we\ndemonstrate our model outputs comparable and in certain cases superior results\nto that of autoregressive models an order of magnitude larger than AEN's size.\nThe architecture's ability to preprocess and cache condition embeddings makes\nit ideal for edge computing applications and real-time monitoring systems."
                },
                "authors": [
                    {
                        "name": "Stan Loosmore"
                    },
                    {
                        "name": "Alexander Titus"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Titus"
                },
                "author": "Alexander Titus",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13676v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13676v1",
                "updated": "2024-11-20T19:51:25Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:51:25Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    51,
                    25,
                    2,
                    325,
                    0
                ],
                "title": "Hymba: A Hybrid-head Architecture for Small Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hymba: A Hybrid-head Architecture for Small Language Models"
                },
                "summary": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Hymba, a family of small language models featuring a hybrid-head\nparallel architecture that integrates transformer attention mechanisms with\nstate space models (SSMs) for enhanced efficiency. Attention heads provide\nhigh-resolution recall, while SSM heads enable efficient context summarization.\nAdditionally, we introduce learnable meta tokens that are prepended to prompts,\nstoring critical information and alleviating the \"forced-to-attend\" burden\nassociated with attention mechanisms. This model is further optimized by\nincorporating cross-layer key-value (KV) sharing and partial sliding window\nattention, resulting in a compact cache size. During development, we conducted\na controlled study comparing various architectures under identical settings and\nobserved significant advantages of our proposed architecture. Notably, Hymba\nachieves state-of-the-art results for small LMs: Our Hymba-1.5B-Base model\nsurpasses all sub-2B public models in performance and even outperforms\nLlama-3.2-3B with 1.32% higher average accuracy, an 11.67x cache size\nreduction, and 3.49x throughput."
                },
                "authors": [
                    {
                        "name": "Xin Dong"
                    },
                    {
                        "name": "Yonggan Fu"
                    },
                    {
                        "name": "Shizhe Diao"
                    },
                    {
                        "name": "Wonmin Byeon"
                    },
                    {
                        "name": "Zijia Chen"
                    },
                    {
                        "name": "Ameya Sunil Mahabaleshwarkar"
                    },
                    {
                        "name": "Shih-Yang Liu"
                    },
                    {
                        "name": "Matthijs Van Keirsbilck"
                    },
                    {
                        "name": "Min-Hung Chen"
                    },
                    {
                        "name": "Yoshi Suhara"
                    },
                    {
                        "name": "Yingyan Lin"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    }
                ],
                "author_detail": {
                    "name": "Pavlo Molchanov"
                },
                "author": "Pavlo Molchanov",
                "arxiv_comment": "20 pages, models are available on huggingface",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13676v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13676v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17720v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17720v1",
                "updated": "2024-11-20T19:44:26Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T19:44:26Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    19,
                    44,
                    26,
                    2,
                    325,
                    0
                ],
                "title": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MAS-Attention: Memory-Aware Stream Processing for Attention Acceleration\n  on Resource-Constrained Edge Devices"
                },
                "summary": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models have revolutionized various fields, enabling\nunprecedented task accuracy and flexibility in computational linguistics,\ncomputer vision and other domains. Attention mechanism has become an essential\ncomponent of foundation models, due to their superb capability of capturing\ncorrelations in a sequence. However, attention results in quadratic complexity\nin memory and compute as the context length grows. Although many fusion-based\nexact attention acceleration algorithms have been developed for\ndatacenter-grade GPUs and accelerators leveraging multi-core parallelism and\ndata locality, yet it remains a significant challenge to accelerate attention\non resource-constrained edge neural accelerators with limited compute units and\nstringent on-chip caches. In this paper, we propose a scheme for exact\nattention inference acceleration on memory-constrained edge accelerators, by\nparallelizing the utilization of heterogeneous compute units, i.e., vector\nprocessing units and matrix processing units. Our method involves scheduling\nworkloads onto these different compute units in a multi-tiered tiling scheme to\nprocess tiled vector workloads and matrix workloads in attention as two\nstreams, respecting the workload dependencies. We search for tiling factors to\nmaximize the parallelization of both compute units while considering I/O\noverhead, and propose a proactive cache overwrite strategy to avoid undesirable\ncache spills in reality. Extensive results based on open-sourced simulation\nframeworks show up to 2.75x speedup and 54% reduction in energy consumption as\ncompared to the state-of-the-art attention fusion method (FLAT) in the edge\ncomputing scenario. Further experiments on a real-world edge neural processing\nunit demonstrate speedup of up to 1.76x for attention as compared to FLAT,\nwithout affecting model output accuracy."
                },
                "authors": [
                    {
                        "name": "Mohammadali Shakerdargah"
                    },
                    {
                        "name": "Shan Lu"
                    },
                    {
                        "name": "Chao Gao"
                    },
                    {
                        "name": "Di Niu"
                    }
                ],
                "author_detail": {
                    "name": "Di Niu"
                },
                "author": "Di Niu",
                "arxiv_comment": "10 pages, 6 figures, under review for MLSys 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17720v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17720v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4; I.2.7; I.5.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13532v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13532v1",
                "updated": "2024-11-20T18:31:39Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T18:31:39Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    18,
                    31,
                    39,
                    2,
                    325,
                    0
                ],
                "title": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Distributed-memory Tridiagonal Solver Based on a Specialised Data\n  Structure Optimised for CPU and GPU Architectures"
                },
                "summary": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Various numerical methods used for solving partial differential equations\n(PDE) result in tridiagonal systems. Solving tridiagonal systems on\ndistributed-memory environments is not straightforward, and often requires\nsignificant amount of communication. In this article, we present a novel\ndistributed-memory tridiagonal solver algorithm, DistD2-TDS, based on a\nspecialised data structure. DistD2-TDS algorithm takes advantage of the\ndiagonal dominance in tridiagonal systems to reduce the communications in\ndistributed-memory environments. The underlying data structure plays a crucial\nrole for the performance of the algorithm. First, the data structure improves\ndata localities and makes it possible to minimise data movements via cache\nblocking and kernel fusion strategies. Second, data continuity enables a\ncontiguous data access pattern and results in efficient utilisation of the\navailable memory bandwidth. Finally, the data layout supports vectorisation on\nCPUs and thread level parallelisation on GPUs for improved performance. In\norder to demonstrate the robustness of the algorithm, we implemented and\nbenchmarked the algorithm on CPUs and GPUs. We investigated the single rank\nperformance and compared against existing algorithms. Furthermore, we analysed\nthe strong scaling of the implementation up to 384 NVIDIA H100 GPUs and up to\n8192 AMD EPYC 7742 CPUs. Finally, we demonstrated a practical use case of the\nalgorithm by using compact finite difference schemes to solve a 3D non-linear\nPDE. The results demonstrate that DistD2 algorithm can sustain around 66% of\nthe theoretical peak bandwidth at scale on CPU and GPU based supercomputers."
                },
                "authors": [
                    {
                        "name": "Semih Akkurt"
                    },
                    {
                        "name": "SÃ©bastien Lemaire"
                    },
                    {
                        "name": "Paul Bartholomew"
                    },
                    {
                        "name": "Sylvain Laizet"
                    }
                ],
                "author_detail": {
                    "name": "Sylvain Laizet"
                },
                "author": "Sylvain Laizet",
                "arxiv_comment": "42 pages, 13 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13532v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13532v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v1",
                "updated": "2024-11-20T14:52:36Z",
                "updated_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Plasma\nPhysics, was based on a source of positive hydrogen ions, accelerated to 50 keV\nand for an equivalent neutral beam current of about 5 A at the source. The beam\ncould be modulated and the maximum overall duration was 50 ms. With the upgrade\nof RFX-mod to the present RFX-mod2 machine, the DNBI is being renovated to\nsolve several plant faults and improve the overall reliability of the system.\nThe 50 kV power supply is being improved, as well as the power supplies in the\nhigh voltage deck and its insulation transformer. The control system,\noriginally based on CAMAC technology, was redesigned to be fully replaced. This\ncontribution reviews the technical criticalities emerged in the DNBI check-up\nand the new solutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_comment": "6 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is a\n  preprint for the \"Fusion Engineering and Design\" journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.08646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08646v1",
                "updated": "2024-12-11T18:59:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    54,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:59:54Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    54,
                    2,
                    346,
                    0
                ],
                "title": "StreamChat: Chatting with Streaming Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamChat: Chatting with Streaming Video"
                },
                "summary": "This paper presents StreamChat, a novel approach that enhances the\ninteraction capabilities of Large Multimodal Models (LMMs) with streaming video\ncontent. In streaming interaction scenarios, existing methods rely solely on\nvisual information available at the moment a question is posed, resulting in\nsignificant delays as the model remains unaware of subsequent changes in the\nstreaming video. StreamChat addresses this limitation by innovatively updating\nthe visual context at each decoding step, ensuring that the model utilizes\nup-to-date video content throughout the decoding process. Additionally, we\nintroduce a flexible and efficient crossattention-based architecture to process\ndynamic streaming inputs while maintaining inference efficiency for streaming\ninteractions. Furthermore, we construct a new dense instruction dataset to\nfacilitate the training of streaming interaction models, complemented by a\nparallel 3D-RoPE mechanism that encodes the relative temporal information of\nvisual and text tokens. Experimental results demonstrate that StreamChat\nachieves competitive performance on established image and video benchmarks and\nexhibits superior capabilities in streaming interaction scenarios compared to\nstate-of-the-art video LMM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents StreamChat, a novel approach that enhances the\ninteraction capabilities of Large Multimodal Models (LMMs) with streaming video\ncontent. In streaming interaction scenarios, existing methods rely solely on\nvisual information available at the moment a question is posed, resulting in\nsignificant delays as the model remains unaware of subsequent changes in the\nstreaming video. StreamChat addresses this limitation by innovatively updating\nthe visual context at each decoding step, ensuring that the model utilizes\nup-to-date video content throughout the decoding process. Additionally, we\nintroduce a flexible and efficient crossattention-based architecture to process\ndynamic streaming inputs while maintaining inference efficiency for streaming\ninteractions. Furthermore, we construct a new dense instruction dataset to\nfacilitate the training of streaming interaction models, complemented by a\nparallel 3D-RoPE mechanism that encodes the relative temporal information of\nvisual and text tokens. Experimental results demonstrate that StreamChat\nachieves competitive performance on established image and video benchmarks and\nexhibits superior capabilities in streaming interaction scenarios compared to\nstate-of-the-art video LMM."
                },
                "authors": [
                    {
                        "name": "Jihao Liu"
                    },
                    {
                        "name": "Zhiding Yu"
                    },
                    {
                        "name": "Shiyi Lan"
                    },
                    {
                        "name": "Shihao Wang"
                    },
                    {
                        "name": "Rongyao Fang"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Hongsheng Li"
                    },
                    {
                        "name": "Jose M. Alvare"
                    }
                ],
                "author_detail": {
                    "name": "Jose M. Alvare"
                },
                "author": "Jose M. Alvare",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08642v1",
                "updated": "2024-12-11T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    50,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    50,
                    2,
                    346,
                    0
                ],
                "title": "Generative Semantic Communication: Architectures, Technologies, and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Semantic Communication: Architectures, Technologies, and\n  Applications"
                },
                "summary": "This paper delves into the applications of generative artificial intelligence\n(GAI) in semantic communication (SemCom) and presents a thorough study. Three\npopular SemCom systems enabled by classical GAI models are first introduced,\nincluding variational autoencoders, generative adversarial networks, and\ndiffusion models. For each system, the fundamental concept of the GAI model,\nthe corresponding SemCom architecture, and the associated literature review of\nrecent efforts are elucidated. Then, a novel generative SemCom system is\nproposed by incorporating the cutting-edge GAI technology-large language models\n(LLMs). This system features two LLM-based AI agents at both the transmitter\nand receiver, serving as \"brains\" to enable powerful information understanding\nand content regeneration capabilities, respectively. This innovative design\nallows the receiver to directly generate the desired content, instead of\nrecovering the bit stream, based on the coded semantic information conveyed by\nthe transmitter. Therefore, it shifts the communication mindset from\n\"information recovery\" to \"information regeneration\" and thus ushers in a new\nera of generative SemCom. A case study on point-to-point video retrieval is\npresented to demonstrate the superiority of the proposed generative SemCom\nsystem, showcasing a 99.98% reduction in communication overhead and a 53%\nimprovement in retrieval accuracy compared to the traditional communication\nsystem. Furthermore, four typical application scenarios for generative SemCom\nare delineated, followed by a discussion of three open issues warranting future\ninvestigation. In a nutshell, this paper provides a holistic set of guidelines\nfor applying GAI in SemCom, paving the way for the efficient implementation of\ngenerative SemCom in future wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper delves into the applications of generative artificial intelligence\n(GAI) in semantic communication (SemCom) and presents a thorough study. Three\npopular SemCom systems enabled by classical GAI models are first introduced,\nincluding variational autoencoders, generative adversarial networks, and\ndiffusion models. For each system, the fundamental concept of the GAI model,\nthe corresponding SemCom architecture, and the associated literature review of\nrecent efforts are elucidated. Then, a novel generative SemCom system is\nproposed by incorporating the cutting-edge GAI technology-large language models\n(LLMs). This system features two LLM-based AI agents at both the transmitter\nand receiver, serving as \"brains\" to enable powerful information understanding\nand content regeneration capabilities, respectively. This innovative design\nallows the receiver to directly generate the desired content, instead of\nrecovering the bit stream, based on the coded semantic information conveyed by\nthe transmitter. Therefore, it shifts the communication mindset from\n\"information recovery\" to \"information regeneration\" and thus ushers in a new\nera of generative SemCom. A case study on point-to-point video retrieval is\npresented to demonstrate the superiority of the proposed generative SemCom\nsystem, showcasing a 99.98% reduction in communication overhead and a 53%\nimprovement in retrieval accuracy compared to the traditional communication\nsystem. Furthermore, four typical application scenarios for generative SemCom\nare delineated, followed by a discussion of three open issues warranting future\ninvestigation. In a nutshell, this paper provides a holistic set of guidelines\nfor applying GAI in SemCom, paving the way for the efficient implementation of\ngenerative SemCom in future wireless networks."
                },
                "authors": [
                    {
                        "name": "Jinke Ren"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Weiwen Yuan"
                    },
                    {
                        "name": "Chongjie Wang"
                    },
                    {
                        "name": "Xianda Wang"
                    },
                    {
                        "name": "Yingbin Zhou"
                    },
                    {
                        "name": "Ziwei Zhu"
                    },
                    {
                        "name": "Fangxin Wang"
                    },
                    {
                        "name": "Shuguang Cui"
                    }
                ],
                "author_detail": {
                    "name": "Shuguang Cui"
                },
                "author": "Shuguang Cui",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08641v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08641v1",
                "updated": "2024-12-11T18:59:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    17,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:59:17Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    17,
                    2,
                    346,
                    0
                ],
                "title": "3D Mesh Editing using Masked LRMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Mesh Editing using Masked LRMs"
                },
                "summary": "We present a novel approach to mesh shape editing, building on recent\nprogress in 3D reconstruction from multi-view images. We formulate shape\nediting as a conditional reconstruction problem, where the model must\nreconstruct the input shape with the exception of a specified 3D region, in\nwhich the geometry should be generated from the conditional signal. To this\nend, we train a conditional Large Reconstruction Model (LRM) for masked\nreconstruction, using multi-view consistent masks rendered from a randomly\ngenerated 3D occlusion, and using one clean viewpoint as the conditional\nsignal. During inference, we manually define a 3D region to edit and provide an\nedited image from a canonical viewpoint to fill in that region. We demonstrate\nthat, in just a single forward pass, our method not only preserves the input\ngeometry in the unmasked region through reconstruction capabilities on par with\nSoTA, but is also expressive enough to perform a variety of mesh edits from a\nsingle image guidance that past works struggle with, while being 10x faster\nthan the top-performing competing prior work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel approach to mesh shape editing, building on recent\nprogress in 3D reconstruction from multi-view images. We formulate shape\nediting as a conditional reconstruction problem, where the model must\nreconstruct the input shape with the exception of a specified 3D region, in\nwhich the geometry should be generated from the conditional signal. To this\nend, we train a conditional Large Reconstruction Model (LRM) for masked\nreconstruction, using multi-view consistent masks rendered from a randomly\ngenerated 3D occlusion, and using one clean viewpoint as the conditional\nsignal. During inference, we manually define a 3D region to edit and provide an\nedited image from a canonical viewpoint to fill in that region. We demonstrate\nthat, in just a single forward pass, our method not only preserves the input\ngeometry in the unmasked region through reconstruction capabilities on par with\nSoTA, but is also expressive enough to perform a variety of mesh edits from a\nsingle image guidance that past works struggle with, while being 10x faster\nthan the top-performing competing prior work."
                },
                "authors": [
                    {
                        "name": "Will Gao"
                    },
                    {
                        "name": "Dilin Wang"
                    },
                    {
                        "name": "Yuchen Fan"
                    },
                    {
                        "name": "Aljaz Bozic"
                    },
                    {
                        "name": "Tuur Stuyck"
                    },
                    {
                        "name": "Zhengqin Li"
                    },
                    {
                        "name": "Zhao Dong"
                    },
                    {
                        "name": "Rakesh Ranjan"
                    },
                    {
                        "name": "Nikolaos Sarafianos"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Sarafianos"
                },
                "author": "Nikolaos Sarafianos",
                "arxiv_comment": "Project Page: https://chocolatebiscuit.github.io/MaskedLRM/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08641v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08641v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08639v1",
                "updated": "2024-12-11T18:58:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    58,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:58:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    58,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "Fast Prompt Alignment for Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Prompt Alignment for Text-to-Image Generation"
                },
                "summary": "Text-to-image generation has advanced rapidly, yet aligning complex textual\nprompts with generated visuals remains challenging, especially with intricate\nobject relationships and fine-grained details. This paper introduces Fast\nPrompt Alignment (FPA), a prompt optimization framework that leverages a\none-pass approach, enhancing text-to-image alignment efficiency without the\niterative overhead typical of current methods like OPT2I. FPA uses large\nlanguage models (LLMs) for single-iteration prompt paraphrasing, followed by\nfine-tuning or in-context learning with optimized prompts to enable real-time\ninference, reducing computational demands while preserving alignment fidelity.\nExtensive evaluations on the COCO Captions and PartiPrompts datasets\ndemonstrate that FPA achieves competitive text-image alignment scores at a\nfraction of the processing time, as validated through both automated metrics\n(TIFA, VQA) and human evaluation. A human study with expert annotators further\nreveals a strong correlation between human alignment judgments and automated\nscores, underscoring the robustness of FPA's improvements. The proposed method\nshowcases a scalable, efficient alternative to iterative prompt optimization,\nenabling broader applicability in real-time, high-demand settings. The codebase\nis provided to facilitate further research:\nhttps://github.com/tiktok/fast_prompt_alignment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation has advanced rapidly, yet aligning complex textual\nprompts with generated visuals remains challenging, especially with intricate\nobject relationships and fine-grained details. This paper introduces Fast\nPrompt Alignment (FPA), a prompt optimization framework that leverages a\none-pass approach, enhancing text-to-image alignment efficiency without the\niterative overhead typical of current methods like OPT2I. FPA uses large\nlanguage models (LLMs) for single-iteration prompt paraphrasing, followed by\nfine-tuning or in-context learning with optimized prompts to enable real-time\ninference, reducing computational demands while preserving alignment fidelity.\nExtensive evaluations on the COCO Captions and PartiPrompts datasets\ndemonstrate that FPA achieves competitive text-image alignment scores at a\nfraction of the processing time, as validated through both automated metrics\n(TIFA, VQA) and human evaluation. A human study with expert annotators further\nreveals a strong correlation between human alignment judgments and automated\nscores, underscoring the robustness of FPA's improvements. The proposed method\nshowcases a scalable, efficient alternative to iterative prompt optimization,\nenabling broader applicability in real-time, high-demand settings. The codebase\nis provided to facilitate further research:\nhttps://github.com/tiktok/fast_prompt_alignment"
                },
                "authors": [
                    {
                        "name": "Khalil Mrini"
                    },
                    {
                        "name": "Hanlin Lu"
                    },
                    {
                        "name": "Linjie Yang"
                    },
                    {
                        "name": "Weilin Huang"
                    },
                    {
                        "name": "Heng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Wang"
                },
                "author": "Heng Wang",
                "arxiv_comment": "TikTok Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.09111v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.09111v4",
                "updated": "2024-12-11T18:50:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    50,
                    30,
                    2,
                    346,
                    0
                ],
                "published": "2024-11-14T00:59:13Z",
                "published_parsed": [
                    2024,
                    11,
                    14,
                    0,
                    59,
                    13,
                    3,
                    319,
                    0
                ],
                "title": "Reducing Reasoning Costs -- The Path of Optimization for Chain of\n  Thought via Sparse Attention Mechanism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing Reasoning Costs -- The Path of Optimization for Chain of\n  Thought via Sparse Attention Mechanism"
                },
                "summary": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In order to address the chain of thought in the large language model\ninference cost surge, this research proposes to use a sparse attention\nmechanism that only focuses on a few relevant tokens. The researcher\nconstructed a new attention mechanism and used GiantRabbit trained with custom\nGPTs as an experimental tool. The experiment tested and compared the reasoning\ntime, correctness score and chain of thought length of this model and o1\nPreview in solving the linear algebra test questions of MIT OpenCourseWare. The\nresults show that GiantRabbit's reasoning time and chain of thought length are\nsignificantly lower than o1 Preview. It verifies the feasibility of sparse\nattention mechanism for optimizing chain of thought reasoning. Detailed\narchitectural details and experimental process have been uploaded to Github,\nthe link is:https://github.com/brucewang123456789/GeniusTrail.git."
                },
                "authors": [
                    {
                        "name": "Libo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Libo Wang"
                },
                "author": "Libo Wang",
                "arxiv_comment": "The main text is 5 pages, totaling 9 pages; 4 figures, 1 table. It\n  have been submitted to NeurIPS 2024 Workshop MusIML and OpenReview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.09111v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.09111v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08628v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08628v1",
                "updated": "2024-12-11T18:48:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    48,
                    20,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:48:20Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    48,
                    20,
                    2,
                    346,
                    0
                ],
                "title": "EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EOV-Seg: Efficient Open-Vocabulary Panoptic Segmentation"
                },
                "summary": "Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatial-aware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.2 PQ, 31.6 mIoU, and\n12.7 FPS on the ADE20K dataset for panoptic and semantic segmentation tasks and\nthe inference time of EOV-Seg is 4-21 times faster than state-of-the-art\nmethods. Especially, equipped with ResNet-50 backbone, EOV-Seg runs 25 FPS with\nonly 71M parameters on a single RTX 3090 GPU. Code is available at\n\\url{https://github.com/nhw649/EOV-Seg}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-vocabulary panoptic segmentation aims to segment and classify everything\nin diverse scenes across an unbounded vocabulary. Existing methods typically\nemploy two-stage or single-stage framework. The two-stage framework involves\ncropping the image multiple times using masks generated by a mask generator,\nfollowed by feature extraction, while the single-stage framework relies on a\nheavyweight mask decoder to make up for the lack of spatial position\ninformation through self-attention and cross-attention in multiple stacked\nTransformer blocks. Both methods incur substantial computational overhead,\nthereby hindering the efficiency of model inference. To fill the gap in\nefficiency, we propose EOV-Seg, a novel single-stage, shared, efficient, and\nspatial-aware framework designed for open-vocabulary panoptic segmentation.\nSpecifically, EOV-Seg innovates in two aspects. First, a Vocabulary-Aware\nSelection (VAS) module is proposed to improve the semantic comprehension of\nvisual aggregated features and alleviate the feature interaction burden on the\nmask decoder. Second, we introduce a Two-way Dynamic Embedding Experts (TDEE),\nwhich efficiently utilizes the spatial awareness capabilities of ViT-based CLIP\nbackbone. To the best of our knowledge, EOV-Seg is the first open-vocabulary\npanoptic segmentation framework towards efficiency, which runs faster and\nachieves competitive performance compared with state-of-the-art methods.\nSpecifically, with COCO training only, EOV-Seg achieves 24.2 PQ, 31.6 mIoU, and\n12.7 FPS on the ADE20K dataset for panoptic and semantic segmentation tasks and\nthe inference time of EOV-Seg is 4-21 times faster than state-of-the-art\nmethods. Especially, equipped with ResNet-50 backbone, EOV-Seg runs 25 FPS with\nonly 71M parameters on a single RTX 3090 GPU. Code is available at\n\\url{https://github.com/nhw649/EOV-Seg}."
                },
                "authors": [
                    {
                        "name": "Hongwei Niu"
                    },
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Jianghang Lin"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Shengchuan Zhang"
                },
                "author": "Shengchuan Zhang",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08628v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08628v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08623v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08623v1",
                "updated": "2024-12-11T18:44:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    44,
                    2,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:44:02Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    44,
                    2,
                    2,
                    346,
                    0
                ],
                "title": "A 1% accurate method to include baryonic effects in galaxy-galaxy\n  lensing models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A 1% accurate method to include baryonic effects in galaxy-galaxy\n  lensing models"
                },
                "summary": "Galaxy clustering and galaxy-galaxy lensing are two of the main observational\nprobes in Stage-IV large-scale structure surveys. Unfortunately, the\ncomplicated relationship between galaxies and matter limits the exploitation of\nthis data. Galaxy bias models -- such as the hybrid Lagrangian bias expansion\n-- allow describing galaxy clustering down to scales as small as $k =\n0.7h$/Mpc. However, the galaxy-matter cross-power spectra are already affected\nby baryons on these scales, directly impacting the modelling of galaxy-galaxy\nlensing. We propose to extend models of the galaxy-matter cross-power spectrum\n$P_{\\rm gm}(k)$ (currently only accounting for dark matter) by including a\nbaryonic correction inferred from the matter component ($S_{\\rm mm}(k)$), so\nthat $P_{\\rm gm, full \\, physics} (k) = \\sqrt{S_{\\rm mm}} P_{\\rm gm, gravity \\,\nonly}$. We use the FLAMINGO simulations to measure the effect of baryons on the\ngalaxy-matter cross-power spectrum and to assess the performance of our model.\nWe perform a Bayesian analysis of synthetic data, implementing a model based on\nBACCO's hybrid Lagrangian bias expansion (for the nonlinear galaxy bias) and\nBaryon Correction Model. Ignoring baryons in the galaxy-matter cross-power\nspectrum leads to a biased inference of the galaxy bias, while ignoring baryons\nin both the galaxy-matter and matter-matter power spectra leads to a biased\ninference of both the galaxy bias and cosmological parameters. In contrast, our\nmethod is 1% accurate compared to all physics variations in FLAMINGO and on all\nscales described by hybrid perturbative models ($k < 0.7h$/Mpc). Moreover, our\nmodel leads to inferred bias and cosmological parameters compatible within\n1$\\sigma$ with their reference values. We anticipate that our method will be a\npromising candidate for analysing forthcoming Stage-IV survey data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Galaxy clustering and galaxy-galaxy lensing are two of the main observational\nprobes in Stage-IV large-scale structure surveys. Unfortunately, the\ncomplicated relationship between galaxies and matter limits the exploitation of\nthis data. Galaxy bias models -- such as the hybrid Lagrangian bias expansion\n-- allow describing galaxy clustering down to scales as small as $k =\n0.7h$/Mpc. However, the galaxy-matter cross-power spectra are already affected\nby baryons on these scales, directly impacting the modelling of galaxy-galaxy\nlensing. We propose to extend models of the galaxy-matter cross-power spectrum\n$P_{\\rm gm}(k)$ (currently only accounting for dark matter) by including a\nbaryonic correction inferred from the matter component ($S_{\\rm mm}(k)$), so\nthat $P_{\\rm gm, full \\, physics} (k) = \\sqrt{S_{\\rm mm}} P_{\\rm gm, gravity \\,\nonly}$. We use the FLAMINGO simulations to measure the effect of baryons on the\ngalaxy-matter cross-power spectrum and to assess the performance of our model.\nWe perform a Bayesian analysis of synthetic data, implementing a model based on\nBACCO's hybrid Lagrangian bias expansion (for the nonlinear galaxy bias) and\nBaryon Correction Model. Ignoring baryons in the galaxy-matter cross-power\nspectrum leads to a biased inference of the galaxy bias, while ignoring baryons\nin both the galaxy-matter and matter-matter power spectra leads to a biased\ninference of both the galaxy bias and cosmological parameters. In contrast, our\nmethod is 1% accurate compared to all physics variations in FLAMINGO and on all\nscales described by hybrid perturbative models ($k < 0.7h$/Mpc). Moreover, our\nmodel leads to inferred bias and cosmological parameters compatible within\n1$\\sigma$ with their reference values. We anticipate that our method will be a\npromising candidate for analysing forthcoming Stage-IV survey data."
                },
                "authors": [
                    {
                        "name": "Matteo Zennaro"
                    },
                    {
                        "name": "Giovanni AricÃ²"
                    },
                    {
                        "name": "Carlos GarcÃ­a-GarcÃ­a"
                    },
                    {
                        "name": "RaÃºl E. Angulo"
                    },
                    {
                        "name": "Lurdes Ondaro-Mallea"
                    },
                    {
                        "name": "Sergio Contreras"
                    },
                    {
                        "name": "Andrina Nicola"
                    },
                    {
                        "name": "Matthieu Schaller"
                    },
                    {
                        "name": "Joop Schaye"
                    }
                ],
                "author_detail": {
                    "name": "Joop Schaye"
                },
                "author": "Joop Schaye",
                "arxiv_comment": "21 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08623v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08623v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08619v1",
                "updated": "2024-12-11T18:40:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    40,
                    16,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:40:16Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    40,
                    16,
                    2,
                    346,
                    0
                ],
                "title": "Synthetic Vision: Training Vision-Language Models to Understand Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Vision: Training Vision-Language Models to Understand Physics"
                },
                "summary": "Physical reasoning, which involves the interpretation, understanding, and\nprediction of object behavior in dynamic environments, remains a significant\nchallenge for current Vision-Language Models (VLMs). In this work, we propose\ntwo methods to enhance VLMs' physical reasoning capabilities using simulated\ndata. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs\ngenerated from simulations relevant to physical reasoning tasks. Second, we\nintroduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to\ncreate scene descriptions enriched with physical properties and processes.\nDuring physical reasoning tasks, these PCBs can be leveraged as context to\nassist a Large Language Model (LLM) to improve its performance. We evaluate\nboth of our approaches using multiple benchmarks, including a new stability\ndetection QA dataset called Falling Tower, which includes both simulated and\nreal-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM\ncan significantly outperform larger state-of-the-art foundational models. We\nalso show that integrating PCBs boosts the performance of foundational LLMs on\nphysical reasoning tasks. Using the real-world scenes from the Falling Tower\ndataset, we also validate the robustness of both approaches in Sim2Real\ntransfer. Our results highlight the utility that simulated data can have in the\ncreation of learning systems capable of advanced physical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical reasoning, which involves the interpretation, understanding, and\nprediction of object behavior in dynamic environments, remains a significant\nchallenge for current Vision-Language Models (VLMs). In this work, we propose\ntwo methods to enhance VLMs' physical reasoning capabilities using simulated\ndata. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs\ngenerated from simulations relevant to physical reasoning tasks. Second, we\nintroduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to\ncreate scene descriptions enriched with physical properties and processes.\nDuring physical reasoning tasks, these PCBs can be leveraged as context to\nassist a Large Language Model (LLM) to improve its performance. We evaluate\nboth of our approaches using multiple benchmarks, including a new stability\ndetection QA dataset called Falling Tower, which includes both simulated and\nreal-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM\ncan significantly outperform larger state-of-the-art foundational models. We\nalso show that integrating PCBs boosts the performance of foundational LLMs on\nphysical reasoning tasks. Using the real-world scenes from the Falling Tower\ndataset, we also validate the robustness of both approaches in Sim2Real\ntransfer. Our results highlight the utility that simulated data can have in the\ncreation of learning systems capable of advanced physical reasoning."
                },
                "authors": [
                    {
                        "name": "Vahid Balazadeh"
                    },
                    {
                        "name": "Mohammadmehdi Ataei"
                    },
                    {
                        "name": "Hyunmin Cheong"
                    },
                    {
                        "name": "Amir Hosein Khasahmadi"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Rahul G. Krishnan"
                },
                "author": "Rahul G. Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08615v1",
                "updated": "2024-12-11T18:37:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    37,
                    56,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:37:56Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    37,
                    56,
                    2,
                    346,
                    0
                ],
                "title": "Exploiting the Index Gradients for Optimization-Based Jailbreaking on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting the Index Gradients for Optimization-Based Jailbreaking on\n  Large Language Models"
                },
                "summary": "Despite the advancements in training Large Language Models (LLMs) with\nalignment techniques to enhance the safety of generated content, these models\nremain susceptible to jailbreak, an adversarial attack method that exposes\nsecurity vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG)\nmethod has demonstrated the ability to automatically generate adversarial\nsuffixes that jailbreak state-of-the-art LLMs. However, the optimization\nprocess involved in GCG is highly time-consuming, rendering the jailbreaking\npipeline inefficient. In this paper, we investigate the process of GCG and\nidentify an issue of Indirect Effect, the key bottleneck of the GCG\noptimization. To this end, we propose the Model Attack Gradient Index GCG\n(MAGIC), that addresses the Indirect Effect by exploiting the gradient\ninformation of the suffix tokens, thereby accelerating the procedure by having\nless computation and fewer iterations. Our experiments on AdvBench show that\nMAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates\n(ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of\n74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on\nGPT-3.5. Code is available at https://github.com/jiah-li/magic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advancements in training Large Language Models (LLMs) with\nalignment techniques to enhance the safety of generated content, these models\nremain susceptible to jailbreak, an adversarial attack method that exposes\nsecurity vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG)\nmethod has demonstrated the ability to automatically generate adversarial\nsuffixes that jailbreak state-of-the-art LLMs. However, the optimization\nprocess involved in GCG is highly time-consuming, rendering the jailbreaking\npipeline inefficient. In this paper, we investigate the process of GCG and\nidentify an issue of Indirect Effect, the key bottleneck of the GCG\noptimization. To this end, we propose the Model Attack Gradient Index GCG\n(MAGIC), that addresses the Indirect Effect by exploiting the gradient\ninformation of the suffix tokens, thereby accelerating the procedure by having\nless computation and fewer iterations. Our experiments on AdvBench show that\nMAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates\n(ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of\n74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on\nGPT-3.5. Code is available at https://github.com/jiah-li/magic."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Yongchang Hao"
                    },
                    {
                        "name": "Haoyu Xu"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Yu Hong"
                    }
                ],
                "author_detail": {
                    "name": "Yu Hong"
                },
                "author": "Yu Hong",
                "arxiv_comment": "13 pages,2 figures, accepted by The 31st International Conference on\n  Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07012v2",
                "updated": "2024-12-11T18:28:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    28,
                    0,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-09T21:44:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    21,
                    44,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for\n  Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProVision: Programmatically Scaling Vision-centric Instruction Data for\n  Multimodal Language Models"
                },
                "summary": "With the rise of multimodal applications, instruction data has become\ncritical for training multimodal language models capable of understanding\ncomplex image-based queries. Existing practices rely on powerful but costly\nlarge language models (LLMs) or multimodal language models (MLMs) to produce\ninstruction data. These are often prone to hallucinations, licensing issues and\nthe generation process is often hard to scale and interpret. In this work, we\npresent a programmatic approach that employs scene graphs as symbolic\nrepresentations of images and human-written programs to systematically\nsynthesize vision-centric instruction data. Our approach ensures the\ninterpretability and controllability of the data generation process and scales\nefficiently while maintaining factual accuracy. By implementing a suite of 24\nsingle-image, 14 multi-image instruction generators, and a scene graph\ngeneration pipeline, we build a scalable, cost-effective system: ProVision\nwhich produces diverse question-answer pairs concerning objects, attributes,\nrelations, depth, etc., for any given image. Applied to Visual Genome and\nDataComp datasets, we generate over 10 million instruction data points,\nProVision-10M, and leverage them in both pretraining and instruction tuning\nstages of MLMs. When adopted in the instruction tuning stage, our single-image\ninstruction data yields up to a 7% improvement on the 2D split and 8% on the 3D\nsplit of CVBench, along with a 3% increase in performance on QBench2,\nRealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%\nimprovement on Mantis-Eval. Incorporation of our data in both pre-training and\nfine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%\nacross 11 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of multimodal applications, instruction data has become\ncritical for training multimodal language models capable of understanding\ncomplex image-based queries. Existing practices rely on powerful but costly\nlarge language models (LLMs) or multimodal language models (MLMs) to produce\ninstruction data. These are often prone to hallucinations, licensing issues and\nthe generation process is often hard to scale and interpret. In this work, we\npresent a programmatic approach that employs scene graphs as symbolic\nrepresentations of images and human-written programs to systematically\nsynthesize vision-centric instruction data. Our approach ensures the\ninterpretability and controllability of the data generation process and scales\nefficiently while maintaining factual accuracy. By implementing a suite of 24\nsingle-image, 14 multi-image instruction generators, and a scene graph\ngeneration pipeline, we build a scalable, cost-effective system: ProVision\nwhich produces diverse question-answer pairs concerning objects, attributes,\nrelations, depth, etc., for any given image. Applied to Visual Genome and\nDataComp datasets, we generate over 10 million instruction data points,\nProVision-10M, and leverage them in both pretraining and instruction tuning\nstages of MLMs. When adopted in the instruction tuning stage, our single-image\ninstruction data yields up to a 7% improvement on the 2D split and 8% on the 3D\nsplit of CVBench, along with a 3% increase in performance on QBench2,\nRealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%\nimprovement on Mantis-Eval. Incorporation of our data in both pre-training and\nfine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%\nacross 11 benchmarks."
                },
                "authors": [
                    {
                        "name": "Jieyu Zhang"
                    },
                    {
                        "name": "Le Xue"
                    },
                    {
                        "name": "Linxin Song"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weikai Huang"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "An Yan"
                    },
                    {
                        "name": "Zixian Ma"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "silvio savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ran Xu"
                },
                "author": "Ran Xu",
                "arxiv_comment": "code: https://github.com/JieyuZ2/ProVision dataset:\n  https://huggingface.co/datasets/Salesforce/ProVision-10M",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08604v1",
                "updated": "2024-12-11T18:26:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    26,
                    55,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:26:55Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    26,
                    55,
                    2,
                    346,
                    0
                ],
                "title": "Preference Discerning with LLM-Enhanced Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Discerning with LLM-Enhanced Generative Retrieval"
                },
                "summary": "Sequential recommendation systems aim to provide personalized recommendations\nfor users based on their interaction history. To achieve this, they often\nincorporate auxiliary information, such as textual descriptions of items and\nauxiliary tasks, like predicting user preferences and intent. Despite numerous\nefforts to enhance these models, they still suffer from limited\npersonalization. To address this issue, we propose a new paradigm, which we\nterm preference discerning. In preference dscerning, we explicitly condition a\ngenerative sequential recommendation system on user preferences within its\ncontext. To this end, we generate user preferences using Large Language Models\n(LLMs) based on user reviews and item-specific data. To evaluate preference\ndiscerning capabilities of sequential recommendation systems, we introduce a\nnovel benchmark that provides a holistic evaluation across various scenarios,\nincluding preference steering and sentiment following. We assess current\nstate-of-the-art methods using our benchmark and show that they struggle to\naccurately discern user preferences. Therefore, we propose a new method named\nMender ($\\textbf{M}$ultimodal Prefer$\\textbf{en}$ce\n$\\textbf{d}$iscern$\\textbf{er}$), which improves upon existing methods and\nachieves state-of-the-art performance on our benchmark. Our results show that\nMender can be effectively guided by human preferences even though they have not\nbeen observed during training, paving the way toward more personalized\nsequential recommendation systems. We will open-source the code and benchmarks\nupon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems aim to provide personalized recommendations\nfor users based on their interaction history. To achieve this, they often\nincorporate auxiliary information, such as textual descriptions of items and\nauxiliary tasks, like predicting user preferences and intent. Despite numerous\nefforts to enhance these models, they still suffer from limited\npersonalization. To address this issue, we propose a new paradigm, which we\nterm preference discerning. In preference dscerning, we explicitly condition a\ngenerative sequential recommendation system on user preferences within its\ncontext. To this end, we generate user preferences using Large Language Models\n(LLMs) based on user reviews and item-specific data. To evaluate preference\ndiscerning capabilities of sequential recommendation systems, we introduce a\nnovel benchmark that provides a holistic evaluation across various scenarios,\nincluding preference steering and sentiment following. We assess current\nstate-of-the-art methods using our benchmark and show that they struggle to\naccurately discern user preferences. Therefore, we propose a new method named\nMender ($\\textbf{M}$ultimodal Prefer$\\textbf{en}$ce\n$\\textbf{d}$iscern$\\textbf{er}$), which improves upon existing methods and\nachieves state-of-the-art performance on our benchmark. Our results show that\nMender can be effectively guided by human preferences even though they have not\nbeen observed during training, paving the way toward more personalized\nsequential recommendation systems. We will open-source the code and benchmarks\nupon publication."
                },
                "authors": [
                    {
                        "name": "Fabian Paischer"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Linfeng Liu"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Kaveh Hassani"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Ricky Chen"
                    },
                    {
                        "name": "Zhang Gabriel Li"
                    },
                    {
                        "name": "Xialo Gao"
                    },
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Xue Feng"
                    },
                    {
                        "name": "Nima Noorshams"
                    },
                    {
                        "name": "Sem Park"
                    },
                    {
                        "name": "Bo Long"
                    },
                    {
                        "name": "Hamid Eghbalzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Eghbalzadeh"
                },
                "author": "Hamid Eghbalzadeh",
                "arxiv_comment": "11 pages + references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12151v3",
                "updated": "2024-12-11T18:12:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    12,
                    43,
                    2,
                    346,
                    0
                ],
                "published": "2024-03-18T18:08:44Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    18,
                    8,
                    44,
                    0,
                    78,
                    0
                ],
                "title": "Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification"
                },
                "summary": "Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach."
                },
                "authors": [
                    {
                        "name": "Filippos Gouidis"
                    },
                    {
                        "name": "Katerina Papantoniou"
                    },
                    {
                        "name": "Konstantinos Papoutsakis"
                    },
                    {
                        "name": "Theodore Patkos"
                    },
                    {
                        "name": "Antonis Argyros"
                    },
                    {
                        "name": "Dimitris Plexousakis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Plexousakis"
                },
                "author": "Dimitris Plexousakis",
                "arxiv_doi": "10.1609/aaaiss.v3i1.31190",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1609/aaaiss.v3i1.31190",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.12151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the AAAI-MAKE 2024",
                "arxiv_journal_ref": "Proceedings of the AAAI Spring Symposium, 2024, pages 115-124",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08593v1",
                "updated": "2024-12-11T18:11:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    11,
                    39,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:11:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    11,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based\n  Automated Requirement Traceability and Compliance Checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based\n  Automated Requirement Traceability and Compliance Checks"
                },
                "summary": "Ensuring that Software Requirements Specifications (SRS) align with\nhigher-level organizational or national requirements is vital, particularly in\nregulated environments such as finance and aerospace. In these domains,\nmaintaining consistency, adhering to regulatory frameworks, minimizing errors,\nand meeting critical expectations are essential for the reliable functioning of\nsystems. The widespread adoption of large language models (LLMs) highlights\ntheir immense potential, yet there remains considerable scope for improvement\nin retrieving relevant information and enhancing reasoning capabilities. This\nstudy demonstrates that integrating a robust Graph-RAG framework with advanced\nprompt engineering techniques, such as Chain of Thought and Tree of Thought,\ncan significantly enhance performance. Compared to baseline RAG methods and\nsimple prompting strategies, this approach delivers more accurate and\ncontext-aware results. While this method demonstrates significant improvements\nin performance, it comes with challenges. It is both costly and more complex to\nimplement across diverse contexts, requiring careful adaptation to specific\nscenarios. Additionally, its effectiveness heavily relies on having complete\nand accurate input data, which may not always be readily available, posing\nfurther limitations to its scalability and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Software Requirements Specifications (SRS) align with\nhigher-level organizational or national requirements is vital, particularly in\nregulated environments such as finance and aerospace. In these domains,\nmaintaining consistency, adhering to regulatory frameworks, minimizing errors,\nand meeting critical expectations are essential for the reliable functioning of\nsystems. The widespread adoption of large language models (LLMs) highlights\ntheir immense potential, yet there remains considerable scope for improvement\nin retrieving relevant information and enhancing reasoning capabilities. This\nstudy demonstrates that integrating a robust Graph-RAG framework with advanced\nprompt engineering techniques, such as Chain of Thought and Tree of Thought,\ncan significantly enhance performance. Compared to baseline RAG methods and\nsimple prompting strategies, this approach delivers more accurate and\ncontext-aware results. While this method demonstrates significant improvements\nin performance, it comes with challenges. It is both costly and more complex to\nimplement across diverse contexts, requiring careful adaptation to specific\nscenarios. Additionally, its effectiveness heavily relies on having complete\nand accurate input data, which may not always be readily available, posing\nfurther limitations to its scalability and practicality."
                },
                "authors": [
                    {
                        "name": "Arsalan Masoudifard"
                    },
                    {
                        "name": "Mohammad Mowlavi Sorond"
                    },
                    {
                        "name": "Moein Madadi"
                    },
                    {
                        "name": "Mohammad Sabokrou"
                    },
                    {
                        "name": "Elahe Habibi"
                    }
                ],
                "author_detail": {
                    "name": "Elahe Habibi"
                },
                "author": "Elahe Habibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16822v3",
                "updated": "2024-12-11T18:07:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    7,
                    25,
                    2,
                    346,
                    0
                ],
                "published": "2024-02-26T18:47:27Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    18,
                    47,
                    27,
                    0,
                    57,
                    0
                ],
                "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nprompts generated by Rainbow Teaming are highly transferable and that\nfine-tuning models with synthetic data generated by our method significantly\nenhances their safety without sacrificing general performance or helpfulness.\nWe additionally explore the versatility of Rainbow Teaming by applying it to\nquestion answering and cybersecurity, showcasing its potential to drive robust\nopen-ended self-improvement in a wide range of applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nprompts generated by Rainbow Teaming are highly transferable and that\nfine-tuning models with synthetic data generated by our method significantly\nenhances their safety without sacrificing general performance or helpfulness.\nWe additionally explore the versatility of Rainbow Teaming by applying it to\nquestion answering and cybersecurity, showcasing its potential to drive robust\nopen-ended self-improvement in a wide range of applications."
                },
                "authors": [
                    {
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "name": "Sharath Chandra Raparthy"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Eric Hambro"
                    },
                    {
                        "name": "Aram H. Markosyan"
                    },
                    {
                        "name": "Manish Bhatt"
                    },
                    {
                        "name": "Yuning Mao"
                    },
                    {
                        "name": "Minqi Jiang"
                    },
                    {
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Tim RocktÃ¤schel"
                    },
                    {
                        "name": "Roberta Raileanu"
                    }
                ],
                "author_detail": {
                    "name": "Roberta Raileanu"
                },
                "author": "Roberta Raileanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08587v1",
                "updated": "2024-12-11T18:06:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    6,
                    44,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:06:44Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    6,
                    44,
                    2,
                    346,
                    0
                ],
                "title": "Advancing Single- and Multi-task Text Classification through Large\n  Language Model Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Single- and Multi-task Text Classification through Large\n  Language Model Fine-tuning"
                },
                "summary": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance."
                },
                "authors": [
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Qile P. Chen"
                    },
                    {
                        "name": "Yijing Barry Zhang"
                    },
                    {
                        "name": "Gang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Yang"
                },
                "author": "Gang Yang",
                "arxiv_comment": "9 pages, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v1",
                "updated": "2024-12-11T18:03:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08581v1",
                "updated": "2024-12-11T17:57:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    57,
                    23,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:57:23Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    57,
                    23,
                    2,
                    346,
                    0
                ],
                "title": "Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge:\n  Feasibility, Challenges, and Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge:\n  Feasibility, Challenges, and Road Ahead"
                },
                "summary": "Exploratory testing (ET) harnesses tester's knowledge, creativity, and\nexperience to create varying tests that uncover unexpected bugs from the\nend-user's perspective. Although ET has proven effective in system-level\ntesting of interactive systems, the need for manual execution has hindered\nlarge-scale adoption. In this work, we explore the feasibility, challenges and\nroad ahead of automated scenario-based ET (a.k.a soap opera testing). We\nconduct a formative study, identifying key insights for effective manual soap\nopera testing and challenges in automating the process. We then develop a\nmulti-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to\nautomate soap opera testing. The system consists of three multi-modal agents,\nPlanner, Player, and Detector that collaborate to execute tests and identify\npotential bugs. Experimental results demonstrate the potential of automated\nsoap opera testing, but there remains a significant gap compared to manual\nexecution, especially under-explored scenario boundaries and incorrectly\nidentified bugs. Based on the observation, we envision road ahead for the\nfuture of automated soap opera testing, focusing on three key aspects: the\nsynergy of neural and symbolic approaches, human-AI co-learning, and the\nintegration of soap opera testing with broader software engineering practices.\nThese insights aim to guide and inspire the future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploratory testing (ET) harnesses tester's knowledge, creativity, and\nexperience to create varying tests that uncover unexpected bugs from the\nend-user's perspective. Although ET has proven effective in system-level\ntesting of interactive systems, the need for manual execution has hindered\nlarge-scale adoption. In this work, we explore the feasibility, challenges and\nroad ahead of automated scenario-based ET (a.k.a soap opera testing). We\nconduct a formative study, identifying key insights for effective manual soap\nopera testing and challenges in automating the process. We then develop a\nmulti-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to\nautomate soap opera testing. The system consists of three multi-modal agents,\nPlanner, Player, and Detector that collaborate to execute tests and identify\npotential bugs. Experimental results demonstrate the potential of automated\nsoap opera testing, but there remains a significant gap compared to manual\nexecution, especially under-explored scenario boundaries and incorrectly\nidentified bugs. Based on the observation, we envision road ahead for the\nfuture of automated soap opera testing, focusing on three key aspects: the\nsynergy of neural and symbolic approaches, human-AI co-learning, and the\nintegration of soap opera testing with broader software engineering practices.\nThese insights aim to guide and inspire the future research."
                },
                "authors": [
                    {
                        "name": "Yanqi Su"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08577v1",
                "updated": "2024-12-11T17:51:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    51,
                    44,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:51:44Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    51,
                    44,
                    2,
                    346,
                    0
                ],
                "title": "Mel-Refine: A Plug-and-Play Approach to Refine Mel-Spectrogram in Audio\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mel-Refine: A Plug-and-Play Approach to Refine Mel-Spectrogram in Audio\n  Generation"
                },
                "summary": "Text-to-audio (TTA) model is capable of generating diverse audio from textual\nprompts. However, most mainstream TTA models, which predominantly rely on\nMel-spectrograms, still face challenges in producing audio with rich content.\nThe intricate details and texture required in Mel-spectrograms for such audio\noften surpass the models' capacity, leading to outputs that are blurred or lack\ncoherence. In this paper, we begin by investigating the critical role of U-Net\nin Mel-spectrogram generation. Our analysis shows that in U-Net structure,\nhigh-frequency components in skip-connections and the backbone influence\ntexture and detail, while low-frequency components in the backbone are critical\nfor the diffusion denoising process. We further propose ``Mel-Refine'', a\nplug-and-play approach that enhances Mel-spectrogram texture and detail by\nadjusting different component weights during inference. Our method requires no\nadditional training or fine-tuning and is fully compatible with any\ndiffusion-based TTA architecture. Experimental results show that our approach\nboosts performance metrics of the latest TTA model Tango2 by 25\\%,\ndemonstrating its effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-audio (TTA) model is capable of generating diverse audio from textual\nprompts. However, most mainstream TTA models, which predominantly rely on\nMel-spectrograms, still face challenges in producing audio with rich content.\nThe intricate details and texture required in Mel-spectrograms for such audio\noften surpass the models' capacity, leading to outputs that are blurred or lack\ncoherence. In this paper, we begin by investigating the critical role of U-Net\nin Mel-spectrogram generation. Our analysis shows that in U-Net structure,\nhigh-frequency components in skip-connections and the backbone influence\ntexture and detail, while low-frequency components in the backbone are critical\nfor the diffusion denoising process. We further propose ``Mel-Refine'', a\nplug-and-play approach that enhances Mel-spectrogram texture and detail by\nadjusting different component weights during inference. Our method requires no\nadditional training or fine-tuning and is fully compatible with any\ndiffusion-based TTA architecture. Experimental results show that our approach\nboosts performance metrics of the latest TTA model Tango2 by 25\\%,\ndemonstrating its effectiveness."
                },
                "authors": [
                    {
                        "name": "Hongming Guo"
                    },
                    {
                        "name": "Ruibo Fu"
                    },
                    {
                        "name": "Yizhong Geng"
                    },
                    {
                        "name": "Shuai Liu"
                    },
                    {
                        "name": "Shuchen Shi"
                    },
                    {
                        "name": "Tao Wang"
                    },
                    {
                        "name": "Chunyu Qiang"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Ya Li"
                    },
                    {
                        "name": "Zhengqi Wen"
                    },
                    {
                        "name": "Yukun Liu"
                    },
                    {
                        "name": "Xuefei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xuefei Liu"
                },
                "author": "Xuefei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08564v1",
                "updated": "2024-12-11T17:32:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    32,
                    21,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:32:21Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    32,
                    21,
                    2,
                    346,
                    0
                ],
                "title": "Can We Generate Visual Programs Without Prompting LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Generate Visual Programs Without Prompting LLMs?"
                },
                "summary": "Visual programming prompts LLMs (large language mod-els) to generate\nexecutable code for visual tasks like visual question answering (VQA).\nPrompt-based methods are difficult to improve while also being unreliable and\ncostly in both time and money. Our goal is to develop an efficient visual\nprogramming system without 1) using prompt-based LLMs at inference time and 2)\na large set of program and answer annotations. We develop a synthetic data\naugmentation approach and alternative program generation method based on\ndecoupling programs into higher-level skills called templates and the\ncorresponding arguments. Our results show that with data augmentation,\nprompt-free smaller LLMs ($\\approx$ 1B parameters) are competitive with\nstate-of-the art models with the added benefit of much faster inference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual programming prompts LLMs (large language mod-els) to generate\nexecutable code for visual tasks like visual question answering (VQA).\nPrompt-based methods are difficult to improve while also being unreliable and\ncostly in both time and money. Our goal is to develop an efficient visual\nprogramming system without 1) using prompt-based LLMs at inference time and 2)\na large set of program and answer annotations. We develop a synthetic data\naugmentation approach and alternative program generation method based on\ndecoupling programs into higher-level skills called templates and the\ncorresponding arguments. Our results show that with data augmentation,\nprompt-free smaller LLMs ($\\approx$ 1B parameters) are competitive with\nstate-of-the art models with the added benefit of much faster inference"
                },
                "authors": [
                    {
                        "name": "Michal Shlapentokh-Rothman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "name": "Derek Hoiem"
                    }
                ],
                "author_detail": {
                    "name": "Derek Hoiem"
                },
                "author": "Derek Hoiem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01999v2",
                "updated": "2024-12-11T17:31:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    31,
                    19,
                    2,
                    346,
                    0
                ],
                "published": "2024-10-02T20:04:02Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    20,
                    4,
                    2,
                    2,
                    276,
                    0
                ],
                "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding\n  Capabilities of CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding\n  Capabilities of CodeLLMs"
                },
                "summary": "Recent advancements in Code Large Language Models (CodeLLMs) have\npredominantly focused on open-ended code generation tasks, often neglecting the\ncritical aspect of code understanding and comprehension. To bridge this gap, we\npresent CodeMMLU, a comprehensive multiple-choice question-answer benchmark\ndesigned to evaluate the depth of software and code understanding in LLMs.\nCodeMMLU includes over 10,000 questions sourced from diverse domains,\nencompassing tasks such as code analysis, defect detection, and software\nengineering principles across multiple programming languages. Unlike\ntraditional benchmarks, CodeMMLU assesses models's ability to reason about code\nrather than merely generate it, providing deeper insights into their grasp of\ncomplex software concepts and systems. Our extensive evaluation reveals that\neven state-of-the-art models face significant challenges with CodeMMLU,\nhighlighting deficiencies in comprehension beyond code generation. By\nunderscoring the crucial relationship between code understanding and effective\ngeneration, CodeMMLU serves as a vital resource for advancing AI-assisted\nsoftware development, ultimately aiming to create more reliable and capable\ncoding assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Code Large Language Models (CodeLLMs) have\npredominantly focused on open-ended code generation tasks, often neglecting the\ncritical aspect of code understanding and comprehension. To bridge this gap, we\npresent CodeMMLU, a comprehensive multiple-choice question-answer benchmark\ndesigned to evaluate the depth of software and code understanding in LLMs.\nCodeMMLU includes over 10,000 questions sourced from diverse domains,\nencompassing tasks such as code analysis, defect detection, and software\nengineering principles across multiple programming languages. Unlike\ntraditional benchmarks, CodeMMLU assesses models's ability to reason about code\nrather than merely generate it, providing deeper insights into their grasp of\ncomplex software concepts and systems. Our extensive evaluation reveals that\neven state-of-the-art models face significant challenges with CodeMMLU,\nhighlighting deficiencies in comprehension beyond code generation. By\nunderscoring the crucial relationship between code understanding and effective\ngeneration, CodeMMLU serves as a vital resource for advancing AI-assisted\nsoftware development, ultimately aiming to create more reliable and capable\ncoding assistants."
                },
                "authors": [
                    {
                        "name": "Dung Nguyen Manh"
                    },
                    {
                        "name": "Thang Phan Chau"
                    },
                    {
                        "name": "Nam Le Hai"
                    },
                    {
                        "name": "Thong T. Doan"
                    },
                    {
                        "name": "Nam V. Nguyen"
                    },
                    {
                        "name": "Quang Pham"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.04436v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.04436v3",
                "updated": "2024-12-11T17:22:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    22,
                    54,
                    2,
                    346,
                    0
                ],
                "published": "2024-02-06T22:13:51Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    22,
                    13,
                    51,
                    1,
                    37,
                    0
                ],
                "title": "Continuous Multidimensional Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Multidimensional Scaling"
                },
                "summary": "Multidimensional scaling (MDS) is the act of embedding proximity information\nabout a set of $n$ objects in $d$-dimensional Euclidean space. As originally\nconceived by the psychometric community, MDS was concerned with embedding a\nfixed set of proximities associated with a fixed set of objects. Modern\nconcerns, e.g., that arise in developing asymptotic theories for statistical\ninference on random graphs, more typically involve studying the limiting\nbehavior of a sequence of proximities associated with an increasing set of\nobjects. Here we are concerned with embedding dissimilarities by minimizing\nKruskal's (1964) raw stress criterion. Standard results from the theory of\npoint-to-set maps can be used to establish that, if $n$ is fixed and a sequence\nof dissimilarity matrices converges, then the limit of their embedded\nstructures is the embedded structure of the limiting dissimilarity matrix. But\nwhat if $n$ increases? It then becomes necessary to reformulate MDS so that the\nentire sequence of embedding problems can be viewed as a sequence of\noptimization problems in a fixed space. We present such a reformulation, {\\em\ncontinuous MDS}. Within the continuous MDS framework, we derive two $L^p$\nconsistency results, one for embedding without constraints on the\nconfiguration, the other for embedding subject to {\\em approximate Lipschitz\nconstraints}\\/ that encourage smoothness of the embedding function. The latter\napproach, {\\em Approximate Lipschitz Embedding}\\/ (ALE) is new. Finally, we\ndemonstrate that embedded structures produced by ALE can be interpolated in a\nway that results in uniform convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multidimensional scaling (MDS) is the act of embedding proximity information\nabout a set of $n$ objects in $d$-dimensional Euclidean space. As originally\nconceived by the psychometric community, MDS was concerned with embedding a\nfixed set of proximities associated with a fixed set of objects. Modern\nconcerns, e.g., that arise in developing asymptotic theories for statistical\ninference on random graphs, more typically involve studying the limiting\nbehavior of a sequence of proximities associated with an increasing set of\nobjects. Here we are concerned with embedding dissimilarities by minimizing\nKruskal's (1964) raw stress criterion. Standard results from the theory of\npoint-to-set maps can be used to establish that, if $n$ is fixed and a sequence\nof dissimilarity matrices converges, then the limit of their embedded\nstructures is the embedded structure of the limiting dissimilarity matrix. But\nwhat if $n$ increases? It then becomes necessary to reformulate MDS so that the\nentire sequence of embedding problems can be viewed as a sequence of\noptimization problems in a fixed space. We present such a reformulation, {\\em\ncontinuous MDS}. Within the continuous MDS framework, we derive two $L^p$\nconsistency results, one for embedding without constraints on the\nconfiguration, the other for embedding subject to {\\em approximate Lipschitz\nconstraints}\\/ that encourage smoothness of the embedding function. The latter\napproach, {\\em Approximate Lipschitz Embedding}\\/ (ALE) is new. Finally, we\ndemonstrate that embedded structures produced by ALE can be interpolated in a\nway that results in uniform convergence."
                },
                "authors": [
                    {
                        "name": "Michael W. Trosset"
                    },
                    {
                        "name": "Carey E. Priebe"
                    }
                ],
                "author_detail": {
                    "name": "Carey E. Priebe"
                },
                "author": "Carey E. Priebe",
                "arxiv_comment": "25 pages. Modified previous material for greater clarity; added new\n  material about approximate Lipschitz constraints, Approximate Lipschitz\n  Embedding (ALE), and uniform convergence; added material on constrained\n  minimization of raw stress to the appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.04436v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.04436v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08559v1",
                "updated": "2024-12-11T17:22:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    22,
                    7,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:22:07Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    22,
                    7,
                    2,
                    346,
                    0
                ],
                "title": "Underestimated Privacy Risks for Minority Populations in Large Language\n  Model Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underestimated Privacy Risks for Minority Populations in Large Language\n  Model Unlearning"
                },
                "summary": "Large Language Models are trained on extensive datasets that often contain\nsensitive, human-generated information, raising significant concerns about\nprivacy breaches. While certified unlearning approaches offer strong privacy\nguarantees, they rely on restrictive model assumptions that are not applicable\nto LLMs. As a result, various unlearning heuristics have been proposed, with\nthe associated privacy risks assessed only empirically. The standard evaluation\npipelines typically randomly select data for removal from the training set,\napply unlearning techniques, and use membership inference attacks to compare\nthe unlearned models against models retrained without the to-be-unlearned data.\nHowever, since every data point is subject to the right to be forgotten,\nunlearning should be considered in the worst-case scenario from the privacy\nperspective. Prior work shows that data outliers may exhibit higher\nmemorization effects. Intuitively, they are harder to be unlearn and thus the\nprivacy risk of unlearning them is underestimated in the current evaluation. In\nthis paper, we leverage minority data to identify such a critical flaw in\npreviously widely adopted evaluations. We substantiate this claim through\ncarefully designed experiments, including unlearning canaries related to\nminority groups, inspired by privacy auditing literature. Using personally\nidentifiable information as a representative minority identifier, we\ndemonstrate that minority groups experience at least 20% more privacy leakage\nin most cases across six unlearning approaches, three MIAs, three benchmark\ndatasets, and two LLMs of different scales. Given that the right to be\nforgotten should be upheld for every individual, we advocate for a more\nrigorous evaluation of LLM unlearning methods. Our minority-aware evaluation\nframework represents an initial step toward ensuring more equitable assessments\nof LLM unlearning efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are trained on extensive datasets that often contain\nsensitive, human-generated information, raising significant concerns about\nprivacy breaches. While certified unlearning approaches offer strong privacy\nguarantees, they rely on restrictive model assumptions that are not applicable\nto LLMs. As a result, various unlearning heuristics have been proposed, with\nthe associated privacy risks assessed only empirically. The standard evaluation\npipelines typically randomly select data for removal from the training set,\napply unlearning techniques, and use membership inference attacks to compare\nthe unlearned models against models retrained without the to-be-unlearned data.\nHowever, since every data point is subject to the right to be forgotten,\nunlearning should be considered in the worst-case scenario from the privacy\nperspective. Prior work shows that data outliers may exhibit higher\nmemorization effects. Intuitively, they are harder to be unlearn and thus the\nprivacy risk of unlearning them is underestimated in the current evaluation. In\nthis paper, we leverage minority data to identify such a critical flaw in\npreviously widely adopted evaluations. We substantiate this claim through\ncarefully designed experiments, including unlearning canaries related to\nminority groups, inspired by privacy auditing literature. Using personally\nidentifiable information as a representative minority identifier, we\ndemonstrate that minority groups experience at least 20% more privacy leakage\nin most cases across six unlearning approaches, three MIAs, three benchmark\ndatasets, and two LLMs of different scales. Given that the right to be\nforgotten should be upheld for every individual, we advocate for a more\nrigorous evaluation of LLM unlearning methods. Our minority-aware evaluation\nframework represents an initial step toward ensuring more equitable assessments\nof LLM unlearning efficacy."
                },
                "authors": [
                    {
                        "name": "Rongzhe Wei"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Mohsen Ghassemi"
                    },
                    {
                        "name": "Eleonora KreaÄiÄ"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Vamsi K. Potluru"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Eli Chien"
                    }
                ],
                "author_detail": {
                    "name": "Eli Chien"
                },
                "author": "Eli Chien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08555v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08555v1",
                "updated": "2024-12-11T17:17:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    17,
                    2,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:17:02Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    17,
                    2,
                    2,
                    346,
                    0
                ],
                "title": "Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks\n  Defending against Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grimm: A Plug-and-Play Perturbation Rectifier for Graph Neural Networks\n  Defending against Poisoning Attacks"
                },
                "summary": "End-to-end training with global optimization have popularized graph neural\nnetworks (GNNs) for node classification, yet inadvertently introduced\nvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit\nthe inherent opened interfaces of GNNs' input and output, perturbing critical\nedges and thus manipulating the classification results. Current defenses, due\nto their persistent utilization of global-optimization-based end-to-end\ntraining schemes, inherently encapsulate the vulnerabilities of GNNs. This is\nspecifically evidenced in their inability to defend against targeted secondary\nattacks. In this paper, we propose the Graph Agent Network (GAgN) to address\nthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent\nnetwork in which each node is designed as an 1-hop-view agent. Through the\ndecentralized interactions between agents, they can learn to infer global\nperceptions to perform tasks including inferring embeddings, degrees and\nneighbor relationships for given nodes. This empowers nodes to filtering\nadversarial edges while carrying out classification tasks. Furthermore, agents'\nlimited view prevents malicious messages from propagating globally in GAgN,\nthereby resisting global-optimization-based secondary attacks. We prove that\nsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient\nto achieve these functionalities. Experimental results show that GAgN\neffectively implements all its intended capabilities and, compared to\nstate-of-the-art defenses, achieves optimal classification accuracy on the\nperturbed datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end training with global optimization have popularized graph neural\nnetworks (GNNs) for node classification, yet inadvertently introduced\nvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit\nthe inherent opened interfaces of GNNs' input and output, perturbing critical\nedges and thus manipulating the classification results. Current defenses, due\nto their persistent utilization of global-optimization-based end-to-end\ntraining schemes, inherently encapsulate the vulnerabilities of GNNs. This is\nspecifically evidenced in their inability to defend against targeted secondary\nattacks. In this paper, we propose the Graph Agent Network (GAgN) to address\nthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent\nnetwork in which each node is designed as an 1-hop-view agent. Through the\ndecentralized interactions between agents, they can learn to infer global\nperceptions to perform tasks including inferring embeddings, degrees and\nneighbor relationships for given nodes. This empowers nodes to filtering\nadversarial edges while carrying out classification tasks. Furthermore, agents'\nlimited view prevents malicious messages from propagating globally in GAgN,\nthereby resisting global-optimization-based secondary attacks. We prove that\nsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient\nto achieve these functionalities. Experimental results show that GAgN\neffectively implements all its intended capabilities and, compared to\nstate-of-the-art defenses, achieves optimal classification accuracy on the\nperturbed datasets."
                },
                "authors": [
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Wenshan Li"
                    },
                    {
                        "name": "Beibei Li"
                    },
                    {
                        "name": "Wengang Ma"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Pan Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Pan Zhou"
                },
                "author": "Pan Zhou",
                "arxiv_comment": "19 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08555v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08555v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08542v1",
                "updated": "2024-12-11T16:59:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    59,
                    31,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:59:31Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    59,
                    31,
                    2,
                    346,
                    0
                ],
                "title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaestroMotif: Skill Design from Artificial Intelligence Feedback"
                },
                "summary": "Describing skills in natural language has the potential to provide an\naccessible way to inject human knowledge about decision-making into an AI\nsystem. We present MaestroMotif, a method for AI-assisted skill design, which\nyields high-performing and adaptable agents. MaestroMotif leverages the\ncapabilities of Large Language Models (LLMs) to effectively create and reuse\nskills. It first uses an LLM's feedback to automatically design rewards\ncorresponding to each skill, starting from their natural language description.\nThen, it employs an LLM's code generation abilities, together with\nreinforcement learning, for training the skills and combining them to implement\ncomplex behaviors specified in language. We evaluate MaestroMotif using a suite\nof complex tasks in the NetHack Learning Environment (NLE), demonstrating that\nit surpasses existing approaches in both performance and usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Describing skills in natural language has the potential to provide an\naccessible way to inject human knowledge about decision-making into an AI\nsystem. We present MaestroMotif, a method for AI-assisted skill design, which\nyields high-performing and adaptable agents. MaestroMotif leverages the\ncapabilities of Large Language Models (LLMs) to effectively create and reuse\nskills. It first uses an LLM's feedback to automatically design rewards\ncorresponding to each skill, starting from their natural language description.\nThen, it employs an LLM's code generation abilities, together with\nreinforcement learning, for training the skills and combining them to implement\ncomplex behaviors specified in language. We evaluate MaestroMotif using a suite\nof complex tasks in the NetHack Learning Environment (NLE), demonstrating that\nit surpasses existing approaches in both performance and usability."
                },
                "authors": [
                    {
                        "name": "Martin Klissarov"
                    },
                    {
                        "name": "Mikael Henaff"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Shagun Sodhani"
                    },
                    {
                        "name": "Pascal Vincent"
                    },
                    {
                        "name": "Amy Zhang"
                    },
                    {
                        "name": "Pierre-Luc Bacon"
                    },
                    {
                        "name": "Doina Precup"
                    },
                    {
                        "name": "Marlos C. Machado"
                    },
                    {
                        "name": "Pierluca D'Oro"
                    }
                ],
                "author_detail": {
                    "name": "Pierluca D'Oro"
                },
                "author": "Pierluca D'Oro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05467v3",
                "updated": "2024-12-11T16:49:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    49,
                    22,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-06T23:43:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    23,
                    43,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "The BrowserGym Ecosystem for Web Agent Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym Ecosystem for Web Agent Research"
                },
                "summary": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models."
                },
                "authors": [
                    {
                        "name": "Thibault Le Sellier De Chezelles"
                    },
                    {
                        "name": "Maxime Gasse"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Massimo Caccia"
                    },
                    {
                        "name": "LÃ©o Boisvert"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Tom Marty"
                    },
                    {
                        "name": "Rim Assouel"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Lawrence Keunho Jang"
                    },
                    {
                        "name": "Xing Han LÃ¹"
                    },
                    {
                        "name": "Ori Yoran"
                    },
                    {
                        "name": "Dehan Kong"
                    },
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Quentin Cappart"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Lacoste"
                },
                "author": "Alexandre Lacoste",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06644v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06644v2",
                "updated": "2024-12-11T16:49:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    49,
                    4,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-09T16:37:43Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    16,
                    37,
                    43,
                    0,
                    344,
                    0
                ],
                "title": "\"Bursts, Beats, and Beyond\": Uncovering the landscape from accretion to\n  ignition of 4U 1728-34 using AstroSat",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Bursts, Beats, and Beyond\": Uncovering the landscape from accretion to\n  ignition of 4U 1728-34 using AstroSat"
                },
                "summary": "A comprehensive study on persistent and thermonuclear burst emission of 4U\n1728-34, commonly known as 'Slow Burster' is performed using seven archival\nobservations of AstroSat spanning from 2016-2019. The burst-free persistent\nspectra can be well fitted with a blackbody (bbody) and a powerlaw (powerlaw)\ncomponents, with a powerlaw photon index (Gamma) was found to be ~2 indicating\nthe source was in \"high/soft\" bananna state or intermediate state. The time\naveraged power density spectrum reveals the presence of twin kilohertz Quasi\nPeriodic Oscillations (kHz QPOs) with centroid frequencies 619+/-10 Hz and\n965+/-6 Hz with a maximum fractional root mean squared amplitude of\n6.24+/-1.31% at ~16 keV. From the upper kHz QPO, we infer the magnetospheric\ndisk radius to be ~17 km, corresponding to a magnetic field strength of\n0.35-1.27 * 10^7 G. The burst spectral evolution indicates Photospheric Radius\nExpansion (PRE) in five bursts, yeilding a touchdown radius of 3.1-5.47 km.\nThese bursts reached near-Eddington luminosities, through which the distance of\nthe source was calculated to be 5.18-5.21 kpc. Two of the bursts show coherent\noscillations at 362.81-363.93 Hz. The presence of twin kHz QPOs and coherent\nBurst Oscillations allows us to provide two different estimates for the spin\nfrequency of the Neutron Star in the system, for the first time using AstroSat.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A comprehensive study on persistent and thermonuclear burst emission of 4U\n1728-34, commonly known as 'Slow Burster' is performed using seven archival\nobservations of AstroSat spanning from 2016-2019. The burst-free persistent\nspectra can be well fitted with a blackbody (bbody) and a powerlaw (powerlaw)\ncomponents, with a powerlaw photon index (Gamma) was found to be ~2 indicating\nthe source was in \"high/soft\" bananna state or intermediate state. The time\naveraged power density spectrum reveals the presence of twin kilohertz Quasi\nPeriodic Oscillations (kHz QPOs) with centroid frequencies 619+/-10 Hz and\n965+/-6 Hz with a maximum fractional root mean squared amplitude of\n6.24+/-1.31% at ~16 keV. From the upper kHz QPO, we infer the magnetospheric\ndisk radius to be ~17 km, corresponding to a magnetic field strength of\n0.35-1.27 * 10^7 G. The burst spectral evolution indicates Photospheric Radius\nExpansion (PRE) in five bursts, yeilding a touchdown radius of 3.1-5.47 km.\nThese bursts reached near-Eddington luminosities, through which the distance of\nthe source was calculated to be 5.18-5.21 kpc. Two of the bursts show coherent\noscillations at 362.81-363.93 Hz. The presence of twin kHz QPOs and coherent\nBurst Oscillations allows us to provide two different estimates for the spin\nfrequency of the Neutron Star in the system, for the first time using AstroSat."
                },
                "authors": [
                    {
                        "name": "Anirudh Salgundi"
                    },
                    {
                        "name": "Suman Bala"
                    },
                    {
                        "name": "Gayathri Raman"
                    },
                    {
                        "name": "Utkarsh Pathak"
                    },
                    {
                        "name": "Varun Bhalerao"
                    }
                ],
                "author_detail": {
                    "name": "Varun Bhalerao"
                },
                "arxiv_affiliation": "Department of Physics, Indian Institute of Technology Bombay, Powai, Mumbai, India",
                "author": "Varun Bhalerao",
                "arxiv_comment": "18 pages, 13 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06644v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06644v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08533v1",
                "updated": "2024-12-11T16:46:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    46,
                    17,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:46:17Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    46,
                    17,
                    2,
                    346,
                    0
                ],
                "title": "Rate accelerated inference for integrals of multivariate random\n  functions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rate accelerated inference for integrals of multivariate random\n  functions"
                },
                "summary": "The computation of integrals is a fundamental task in the analysis of\nfunctional data, which are typically considered as random elements in a space\nof squared integrable functions. Borrowing ideas from recent advances in the\nMonte Carlo integration literature, we propose effective unbiased estimation\nand inference procedures for integrals of uni- and multivariate random\nfunctions. Several applications to key problems in functional data analysis\n(FDA) involving random design points are studied and illustrated. In the\nabsence of noise, the proposed estimates converge faster than the sample mean\nand the usual algorithms for numerical integration. Moreover, the proposed\nestimator facilitates effective inference by generally providing better\ncoverage with shorter confidence and prediction intervals, in both noisy and\nnoiseless setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The computation of integrals is a fundamental task in the analysis of\nfunctional data, which are typically considered as random elements in a space\nof squared integrable functions. Borrowing ideas from recent advances in the\nMonte Carlo integration literature, we propose effective unbiased estimation\nand inference procedures for integrals of uni- and multivariate random\nfunctions. Several applications to key problems in functional data analysis\n(FDA) involving random design points are studied and illustrated. In the\nabsence of noise, the proposed estimates converge faster than the sample mean\nand the usual algorithms for numerical integration. Moreover, the proposed\nestimator facilitates effective inference by generally providing better\ncoverage with shorter confidence and prediction intervals, in both noisy and\nnoiseless setups."
                },
                "authors": [
                    {
                        "name": "Valentin Patilea"
                    },
                    {
                        "name": "Sunny G. W. Wang"
                    }
                ],
                "author_detail": {
                    "name": "Sunny G. W. Wang"
                },
                "author": "Sunny G. W. Wang",
                "arxiv_comment": "26 pages, Supplementary Material separately available upon request",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62R10, 62G08, 62M99, 62-08",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14654v2",
                "updated": "2024-12-11T16:38:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    38,
                    1,
                    2,
                    346,
                    0
                ],
                "published": "2024-11-22T00:59:25Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    59,
                    25,
                    4,
                    327,
                    0
                ],
                "title": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jinming Xing"
                    },
                    {
                        "name": "Ruilin Xing"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07365v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07365v5",
                "updated": "2024-12-11T16:33:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    33,
                    21,
                    2,
                    346,
                    0
                ],
                "published": "2024-01-14T20:43:43Z",
                "published_parsed": [
                    2024,
                    1,
                    14,
                    20,
                    43,
                    43,
                    6,
                    14,
                    0
                ],
                "title": "Sequential Monte-Carlo testing by betting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential Monte-Carlo testing by betting"
                },
                "summary": "In a Monte-Carlo test, the observed dataset is fixed, and several resampled\nor permuted versions of the dataset are generated in order to test a null\nhypothesis that the original dataset is exchangeable with the\nresampled/permuted ones. Sequential Monte-Carlo tests aim to save computational\nresources by generating these additional datasets sequentially one by one, and\npotentially stopping early. While earlier tests yield valid inference at a\nparticular prespecified stopping rule, our work develops a new anytime-valid\nMonte-Carlo test that can be continuously monitored, yielding a p-value or\ne-value at any stopping time possibly not specified in advance. It generalizes\nthe well-known method by Besag and Clifford, allowing it to stop at any time,\nbut also encompasses new sequential Monte-Carlo tests that tend to stop sooner\nunder the null and alternative without compromising power. The core technical\nadvance is the development of new test martingales (nonnegative martingales\nwith initial value one) for testing exchangeability against a very particular\nalternative. These test martingales are constructed using new and simple\nbetting strategies that smartly bet on whether a generated test statistic is\ngreater or smaller than the observed one. The betting strategies are guided by\nthe derivation of a simple log-optimal betting strategy, have closed form\nexpressions for the wealth process, provable guarantees on resampling risk, and\ndisplay excellent power in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In a Monte-Carlo test, the observed dataset is fixed, and several resampled\nor permuted versions of the dataset are generated in order to test a null\nhypothesis that the original dataset is exchangeable with the\nresampled/permuted ones. Sequential Monte-Carlo tests aim to save computational\nresources by generating these additional datasets sequentially one by one, and\npotentially stopping early. While earlier tests yield valid inference at a\nparticular prespecified stopping rule, our work develops a new anytime-valid\nMonte-Carlo test that can be continuously monitored, yielding a p-value or\ne-value at any stopping time possibly not specified in advance. It generalizes\nthe well-known method by Besag and Clifford, allowing it to stop at any time,\nbut also encompasses new sequential Monte-Carlo tests that tend to stop sooner\nunder the null and alternative without compromising power. The core technical\nadvance is the development of new test martingales (nonnegative martingales\nwith initial value one) for testing exchangeability against a very particular\nalternative. These test martingales are constructed using new and simple\nbetting strategies that smartly bet on whether a generated test statistic is\ngreater or smaller than the observed one. The betting strategies are guided by\nthe derivation of a simple log-optimal betting strategy, have closed form\nexpressions for the wealth process, provable guarantees on resampling risk, and\ndisplay excellent power in practice."
                },
                "authors": [
                    {
                        "name": "Lasse Fischer"
                    },
                    {
                        "name": "Aaditya Ramdas"
                    }
                ],
                "author_detail": {
                    "name": "Aaditya Ramdas"
                },
                "author": "Aaditya Ramdas",
                "arxiv_comment": "39 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.07365v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07365v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08519v1",
                "updated": "2024-12-11T16:32:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    32,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:32:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    32,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "Bridging Relevance and Reasoning: Rationale Distillation in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Relevance and Reasoning: Rationale Distillation in\n  Retrieval-Augmented Generation"
                },
                "summary": "The reranker and generator are two critical components in the\nRetrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking\nrelevant documents and generating responses. However, due to differences in\npre-training data and objectives, there is an inevitable gap between the\ndocuments ranked as relevant by the reranker and those required by the\ngenerator to support answering the query. To address this gap, we propose\nRADIO, a novel and practical preference alignment framework with RAtionale\nDIstillatiOn. Specifically, We first propose a rationale extraction method that\nleverages the reasoning capabilities of Large Language Models (LLMs) to extract\nthe rationales necessary for answering the query. Subsequently, a\nrationale-based alignment process is designed to rerank the documents based on\nthe extracted rationales, and fine-tune the reranker to align the preferences.\nWe conduct extensive experiments on two tasks across three datasets to\ndemonstrate the effectiveness of our approach compared to baseline methods. Our\ncode is released online to ease reproduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reranker and generator are two critical components in the\nRetrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking\nrelevant documents and generating responses. However, due to differences in\npre-training data and objectives, there is an inevitable gap between the\ndocuments ranked as relevant by the reranker and those required by the\ngenerator to support answering the query. To address this gap, we propose\nRADIO, a novel and practical preference alignment framework with RAtionale\nDIstillatiOn. Specifically, We first propose a rationale extraction method that\nleverages the reasoning capabilities of Large Language Models (LLMs) to extract\nthe rationales necessary for answering the query. Subsequently, a\nrationale-based alignment process is designed to rerank the documents based on\nthe extracted rationales, and fine-tune the reranker to align the preferences.\nWe conduct extensive experiments on two tasks across three datasets to\ndemonstrate the effectiveness of our approach compared to baseline methods. Our\ncode is released online to ease reproduction."
                },
                "authors": [
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08516v1",
                "updated": "2024-12-11T16:28:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    28,
                    18,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:28:18Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    28,
                    18,
                    2,
                    346,
                    0
                ],
                "title": "AltFS: Agency-light Feature Selection with Large Language Models in Deep\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AltFS: Agency-light Feature Selection with Large Language Models in Deep\n  Recommender Systems"
                },
                "summary": "Feature selection is crucial in recommender systems for improving model\nefficiency and predictive performance. Traditional methods rely on agency\nmodels, such as decision trees or neural networks, to estimate feature\nimportance. However, this approach is inherently limited, as the agency models\nmay fail to learn effectively in all scenarios due to suboptimal training\nconditions (e.g., feature collinearity, high-dimensional sparsity, and data\ninsufficiency). In this paper, we propose AltFS, an Agency-light Feature\nSelection method for deep recommender systems. AltFS integrates semantic\nreasoning from Large Language Models (LLMs) with task-specific learning from\nagency models. Initially, LLMs will generate a semantic ranking of feature\nimportance, which is then refined by an agency model, combining world knowledge\nwith task-specific insights. Extensive experiments on three public datasets\nfrom real-world recommender platforms demonstrate the effectiveness of AltFS.\nOur code is publicly available for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature selection is crucial in recommender systems for improving model\nefficiency and predictive performance. Traditional methods rely on agency\nmodels, such as decision trees or neural networks, to estimate feature\nimportance. However, this approach is inherently limited, as the agency models\nmay fail to learn effectively in all scenarios due to suboptimal training\nconditions (e.g., feature collinearity, high-dimensional sparsity, and data\ninsufficiency). In this paper, we propose AltFS, an Agency-light Feature\nSelection method for deep recommender systems. AltFS integrates semantic\nreasoning from Large Language Models (LLMs) with task-specific learning from\nagency models. Initially, LLMs will generate a semantic ranking of feature\nimportance, which is then refined by an agency model, combining world knowledge\nwith task-specific insights. Extensive experiments on three public datasets\nfrom real-world recommender platforms demonstrate the effectiveness of AltFS.\nOur code is publicly available for reproducibility."
                },
                "authors": [
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01179v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01179v2",
                "updated": "2024-12-11T16:19:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    19,
                    47,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-02T11:19:54Z",
                "published_parsed": [
                    2024,
                    9,
                    2,
                    11,
                    19,
                    54,
                    0,
                    246,
                    0
                ],
                "title": "Recoverable Compression: A Multimodal Vision Token Recovery Mechanism\n  Guided by Text Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recoverable Compression: A Multimodal Vision Token Recovery Mechanism\n  Guided by Text Information"
                },
                "summary": "With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advancement of large-scale language modeling techniques, large\nmultimodal models combining visual encoders with large language models have\ndemonstrated exceptional performance in various visual tasks. Most of the\ncurrent large-scale multimodal models achieve this by mapping visual features\nobtained from the visual encoder into a large language model and using them as\ninputs alongside text for downstream tasks. Therefore, the number of visual\ntokens directly affects the training and inference speed of the model. There\nhas been significant work on token pruning for visual transformers, but for\nlarge multimodal models, only relying on visual information for token pruning\nor compression may lead to significant loss of important information. On the\nother hand, the textual input in the form of a question may contain valuable\ninformation that can aid in answering the question, providing additional\nknowledge to the model. To address the potential oversimplification and\nexcessive pruning that can occur with most purely visual token pruning methods,\nwe propose a text information-guided dynamic visual token recovery mechanism\nthat does not require training. This mechanism leverages the similarity between\nthe question text and visual tokens to recover visually meaningful tokens with\nimportant text information while merging other less important tokens.\nExperimental results demonstrate that our proposed method achieves comparable\nperformance to the original approach while compressing the visual tokens to an\naverage of 10% of the original quantity. Our source code will be made publicly\navailable following acceptance."
                },
                "authors": [
                    {
                        "name": "Yi Chen"
                    },
                    {
                        "name": "Jian Xu"
                    },
                    {
                        "name": "Xu-Yao Zhang"
                    },
                    {
                        "name": "Wen-Zhuo Liu"
                    },
                    {
                        "name": "Yang-Yang Liu"
                    },
                    {
                        "name": "Cheng-Lin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Cheng-Lin Liu"
                },
                "author": "Cheng-Lin Liu",
                "arxiv_comment": "AAAI2025 Accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01179v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01179v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02365v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02365v3",
                "updated": "2024-12-11T15:46:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    46,
                    18,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-05T10:22:31Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    10,
                    22,
                    31,
                    0,
                    218,
                    0
                ],
                "title": "Nonparametric late-time expansion history reconstruction and\n  implications for the Hubble tension in light of recent DESI and type Ia\n  supernovae data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonparametric late-time expansion history reconstruction and\n  implications for the Hubble tension in light of recent DESI and type Ia\n  supernovae data"
                },
                "summary": "We nonparametrically reconstruct the late-time expansion history in light of\nthe latest Baryon Acoustic Oscillation (BAO) measurements from DESI combined\nwith various Type Ia Supernovae (SNeIa) catalogs, using interpolation through\npiece-wise natural cubic splines, and a reconstruction procedure based on\nGaussian Processes (GPs). Applied to DESI BAO and PantheonPlus SNeIa data, both\nmethods indicate that deviations from a reference $\\Lambda$CDM model in the $z\n\\lesssim 2$ unnormalized expansion rate $E(z)$ are constrained to be $\\lesssim\n10\\%$, but also consistently identify two features in $E(z)$: a bump at $z \\sim\n0.5$, and a depression at $z \\sim 0.9$, which cannot be simultaneously captured\nby a $w_0w_a$CDM fit. These features, which are stable against assumptions\nregarding spatial curvature, interpolation knots, and GP kernel, disappear if\none adopts the older SDSS BAO measurements in place of DESI, and decrease in\nsignificance when replacing the PantheonPlus catalog with the Union3 and DESY5\nones. We infer $c/(r_dH_0)=29.90 \\pm 0.33$, with $r_d$ the sound horizon at\nbaryon drag and $H_0$ the Hubble constant. Breaking the $r_d$-$H_0$ degeneracy\nwith the SH0ES prior on $H_0$, the significance of the tension between our\nnonparametric determination of $r_d=136.20^{+2.20}_{-2.40}\\,{\\text{Mpc}}$ and\nthe \\textit{Planck} $\\Lambda$CDM-based determination is at the $5\\sigma$ level,\nslightly lower than the $6\\sigma$ obtained when adopting the older SDSS dataset\nin place of DESI. This indicates the persistence at very high significance of\nthe ``sound horizon tension'', reinforcing the need for pre-recombination new\nphysics. If substantiated in forthcoming data releases, our results tentatively\npoint to oscillatory/nonmonotonic features in the shape of the expansion rate\nat $z \\lesssim 2$, of potential interest for dark energy model-building.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We nonparametrically reconstruct the late-time expansion history in light of\nthe latest Baryon Acoustic Oscillation (BAO) measurements from DESI combined\nwith various Type Ia Supernovae (SNeIa) catalogs, using interpolation through\npiece-wise natural cubic splines, and a reconstruction procedure based on\nGaussian Processes (GPs). Applied to DESI BAO and PantheonPlus SNeIa data, both\nmethods indicate that deviations from a reference $\\Lambda$CDM model in the $z\n\\lesssim 2$ unnormalized expansion rate $E(z)$ are constrained to be $\\lesssim\n10\\%$, but also consistently identify two features in $E(z)$: a bump at $z \\sim\n0.5$, and a depression at $z \\sim 0.9$, which cannot be simultaneously captured\nby a $w_0w_a$CDM fit. These features, which are stable against assumptions\nregarding spatial curvature, interpolation knots, and GP kernel, disappear if\none adopts the older SDSS BAO measurements in place of DESI, and decrease in\nsignificance when replacing the PantheonPlus catalog with the Union3 and DESY5\nones. We infer $c/(r_dH_0)=29.90 \\pm 0.33$, with $r_d$ the sound horizon at\nbaryon drag and $H_0$ the Hubble constant. Breaking the $r_d$-$H_0$ degeneracy\nwith the SH0ES prior on $H_0$, the significance of the tension between our\nnonparametric determination of $r_d=136.20^{+2.20}_{-2.40}\\,{\\text{Mpc}}$ and\nthe \\textit{Planck} $\\Lambda$CDM-based determination is at the $5\\sigma$ level,\nslightly lower than the $6\\sigma$ obtained when adopting the older SDSS dataset\nin place of DESI. This indicates the persistence at very high significance of\nthe ``sound horizon tension'', reinforcing the need for pre-recombination new\nphysics. If substantiated in forthcoming data releases, our results tentatively\npoint to oscillatory/nonmonotonic features in the shape of the expansion rate\nat $z \\lesssim 2$, of potential interest for dark energy model-building."
                },
                "authors": [
                    {
                        "name": "Jun-Qian Jiang"
                    },
                    {
                        "name": "Davide Pedrotti"
                    },
                    {
                        "name": "Simony Santos da Costa"
                    },
                    {
                        "name": "Sunny Vagnozzi"
                    }
                ],
                "author_detail": {
                    "name": "Sunny Vagnozzi"
                },
                "author": "Sunny Vagnozzi",
                "arxiv_doi": "10.1103/PhysRevD.110.123519",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.110.123519",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2408.02365v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02365v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 8 sub-figures arranged into 5 figures. v2: additional\n  references added, minor clarifications to analysis, slight changes to title,\n  abstract, and figures. v3: slight changes with hyphenation in title,\n  abstract, and main text to reflect published version. Version published in\n  PRD",
                "arxiv_journal_ref": "Phys. Rev. D 110 (2024) 123519",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08478v1",
                "updated": "2024-12-11T15:45:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    45,
                    44,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:45:44Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    45,
                    44,
                    2,
                    346,
                    0
                ],
                "title": "ECSeptional DNS Data: Evaluating Nameserver ECS Deployments with\n  Response-Aware Scanning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECSeptional DNS Data: Evaluating Nameserver ECS Deployments with\n  Response-Aware Scanning"
                },
                "summary": "DNS is one of the cornerstones of the Internet. Nowadays, a substantial\nfraction of DNS queries are handled by public resolvers (e.g., Google Public\nDNS and Cisco's OpenDNS) rather than ISP nameservers. This behavior makes it\ndifficult for authoritative nameservers to provide answers based on the\nrequesting resolver. The impact is especially important for entities that make\nclient origin inferences to perform DNS-based load balancing (e.g., CDNS). The\nEDNS0 Client Subnet (ECS) option adds the client's IP prefix to DNS queries,\nwhich allows authoritative nameservers to provide prefix-based responses. In\nthis study, we introduce a new method for conducting ECS scans, which provides\ninsights into ECS behavior and significantly reduces the required number of\nqueries by up to 97% compared to state-of-the-art techniques. Our approach is\nalso the first to facilitate ECS scans for IPv6. We conduct a comprehensive\nevaluation of the ECS landscape, examining the usage and implementation of ECS\nacross various services. Overall, 53% of all nameservers support prefix-based\nresponses. Furthermore, we find that Google nameservers do not comply with the\nGoogle Public DNS guidelines. Lastly, we plan to make our tool, and data\npublicly available to foster further research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNS is one of the cornerstones of the Internet. Nowadays, a substantial\nfraction of DNS queries are handled by public resolvers (e.g., Google Public\nDNS and Cisco's OpenDNS) rather than ISP nameservers. This behavior makes it\ndifficult for authoritative nameservers to provide answers based on the\nrequesting resolver. The impact is especially important for entities that make\nclient origin inferences to perform DNS-based load balancing (e.g., CDNS). The\nEDNS0 Client Subnet (ECS) option adds the client's IP prefix to DNS queries,\nwhich allows authoritative nameservers to provide prefix-based responses. In\nthis study, we introduce a new method for conducting ECS scans, which provides\ninsights into ECS behavior and significantly reduces the required number of\nqueries by up to 97% compared to state-of-the-art techniques. Our approach is\nalso the first to facilitate ECS scans for IPv6. We conduct a comprehensive\nevaluation of the ECS landscape, examining the usage and implementation of ECS\nacross various services. Overall, 53% of all nameservers support prefix-based\nresponses. Furthermore, we find that Google nameservers do not comply with the\nGoogle Public DNS guidelines. Lastly, we plan to make our tool, and data\npublicly available to foster further research in the area."
                },
                "authors": [
                    {
                        "name": "Patrick Sattler"
                    },
                    {
                        "name": "Johannes Zirngibl"
                    },
                    {
                        "name": "Fahad Hilal"
                    },
                    {
                        "name": "Oliver Gasser"
                    },
                    {
                        "name": "Kevin Vermeulen"
                    },
                    {
                        "name": "Georg Carle"
                    },
                    {
                        "name": "Mattijs Jonker"
                    }
                ],
                "author_detail": {
                    "name": "Mattijs Jonker"
                },
                "author": "Mattijs Jonker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17196v3",
                "updated": "2024-12-11T15:45:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    45,
                    21,
                    2,
                    346,
                    0
                ],
                "published": "2024-10-22T17:15:20Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    15,
                    20,
                    1,
                    296,
                    0
                ],
                "title": "VoiceBench: Benchmarking LLM-Based Voice Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoiceBench: Benchmarking LLM-Based Voice Assistants"
                },
                "summary": "Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Xianghu Yue"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Robby T. Tan"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "Work in progress. Data is available at\n  https://github.com/MatthewCYM/VoiceBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08476v1",
                "updated": "2024-12-11T15:43:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    43,
                    36,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:43:36Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    43,
                    36,
                    2,
                    346,
                    0
                ],
                "title": "Potential Interior Structures and Habitability of Super-Earth Exoplanets\n  LHS 1140 b, K2-18 b, TOI-1452 b and TOI-1468 c",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Potential Interior Structures and Habitability of Super-Earth Exoplanets\n  LHS 1140 b, K2-18 b, TOI-1452 b and TOI-1468 c"
                },
                "summary": "We analyze four super-Earth exoplanets, LHS 1140 b, K2-18 b, TOI-1452 b, and\nTOI-1468 c, which orbit M-dwarf stars in the habitable zone. Their relative\nproximity, within 40 parsecs, makes them prime candidates for follow-up\nobservations and atmospheric and habitability studies. This paper aims to\nassess their internal structure and habitability, considering their tidal\nheating, atmospheric heating, and global transport. We model the interior\nstructure of the planets by applying Bayesian inference to an exoplanet's\ninterior model. A constant quality factor model is used to calculate the range\nof tidal heating, and a one-dimensional analytical model of tidally locked\nplanets is used to assess their surface temperature distribution and\nhabitability. Assuming no or only thin atmospheres, K2-18 b and TOI-1468 c are\nlikely to be water worlds. However, TOI-1452 b and LHS 1140 b may have rocky\nsurfaces. We find that tidal heating is not enough to raise the global mean\nsurface temperature, but greenhouse heating can effectively do so. If the\nconsidered planets have retained thick atmospheres, K2-18 b, TOI-1468 c, and\nTOI-1452 b may, for significant atmospheric heating and heat transport factors,\nbe too hot to sustain liquid water on their surface. However, the lower\ninstellation of LHS 1140 b and the non-zero probability of it having a rocky\nsurface give more space for habitable conditions on the planet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We analyze four super-Earth exoplanets, LHS 1140 b, K2-18 b, TOI-1452 b, and\nTOI-1468 c, which orbit M-dwarf stars in the habitable zone. Their relative\nproximity, within 40 parsecs, makes them prime candidates for follow-up\nobservations and atmospheric and habitability studies. This paper aims to\nassess their internal structure and habitability, considering their tidal\nheating, atmospheric heating, and global transport. We model the interior\nstructure of the planets by applying Bayesian inference to an exoplanet's\ninterior model. A constant quality factor model is used to calculate the range\nof tidal heating, and a one-dimensional analytical model of tidally locked\nplanets is used to assess their surface temperature distribution and\nhabitability. Assuming no or only thin atmospheres, K2-18 b and TOI-1468 c are\nlikely to be water worlds. However, TOI-1452 b and LHS 1140 b may have rocky\nsurfaces. We find that tidal heating is not enough to raise the global mean\nsurface temperature, but greenhouse heating can effectively do so. If the\nconsidered planets have retained thick atmospheres, K2-18 b, TOI-1468 c, and\nTOI-1452 b may, for significant atmospheric heating and heat transport factors,\nbe too hot to sustain liquid water on their surface. However, the lower\ninstellation of LHS 1140 b and the non-zero probability of it having a rocky\nsurface give more space for habitable conditions on the planet."
                },
                "authors": [
                    {
                        "name": "Mangesh Daspute"
                    },
                    {
                        "name": "Amri Wandel"
                    },
                    {
                        "name": "Ravi Kumar Kopparapu"
                    },
                    {
                        "name": "Volker Perdelwitz"
                    },
                    {
                        "name": "Jerusalem Tamirat Teklu"
                    },
                    {
                        "name": "Lev Tal-Or"
                    }
                ],
                "author_detail": {
                    "name": "Lev Tal-Or"
                },
                "author": "Lev Tal-Or",
                "arxiv_comment": "Accepted to ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08475v1",
                "updated": "2024-12-11T15:42:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    42,
                    35,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:42:35Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    42,
                    35,
                    2,
                    346,
                    0
                ],
                "title": "Rethinking Mean Square Error: Why Information is a Superior Assessment\n  of Estimators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Mean Square Error: Why Information is a Superior Assessment\n  of Estimators"
                },
                "summary": "James-Stein (JS) estimators have been described as showing the inadequacy of\nmaximum likelihood estimation when assessed using mean square error (MSE). We\nclaim the problem is not with maximum likelihood (ML) but with MSE. When MSE is\nreplaced with a measure $\\Lambda$ of the information utilized by a statistic,\nlikelihood based methods are superior. The information measure $\\Lambda$\ndescribes not just point estimators but extends to Fisher's view of estimation\nso that we not only reconsider how estimators are assessed but also how we\ndefine an estimator. Fisher information and his views on the role of\nparameters, interpretation of probability, and logic of statistical inference\nfit well with $\\Lambda$ as measure of information.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "James-Stein (JS) estimators have been described as showing the inadequacy of\nmaximum likelihood estimation when assessed using mean square error (MSE). We\nclaim the problem is not with maximum likelihood (ML) but with MSE. When MSE is\nreplaced with a measure $\\Lambda$ of the information utilized by a statistic,\nlikelihood based methods are superior. The information measure $\\Lambda$\ndescribes not just point estimators but extends to Fisher's view of estimation\nso that we not only reconsider how estimators are assessed but also how we\ndefine an estimator. Fisher information and his views on the role of\nparameters, interpretation of probability, and logic of statistical inference\nfit well with $\\Lambda$ as measure of information."
                },
                "authors": [
                    {
                        "name": "Paul Vos"
                    }
                ],
                "author_detail": {
                    "name": "Paul Vos"
                },
                "author": "Paul Vos",
                "arxiv_comment": "14 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08468v1",
                "updated": "2024-12-11T15:33:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    33,
                    35,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:33:35Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    33,
                    35,
                    2,
                    346,
                    0
                ],
                "title": "Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp\n  Generation"
                },
                "summary": "Multi-hand semantic grasp generation aims to generate feasible and\nsemantically appropriate grasp poses for different robotic hands based on\nnatural language instructions. Although the task is highly valuable, due to the\nlack of multi-hand grasp datasets with fine-grained contact description between\nrobotic hands and objects, it is still a long-standing difficult task. In this\npaper, we present Multi-GraspSet, the first large-scale multi-hand grasp\ndataset with automatically contact annotations. Based on Multi-GraspSet, we\npropose Multi-GraspLLM, a unified language-guided grasp generation framework.\nIt leverages large language models (LLM) to handle variable-length sequences,\ngenerating grasp poses for diverse robotic hands in a single unified\narchitecture. Multi-GraspLLM first aligns the encoded point cloud features and\ntext features into a unified semantic space. It then generates grasp bin tokens\nwhich are subsequently converted into grasp pose for each robotic hand via\nhand-aware linear mapping. The experimental results demonstrate that our\napproach significantly outperforms existing methods on Multi-GraspSet. More\ninformation can be found on our project page https://multi-graspllm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hand semantic grasp generation aims to generate feasible and\nsemantically appropriate grasp poses for different robotic hands based on\nnatural language instructions. Although the task is highly valuable, due to the\nlack of multi-hand grasp datasets with fine-grained contact description between\nrobotic hands and objects, it is still a long-standing difficult task. In this\npaper, we present Multi-GraspSet, the first large-scale multi-hand grasp\ndataset with automatically contact annotations. Based on Multi-GraspSet, we\npropose Multi-GraspLLM, a unified language-guided grasp generation framework.\nIt leverages large language models (LLM) to handle variable-length sequences,\ngenerating grasp poses for diverse robotic hands in a single unified\narchitecture. Multi-GraspLLM first aligns the encoded point cloud features and\ntext features into a unified semantic space. It then generates grasp bin tokens\nwhich are subsequently converted into grasp pose for each robotic hand via\nhand-aware linear mapping. The experimental results demonstrate that our\napproach significantly outperforms existing methods on Multi-GraspSet. More\ninformation can be found on our project page https://multi-graspllm.github.io."
                },
                "authors": [
                    {
                        "name": "Haosheng Li"
                    },
                    {
                        "name": "Weixin Mao"
                    },
                    {
                        "name": "Weipeng Deng"
                    },
                    {
                        "name": "Chenyu Meng"
                    },
                    {
                        "name": "Haoqiang Fan"
                    },
                    {
                        "name": "Tiancai Wang"
                    },
                    {
                        "name": "Ping Tan"
                    },
                    {
                        "name": "Hongan Wang"
                    },
                    {
                        "name": "Xiaoming Deng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Deng"
                },
                "author": "Xiaoming Deng",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2307.03639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2307.03639v3",
                "updated": "2024-12-11T15:33:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    33,
                    25,
                    2,
                    346,
                    0
                ],
                "published": "2023-07-07T14:58:20Z",
                "published_parsed": [
                    2023,
                    7,
                    7,
                    14,
                    58,
                    20,
                    4,
                    188,
                    0
                ],
                "title": "Fast and Optimal Inference for Change Points in Piecewise Polynomials\n  via Differencing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and Optimal Inference for Change Points in Piecewise Polynomials\n  via Differencing"
                },
                "summary": "We consider the problem of uncertainty quantification in change point\nregressions, where the signal can be piecewise polynomial of arbitrary but\nfixed degree. That is we seek disjoint intervals which, uniformly at a given\nconfidence level, must each contain a change point location. We propose a\nprocedure based on performing local tests at a number of scales and locations\non a sparse grid, which adapts to the choice of grid in the sense that by\nchoosing a sparser grid one explicitly pays a lower price for multiple testing.\nThe procedure is fast as its computational complexity is always of the order\n$\\mathcal{O} (n \\log (n))$ where $n$ is the length of the data, and optimal in\nthe sense that under certain mild conditions every change point is detected\nwith high probability and the widths of the intervals returned match the\nmini-max localisation rates for the associated change point problem up to log\nfactors. A detailed simulation study shows our procedure is competitive against\nstate of the art algorithms for similar problems. Our procedure is implemented\nin the R package ChangePointInference which is available via\nhttps://github.com/gaviosha/ChangePointInference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of uncertainty quantification in change point\nregressions, where the signal can be piecewise polynomial of arbitrary but\nfixed degree. That is we seek disjoint intervals which, uniformly at a given\nconfidence level, must each contain a change point location. We propose a\nprocedure based on performing local tests at a number of scales and locations\non a sparse grid, which adapts to the choice of grid in the sense that by\nchoosing a sparser grid one explicitly pays a lower price for multiple testing.\nThe procedure is fast as its computational complexity is always of the order\n$\\mathcal{O} (n \\log (n))$ where $n$ is the length of the data, and optimal in\nthe sense that under certain mild conditions every change point is detected\nwith high probability and the widths of the intervals returned match the\nmini-max localisation rates for the associated change point problem up to log\nfactors. A detailed simulation study shows our procedure is competitive against\nstate of the art algorithms for similar problems. Our procedure is implemented\nin the R package ChangePointInference which is available via\nhttps://github.com/gaviosha/ChangePointInference."
                },
                "authors": [
                    {
                        "name": "Shakeel Gavioli-Akilagun"
                    },
                    {
                        "name": "Piotr Fryzlewicz"
                    }
                ],
                "author_detail": {
                    "name": "Piotr Fryzlewicz"
                },
                "author": "Piotr Fryzlewicz",
                "arxiv_comment": "69 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2307.03639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2307.03639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2210.06639v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2210.06639v3",
                "updated": "2024-12-11T15:30:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    30,
                    25,
                    2,
                    346,
                    0
                ],
                "published": "2022-10-13T00:32:58Z",
                "published_parsed": [
                    2022,
                    10,
                    13,
                    0,
                    32,
                    58,
                    3,
                    286,
                    0
                ],
                "title": "Robust Estimation and Inference in Panels with Interactive Fixed Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Estimation and Inference in Panels with Interactive Fixed Effects"
                },
                "summary": "We consider estimation and inference for a regression coefficient in panels\nwith interactive fixed effects (i.e., with a factor structure). We demonstrate\nthat existing estimators and confidence intervals (CIs) can be heavily biased\nand size-distorted when some of the factors are weak. We propose estimators\nwith improved rates of convergence and bias-aware CIs that remain valid\nuniformly, regardless of factor strength. Our approach applies the theory of\nminimax linear estimation to form a debiased estimate, using a nuclear norm\nbound on the error of an initial estimate of the interactive fixed effects. Our\nresulting bias-aware CIs take into account the remaining bias caused by weak\nfactors. Monte Carlo experiments show substantial improvements over\nconventional methods when factors are weak, with minimal costs to estimation\naccuracy when factors are strong.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider estimation and inference for a regression coefficient in panels\nwith interactive fixed effects (i.e., with a factor structure). We demonstrate\nthat existing estimators and confidence intervals (CIs) can be heavily biased\nand size-distorted when some of the factors are weak. We propose estimators\nwith improved rates of convergence and bias-aware CIs that remain valid\nuniformly, regardless of factor strength. Our approach applies the theory of\nminimax linear estimation to form a debiased estimate, using a nuclear norm\nbound on the error of an initial estimate of the interactive fixed effects. Our\nresulting bias-aware CIs take into account the remaining bias caused by weak\nfactors. Monte Carlo experiments show substantial improvements over\nconventional methods when factors are weak, with minimal costs to estimation\naccuracy when factors are strong."
                },
                "authors": [
                    {
                        "name": "Timothy B. Armstrong"
                    },
                    {
                        "name": "Martin Weidner"
                    },
                    {
                        "name": "Andrei Zeleneev"
                    }
                ],
                "author_detail": {
                    "name": "Andrei Zeleneev"
                },
                "author": "Andrei Zeleneev",
                "arxiv_comment": "Implementation of our method in R is available here:\n  https://github.com/chenweihsiang/PanelIFE/tree/main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2210.06639v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2210.06639v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08458v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08458v1",
                "updated": "2024-12-11T15:24:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    24,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:24:40Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    24,
                    40,
                    2,
                    346,
                    0
                ],
                "title": "Heavy Tail Robust Estimation and Inference for Average Treatment Effects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heavy Tail Robust Estimation and Inference for Average Treatment Effects"
                },
                "summary": "We study the probability tail properties of Inverse Probability Weighting\n(IPW) estimators of the Average Treatment Effect (ATE) when there is limited\noverlap between the covariate distributions of the treatment and control\ngroups. Under unconfoundedness of treatment assignment conditional on\ncovariates, such limited overlap is manifested in the propensity score for\ncertain units being very close (but not equal) to 0 or 1. This renders IPW\nestimators possibly heavy tailed, and with a slower than sqrt(n) rate of\nconvergence. Trimming or truncation is ultimately based on the covariates,\nignoring important information about the inverse probability weighted random\nvariable Z that identifies ATE by E[Z]= ATE. We propose a tail-trimmed IPW\nestimator whose performance is robust to limited overlap. In terms of the\npropensity score, which is generally unknown, we plug-in its parametric\nestimator in the infeasible Z, and then negligibly trim the resulting feasible\nZ adaptively by its large values. Trimming leads to bias if Z has an asymmetric\ndistribution and an infinite variance, hence we estimate and remove the bias\nusing important improvements on existing theory and methods. Our estimator\nsidesteps dimensionality, bias and poor correspondence properties associated\nwith trimming by the covariates or propensity score. Monte Carlo experiments\ndemonstrate that trimming by the covariates or the propensity score requires\nthe removal of a substantial portion of the sample to render a low bias and\nclose to normal estimator, while our estimator has low bias and mean-squared\nerror, and is close to normal, based on the removal of very few sample\nextremes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the probability tail properties of Inverse Probability Weighting\n(IPW) estimators of the Average Treatment Effect (ATE) when there is limited\noverlap between the covariate distributions of the treatment and control\ngroups. Under unconfoundedness of treatment assignment conditional on\ncovariates, such limited overlap is manifested in the propensity score for\ncertain units being very close (but not equal) to 0 or 1. This renders IPW\nestimators possibly heavy tailed, and with a slower than sqrt(n) rate of\nconvergence. Trimming or truncation is ultimately based on the covariates,\nignoring important information about the inverse probability weighted random\nvariable Z that identifies ATE by E[Z]= ATE. We propose a tail-trimmed IPW\nestimator whose performance is robust to limited overlap. In terms of the\npropensity score, which is generally unknown, we plug-in its parametric\nestimator in the infeasible Z, and then negligibly trim the resulting feasible\nZ adaptively by its large values. Trimming leads to bias if Z has an asymmetric\ndistribution and an infinite variance, hence we estimate and remove the bias\nusing important improvements on existing theory and methods. Our estimator\nsidesteps dimensionality, bias and poor correspondence properties associated\nwith trimming by the covariates or propensity score. Monte Carlo experiments\ndemonstrate that trimming by the covariates or the propensity score requires\nthe removal of a substantial portion of the sample to render a low bias and\nclose to normal estimator, while our estimator has low bias and mean-squared\nerror, and is close to normal, based on the removal of very few sample\nextremes."
                },
                "authors": [
                    {
                        "name": "Jonathan B. Hill"
                    },
                    {
                        "name": "Saraswata Chaudhuri"
                    }
                ],
                "author_detail": {
                    "name": "Saraswata Chaudhuri"
                },
                "author": "Saraswata Chaudhuri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08458v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08458v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62F12, 62F35",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08457v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08457v1",
                "updated": "2024-12-11T15:24:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    24,
                    7,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:24:07Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    24,
                    7,
                    2,
                    346,
                    0
                ],
                "title": "Efficient Rectification of Neuro-Symbolic Reasoning Inconsistencies by\n  Abductive Reflection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Rectification of Neuro-Symbolic Reasoning Inconsistencies by\n  Abductive Reflection"
                },
                "summary": "Neuro-Symbolic (NeSy) AI could be regarded as an analogy to human\ndual-process cognition, modeling the intuitive System 1 with neural networks\nand the algorithmic System 2 with symbolic reasoning. However, for complex\nlearning targets, NeSy systems often generate outputs inconsistent with domain\nknowledge and it is challenging to rectify them. Inspired by the human\nCognitive Reflection, which promptly detects errors in our intuitive response\nand revises them by invoking the System 2 reasoning, we propose to improve NeSy\nsystems by introducing Abductive Reflection (ABL-Refl) based on the Abductive\nLearning (ABL) framework. ABL-Refl leverages domain knowledge to abduce a\nreflection vector during training, which can then flag potential errors in the\nneural network outputs and invoke abduction to rectify them and generate\nconsistent outputs during inference. ABL-Refl is highly efficient in contrast\nto previous ABL implementations. Experiments show that ABL-Refl outperforms\nstate-of-the-art NeSy methods, achieving excellent accuracy with fewer training\nresources and enhanced efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuro-Symbolic (NeSy) AI could be regarded as an analogy to human\ndual-process cognition, modeling the intuitive System 1 with neural networks\nand the algorithmic System 2 with symbolic reasoning. However, for complex\nlearning targets, NeSy systems often generate outputs inconsistent with domain\nknowledge and it is challenging to rectify them. Inspired by the human\nCognitive Reflection, which promptly detects errors in our intuitive response\nand revises them by invoking the System 2 reasoning, we propose to improve NeSy\nsystems by introducing Abductive Reflection (ABL-Refl) based on the Abductive\nLearning (ABL) framework. ABL-Refl leverages domain knowledge to abduce a\nreflection vector during training, which can then flag potential errors in the\nneural network outputs and invoke abduction to rectify them and generate\nconsistent outputs during inference. ABL-Refl is highly efficient in contrast\nto previous ABL implementations. Experiments show that ABL-Refl outperforms\nstate-of-the-art NeSy methods, achieving excellent accuracy with fewer training\nresources and enhanced efficiency."
                },
                "authors": [
                    {
                        "name": "Wen-Chao Hu"
                    },
                    {
                        "name": "Wang-Zhou Dai"
                    },
                    {
                        "name": "Yuan Jiang"
                    },
                    {
                        "name": "Zhi-Hua Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Zhi-Hua Zhou"
                },
                "author": "Zhi-Hua Zhou",
                "arxiv_comment": "Accepted to AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08457v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08457v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2304.14385v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2304.14385v3",
                "updated": "2024-12-11T15:22:52Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    22,
                    52,
                    2,
                    346,
                    0
                ],
                "published": "2023-04-27T17:52:06Z",
                "published_parsed": [
                    2023,
                    4,
                    27,
                    17,
                    52,
                    6,
                    3,
                    117,
                    0
                ],
                "title": "Dynamic Pricing and Advertising with Demand Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Pricing and Advertising with Demand Learning"
                },
                "summary": "We consider a novel pricing and advertising framework, where a seller not\nonly sets product price but also designs flexible 'advertising schemes' to\ninfluence customers' valuation of the product. We impose no structural\nrestriction on the seller's feasible advertising strategies and allow her to\nadvertise the product by disclosing or concealing any information. Following\nthe literature in information design, this fully flexible advertising can be\nmodeled as the seller being able to choose any information policy that signals\nthe product quality/characteristic to the customers. Customers observe the\nadvertising signal and infer a Bayesian belief over the products. We aim to\ninvestigate two questions in this work: (1) What is the value of advertising?\nTo what extent can advertising enhance a seller's revenue? (2) Without any\napriori knowledge of the customers' demand function, how can a seller\nadaptively learn and optimize both pricing and advertising strategies using\npast purchase responses?\n  To study the first question, we introduce and study the value of advertising\n- a revenue gap between using advertising vs not advertising, and we provide a\ncrisp tight characterization for this notion for a broad family of problems.\nFor the second question, we study the seller's dynamic pricing and advertising\nproblem with demand uncertainty. Our main result for this question is a\ncomputationally efficient online algorithm that achieves an optimal\n$O(T^{2/3}(m\\log T)^{1/3})$ regret rate when the valuation function is linear\nin the product quality. Here $m$ is the cardinality of the discrete product\nquality domain and $T$ is the time horizon. This result requires some mild\nregularity assumptions on the valuation function, but no Lipschitz or\nsmoothness assumption on the customers' demand function. We also obtain several\nimproved results for the widely considered special case of additive valuations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a novel pricing and advertising framework, where a seller not\nonly sets product price but also designs flexible 'advertising schemes' to\ninfluence customers' valuation of the product. We impose no structural\nrestriction on the seller's feasible advertising strategies and allow her to\nadvertise the product by disclosing or concealing any information. Following\nthe literature in information design, this fully flexible advertising can be\nmodeled as the seller being able to choose any information policy that signals\nthe product quality/characteristic to the customers. Customers observe the\nadvertising signal and infer a Bayesian belief over the products. We aim to\ninvestigate two questions in this work: (1) What is the value of advertising?\nTo what extent can advertising enhance a seller's revenue? (2) Without any\napriori knowledge of the customers' demand function, how can a seller\nadaptively learn and optimize both pricing and advertising strategies using\npast purchase responses?\n  To study the first question, we introduce and study the value of advertising\n- a revenue gap between using advertising vs not advertising, and we provide a\ncrisp tight characterization for this notion for a broad family of problems.\nFor the second question, we study the seller's dynamic pricing and advertising\nproblem with demand uncertainty. Our main result for this question is a\ncomputationally efficient online algorithm that achieves an optimal\n$O(T^{2/3}(m\\log T)^{1/3})$ regret rate when the valuation function is linear\nin the product quality. Here $m$ is the cardinality of the discrete product\nquality domain and $T$ is the time horizon. This result requires some mild\nregularity assumptions on the valuation function, but no Lipschitz or\nsmoothness assumption on the customers' demand function. We also obtain several\nimproved results for the widely considered special case of additive valuations."
                },
                "authors": [
                    {
                        "name": "Shipra Agrawal"
                    },
                    {
                        "name": "Yiding Feng"
                    },
                    {
                        "name": "Wei Tang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Tang"
                },
                "author": "Wei Tang",
                "arxiv_comment": "Added new results, including a new section for detailed analysis of\n  value of advertising, a section for numerical results. Also rewrite the\n  introduction and setting section",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2304.14385v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2304.14385v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08445v1",
                "updated": "2024-12-11T15:09:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    9,
                    54,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:09:54Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    9,
                    54,
                    2,
                    346,
                    0
                ],
                "title": "TapeAgents: a Holistic Framework for Agent Development and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TapeAgents: a Holistic Framework for Agent Development and Optimization"
                },
                "summary": "We present TapeAgents, an agent framework built around a granular, structured\nlog tape of the agent session that also plays the role of the session's\nresumable state. In TapeAgents we leverage tapes to facilitate all stages of\nthe LLM Agent development lifecycle. The agent reasons by processing the tape\nand the LLM output to produce new thought and action steps and append them to\nthe tape. The environment then reacts to the agent's actions by likewise\nappending observation steps to the tape. By virtue of this tape-centred design,\nTapeAgents can provide AI practitioners with holistic end-to-end support. At\nthe development stage, tapes facilitate session persistence, agent auditing,\nand step-by-step debugging. Post-deployment, one can reuse tapes for\nevaluation, fine-tuning, and prompt-tuning; crucially, one can adapt tapes from\nother agents or use revised historical tapes. In this report, we explain the\nTapeAgents design in detail. We demonstrate possible applications of TapeAgents\nwith several concrete examples of building monolithic agents and multi-agent\nteams, of optimizing agent prompts and finetuning the agent's LLM. We present\ntooling prototypes and report a case study where we use TapeAgents to finetune\na Llama-3.1-8B form-filling assistant to perform as well as GPT-4o while being\norders of magnitude cheaper. Lastly, our comparative analysis shows that\nTapeAgents's advantages over prior frameworks stem from our novel design of the\nLLM agent as a resumable, modular state machine with a structured\nconfiguration, that generates granular, structured logs and that can transform\nthese logs into training text -- a unique combination of features absent in\nprevious work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TapeAgents, an agent framework built around a granular, structured\nlog tape of the agent session that also plays the role of the session's\nresumable state. In TapeAgents we leverage tapes to facilitate all stages of\nthe LLM Agent development lifecycle. The agent reasons by processing the tape\nand the LLM output to produce new thought and action steps and append them to\nthe tape. The environment then reacts to the agent's actions by likewise\nappending observation steps to the tape. By virtue of this tape-centred design,\nTapeAgents can provide AI practitioners with holistic end-to-end support. At\nthe development stage, tapes facilitate session persistence, agent auditing,\nand step-by-step debugging. Post-deployment, one can reuse tapes for\nevaluation, fine-tuning, and prompt-tuning; crucially, one can adapt tapes from\nother agents or use revised historical tapes. In this report, we explain the\nTapeAgents design in detail. We demonstrate possible applications of TapeAgents\nwith several concrete examples of building monolithic agents and multi-agent\nteams, of optimizing agent prompts and finetuning the agent's LLM. We present\ntooling prototypes and report a case study where we use TapeAgents to finetune\na Llama-3.1-8B form-filling assistant to perform as well as GPT-4o while being\norders of magnitude cheaper. Lastly, our comparative analysis shows that\nTapeAgents's advantages over prior frameworks stem from our novel design of the\nLLM agent as a resumable, modular state machine with a structured\nconfiguration, that generates granular, structured logs and that can transform\nthese logs into training text -- a unique combination of features absent in\nprevious work."
                },
                "authors": [
                    {
                        "name": "Dzmitry Bahdanau"
                    },
                    {
                        "name": "Nicolas Gontier"
                    },
                    {
                        "name": "Gabriel Huang"
                    },
                    {
                        "name": "Ehsan Kamalloo"
                    },
                    {
                        "name": "Rafael Pardinas"
                    },
                    {
                        "name": "Alex PichÃ©"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Oleh Shliazhko"
                    },
                    {
                        "name": "Jordan Prince Tremblay"
                    },
                    {
                        "name": "Karam Ghanem"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Mitul Tiwari"
                    },
                    {
                        "name": "Quaizar Vohra"
                    }
                ],
                "author_detail": {
                    "name": "Quaizar Vohra"
                },
                "author": "Quaizar Vohra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08442v1",
                "updated": "2024-12-11T15:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    6,
                    25,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:06:25Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    6,
                    25,
                    2,
                    346,
                    0
                ],
                "title": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons"
                },
                "summary": "We examine the capability of Multimodal Large Language Models (MLLMs) to\ntackle diverse domains that extend beyond the traditional language and vision\ntasks these models are typically trained on. Specifically, our focus lies in\nareas such as Embodied AI, Games, UI Control, and Planning. To this end, we\nintroduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).\nGEA is a single unified model capable of grounding itself across these varied\ndomains through a multi-embodiment action tokenizer. GEA is trained with\nsupervised learning on a large dataset of embodied experiences and with online\nRL in interactive simulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal the importance of\ntraining with cross-domain data and online RL for building generalist agents.\nThe final GEA model achieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist models and\nbenchmark-specific approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the capability of Multimodal Large Language Models (MLLMs) to\ntackle diverse domains that extend beyond the traditional language and vision\ntasks these models are typically trained on. Specifically, our focus lies in\nareas such as Embodied AI, Games, UI Control, and Planning. To this end, we\nintroduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).\nGEA is a single unified model capable of grounding itself across these varied\ndomains through a multi-embodiment action tokenizer. GEA is trained with\nsupervised learning on a large dataset of embodied experiences and with online\nRL in interactive simulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal the importance of\ntraining with cross-domain data and online RL for building generalist agents.\nThe final GEA model achieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist models and\nbenchmark-specific approaches."
                },
                "authors": [
                    {
                        "name": "Andrew Szot"
                    },
                    {
                        "name": "Bogdan Mazoure"
                    },
                    {
                        "name": "Omar Attia"
                    },
                    {
                        "name": "Aleksei Timofeev"
                    },
                    {
                        "name": "Harsh Agrawal"
                    },
                    {
                        "name": "Devon Hjelm"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Alexander Toshev"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Toshev"
                },
                "author": "Alexander Toshev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08775v2",
                "updated": "2024-12-11T14:58:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    58,
                    53,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-13T12:34:14Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "title": "What Should We Engineer in Prompts? Training Humans in\n  Requirement-Driven LLM Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Should We Engineer in Prompts? Training Humans in\n  Requirement-Driven LLM Use"
                },
                "summary": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications."
                },
                "authors": [
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Weirui Peng"
                    },
                    {
                        "name": "Chenyang Yang"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08430v1",
                "updated": "2024-12-11T14:51:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    51,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:51:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    51,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "Assessing Personalized AI Mentoring with Large Language Models in the\n  Computing Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Personalized AI Mentoring with Large Language Models in the\n  Computing Field"
                },
                "summary": "This paper provides an in-depth evaluation of three state-of-the-art Large\nLanguage Models (LLMs) for personalized career mentoring in the computing\nfield, using three distinct student profiles that consider gender, race, and\nprofessional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2\nusing a zero-shot learning approach without human intervention. A quantitative\nevaluation was conducted through a custom natural language processing analytics\npipeline to highlight the uniqueness of the responses and to identify words\nreflecting each student's profile, including race, gender, or professional\nlevel. The analysis of frequently used words in the responses indicates that\nGPT-4 offers more personalized mentoring compared to the other two LLMs.\nAdditionally, a qualitative evaluation was performed to see if human experts\nreached similar conclusions. The analysis of survey responses shows that GPT-4\noutperformed the other two LLMs in delivering more accurate and useful\nmentoring while addressing specific challenges with encouragement languages.\nOur work establishes a foundation for developing personalized mentoring tools\nbased on LLMs, incorporating human mentors in the process to deliver a more\nimpactful and tailored mentoring experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an in-depth evaluation of three state-of-the-art Large\nLanguage Models (LLMs) for personalized career mentoring in the computing\nfield, using three distinct student profiles that consider gender, race, and\nprofessional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2\nusing a zero-shot learning approach without human intervention. A quantitative\nevaluation was conducted through a custom natural language processing analytics\npipeline to highlight the uniqueness of the responses and to identify words\nreflecting each student's profile, including race, gender, or professional\nlevel. The analysis of frequently used words in the responses indicates that\nGPT-4 offers more personalized mentoring compared to the other two LLMs.\nAdditionally, a qualitative evaluation was performed to see if human experts\nreached similar conclusions. The analysis of survey responses shows that GPT-4\noutperformed the other two LLMs in delivering more accurate and useful\nmentoring while addressing specific challenges with encouragement languages.\nOur work establishes a foundation for developing personalized mentoring tools\nbased on LLMs, incorporating human mentors in the process to deliver a more\nimpactful and tailored mentoring experience."
                },
                "authors": [
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Sean O'Connell"
                    },
                    {
                        "name": "Shamima Mithun"
                    }
                ],
                "author_detail": {
                    "name": "Shamima Mithun"
                },
                "author": "Shamima Mithun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08428v1",
                "updated": "2024-12-11T14:48:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    48,
                    19,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:48:19Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    48,
                    19,
                    2,
                    346,
                    0
                ],
                "title": "SwarmGPT-Primitive: A Language-Driven Choreographer for Drone Swarms\n  Using Safe Motion Primitive Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwarmGPT-Primitive: A Language-Driven Choreographer for Drone Swarms\n  Using Safe Motion Primitive Composition"
                },
                "summary": "Catalyzed by advancements in hardware and software, drone performances are\nincreasingly making their mark in the entertainment industry. However,\ndesigning smooth and safe choreographies for drone swarms is complex and often\nrequires expert domain knowledge. In this work, we introduce\nSwarmGPT-Primitive, a language-based choreographer that integrates the\nreasoning capabilities of large language models (LLMs) with safe motion\nplanning to facilitate deployable drone swarm choreographies. The LLM composes\nchoreographies for a given piece of music by utilizing a library of motion\nprimitives; the language-based choreographer is augmented with an\noptimization-based safety filter, which certifies the choreography for\nreal-world deployment by making minimal adjustments when feasibility and safety\nconstraints are violated. The overall SwarmGPT-Primitive framework decouples\nchoreographic design from safe motion planning, which allows non-expert users\nto re-prompt and refine compositions without concerns about compliance with\nconstraints such as avoiding collisions or downwash effects or satisfying\nactuation limits. We demonstrate our approach through simulations and\nexperiments with swarms of up to 20 drones performing choreographies designed\nbased on various songs, highlighting the system's ability to generate effective\nand synchronized drone choreographies for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catalyzed by advancements in hardware and software, drone performances are\nincreasingly making their mark in the entertainment industry. However,\ndesigning smooth and safe choreographies for drone swarms is complex and often\nrequires expert domain knowledge. In this work, we introduce\nSwarmGPT-Primitive, a language-based choreographer that integrates the\nreasoning capabilities of large language models (LLMs) with safe motion\nplanning to facilitate deployable drone swarm choreographies. The LLM composes\nchoreographies for a given piece of music by utilizing a library of motion\nprimitives; the language-based choreographer is augmented with an\noptimization-based safety filter, which certifies the choreography for\nreal-world deployment by making minimal adjustments when feasibility and safety\nconstraints are violated. The overall SwarmGPT-Primitive framework decouples\nchoreographic design from safe motion planning, which allows non-expert users\nto re-prompt and refine compositions without concerns about compliance with\nconstraints such as avoiding collisions or downwash effects or satisfying\nactuation limits. We demonstrate our approach through simulations and\nexperiments with swarms of up to 20 drones performing choreographies designed\nbased on various songs, highlighting the system's ability to generate effective\nand synchronized drone choreographies for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Vedant Vyas"
                    },
                    {
                        "name": "Martin Schuck"
                    },
                    {
                        "name": "Dinushka O. Dahanaggamaarachchi"
                    },
                    {
                        "name": "Siqi Zhou"
                    },
                    {
                        "name": "Angela P. Schoellig"
                    }
                ],
                "author_detail": {
                    "name": "Angela P. Schoellig"
                },
                "author": "Angela P. Schoellig",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05185v2",
                "updated": "2024-12-11T14:43:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    43,
                    2,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-06T17:04:42Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    42,
                    4,
                    341,
                    0
                ],
                "title": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos"
                },
                "summary": "Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Lishuai Gao"
                    },
                    {
                        "name": "Yujie Zhong"
                    },
                    {
                        "name": "Yingsen Zeng"
                    },
                    {
                        "name": "Haoxian Tan"
                    },
                    {
                        "name": "Dengjie Li"
                    },
                    {
                        "name": "Zheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhao"
                },
                "author": "Zheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2309.01745v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2309.01745v3",
                "updated": "2024-12-11T14:40:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    40,
                    46,
                    2,
                    346,
                    0
                ],
                "published": "2023-09-04T18:01:42Z",
                "published_parsed": [
                    2023,
                    9,
                    4,
                    18,
                    1,
                    42,
                    0,
                    247,
                    0
                ],
                "title": "Benchmarking Autoregressive Conditional Diffusion Models for Turbulent\n  Flow Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Autoregressive Conditional Diffusion Models for Turbulent\n  Flow Simulation"
                },
                "summary": "Simulating turbulent flows is crucial for a wide range of applications, and\nmachine learning-based solvers are gaining increasing relevance. However,\nachieving temporal stability when generalizing to longer rollout horizons\nremains a persistent challenge for learned PDE solvers. In this work, we\nanalyze if fully data-driven fluid solvers that utilize an autoregressive\nrollout based on conditional diffusion models are a viable option to address\nthis challenge. We investigate accuracy, posterior sampling, spectral behavior,\nand temporal stability, while requiring that methods generalize to flow\nparameters beyond the training regime. To quantitatively and qualitatively\nbenchmark the performance of various flow prediction approaches, three\nchallenging 2D scenarios including incompressible and transonic flows, as well\nas isotropic turbulence are employed. We find that even simple diffusion-based\napproaches can outperform multiple established flow prediction methods in terms\nof accuracy and temporal stability, while being on par with state-of-the-art\nstabilization techniques like unrolling at training time. Such traditional\narchitectures are superior in terms of inference speed, however, the\nprobabilistic nature of diffusion approaches allows for inferring multiple\npredictions that align with the statistics of the underlying physics. Overall,\nour benchmark contains three carefully chosen data sets that are suitable for\nprobabilistic evaluation alongside various established flow prediction\narchitectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating turbulent flows is crucial for a wide range of applications, and\nmachine learning-based solvers are gaining increasing relevance. However,\nachieving temporal stability when generalizing to longer rollout horizons\nremains a persistent challenge for learned PDE solvers. In this work, we\nanalyze if fully data-driven fluid solvers that utilize an autoregressive\nrollout based on conditional diffusion models are a viable option to address\nthis challenge. We investigate accuracy, posterior sampling, spectral behavior,\nand temporal stability, while requiring that methods generalize to flow\nparameters beyond the training regime. To quantitatively and qualitatively\nbenchmark the performance of various flow prediction approaches, three\nchallenging 2D scenarios including incompressible and transonic flows, as well\nas isotropic turbulence are employed. We find that even simple diffusion-based\napproaches can outperform multiple established flow prediction methods in terms\nof accuracy and temporal stability, while being on par with state-of-the-art\nstabilization techniques like unrolling at training time. Such traditional\narchitectures are superior in terms of inference speed, however, the\nprobabilistic nature of diffusion approaches allows for inferring multiple\npredictions that align with the statistics of the underlying physics. Overall,\nour benchmark contains three carefully chosen data sets that are suitable for\nprobabilistic evaluation alongside various established flow prediction\narchitectures."
                },
                "authors": [
                    {
                        "name": "Georg Kohl"
                    },
                    {
                        "name": "Li-Wei Chen"
                    },
                    {
                        "name": "Nils Thuerey"
                    }
                ],
                "author_detail": {
                    "name": "Nils Thuerey"
                },
                "author": "Nils Thuerey",
                "arxiv_comment": "Source code available at\n  https://github.com/tum-pbs/autoreg-pde-diffusion and further information and\n  videos at https://ge.in.tum.de/publications/2023-acdm-kohl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2309.01745v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2309.01745v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08414v1",
                "updated": "2024-12-11T14:31:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    31,
                    39,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:31:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    31,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Detecting Conversational Mental Manipulation with Intent-Aware Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Conversational Mental Manipulation with Intent-Aware Prompting"
                },
                "summary": "Mental manipulation severely undermines mental wellness by covertly and\nnegatively distorting decision-making. While there is an increasing interest in\nmental health care within the natural language processing community, progress\nin tackling manipulation remains limited due to the complexity of detecting\nsubtle, covert tactics in conversations. In this paper, we propose Intent-Aware\nPrompting (IAP), a novel approach for detecting mental manipulations using\nlarge language models (LLMs), providing a deeper understanding of manipulative\ntactics by capturing the underlying intents of participants. Experimental\nresults on the MentalManip dataset demonstrate superior effectiveness of IAP\nagainst other advanced prompting strategies. Notably, our approach\nsubstantially reduces false negatives, helping detect more instances of mental\nmanipulation with minimal misjudgment of positive cases. The code of this paper\nis available at https://github.com/Anton-Jiayuan-MA/Manip-IAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental manipulation severely undermines mental wellness by covertly and\nnegatively distorting decision-making. While there is an increasing interest in\nmental health care within the natural language processing community, progress\nin tackling manipulation remains limited due to the complexity of detecting\nsubtle, covert tactics in conversations. In this paper, we propose Intent-Aware\nPrompting (IAP), a novel approach for detecting mental manipulations using\nlarge language models (LLMs), providing a deeper understanding of manipulative\ntactics by capturing the underlying intents of participants. Experimental\nresults on the MentalManip dataset demonstrate superior effectiveness of IAP\nagainst other advanced prompting strategies. Notably, our approach\nsubstantially reduces false negatives, helping detect more instances of mental\nmanipulation with minimal misjudgment of positive cases. The code of this paper\nis available at https://github.com/Anton-Jiayuan-MA/Manip-IAP."
                },
                "authors": [
                    {
                        "name": "Jiayuan Ma"
                    },
                    {
                        "name": "Hongbin Na"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_journal_ref": "COLING2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08412v1",
                "updated": "2024-12-11T14:30:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    30,
                    24,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:30:24Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    30,
                    24,
                    2,
                    346,
                    0
                ],
                "title": "Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D\n  Reconstruction from Unposed Sparse Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pragmatist: Multiview Conditional Diffusion Models for High-Fidelity 3D\n  Reconstruction from Unposed Sparse Views"
                },
                "summary": "Inferring 3D structures from sparse, unposed observations is challenging due\nto its unconstrained nature. Recent methods propose to predict implicit\nrepresentations directly from unposed inputs in a data-driven manner, achieving\npromising results. However, these methods do not utilize geometric priors and\ncannot hallucinate the appearance of unseen regions, thus making it challenging\nto reconstruct fine geometric and textural details. To tackle this challenge,\nour key idea is to reformulate this ill-posed problem as conditional novel view\nsynthesis, aiming to generate complete observations from limited input views to\nfacilitate reconstruction. With complete observations, the poses of the input\nviews can be easily recovered and further used to optimize the reconstructed\nobject. To this end, we propose a novel pipeline Pragmatist. First, we generate\na complete observation of the object via a multiview conditional diffusion\nmodel. Then, we use a feed-forward large reconstruction model to obtain the\nreconstructed mesh. To further improve the reconstruction quality, we recover\nthe poses of input views by inverting the obtained 3D representations and\nfurther optimize the texture using detailed input views. Unlike previous\napproaches, our pipeline improves reconstruction by efficiently leveraging\nunposed inputs and generative priors, circumventing the direct resolution of\nhighly ill-posed problems. Extensive experiments show that our approach\nachieves promising performance in several benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring 3D structures from sparse, unposed observations is challenging due\nto its unconstrained nature. Recent methods propose to predict implicit\nrepresentations directly from unposed inputs in a data-driven manner, achieving\npromising results. However, these methods do not utilize geometric priors and\ncannot hallucinate the appearance of unseen regions, thus making it challenging\nto reconstruct fine geometric and textural details. To tackle this challenge,\nour key idea is to reformulate this ill-posed problem as conditional novel view\nsynthesis, aiming to generate complete observations from limited input views to\nfacilitate reconstruction. With complete observations, the poses of the input\nviews can be easily recovered and further used to optimize the reconstructed\nobject. To this end, we propose a novel pipeline Pragmatist. First, we generate\na complete observation of the object via a multiview conditional diffusion\nmodel. Then, we use a feed-forward large reconstruction model to obtain the\nreconstructed mesh. To further improve the reconstruction quality, we recover\nthe poses of input views by inverting the obtained 3D representations and\nfurther optimize the texture using detailed input views. Unlike previous\napproaches, our pipeline improves reconstruction by efficiently leveraging\nunposed inputs and generative priors, circumventing the direct resolution of\nhighly ill-posed problems. Extensive experiments show that our approach\nachieves promising performance in several benchmarks."
                },
                "authors": [
                    {
                        "name": "Songchun Zhang"
                    },
                    {
                        "name": "Chunhui Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Chunhui Zhao"
                },
                "author": "Chunhui Zhao",
                "arxiv_comment": "Accepted by AAAI 2025. 13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08398v1",
                "updated": "2024-12-11T14:17:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    17,
                    17,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:17:17Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    17,
                    17,
                    2,
                    346,
                    0
                ],
                "title": "Grasp Diffusion Network: Learning Grasp Generators from Partial Point\n  Clouds with Diffusion Models in SO(3)xR3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasp Diffusion Network: Learning Grasp Generators from Partial Point\n  Clouds with Diffusion Models in SO(3)xR3"
                },
                "summary": "Grasping objects successfully from a single-view camera is crucial in many\nrobot manipulation tasks. An approach to solve this problem is to leverage\nsimulation to create large datasets of pairs of objects and grasp poses, and\nthen learn a conditional generative model that can be prompted quickly during\ndeployment. However, the grasp pose data is highly multimodal since there are\nseveral ways to grasp an object. Hence, in this work, we learn a grasp\ngenerative model with diffusion models to sample candidate grasp poses given a\npartial point cloud of an object. A novel aspect of our method is to consider\ndiffusion in the manifold space of rotations and to propose a\ncollision-avoidance cost guidance to improve the grasp success rate during\ninference. To accelerate grasp sampling we use recent techniques from the\ndiffusion literature to achieve faster inference times. We show in simulation\nand real-world experiments that our approach can grasp several objects from raw\ndepth images with $90\\%$ success rate and benchmark it against several\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasping objects successfully from a single-view camera is crucial in many\nrobot manipulation tasks. An approach to solve this problem is to leverage\nsimulation to create large datasets of pairs of objects and grasp poses, and\nthen learn a conditional generative model that can be prompted quickly during\ndeployment. However, the grasp pose data is highly multimodal since there are\nseveral ways to grasp an object. Hence, in this work, we learn a grasp\ngenerative model with diffusion models to sample candidate grasp poses given a\npartial point cloud of an object. A novel aspect of our method is to consider\ndiffusion in the manifold space of rotations and to propose a\ncollision-avoidance cost guidance to improve the grasp success rate during\ninference. To accelerate grasp sampling we use recent techniques from the\ndiffusion literature to achieve faster inference times. We show in simulation\nand real-world experiments that our approach can grasp several objects from raw\ndepth images with $90\\%$ success rate and benchmark it against several\nbaselines."
                },
                "authors": [
                    {
                        "name": "Joao Carvalho"
                    },
                    {
                        "name": "An T. Le"
                    },
                    {
                        "name": "Philipp Jahr"
                    },
                    {
                        "name": "Qiao Sun"
                    },
                    {
                        "name": "Julen Urain"
                    },
                    {
                        "name": "Dorothea Koert"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.06909v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.06909v5",
                "updated": "2024-12-11T14:08:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    8,
                    57,
                    2,
                    346,
                    0
                ],
                "published": "2023-06-12T07:27:31Z",
                "published_parsed": [
                    2023,
                    6,
                    12,
                    7,
                    27,
                    31,
                    0,
                    163,
                    0
                ],
                "title": "Graph Agent Network: Empowering Nodes with Inference Capabilities for\n  Adversarial Resilience",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Agent Network: Empowering Nodes with Inference Capabilities for\n  Adversarial Resilience"
                },
                "summary": "End-to-end training with global optimization have popularized graph neural\nnetworks (GNNs) for node classification, yet inadvertently introduced\nvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit\nthe inherent opened interfaces of GNNs' input and output, perturbing critical\nedges and thus manipulating the classification results. Current defenses, due\nto their persistent utilization of global-optimization-based end-to-end\ntraining schemes, inherently encapsulate the vulnerabilities of GNNs. This is\nspecifically evidenced in their inability to defend against targeted secondary\nattacks. In this paper, we propose the Graph Agent Network (GAgN) to address\nthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent\nnetwork in which each node is designed as an 1-hop-view agent. Through the\ndecentralized interactions between agents, they can learn to infer global\nperceptions to perform tasks including inferring embeddings, degrees and\nneighbor relationships for given nodes. This empowers nodes to filtering\nadversarial edges while carrying out classification tasks. Furthermore, agents'\nlimited view prevents malicious messages from propagating globally in GAgN,\nthereby resisting global-optimization-based secondary attacks. We prove that\nsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient\nto achieve these functionalities. Experimental results show that GAgN\neffectively implements all its intended capabilities and, compared to\nstate-of-the-art defenses, achieves optimal classification accuracy on the\nperturbed datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "End-to-end training with global optimization have popularized graph neural\nnetworks (GNNs) for node classification, yet inadvertently introduced\nvulnerabilities to adversarial edge-perturbing attacks. Adversaries can exploit\nthe inherent opened interfaces of GNNs' input and output, perturbing critical\nedges and thus manipulating the classification results. Current defenses, due\nto their persistent utilization of global-optimization-based end-to-end\ntraining schemes, inherently encapsulate the vulnerabilities of GNNs. This is\nspecifically evidenced in their inability to defend against targeted secondary\nattacks. In this paper, we propose the Graph Agent Network (GAgN) to address\nthe aforementioned vulnerabilities of GNNs. GAgN is a graph-structured agent\nnetwork in which each node is designed as an 1-hop-view agent. Through the\ndecentralized interactions between agents, they can learn to infer global\nperceptions to perform tasks including inferring embeddings, degrees and\nneighbor relationships for given nodes. This empowers nodes to filtering\nadversarial edges while carrying out classification tasks. Furthermore, agents'\nlimited view prevents malicious messages from propagating globally in GAgN,\nthereby resisting global-optimization-based secondary attacks. We prove that\nsingle-hidden-layer multilayer perceptrons (MLPs) are theoretically sufficient\nto achieve these functionalities. Experimental results show that GAgN\neffectively implements all its intended capabilities and, compared to\nstate-of-the-art defenses, achieves optimal classification accuracy on the\nperturbed datasets."
                },
                "authors": [
                    {
                        "name": "Ao Liu"
                    },
                    {
                        "name": "Wenshan Li"
                    },
                    {
                        "name": "Tao Li"
                    },
                    {
                        "name": "Beibei Li"
                    },
                    {
                        "name": "Guangquan Xu"
                    },
                    {
                        "name": "Pan Zhou"
                    },
                    {
                        "name": "Wengang Ma"
                    },
                    {
                        "name": "Hanyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Hanyuan Huang"
                },
                "author": "Hanyuan Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.06909v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.06909v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08393v1",
                "updated": "2024-12-11T14:05:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    5,
                    4,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:05:04Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    5,
                    4,
                    2,
                    346,
                    0
                ],
                "title": "Learning to Reason via Self-Iterative Process Feedback for Small\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason via Self-Iterative Process Feedback for Small\n  Language Models"
                },
                "summary": "Small language models (SLMs) are more efficient, cost-effective, and\ncustomizable than large language models (LLMs), though they often underperform\nin specific areas like reasoning. Past methods for enhancing SLMs' reasoning,\nsuch as supervised fine-tuning and distillation, often depend on costly\nexternal signals, resulting in SLMs being overly confident with limited\nsupervision signals, thus limiting their abilities. Therefore, this study\nenables SLMs to learn to reason from self-iterative feedback. By combining odds\nratio preference optimization (ORPO), we fine-tune and align SLMs using\npositive and negative signals generated by themselves. Additionally, we\nintroduce process supervision for rewards in preference alignment by\nsampling-based inference simulation and process reward models. Compared to\nSupervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B\nby 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed\nmethod also demonstrated superior out-of-domain generalization capabilities on\nMMLU_Math and HumanEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) are more efficient, cost-effective, and\ncustomizable than large language models (LLMs), though they often underperform\nin specific areas like reasoning. Past methods for enhancing SLMs' reasoning,\nsuch as supervised fine-tuning and distillation, often depend on costly\nexternal signals, resulting in SLMs being overly confident with limited\nsupervision signals, thus limiting their abilities. Therefore, this study\nenables SLMs to learn to reason from self-iterative feedback. By combining odds\nratio preference optimization (ORPO), we fine-tune and align SLMs using\npositive and negative signals generated by themselves. Additionally, we\nintroduce process supervision for rewards in preference alignment by\nsampling-based inference simulation and process reward models. Compared to\nSupervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B\nby 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed\nmethod also demonstrated superior out-of-domain generalization capabilities on\nMMLU_Math and HumanEval."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Xuejie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuejie Zhang"
                },
                "author": "Xuejie Zhang",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.04876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.04876v2",
                "updated": "2024-12-11T14:00:27Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    0,
                    27,
                    2,
                    346,
                    0
                ],
                "published": "2023-01-12T08:47:59Z",
                "published_parsed": [
                    2023,
                    1,
                    12,
                    8,
                    47,
                    59,
                    3,
                    12,
                    0
                ],
                "title": "Interacting Treatments with Endogenous Takeup",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interacting Treatments with Endogenous Takeup"
                },
                "summary": "We study causal inference in randomized experiments (or quasi-experiments)\nfollowing a $2\\times 2$ factorial design. There are two treatments, denoted $A$\nand $B$, and units are randomly assigned to one of four categories: treatment\n$A$ alone, treatment $B$ alone, joint treatment, or none. Allowing for\nendogenous non-compliance with the two binary instruments representing the\nintended assignment, as well as unrestricted interference across the two\ntreatments, we derive the causal interpretation of various instrumental\nvariable estimands under more general compliance conditions than in the\nliterature. In general, if treatment takeup is driven by both instruments for\nsome units, it becomes difficult to separate treatment interaction from\ntreatment effect heterogeneity. We provide auxiliary conditions and various\nbounding strategies that may help zero in on causally interesting parameters.\nAs an empirical illustration, we apply our results to a program randomly\noffering two different treatments, namely tutoring and financial incentives, to\nfirst year college students, in order to assess the treatments' effects on\nacademic performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study causal inference in randomized experiments (or quasi-experiments)\nfollowing a $2\\times 2$ factorial design. There are two treatments, denoted $A$\nand $B$, and units are randomly assigned to one of four categories: treatment\n$A$ alone, treatment $B$ alone, joint treatment, or none. Allowing for\nendogenous non-compliance with the two binary instruments representing the\nintended assignment, as well as unrestricted interference across the two\ntreatments, we derive the causal interpretation of various instrumental\nvariable estimands under more general compliance conditions than in the\nliterature. In general, if treatment takeup is driven by both instruments for\nsome units, it becomes difficult to separate treatment interaction from\ntreatment effect heterogeneity. We provide auxiliary conditions and various\nbounding strategies that may help zero in on causally interesting parameters.\nAs an empirical illustration, we apply our results to a program randomly\noffering two different treatments, namely tutoring and financial incentives, to\nfirst year college students, in order to assess the treatments' effects on\nacademic performance."
                },
                "authors": [
                    {
                        "name": "Mate Kormos"
                    },
                    {
                        "name": "Robert P. Lieli"
                    },
                    {
                        "name": "Martin Huber"
                    }
                ],
                "author_detail": {
                    "name": "Martin Huber"
                },
                "author": "Martin Huber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2301.04876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.04876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05587v2",
                "updated": "2024-12-11T13:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    56,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-07T08:50:24Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    8,
                    50,
                    24,
                    5,
                    342,
                    0
                ],
                "title": "GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on\n  the Google Earth Engine Platform Powered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on\n  the Google Earth Engine Platform Powered by Large Language Models"
                },
                "summary": "As the scale and complexity of spatiotemporal data continue to grow rapidly,\nthe use of geospatial modeling on the Google Earth Engine (GEE) platform\npresents dual challenges: improving the coding efficiency of domain experts and\nenhancing the coding capabilities of interdisciplinary users. To address these\nchallenges and improve the performance of large language models (LLMs) in\ngeospatial code generation tasks, we propose a framework for building a\ngeospatial operator knowledge base tailored to the GEE JavaScript API. This\nframework consists of an operator syntax knowledge table, an operator\nrelationship frequency table, an operator frequent pattern knowledge table, and\nan operator relationship chain knowledge table. By leveraging Abstract Syntax\nTree (AST) techniques and frequent itemset mining, we systematically extract\noperator knowledge from 185,236 real GEE scripts and syntax documentation,\nforming a structured knowledge base. Experimental results demonstrate that the\nframework achieves over 90% accuracy, recall, and F1 score in operator\nknowledge extraction. When integrated with the Retrieval-Augmented Generation\n(RAG) strategy for LLM-based geospatial code generation tasks, the knowledge\nbase improves performance by 20-30%. Ablation studies further quantify the\nnecessity of each knowledge table in the knowledge base construction. This work\nprovides robust support for the advancement and application of geospatial code\nmodeling techniques, offering an innovative approach to constructing\ndomain-specific knowledge bases that enhance the code generation capabilities\nof LLMs, and fostering the deeper integration of generative AI technologies\nwithin the field of geoinformatics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the scale and complexity of spatiotemporal data continue to grow rapidly,\nthe use of geospatial modeling on the Google Earth Engine (GEE) platform\npresents dual challenges: improving the coding efficiency of domain experts and\nenhancing the coding capabilities of interdisciplinary users. To address these\nchallenges and improve the performance of large language models (LLMs) in\ngeospatial code generation tasks, we propose a framework for building a\ngeospatial operator knowledge base tailored to the GEE JavaScript API. This\nframework consists of an operator syntax knowledge table, an operator\nrelationship frequency table, an operator frequent pattern knowledge table, and\nan operator relationship chain knowledge table. By leveraging Abstract Syntax\nTree (AST) techniques and frequent itemset mining, we systematically extract\noperator knowledge from 185,236 real GEE scripts and syntax documentation,\nforming a structured knowledge base. Experimental results demonstrate that the\nframework achieves over 90% accuracy, recall, and F1 score in operator\nknowledge extraction. When integrated with the Retrieval-Augmented Generation\n(RAG) strategy for LLM-based geospatial code generation tasks, the knowledge\nbase improves performance by 20-30%. Ablation studies further quantify the\nnecessity of each knowledge table in the knowledge base construction. This work\nprovides robust support for the advancement and application of geospatial code\nmodeling techniques, offering an innovative approach to constructing\ndomain-specific knowledge bases that enhance the code generation capabilities\nof LLMs, and fostering the deeper integration of generative AI technologies\nwithin the field of geoinformatics."
                },
                "authors": [
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Jianyuan Liang"
                    },
                    {
                        "name": "Anqi Zhao"
                    },
                    {
                        "name": "Huayi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Huayi Wu"
                },
                "author": "Huayi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08389v1",
                "updated": "2024-12-11T13:56:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    56,
                    4,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T13:56:04Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    56,
                    4,
                    2,
                    346,
                    0
                ],
                "title": "SweetieChat: A Strategy-Enhanced Role-playing Framework for Diverse\n  Scenarios Handling Emotional Support Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SweetieChat: A Strategy-Enhanced Role-playing Framework for Diverse\n  Scenarios Handling Emotional Support Agent"
                },
                "summary": "Large Language Models (LLMs) have demonstrated promising potential in\nproviding empathetic support during interactions. However, their responses\noften become verbose or overly formulaic, failing to adequately address the\ndiverse emotional support needs of real-world scenarios. To tackle this\nchallenge, we propose an innovative strategy-enhanced role-playing framework,\ndesigned to simulate authentic emotional support conversations. Specifically,\nour approach unfolds in two steps: (1) Strategy-Enhanced Role-Playing\nInteractions, which involve three pivotal roles -- Seeker, Strategy Counselor,\nand Supporter -- engaging in diverse scenarios to emulate real-world\ninteractions and promote a broader range of dialogues; and (2) Emotional\nSupport Agent Training, achieved through fine-tuning LLMs using our specially\nconstructed dataset. Within this framework, we develop the \\textbf{ServeForEmo}\ndataset, comprising an extensive collection of 3.7K+ multi-turn dialogues and\n62.8K+ utterances. We further present \\textbf{SweetieChat}, an emotional\nsupport agent capable of handling diverse open-domain scenarios. Extensive\nexperiments and human evaluations confirm the framework's effectiveness in\nenhancing emotional support, highlighting its unique ability to provide more\nnuanced and tailored assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated promising potential in\nproviding empathetic support during interactions. However, their responses\noften become verbose or overly formulaic, failing to adequately address the\ndiverse emotional support needs of real-world scenarios. To tackle this\nchallenge, we propose an innovative strategy-enhanced role-playing framework,\ndesigned to simulate authentic emotional support conversations. Specifically,\nour approach unfolds in two steps: (1) Strategy-Enhanced Role-Playing\nInteractions, which involve three pivotal roles -- Seeker, Strategy Counselor,\nand Supporter -- engaging in diverse scenarios to emulate real-world\ninteractions and promote a broader range of dialogues; and (2) Emotional\nSupport Agent Training, achieved through fine-tuning LLMs using our specially\nconstructed dataset. Within this framework, we develop the \\textbf{ServeForEmo}\ndataset, comprising an extensive collection of 3.7K+ multi-turn dialogues and\n62.8K+ utterances. We further present \\textbf{SweetieChat}, an emotional\nsupport agent capable of handling diverse open-domain scenarios. Extensive\nexperiments and human evaluations confirm the framework's effectiveness in\nenhancing emotional support, highlighting its unique ability to provide more\nnuanced and tailored assistance."
                },
                "authors": [
                    {
                        "name": "Jing Ye"
                    },
                    {
                        "name": "Lu Xiang"
                    },
                    {
                        "name": "Yaping Zhang"
                    },
                    {
                        "name": "Chengqing Zong"
                    }
                ],
                "author_detail": {
                    "name": "Chengqing Zong"
                },
                "author": "Chengqing Zong",
                "arxiv_comment": "24 pages. Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08385v1",
                "updated": "2024-12-11T13:50:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    50,
                    17,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T13:50:17Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    50,
                    17,
                    2,
                    346,
                    0
                ],
                "title": "NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment\n  Prediction Dataset and Specialized Language Model for Enhanced Decision\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment\n  Prediction Dataset and Specialized Language Model for Enhanced Decision\n  Analysis"
                },
                "summary": "The integration of artificial intelligence (AI) in legal judgment prediction\n(LJP) has the potential to transform the legal landscape, particularly in\njurisdictions like India, where a significant backlog of cases burdens the\nlegal system. This paper introduces NyayaAnumana, the largest and most diverse\ncorpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945\npreprocessed cases. NyayaAnumana, which combines the words \"Nyay\" (judgment)\nand \"Anuman\" (prediction or inference) respectively for most major Indian\nlanguages, includes a wide range of cases from the Supreme Court, High Courts,\nTribunal Courts, District Courts, and Daily Orders and, thus, provides\nunparalleled diversity and coverage. Our dataset surpasses existing datasets\nlike PredEx and ILDC, offering a comprehensive foundation for advanced AI\nresearch in the legal domain.\n  In addition to the dataset, we present INLegalLlama, a domain-specific\ngenerative large language model (LLM) tailored to the intricacies of the Indian\nlegal system. It is developed through a two-phase training approach over a base\nLLaMa model. First, Indian legal documents are injected using continual\npretraining. Second, task-specific supervised finetuning is done. This method\nallows the model to achieve a deeper understanding of legal contexts.\n  Our experiments demonstrate that incorporating diverse court data\nsignificantly boosts model accuracy, achieving approximately 90% F1-score in\nprediction tasks. INLegalLlama not only improves prediction accuracy but also\noffers comprehensible explanations, addressing the need for explainability in\nAI-assisted legal decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of artificial intelligence (AI) in legal judgment prediction\n(LJP) has the potential to transform the legal landscape, particularly in\njurisdictions like India, where a significant backlog of cases burdens the\nlegal system. This paper introduces NyayaAnumana, the largest and most diverse\ncorpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945\npreprocessed cases. NyayaAnumana, which combines the words \"Nyay\" (judgment)\nand \"Anuman\" (prediction or inference) respectively for most major Indian\nlanguages, includes a wide range of cases from the Supreme Court, High Courts,\nTribunal Courts, District Courts, and Daily Orders and, thus, provides\nunparalleled diversity and coverage. Our dataset surpasses existing datasets\nlike PredEx and ILDC, offering a comprehensive foundation for advanced AI\nresearch in the legal domain.\n  In addition to the dataset, we present INLegalLlama, a domain-specific\ngenerative large language model (LLM) tailored to the intricacies of the Indian\nlegal system. It is developed through a two-phase training approach over a base\nLLaMa model. First, Indian legal documents are injected using continual\npretraining. Second, task-specific supervised finetuning is done. This method\nallows the model to achieve a deeper understanding of legal contexts.\n  Our experiments demonstrate that incorporating diverse court data\nsignificantly boosts model accuracy, achieving approximately 90% F1-score in\nprediction tasks. INLegalLlama not only improves prediction accuracy but also\noffers comprehensible explanations, addressing the need for explainability in\nAI-assisted legal decisions."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Shivam Mishra"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Accepted on COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08376v1",
                "updated": "2024-12-11T13:36:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    36,
                    18,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T13:36:18Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    36,
                    18,
                    2,
                    346,
                    0
                ],
                "title": "Reloc3r: Large-Scale Training of Relative Camera Pose Regression for\n  Generalizable, Fast, and Accurate Visual Localization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reloc3r: Large-Scale Training of Relative Camera Pose Regression for\n  Generalizable, Fast, and Accurate Visual Localization"
                },
                "summary": "Visual localization aims to determine the camera pose of a query image\nrelative to a database of posed images. In recent years, deep neural networks\nthat directly regress camera poses have gained popularity due to their fast\ninference capabilities. However, existing methods struggle to either generalize\nwell to new scenes or provide accurate camera pose estimates. To address these\nissues, we present \\textbf{Reloc3r}, a simple yet effective visual localization\nframework. It consists of an elegantly designed relative pose regression\nnetwork, and a minimalist motion averaging module for absolute pose estimation.\nTrained on approximately 8 million posed image pairs, Reloc3r achieves\nsurprisingly good performance and generalization ability. We conduct extensive\nexperiments on 6 public datasets, consistently demonstrating the effectiveness\nand efficiency of the proposed method. It provides high-quality camera pose\nestimates in real time and generalizes to novel scenes. Code, weights, and data\nat: \\url{https://github.com/ffrivera0/reloc3r}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual localization aims to determine the camera pose of a query image\nrelative to a database of posed images. In recent years, deep neural networks\nthat directly regress camera poses have gained popularity due to their fast\ninference capabilities. However, existing methods struggle to either generalize\nwell to new scenes or provide accurate camera pose estimates. To address these\nissues, we present \\textbf{Reloc3r}, a simple yet effective visual localization\nframework. It consists of an elegantly designed relative pose regression\nnetwork, and a minimalist motion averaging module for absolute pose estimation.\nTrained on approximately 8 million posed image pairs, Reloc3r achieves\nsurprisingly good performance and generalization ability. We conduct extensive\nexperiments on 6 public datasets, consistently demonstrating the effectiveness\nand efficiency of the proposed method. It provides high-quality camera pose\nestimates in real time and generalizes to novel scenes. Code, weights, and data\nat: \\url{https://github.com/ffrivera0/reloc3r}."
                },
                "authors": [
                    {
                        "name": "Siyan Dong"
                    },
                    {
                        "name": "Shuzhe Wang"
                    },
                    {
                        "name": "Shaohui Liu"
                    },
                    {
                        "name": "Lulu Cai"
                    },
                    {
                        "name": "Qingnan Fan"
                    },
                    {
                        "name": "Juho Kannala"
                    },
                    {
                        "name": "Yanchao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yanchao Yang"
                },
                "author": "Yanchao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04964v2",
                "updated": "2024-12-11T13:27:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    27,
                    0,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-06T11:29:32Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    29,
                    32,
                    4,
                    341,
                    0
                ],
                "title": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n  Large Language Model Inference"
                },
                "summary": "The ever-increasing sizes of large language models necessitate distributed\nsolutions for fast inference that exploit multi-dimensional parallelism, where\ncomputational loads are split across various accelerators such as GPU clusters.\nHowever, this approach often introduces significant communication overhead,\nespecially on devices with limited bandwidth. In this paper, we introduce Flash\nCommunication, a novel low-bit compression technique designed to alleviate the\ntensor-parallelism communication bottleneck during inference. Our method\nsubstantially boosts intra-node communication speed by more than 3x and reduces\nthe time-to-first-token by 2x, with nearly no sacrifice in model accuracy.\nExtensive experiments on various up-to-date LLMs demonstrate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing sizes of large language models necessitate distributed\nsolutions for fast inference that exploit multi-dimensional parallelism, where\ncomputational loads are split across various accelerators such as GPU clusters.\nHowever, this approach often introduces significant communication overhead,\nespecially on devices with limited bandwidth. In this paper, we introduce Flash\nCommunication, a novel low-bit compression technique designed to alleviate the\ntensor-parallelism communication bottleneck during inference. Our method\nsubstantially boosts intra-node communication speed by more than 3x and reduces\nthe time-to-first-token by 2x, with nearly no sacrifice in model accuracy.\nExtensive experiments on various up-to-date LLMs demonstrate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Qingyuan Li"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Liang Ye"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Yuchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Xie"
                },
                "author": "Yuchen Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08578v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08578v2",
                "updated": "2024-12-11T13:07:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    7,
                    45,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-16T07:24:19Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    7,
                    24,
                    19,
                    4,
                    229,
                    0
                ],
                "title": "TAMER: Tree-Aware Transformer for Handwritten Mathematical Expression\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TAMER: Tree-Aware Transformer for Handwritten Mathematical Expression\n  Recognition"
                },
                "summary": "Handwritten Mathematical Expression Recognition (HMER) has extensive\napplications in automated grading and office automation. However, existing\nsequence-based decoding methods, which directly predict $\\LaTeX$ sequences,\nstruggle to understand and model the inherent tree structure of $\\LaTeX$ and\noften fail to ensure syntactic correctness in the decoded results. To address\nthese challenges, we propose a novel model named TAMER (Tree-Aware Transformer)\nfor handwritten mathematical expression recognition. TAMER introduces an\ninnovative Tree-aware Module while maintaining the flexibility and efficient\ntraining of Transformer. TAMER combines the advantages of both sequence\ndecoding and tree decoding models by jointly optimizing sequence prediction and\ntree structure prediction tasks, which enhances the model's understanding and\ngeneralization of complex mathematical expression structures. During inference,\nTAMER employs a Tree Structure Prediction Scoring Mechanism to improve the\nstructural validity of the generated $\\LaTeX$ sequences. Experimental results\non CROHME datasets demonstrate that TAMER outperforms traditional sequence\ndecoding and tree decoding models, especially in handling complex mathematical\nstructures, achieving state-of-the-art (SOTA) performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Handwritten Mathematical Expression Recognition (HMER) has extensive\napplications in automated grading and office automation. However, existing\nsequence-based decoding methods, which directly predict $\\LaTeX$ sequences,\nstruggle to understand and model the inherent tree structure of $\\LaTeX$ and\noften fail to ensure syntactic correctness in the decoded results. To address\nthese challenges, we propose a novel model named TAMER (Tree-Aware Transformer)\nfor handwritten mathematical expression recognition. TAMER introduces an\ninnovative Tree-aware Module while maintaining the flexibility and efficient\ntraining of Transformer. TAMER combines the advantages of both sequence\ndecoding and tree decoding models by jointly optimizing sequence prediction and\ntree structure prediction tasks, which enhances the model's understanding and\ngeneralization of complex mathematical expression structures. During inference,\nTAMER employs a Tree Structure Prediction Scoring Mechanism to improve the\nstructural validity of the generated $\\LaTeX$ sequences. Experimental results\non CROHME datasets demonstrate that TAMER outperforms traditional sequence\ndecoding and tree decoding models, especially in handling complex mathematical\nstructures, achieving state-of-the-art (SOTA) performance."
                },
                "authors": [
                    {
                        "name": "Jianhua Zhu"
                    },
                    {
                        "name": "Wenqi Zhao"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Xingjian Hu"
                    },
                    {
                        "name": "Liangcai Gao"
                    }
                ],
                "author_detail": {
                    "name": "Liangcai Gao"
                },
                "author": "Liangcai Gao",
                "arxiv_journal_ref": "AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08578v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08578v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18353v2",
                "updated": "2024-12-11T13:02:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    2,
                    30,
                    2,
                    346,
                    0
                ],
                "published": "2024-04-29T01:24:14Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    1,
                    24,
                    14,
                    0,
                    120,
                    0
                ],
                "title": "How secure is AI-generated Code: A Large-Scale Comparison of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How secure is AI-generated Code: A Large-Scale Comparison of Large\n  Language Models"
                },
                "summary": "This study compares state-of-the-art Large Language Models (LLMs) on their\ntendency to generate vulnerabilities when writing C programs using a neutral\nzero-shot prompt. Tihanyi et al. introduced the FormAI dataset at PROMISE'23,\nfeaturing 112,000 C programs generated by GPT-3.5-turbo, with over 51.24%\nidentified as vulnerable. We extended that research with a large-scale study\ninvolving 9 state-of-the-art models such as OpenAI's GPT-4o-mini, Google's\nGemini Pro 1.0, TII's 180 billion-parameter Falcon, Meta's 13 billion-parameter\nCode Llama, and several other compact models. Additionally, we introduce the\nFormAI-v2 dataset, which comprises 331 000 compilable C programs generated by\nthese LLMs. Each program in the dataset is labeled based on the vulnerabilities\ndetected in its source code through formal verification, using the Efficient\nSMT-based Context-Bounded Model Checker (ESBMC). This technique minimizes false\npositives by providing a counterexample for the specific vulnerability and\nreduces false negatives by thoroughly completing the verification process. Our\nstudy reveals that at least 62.07% of the generated programs are vulnerable.\nThe differences between the models are minor, as they all show similar coding\nerrors with slight variations. Our research highlights that while LLMs offer\npromising capabilities for code generation, deploying their output in a\nproduction environment requires proper risk assessment and validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares state-of-the-art Large Language Models (LLMs) on their\ntendency to generate vulnerabilities when writing C programs using a neutral\nzero-shot prompt. Tihanyi et al. introduced the FormAI dataset at PROMISE'23,\nfeaturing 112,000 C programs generated by GPT-3.5-turbo, with over 51.24%\nidentified as vulnerable. We extended that research with a large-scale study\ninvolving 9 state-of-the-art models such as OpenAI's GPT-4o-mini, Google's\nGemini Pro 1.0, TII's 180 billion-parameter Falcon, Meta's 13 billion-parameter\nCode Llama, and several other compact models. Additionally, we introduce the\nFormAI-v2 dataset, which comprises 331 000 compilable C programs generated by\nthese LLMs. Each program in the dataset is labeled based on the vulnerabilities\ndetected in its source code through formal verification, using the Efficient\nSMT-based Context-Bounded Model Checker (ESBMC). This technique minimizes false\npositives by providing a counterexample for the specific vulnerability and\nreduces false negatives by thoroughly completing the verification process. Our\nstudy reveals that at least 62.07% of the generated programs are vulnerable.\nThe differences between the models are minor, as they all show similar coding\nerrors with slight variations. Our research highlights that while LLMs offer\npromising capabilities for code generation, deploying their output in a\nproduction environment requires proper risk assessment and validation."
                },
                "authors": [
                    {
                        "name": "Norbert Tihanyi"
                    },
                    {
                        "name": "Tamas Bisztray"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Ridhi Jain"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    }
                ],
                "author_detail": {
                    "name": "Lucas C. Cordeiro"
                },
                "author": "Lucas C. Cordeiro",
                "arxiv_doi": "10.1007/s10664-024-10590-1.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10664-024-10590-1.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.18353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and will be shortly published at Empirical Software\n  Engineering (EMSE). Journal Impact Factor: 3.5 (2023)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07169v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07169v2",
                "updated": "2024-12-11T12:50:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    50,
                    45,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-10T04:03:46Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    4,
                    3,
                    46,
                    1,
                    345,
                    0
                ],
                "title": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rate-In: Information-Driven Adaptive Dropout Rates for Improved\n  Inference-Time Uncertainty Estimation"
                },
                "summary": "Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate uncertainty estimation is crucial for deploying neural networks in\nrisk-sensitive applications such as medical diagnosis. Monte Carlo Dropout is a\nwidely used technique for approximating predictive uncertainty by performing\nstochastic forward passes with dropout during inference. However, using static\ndropout rates across all layers and inputs can lead to suboptimal uncertainty\nestimates, as it fails to adapt to the varying characteristics of individual\ninputs and network layers. Existing approaches optimize dropout rates during\ntraining using labeled data, resulting in fixed inference-time parameters that\ncannot adjust to new data distributions, compromising uncertainty estimates in\nMonte Carlo simulations.\n  In this paper, we propose Rate-In, an algorithm that dynamically adjusts\ndropout rates during inference by quantifying the information loss induced by\ndropout in each layer's feature maps. By treating dropout as controlled noise\ninjection and leveraging information-theoretic principles, Rate-In adapts\ndropout rates per layer and per input instance without requiring ground truth\nlabels. By quantifying the functional information loss in feature maps, we\nadaptively tune dropout rates to maintain perceptual quality across diverse\nmedical imaging tasks and architectural configurations. Our extensive empirical\nstudy on synthetic data and real-world medical imaging tasks demonstrates that\nRate-In improves calibration and sharpens uncertainty estimates compared to\nfixed or heuristic dropout rates without compromising predictive performance.\nRate-In offers a practical, unsupervised, inference-time approach to optimizing\ndropout for more reliable predictive uncertainty estimation in critical\napplications."
                },
                "authors": [
                    {
                        "name": "Tal Zeevi"
                    },
                    {
                        "name": "Ravid Shwartz-Ziv"
                    },
                    {
                        "name": "Yann LeCun"
                    },
                    {
                        "name": "Lawrence H. Staib"
                    },
                    {
                        "name": "John A. Onofrey"
                    }
                ],
                "author_detail": {
                    "name": "John A. Onofrey"
                },
                "author": "John A. Onofrey",
                "arxiv_comment": "Updated author affiliation",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07169v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07169v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07646v2",
                "updated": "2024-12-11T12:50:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    50,
                    3,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-10T16:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    32,
                    19,
                    1,
                    345,
                    0
                ],
                "title": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models"
                },
                "summary": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field."
                },
                "authors": [
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Tessa Verhoef"
                    }
                ],
                "author_detail": {
                    "name": "Tessa Verhoef"
                },
                "author": "Tessa Verhoef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06843v2",
                "updated": "2024-12-11T12:35:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    35,
                    25,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-07T16:35:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    35,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe\n  Responses in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe\n  Responses in LLMs"
                },
                "summary": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is\na significant issue in their applications. While various efforts aim to address\nthis safety concern, previous approaches often demand substantial human data\ncollection or rely on the less dependable option of using another LLM to\ngenerate corrective data. In this paper, we aim to take this problem and\novercome limitations of requiring significant high-quality human data. Our\nmethod requires only a small set of unsafe responses to toxic prompts, easily\nobtained from the unsafe LLM itself. By employing a semantic cost combined with\na negative Earth Mover Distance (EMD) loss, we guide the LLM away from\ngenerating unsafe responses. Additionally, we propose a novel lower bound for\nEMD loss, enabling more efficient optimization. Our results demonstrate\nsuperior performance and data efficiency compared to baselines, and we further\nexamine the nuanced effects of over-alignment and potential degradation of\nlanguage capabilities when using contrastive data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is\na significant issue in their applications. While various efforts aim to address\nthis safety concern, previous approaches often demand substantial human data\ncollection or rely on the less dependable option of using another LLM to\ngenerate corrective data. In this paper, we aim to take this problem and\novercome limitations of requiring significant high-quality human data. Our\nmethod requires only a small set of unsafe responses to toxic prompts, easily\nobtained from the unsafe LLM itself. By employing a semantic cost combined with\na negative Earth Mover Distance (EMD) loss, we guide the LLM away from\ngenerating unsafe responses. Additionally, we propose a novel lower bound for\nEMD loss, enabling more efficient optimization. Our results demonstrate\nsuperior performance and data efficiency compared to baselines, and we further\nexamine the nuanced effects of over-alignment and potential degradation of\nlanguage capabilities when using contrastive data."
                },
                "authors": [
                    {
                        "name": "Yuxiao Lu"
                    },
                    {
                        "name": "Arunesh Sinha"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Varakantham"
                },
                "author": "Pradeep Varakantham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08341v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08341v1",
                "updated": "2024-12-11T12:31:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    31,
                    30,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T12:31:30Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    31,
                    30,
                    2,
                    346,
                    0
                ],
                "title": "ALoRE: Efficient Visual Adaptation via Aggregating Low Rank Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ALoRE: Efficient Visual Adaptation via Aggregating Low Rank Experts"
                },
                "summary": "Parameter-efficient transfer learning (PETL) has become a promising paradigm\nfor adapting large-scale vision foundation models to downstream tasks. Typical\nmethods primarily leverage the intrinsic low rank property to make\ndecomposition, learning task-specific weights while compressing parameter size.\nHowever, such approaches predominantly manipulate within the original feature\nspace utilizing a single-branch structure, which might be suboptimal for\ndecoupling the learned representations and patterns. In this paper, we propose\nALoRE, a novel PETL method that reuses the hypercomplex parameterized space\nconstructed by Kronecker product to Aggregate Low Rank Experts using a\nmulti-branch paradigm, disentangling the learned cognitive patterns during\ntraining. Thanks to the artful design, ALoRE maintains negligible extra\nparameters and can be effortlessly merged into the frozen backbone via\nre-parameterization in a sequential manner, avoiding additional inference\nlatency. We conduct extensive experiments on 24 image classification tasks\nusing various backbone variants. Experimental results demonstrate that ALoRE\noutperforms the full fine-tuning strategy and other state-of-the-art PETL\nmethods in terms of performance and parameter efficiency. For instance, ALoRE\nobtains 3.06% and 9.97% Top-1 accuracy improvement on average compared to full\nfine-tuning on the FGVC datasets and VTAB-1k benchmark by only updating 0.15M\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter-efficient transfer learning (PETL) has become a promising paradigm\nfor adapting large-scale vision foundation models to downstream tasks. Typical\nmethods primarily leverage the intrinsic low rank property to make\ndecomposition, learning task-specific weights while compressing parameter size.\nHowever, such approaches predominantly manipulate within the original feature\nspace utilizing a single-branch structure, which might be suboptimal for\ndecoupling the learned representations and patterns. In this paper, we propose\nALoRE, a novel PETL method that reuses the hypercomplex parameterized space\nconstructed by Kronecker product to Aggregate Low Rank Experts using a\nmulti-branch paradigm, disentangling the learned cognitive patterns during\ntraining. Thanks to the artful design, ALoRE maintains negligible extra\nparameters and can be effortlessly merged into the frozen backbone via\nre-parameterization in a sequential manner, avoiding additional inference\nlatency. We conduct extensive experiments on 24 image classification tasks\nusing various backbone variants. Experimental results demonstrate that ALoRE\noutperforms the full fine-tuning strategy and other state-of-the-art PETL\nmethods in terms of performance and parameter efficiency. For instance, ALoRE\nobtains 3.06% and 9.97% Top-1 accuracy improvement on average compared to full\nfine-tuning on the FGVC datasets and VTAB-1k benchmark by only updating 0.15M\nparameters."
                },
                "authors": [
                    {
                        "name": "Sinan Du"
                    },
                    {
                        "name": "Guosheng Zhang"
                    },
                    {
                        "name": "Keyao Wang"
                    },
                    {
                        "name": "Yuanrui Wang"
                    },
                    {
                        "name": "Haixiao Yue"
                    },
                    {
                        "name": "Gang Zhang"
                    },
                    {
                        "name": "Errui Ding"
                    },
                    {
                        "name": "Jingdong Wang"
                    },
                    {
                        "name": "Zhengzhuo Xu"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "arxiv_comment": "23 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08341v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08341v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08331v1",
                "updated": "2024-12-11T12:18:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    18,
                    30,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T12:18:30Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    18,
                    30,
                    2,
                    346,
                    0
                ],
                "title": "SLGaussian: Fast Language Gaussian Splatting in Sparse Views",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SLGaussian: Fast Language Gaussian Splatting in Sparse Views"
                },
                "summary": "3D semantic field learning is crucial for applications like autonomous\nnavigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from\nlimited viewpoints is essential. Existing methods struggle under sparse view\nconditions, relying on inefficient per-scene multi-view optimizations, which\nare impractical for many real-world tasks. To address this, we propose\nSLGaussian, a feed-forward method for constructing 3D semantic fields from\nsparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring\nconsistent SAM segmentations through video tracking and using low-dimensional\nindexing for high-dimensional CLIP features, SLGaussian efficiently embeds\nlanguage information in 3D space, offering a robust solution for accurate 3D\nscene understanding under sparse view conditions. In experiments on two-view\nsparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,\nSLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,\nand mIoU. Moreover, our model achieves scene inference in under 30 seconds and\nopen-vocabulary querying in just 0.011 seconds per query.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D semantic field learning is crucial for applications like autonomous\nnavigation, AR/VR, and robotics, where accurate comprehension of 3D scenes from\nlimited viewpoints is essential. Existing methods struggle under sparse view\nconditions, relying on inefficient per-scene multi-view optimizations, which\nare impractical for many real-world tasks. To address this, we propose\nSLGaussian, a feed-forward method for constructing 3D semantic fields from\nsparse viewpoints, allowing direct inference of 3DGS-based scenes. By ensuring\nconsistent SAM segmentations through video tracking and using low-dimensional\nindexing for high-dimensional CLIP features, SLGaussian efficiently embeds\nlanguage information in 3D space, offering a robust solution for accurate 3D\nscene understanding under sparse view conditions. In experiments on two-view\nsparse 3D object querying and segmentation in the LERF and 3D-OVS datasets,\nSLGaussian outperforms existing methods in chosen IoU, Localization Accuracy,\nand mIoU. Moreover, our model achieves scene inference in under 30 seconds and\nopen-vocabulary querying in just 0.011 seconds per query."
                },
                "authors": [
                    {
                        "name": "Kangjie Chen"
                    },
                    {
                        "name": "BingQuan Dai"
                    },
                    {
                        "name": "Minghan Qin"
                    },
                    {
                        "name": "Dongbin Zhang"
                    },
                    {
                        "name": "Peihao Li"
                    },
                    {
                        "name": "Yingshuang Zou"
                    },
                    {
                        "name": "Haoqian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoqian Wang"
                },
                "author": "Haoqian Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00380v3",
                "updated": "2024-12-11T11:52:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    52,
                    58,
                    2,
                    346,
                    0
                ],
                "published": "2024-06-01T09:36:16Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    9,
                    36,
                    16,
                    5,
                    153,
                    0
                ],
                "title": "HonestLLM: Toward an Honest and Helpful Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HonestLLM: Toward an Honest and Helpful Large Language Model"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Zhengyan Fu"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08315v1",
                "updated": "2024-12-11T11:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    52,
                    16,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T11:52:16Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    52,
                    16,
                    2,
                    346,
                    0
                ],
                "title": "Lightweight Method for Interactive 3D Medical Image Segmentation with\n  Multi-Round Result Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Method for Interactive 3D Medical Image Segmentation with\n  Multi-Round Result Fusion"
                },
                "summary": "In medical imaging, precise annotation of lesions or organs is often\nrequired. However, 3D volumetric images typically consist of hundreds or\nthousands of slices, making the annotation process extremely time-consuming and\nlaborious. Recently, the Segment Anything Model (SAM) has drawn widespread\nattention due to its remarkable zero-shot generalization capabilities in\ninteractive segmentation. While researchers have explored adapting SAM for\nmedical applications, such as using SAM adapters or constructing 3D SAM models,\na key question remains: Can traditional CNN networks achieve the same strong\nzero-shot generalization in this task? In this paper, we propose the\nLightweight Interactive Network for 3D Medical Image Segmentation (LIM-Net), a\nnovel approach demonstrating the potential of compact CNN-based models. Built\nupon a 2D CNN backbone, LIM-Net initiates segmentation by generating a 2D\nprompt mask from user hints. This mask is then propagated through the 3D\nsequence via the Memory Module. To refine and stabilize results during\ninteraction, the Multi-Round Result Fusion (MRF) Module selects and merges\noptimal masks from multiple rounds. Our extensive experiments across multiple\ndatasets and modalities demonstrate LIM-Net's competitive performance. It\nexhibits stronger generalization to unseen data compared to SAM-based models,\nwith competitive accuracy while requiring fewer interactions. Notably,\nLIM-Net's lightweight design offers significant advantages in deployment and\ninference efficiency, with low GPU memory consumption suitable for\nresource-constrained environments. These promising results demonstrate LIM-Net\ncan serve as a strong baseline, complementing and contrasting with popular SAM\nmodels to further boost effective interactive medical image segmentation. The\ncode will be released at \\url{https://github.com/goodtime-123/LIM-Net}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical imaging, precise annotation of lesions or organs is often\nrequired. However, 3D volumetric images typically consist of hundreds or\nthousands of slices, making the annotation process extremely time-consuming and\nlaborious. Recently, the Segment Anything Model (SAM) has drawn widespread\nattention due to its remarkable zero-shot generalization capabilities in\ninteractive segmentation. While researchers have explored adapting SAM for\nmedical applications, such as using SAM adapters or constructing 3D SAM models,\na key question remains: Can traditional CNN networks achieve the same strong\nzero-shot generalization in this task? In this paper, we propose the\nLightweight Interactive Network for 3D Medical Image Segmentation (LIM-Net), a\nnovel approach demonstrating the potential of compact CNN-based models. Built\nupon a 2D CNN backbone, LIM-Net initiates segmentation by generating a 2D\nprompt mask from user hints. This mask is then propagated through the 3D\nsequence via the Memory Module. To refine and stabilize results during\ninteraction, the Multi-Round Result Fusion (MRF) Module selects and merges\noptimal masks from multiple rounds. Our extensive experiments across multiple\ndatasets and modalities demonstrate LIM-Net's competitive performance. It\nexhibits stronger generalization to unseen data compared to SAM-based models,\nwith competitive accuracy while requiring fewer interactions. Notably,\nLIM-Net's lightweight design offers significant advantages in deployment and\ninference efficiency, with low GPU memory consumption suitable for\nresource-constrained environments. These promising results demonstrate LIM-Net\ncan serve as a strong baseline, complementing and contrasting with popular SAM\nmodels to further boost effective interactive medical image segmentation. The\ncode will be released at \\url{https://github.com/goodtime-123/LIM-Net}."
                },
                "authors": [
                    {
                        "name": "Bingzhi Shen"
                    },
                    {
                        "name": "Lufan Chang"
                    },
                    {
                        "name": "Siqi Chen"
                    },
                    {
                        "name": "Shuxiang Guo"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08310v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08310v1",
                "updated": "2024-12-11T11:44:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    44,
                    55,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T11:44:55Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    44,
                    55,
                    2,
                    346,
                    0
                ],
                "title": "Edge-Splitting MLP: Node Classification on Homophilic and Heterophilic\n  Graphs without Message Passing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge-Splitting MLP: Node Classification on Homophilic and Heterophilic\n  Graphs without Message Passing"
                },
                "summary": "Message Passing Neural Networks (MPNNs) have demonstrated remarkable success\nin node classification on homophilic graphs. It has been shown that they do not\nsolely rely on homophily but on neighborhood distributions of nodes, i.e.,\nconsistency of the neighborhood label distribution within the same class.\nMLP-based models do not use message passing, \\eg Graph-MLP incorporates the\nneighborhood in a separate loss function. These models are faster and more\nrobust to edge noise. Graph-MLP maps adjacent nodes closer in the embedding\nspace but is unaware of the neighborhood pattern of the labels, i.e., relies\nsolely on homophily. Edge Splitting GNN (ES-GNN) is a model specialized for\nheterophilic graphs and splits the edges into task-relevant and\ntask-irrelevant, respectively. To mitigate the limitations of Graph-MLP on\nheterophilic graphs, we propose ES-MLP that combines Graph-MLP with an\nedge-splitting mechanism from ES-GNN. It incorporates the edge splitting into\nthe loss of Graph-MLP to learn two separate adjacency matrices based on\nrelevant and irrelevant feature pairs. Our experiments on seven datasets with\nsix baselines show that ES-MLP is on par with homophilic and heterophilic\nmodels on all datasets without using edges during inference. We show that\nES-MLP is robust to multiple types of edge noise during inference and that its\ninference time is two to five times faster than that of commonly used MPNNs.\nThe source code is available at https://github.com/MatthiasKohn/ES-MLP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Neural Networks (MPNNs) have demonstrated remarkable success\nin node classification on homophilic graphs. It has been shown that they do not\nsolely rely on homophily but on neighborhood distributions of nodes, i.e.,\nconsistency of the neighborhood label distribution within the same class.\nMLP-based models do not use message passing, \\eg Graph-MLP incorporates the\nneighborhood in a separate loss function. These models are faster and more\nrobust to edge noise. Graph-MLP maps adjacent nodes closer in the embedding\nspace but is unaware of the neighborhood pattern of the labels, i.e., relies\nsolely on homophily. Edge Splitting GNN (ES-GNN) is a model specialized for\nheterophilic graphs and splits the edges into task-relevant and\ntask-irrelevant, respectively. To mitigate the limitations of Graph-MLP on\nheterophilic graphs, we propose ES-MLP that combines Graph-MLP with an\nedge-splitting mechanism from ES-GNN. It incorporates the edge splitting into\nthe loss of Graph-MLP to learn two separate adjacency matrices based on\nrelevant and irrelevant feature pairs. Our experiments on seven datasets with\nsix baselines show that ES-MLP is on par with homophilic and heterophilic\nmodels on all datasets without using edges during inference. We show that\nES-MLP is robust to multiple types of edge noise during inference and that its\ninference time is two to five times faster than that of commonly used MPNNs.\nThe source code is available at https://github.com/MatthiasKohn/ES-MLP."
                },
                "authors": [
                    {
                        "name": "Matthias Kohn"
                    },
                    {
                        "name": "Marcel Hoffmann"
                    },
                    {
                        "name": "Ansgar Scherp"
                    }
                ],
                "author_detail": {
                    "name": "Ansgar Scherp"
                },
                "author": "Ansgar Scherp",
                "arxiv_comment": "Published at Learning on Graphs, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08310v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08310v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v2",
                "updated": "2024-12-11T11:39:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    39,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Efficient Large Foundation Model Inference: A Perspective From Model and\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Large Foundation Model Inference: A Perspective From Model and\n  System Co-Design"
                },
                "summary": "As Large Language Models (LLMs) become popular, the need for efficient design\nfor ML models on LLMs grows. We are amazed by the excellent output by the LLMs,\nyet we are still troubled with slow inference speed and large memory\nconsumption of contemporary LLMs. This paper focuses on modern efficient\ninference technologies on LLMs and illustrates them from two perspectives:\nmodel and system design. These methodologies optimize LLM inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become popular, the need for efficient design\nfor ML models on LLMs grows. We are amazed by the excellent output by the LLMs,\nyet we are still troubled with slow inference speed and large memory\nconsumption of contemporary LLMs. This paper focuses on modern efficient\ninference technologies on LLMs and illustrates them from two perspectives:\nmodel and system design. These methodologies optimize LLM inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Zhixin Lai"
                    },
                    {
                        "name": "Yite Wang"
                    },
                    {
                        "name": "Jing Wu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Benjamin Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02232v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02232v4",
                "updated": "2024-12-11T11:18:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    18,
                    54,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-05T04:53:01Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    4,
                    53,
                    1,
                    0,
                    218,
                    0
                ],
                "title": "SpecRover: Code Intent Extraction via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRover: Code Intent Extraction via LLMs"
                },
                "summary": "Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era."
                },
                "authors": [
                    {
                        "name": "Haifeng Ruan"
                    },
                    {
                        "name": "Yuntong Zhang"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "arxiv_comment": "Haifeng Ruan and Yuntong Zhang contributed equally to this work. To\n  appear in ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02232v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02232v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08292v1",
                "updated": "2024-12-11T11:08:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    8,
                    9,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T11:08:09Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    8,
                    9,
                    2,
                    346,
                    0
                ],
                "title": "Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal\n  Iterations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Refining Diffusion Samplers: Enabling Parallelization via Parareal\n  Iterations"
                },
                "summary": "In diffusion models, samples are generated through an iterative refinement\nprocess, requiring hundreds of sequential model evaluations. Several recent\nmethods have introduced approximations (fewer discretization steps or\ndistillation) to trade off speed at the cost of sample quality. In contrast, we\nintroduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality\nand can improve latency at the cost of additional parallel compute. We take\ninspiration from the Parareal algorithm, a popular numerical method for\nparallel-in-time integration of differential equations. In SRDS, a quick but\nrough estimate of a sample is first created and then iteratively refined in\nparallel through Parareal iterations. SRDS is not only guaranteed to accurately\nsolve the ODE and converge to the serial solution but also benefits from\nparallelization across the diffusion trajectory, enabling batched inference and\npipelining. As we demonstrate for pre-trained diffusion models, the early\nconvergence of this refinement procedure drastically reduces the number of\nsteps required to produce a sample, speeding up generation for instance by up\nto 1.7x on a 25-step StableDiffusion-v2 benchmark and up to 4.3x on longer\ntrajectories.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In diffusion models, samples are generated through an iterative refinement\nprocess, requiring hundreds of sequential model evaluations. Several recent\nmethods have introduced approximations (fewer discretization steps or\ndistillation) to trade off speed at the cost of sample quality. In contrast, we\nintroduce Self-Refining Diffusion Samplers (SRDS) that retain sample quality\nand can improve latency at the cost of additional parallel compute. We take\ninspiration from the Parareal algorithm, a popular numerical method for\nparallel-in-time integration of differential equations. In SRDS, a quick but\nrough estimate of a sample is first created and then iteratively refined in\nparallel through Parareal iterations. SRDS is not only guaranteed to accurately\nsolve the ODE and converge to the serial solution but also benefits from\nparallelization across the diffusion trajectory, enabling batched inference and\npipelining. As we demonstrate for pre-trained diffusion models, the early\nconvergence of this refinement procedure drastically reduces the number of\nsteps required to produce a sample, speeding up generation for instance by up\nto 1.7x on a 25-step StableDiffusion-v2 benchmark and up to 4.3x on longer\ntrajectories."
                },
                "authors": [
                    {
                        "name": "Nikil Roashan Selvam"
                    },
                    {
                        "name": "Amil Merchant"
                    },
                    {
                        "name": "Stefano Ermon"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Ermon"
                },
                "author": "Stefano Ermon",
                "arxiv_comment": "NeurIPS 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08291v1",
                "updated": "2024-12-11T11:07:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    7,
                    50,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T11:07:50Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    7,
                    50,
                    2,
                    346,
                    0
                ],
                "title": "Code LLMs: A Taxonomy-based Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code LLMs: A Taxonomy-based Survey"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks and have recently expanded their impact to coding tasks,\nbridging the gap between natural languages (NL) and programming languages (PL).\nThis taxonomy-based survey provides a comprehensive analysis of LLMs in the\nNL-PL domain, investigating how these models are utilized in coding tasks and\nexamining their methodologies, architectures, and training processes. We\npropose a taxonomy-based framework that categorizes relevant concepts,\nproviding a unified classification system to facilitate a deeper understanding\nof this rapidly evolving field. This survey offers insights into the current\nstate and future directions of LLMs in coding tasks, including their\napplications and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks and have recently expanded their impact to coding tasks,\nbridging the gap between natural languages (NL) and programming languages (PL).\nThis taxonomy-based survey provides a comprehensive analysis of LLMs in the\nNL-PL domain, investigating how these models are utilized in coding tasks and\nexamining their methodologies, architectures, and training processes. We\npropose a taxonomy-based framework that categorizes relevant concepts,\nproviding a unified classification system to facilitate a deeper understanding\nof this rapidly evolving field. This survey offers insights into the current\nstate and future directions of LLMs in coding tasks, including their\napplications and limitations."
                },
                "authors": [
                    {
                        "name": "Nishat Raihan"
                    },
                    {
                        "name": "Christian Newman"
                    },
                    {
                        "name": "Marcos Zampieri"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Zampieri"
                },
                "author": "Marcos Zampieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17905v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17905v2",
                "updated": "2024-12-11T11:07:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    7,
                    2,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-25T09:51:09Z",
                "published_parsed": [
                    2024,
                    7,
                    25,
                    9,
                    51,
                    9,
                    3,
                    207,
                    0
                ],
                "title": "StreamMOS: Streaming Moving Object Segmentation with Multi-View\n  Perception and Dual-Span Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamMOS: Streaming Moving Object Segmentation with Multi-View\n  Perception and Dual-Span Memory"
                },
                "summary": "Moving object segmentation based on LiDAR is a crucial and challenging task\nfor autonomous driving and mobile robotics. Most approaches explore\nspatio-temporal information from LiDAR sequences to predict moving objects in\nthe current frame. However, they often focus on transferring temporal cues in a\nsingle inference and regard every prediction as independent of others. This may\ncause inconsistent segmentation results for the same object in different\nframes. To overcome this issue, we propose a streaming network with a memory\nmechanism, called StreamMOS, to build the association of features and\npredictions among multiple inferences. Specifically, we utilize a short-term\nmemory to convey historical features, which can be regarded as spatial prior of\nmoving objects and adopted to enhance current inference by temporal fusion.\nMeanwhile, we build a long-term memory to store previous predictions and\nexploit them to refine the present forecast at voxel and instance levels\nthrough voting. Besides, we present multi-view encoder with cascade projection\nand asymmetric convolution to extract motion feature of objects in different\nrepresentations. Extensive experiments validate that our algorithm gets\ncompetitive performance on SemanticKITTI and Sipailou Campus datasets. Code\nwill be released at https://github.com/NEU-REAL/StreamMOS.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moving object segmentation based on LiDAR is a crucial and challenging task\nfor autonomous driving and mobile robotics. Most approaches explore\nspatio-temporal information from LiDAR sequences to predict moving objects in\nthe current frame. However, they often focus on transferring temporal cues in a\nsingle inference and regard every prediction as independent of others. This may\ncause inconsistent segmentation results for the same object in different\nframes. To overcome this issue, we propose a streaming network with a memory\nmechanism, called StreamMOS, to build the association of features and\npredictions among multiple inferences. Specifically, we utilize a short-term\nmemory to convey historical features, which can be regarded as spatial prior of\nmoving objects and adopted to enhance current inference by temporal fusion.\nMeanwhile, we build a long-term memory to store previous predictions and\nexploit them to refine the present forecast at voxel and instance levels\nthrough voting. Besides, we present multi-view encoder with cascade projection\nand asymmetric convolution to extract motion feature of objects in different\nrepresentations. Extensive experiments validate that our algorithm gets\ncompetitive performance on SemanticKITTI and Sipailou Campus datasets. Code\nwill be released at https://github.com/NEU-REAL/StreamMOS.git."
                },
                "authors": [
                    {
                        "name": "Zhiheng Li"
                    },
                    {
                        "name": "Yubo Cui"
                    },
                    {
                        "name": "Jiexi Zhong"
                    },
                    {
                        "name": "Zheng Fang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Fang"
                },
                "author": "Zheng Fang",
                "arxiv_comment": "Accepted for publication at IEEE Robotics and Automation Letters\n  (RAL)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17905v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17905v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.00343v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.00343v8",
                "updated": "2024-12-11T11:04:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    4,
                    22,
                    2,
                    346,
                    0
                ],
                "published": "2023-12-01T04:35:47Z",
                "published_parsed": [
                    2023,
                    12,
                    1,
                    4,
                    35,
                    47,
                    4,
                    335,
                    0
                ],
                "title": "OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong\n  Baseline",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenStereo: A Comprehensive Benchmark for Stereo Matching and Strong\n  Baseline"
                },
                "summary": "Stereo matching aims to estimate the disparity between matching pixels in a\nstereo image pair, which is important to robotics, autonomous driving, and\nother computer vision tasks. Despite the development of numerous impressive\nmethods in recent years, determining the most suitable architecture for\npractical application remains challenging. Addressing this gap, our paper\nintroduces a comprehensive benchmark focusing on practical applicability rather\nthan solely on individual models for optimized performance. Specifically, we\ndevelop a flexible and efficient stereo matching codebase, called OpenStereo.\nOpenStereo includes training and inference codes of more than 10 network\nmodels, making it, to our knowledge, the most complete stereo matching toolbox\navailable. Based on OpenStereo, we conducted experiments and have achieved or\nsurpassed the performance metrics reported in the original paper. Additionally,\nwe conduct an exhaustive analysis and deconstruction of recent developments in\nstereo matching through comprehensive ablative experiments. These\ninvestigations inspired the creation of StereoBase, a strong baseline model.\nOur StereoBase ranks 1st on SceneFlow, KITTI 2015, 2012 (Reflective) among\npublished methods and achieves the best performance across all metrics. In\naddition, StereoBase has strong cross-dataset generalization. Code is available\nat \\url{https://github.com/XiandaGuo/OpenStereo}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stereo matching aims to estimate the disparity between matching pixels in a\nstereo image pair, which is important to robotics, autonomous driving, and\nother computer vision tasks. Despite the development of numerous impressive\nmethods in recent years, determining the most suitable architecture for\npractical application remains challenging. Addressing this gap, our paper\nintroduces a comprehensive benchmark focusing on practical applicability rather\nthan solely on individual models for optimized performance. Specifically, we\ndevelop a flexible and efficient stereo matching codebase, called OpenStereo.\nOpenStereo includes training and inference codes of more than 10 network\nmodels, making it, to our knowledge, the most complete stereo matching toolbox\navailable. Based on OpenStereo, we conducted experiments and have achieved or\nsurpassed the performance metrics reported in the original paper. Additionally,\nwe conduct an exhaustive analysis and deconstruction of recent developments in\nstereo matching through comprehensive ablative experiments. These\ninvestigations inspired the creation of StereoBase, a strong baseline model.\nOur StereoBase ranks 1st on SceneFlow, KITTI 2015, 2012 (Reflective) among\npublished methods and achieves the best performance across all metrics. In\naddition, StereoBase has strong cross-dataset generalization. Code is available\nat \\url{https://github.com/XiandaGuo/OpenStereo}."
                },
                "authors": [
                    {
                        "name": "Xianda Guo"
                    },
                    {
                        "name": "Chenming Zhang"
                    },
                    {
                        "name": "Juntao Lu"
                    },
                    {
                        "name": "Yiqun Duan"
                    },
                    {
                        "name": "Yiqi Wang"
                    },
                    {
                        "name": "Tian Yang"
                    },
                    {
                        "name": "Zheng Zhu"
                    },
                    {
                        "name": "Long Chen"
                    }
                ],
                "author_detail": {
                    "name": "Long Chen"
                },
                "author": "Long Chen",
                "arxiv_comment": "Code is available at: https://github.com/XiandaGuo/OpenStereo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.00343v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.00343v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08284v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08284v1",
                "updated": "2024-12-11T10:59:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    59,
                    5,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:59:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    59,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "Collaborative Inference for Large Models with Task Offloading and Early\n  Exiting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Inference for Large Models with Task Offloading and Early\n  Exiting"
                },
                "summary": "In 5G smart cities, edge computing is employed to provide nearby computing\nservices for end devices, and the large-scale models (e.g., GPT and LLaMA) can\nbe deployed at the network edge to boost the service quality. However, due to\nthe constraints of memory size and computing capacity, it is difficult to run\nthese large-scale models on a single edge node. To meet the resource\nconstraints, a large-scale model can be partitioned into multiple sub-models\nand deployed across multiple edge nodes. Then tasks are offloaded to the edge\nnodes for collaborative inference. Additionally, we incorporate the early exit\nmechanism to further accelerate inference. However, the heterogeneous system\nand dynamic environment will significantly affect the inference efficiency. To\naddress these challenges, we theoretically analyze the coupled relationship\nbetween task offloading strategy and confidence thresholds, and develop a\ndistributed algorithm, termed DTO-EE, based on the coupled relationship and\nconvex optimization. DTO-EE enables each edge node to jointly optimize its\noffloading strategy and the confidence threshold, so as to achieve a promising\ntrade-off between response delay and inference accuracy. The experimental\nresults show that DTO-EE can reduce the average response delay by 21%-41% and\nimprove the inference accuracy by 1%-4%, compared to the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In 5G smart cities, edge computing is employed to provide nearby computing\nservices for end devices, and the large-scale models (e.g., GPT and LLaMA) can\nbe deployed at the network edge to boost the service quality. However, due to\nthe constraints of memory size and computing capacity, it is difficult to run\nthese large-scale models on a single edge node. To meet the resource\nconstraints, a large-scale model can be partitioned into multiple sub-models\nand deployed across multiple edge nodes. Then tasks are offloaded to the edge\nnodes for collaborative inference. Additionally, we incorporate the early exit\nmechanism to further accelerate inference. However, the heterogeneous system\nand dynamic environment will significantly affect the inference efficiency. To\naddress these challenges, we theoretically analyze the coupled relationship\nbetween task offloading strategy and confidence thresholds, and develop a\ndistributed algorithm, termed DTO-EE, based on the coupled relationship and\nconvex optimization. DTO-EE enables each edge node to jointly optimize its\noffloading strategy and the confidence threshold, so as to achieve a promising\ntrade-off between response delay and inference accuracy. The experimental\nresults show that DTO-EE can reduce the average response delay by 21%-41% and\nimprove the inference accuracy by 1%-4%, compared to the baselines."
                },
                "authors": [
                    {
                        "name": "Zuan Xie"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Yunming Liao"
                    },
                    {
                        "name": "Zhiyuan Yao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Yao"
                },
                "author": "Zhiyuan Yao",
                "arxiv_comment": "9 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08284v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08284v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08283v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08283v1",
                "updated": "2024-12-11T10:58:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    58,
                    14,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:58:14Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    58,
                    14,
                    2,
                    346,
                    0
                ],
                "title": "A Preliminary Analysis of Automatic Word and Syllable Prominence\n  Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Preliminary Analysis of Automatic Word and Syllable Prominence\n  Detection in Non-Native Speech With Text-to-Speech Prosody Embeddings"
                },
                "summary": "Automatic detection of prominence at the word and syllable-levels is critical\nfor building computer-assisted language learning systems. It has been shown\nthat prosody embeddings learned by the current state-of-the-art (SOTA)\ntext-to-speech (TTS) systems could generate word- and syllable-level prominence\nin the synthesized speech as natural as in native speech. To understand the\neffectiveness of prosody embeddings from TTS for prominence detection under\nnonnative context, a comparative analysis is conducted on the embeddings\nextracted from native and non-native speech considering the prominence-related\nembeddings: duration, energy, and pitch from a SOTA TTS named FastSpeech2.\nThese embeddings are extracted under two conditions considering: 1) only text,\n2) both speech and text. For the first condition, the embeddings are extracted\ndirectly from the TTS inference mode, whereas for the second condition, we\npropose to extract from the TTS under training mode. Experiments are conducted\non native speech corpus: Tatoeba, and non-native speech corpus: ISLE. For\nexperimentation, word-level prominence locations are manually annotated for\nboth corpora. The highest relative improvement on word \\& syllable-level\nprominence detection accuracies with the TTS embeddings are found to be 13.7% &\n5.9% and 16.2% & 6.9% compared to those with the heuristic-based features and\nself-supervised Wav2Vec-2.0 representations, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic detection of prominence at the word and syllable-levels is critical\nfor building computer-assisted language learning systems. It has been shown\nthat prosody embeddings learned by the current state-of-the-art (SOTA)\ntext-to-speech (TTS) systems could generate word- and syllable-level prominence\nin the synthesized speech as natural as in native speech. To understand the\neffectiveness of prosody embeddings from TTS for prominence detection under\nnonnative context, a comparative analysis is conducted on the embeddings\nextracted from native and non-native speech considering the prominence-related\nembeddings: duration, energy, and pitch from a SOTA TTS named FastSpeech2.\nThese embeddings are extracted under two conditions considering: 1) only text,\n2) both speech and text. For the first condition, the embeddings are extracted\ndirectly from the TTS inference mode, whereas for the second condition, we\npropose to extract from the TTS under training mode. Experiments are conducted\non native speech corpus: Tatoeba, and non-native speech corpus: ISLE. For\nexperimentation, word-level prominence locations are manually annotated for\nboth corpora. The highest relative improvement on word \\& syllable-level\nprominence detection accuracies with the TTS embeddings are found to be 13.7% &\n5.9% and 16.2% & 6.9% compared to those with the heuristic-based features and\nself-supervised Wav2Vec-2.0 representations, respectively."
                },
                "authors": [
                    {
                        "name": "Anindita Mondal"
                    },
                    {
                        "name": "Rangavajjala Sankara Bharadwaj"
                    },
                    {
                        "name": "Jhansi Mallela"
                    },
                    {
                        "name": "Anil Kumar Vuppala"
                    },
                    {
                        "name": "Chiranjeevi Yarra"
                    }
                ],
                "author_detail": {
                    "name": "Chiranjeevi Yarra"
                },
                "author": "Chiranjeevi Yarra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08283v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08283v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08281v1",
                "updated": "2024-12-11T10:56:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    56,
                    47,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:56:47Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    56,
                    47,
                    2,
                    346,
                    0
                ],
                "title": "Lachesis: Predicting LLM Inference Accuracy using Structural Properties\n  of Reasoning Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lachesis: Predicting LLM Inference Accuracy using Structural Properties\n  of Reasoning Paths"
                },
                "summary": "Large Language Models are increasingly used to build agents to perform more\ncomplex tasks. As LLMs perform more complicated reasoning through longer\ninteractions, self-consistency, i.e., the idea that the answer obtained from\nsampling and marginalising a number of multiple independent inferences is more\nlikely to be correct, has received much attention as a simple validation\ntechnique. This paper aims to empirically verify this intuitive hypothesis by\npredicting the correctness of answers obtained using self-consistency from\nproperties of the samples of reasoning paths. We introduce Lachesis, a\npredictive model for self-consistency based LLM inferences, and empirically\nevaluate it using AutoFL, a recently proposed LLM-based fault localisation\ntechnique, as the target technique that uses self-consistency. Lachesis\nconverts collected reasoning paths from AutoFL using specifically designed\nreasoning path representations, and trains LSTM and GCN models to predict\nwhether a given set of reasoning paths would result in a correct answer. The\nresults suggest that Lachesis can predict the correctness of answers with a\nprecision of up to 0.8136, highlighting the possibility of training a\npredictive model that can allow early termination of inferences that are not\nlikely to be successful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly used to build agents to perform more\ncomplex tasks. As LLMs perform more complicated reasoning through longer\ninteractions, self-consistency, i.e., the idea that the answer obtained from\nsampling and marginalising a number of multiple independent inferences is more\nlikely to be correct, has received much attention as a simple validation\ntechnique. This paper aims to empirically verify this intuitive hypothesis by\npredicting the correctness of answers obtained using self-consistency from\nproperties of the samples of reasoning paths. We introduce Lachesis, a\npredictive model for self-consistency based LLM inferences, and empirically\nevaluate it using AutoFL, a recently proposed LLM-based fault localisation\ntechnique, as the target technique that uses self-consistency. Lachesis\nconverts collected reasoning paths from AutoFL using specifically designed\nreasoning path representations, and trains LSTM and GCN models to predict\nwhether a given set of reasoning paths would result in a correct answer. The\nresults suggest that Lachesis can predict the correctness of answers with a\nprecision of up to 0.8136, highlighting the possibility of training a\npredictive model that can allow early termination of inferences that are not\nlikely to be successful."
                },
                "authors": [
                    {
                        "name": "Naryeong Kim"
                    },
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Gabin An"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "arxiv_comment": "To appear at DeepTest 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19670v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19670v4",
                "updated": "2024-12-11T10:56:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    56,
                    3,
                    2,
                    346,
                    0
                ],
                "published": "2024-05-30T03:44:54Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    3,
                    44,
                    54,
                    3,
                    151,
                    0
                ],
                "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for\n  Retrieval-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for\n  Retrieval-Augmented Large Language Models"
                },
                "summary": "Retrieval-augmented generation (RAG) is a promising way to improve large\nlanguage models (LLMs) for generating more factual, accurate, and up-to-date\ncontent. Existing methods either optimize prompts to guide LLMs in leveraging\nretrieved information or directly fine-tune LLMs to adapt to RAG scenarios.\nAlthough fine-tuning can yield better performance, it often compromises the\nLLMs' general generation capabilities by modifying their parameters. This\nlimitation poses challenges in practical applications, especially when LLMs are\nalready deployed, as parameter adjustments may affect their original\nfunctionality. To address this, we propose a novel method that involves\nlearning scalable and pluggable virtual tokens for RAG. By maintaining the\nLLMs' original parameters and fine-tuning only the embeddings of these\npluggable tokens, our approach not only enhances LLMs' performance but also\npreserves their general generation capabilities. Furthermore, we design several\ntraining strategies to improve the scalability, flexibility, and\ngeneralizability of our method. Comprehensive experiments across 12\nquestion-answering tasks demonstrate the superiority of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a promising way to improve large\nlanguage models (LLMs) for generating more factual, accurate, and up-to-date\ncontent. Existing methods either optimize prompts to guide LLMs in leveraging\nretrieved information or directly fine-tune LLMs to adapt to RAG scenarios.\nAlthough fine-tuning can yield better performance, it often compromises the\nLLMs' general generation capabilities by modifying their parameters. This\nlimitation poses challenges in practical applications, especially when LLMs are\nalready deployed, as parameter adjustments may affect their original\nfunctionality. To address this, we propose a novel method that involves\nlearning scalable and pluggable virtual tokens for RAG. By maintaining the\nLLMs' original parameters and fine-tuning only the embeddings of these\npluggable tokens, our approach not only enhances LLMs' performance but also\npreserves their general generation capabilities. Furthermore, we design several\ntraining strategies to improve the scalability, flexibility, and\ngeneralizability of our method. Comprehensive experiments across 12\nquestion-answering tasks demonstrate the superiority of our approach."
                },
                "authors": [
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Zhaoheng Huang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Accepted by AAAI 2025, repo: https://github.com/DaoD/SPRING/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19670v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19670v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08279v1",
                "updated": "2024-12-11T10:52:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    52,
                    29,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:52:29Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    52,
                    29,
                    2,
                    346,
                    0
                ],
                "title": "Y-NQ: English-YorÃ¹bÃ¡ Evaluation dataset for Open-Book Reading\n  Comprehension and Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Y-NQ: English-YorÃ¹bÃ¡ Evaluation dataset for Open-Book Reading\n  Comprehension and Text Generation"
                },
                "summary": "The purpose of this work is to share an English-Yor\\`ub\\'a evaluation dataset\nfor open-book reading comprehension and text generation to assess the\nperformance of models both in a high- and a low- resource language. The dataset\ncontains 358 questions and answers on 338 English documents and 208 Yor\\`ub\\'a\ndocuments. The average document length is ~ 10k words for English and 430 words\nfor Yor\\`ub\\'a. Experiments show a consistent disparity in performance between\nthe two languages, with Yor\\`ub\\'a falling behind English for automatic metrics\neven if documents are much shorter for this language. For a small set of\ndocuments with comparable length, performance of Yor\\`ub\\'a drops by x2.5\ntimes. When analyzing performance by length, we observe that Yor\\`ub\\'a\ndecreases performance dramatically for documents that reach 1500 words while\nEnglish performance is barely affected at that length. Our dataset opens the\ndoor to showcasing if English LLM reading comprehension capabilities extend to\nYor\\`ub\\'a, which for the evaluated LLMs is not the case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of this work is to share an English-Yor\\`ub\\'a evaluation dataset\nfor open-book reading comprehension and text generation to assess the\nperformance of models both in a high- and a low- resource language. The dataset\ncontains 358 questions and answers on 338 English documents and 208 Yor\\`ub\\'a\ndocuments. The average document length is ~ 10k words for English and 430 words\nfor Yor\\`ub\\'a. Experiments show a consistent disparity in performance between\nthe two languages, with Yor\\`ub\\'a falling behind English for automatic metrics\neven if documents are much shorter for this language. For a small set of\ndocuments with comparable length, performance of Yor\\`ub\\'a drops by x2.5\ntimes. When analyzing performance by length, we observe that Yor\\`ub\\'a\ndecreases performance dramatically for documents that reach 1500 words while\nEnglish performance is barely affected at that length. Our dataset opens the\ndoor to showcasing if English LLM reading comprehension capabilities extend to\nYor\\`ub\\'a, which for the evaluated LLMs is not the case."
                },
                "authors": [
                    {
                        "name": "Marta R. Costa-jussÃ "
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Ifeoluwanimi Adebara"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Eduardo SÃ¡nchez"
                    }
                ],
                "author_detail": {
                    "name": "Eduardo SÃ¡nchez"
                },
                "author": "Eduardo SÃ¡nchez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08268v1",
                "updated": "2024-12-11T10:35:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    35,
                    45,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:35:45Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    35,
                    45,
                    2,
                    346,
                    0
                ],
                "title": "LCFO: Long Context and Long Form Output Dataset and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCFO: Long Context and Long Form Output Dataset and Benchmarking"
                },
                "summary": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI."
                },
                "authors": [
                    {
                        "name": "Marta R. Costa-jussÃ "
                    },
                    {
                        "name": "Pierre Andrews"
                    },
                    {
                        "name": "Mariano Coria Meglioli"
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "David Dale"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Alexandre Mourachko"
                    },
                    {
                        "name": "Eduardo SÃ¡nchez"
                    },
                    {
                        "name": "Holger Schwenk"
                    },
                    {
                        "name": "Tuan Tran"
                    },
                    {
                        "name": "Arina Turkatenko"
                    },
                    {
                        "name": "Carleigh Wood"
                    }
                ],
                "author_detail": {
                    "name": "Carleigh Wood"
                },
                "author": "Carleigh Wood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02286v2",
                "updated": "2024-12-11T10:31:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    31,
                    1,
                    2,
                    346,
                    0
                ],
                "published": "2024-05-03T17:59:27Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    17,
                    59,
                    27,
                    4,
                    124,
                    0
                ],
                "title": "Accurate Standard Siren Cosmology with Joint Gravitational-Wave and\n  $Î³$-Ray Burst Observations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate Standard Siren Cosmology with Joint Gravitational-Wave and\n  $Î³$-Ray Burst Observations"
                },
                "summary": "Joint gravitational-wave (GW) and $\\gamma$-ray burst (GRB) observations are\namong the best prospects for standard siren cosmology. However, the strong\nselection effect for the coincident GRB detection, which is possible only for\nsources with small inclination angles, induces a systematic uncertainty that is\ncurrently not accounted for. We show that this severe source of bias can be\nremoved by inferring the \\emph{a priori} unknown electromagnetic detection\nprobability directly from multimessenger data. This leads at the same time to\nan unbiased measurement of the Hubble constant, to constrain the properties of\nGRB emission, and to accurately measure the viewing angle of each source. Our\ninference scheme is applicable to real data already in the small-statistics\nregime, a scenario that might become reality in the near future. Additionally,\nwe introduce a novel likelihood approximant for GW events which treats the\ndependence on distance and inclination as exact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint gravitational-wave (GW) and $\\gamma$-ray burst (GRB) observations are\namong the best prospects for standard siren cosmology. However, the strong\nselection effect for the coincident GRB detection, which is possible only for\nsources with small inclination angles, induces a systematic uncertainty that is\ncurrently not accounted for. We show that this severe source of bias can be\nremoved by inferring the \\emph{a priori} unknown electromagnetic detection\nprobability directly from multimessenger data. This leads at the same time to\nan unbiased measurement of the Hubble constant, to constrain the properties of\nGRB emission, and to accurately measure the viewing angle of each source. Our\ninference scheme is applicable to real data already in the small-statistics\nregime, a scenario that might become reality in the near future. Additionally,\nwe introduce a novel likelihood approximant for GW events which treats the\ndependence on distance and inclination as exact."
                },
                "authors": [
                    {
                        "name": "Michele Mancarella"
                    },
                    {
                        "name": "Francesco Iacovelli"
                    },
                    {
                        "name": "Stefano Foffa"
                    },
                    {
                        "name": "NiccolÃ² Muttoni"
                    },
                    {
                        "name": "Michele Maggiore"
                    }
                ],
                "author_detail": {
                    "name": "Michele Maggiore"
                },
                "author": "Michele Maggiore",
                "arxiv_comment": "5+6 pages, 5 figures, v2: matches version to appear on PRL, added\n  Sec. III.C and Fig 1 in Supplemental Material",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05074v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05074v5",
                "updated": "2024-12-11T10:14:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    14,
                    32,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-09T14:02:24Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "title": "Improving Mortality Prediction After Radiotherapy with Large Language\n  Model Structuring of Large-Scale Unstructured Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Mortality Prediction After Radiotherapy with Large Language\n  Model Structuring of Large-Scale Unstructured Electronic Health Records"
                },
                "summary": "Accurate survival prediction in radiotherapy (RT) is critical for optimizing\ntreatment decisions. This study developed and validated the RT-Surv framework,\nwhich integrates general-domain, open-source large language models (LLMs) to\nstructure unstructured electronic health records alongside structured clinical\ndata. Using data from 34,276 patients and an external cohort of 852, the\nframework successfully transformed unstructured clinical information into\nstructured formats. Incorporating LLM-structured clinical features improved the\nconcordance index from 0.779 to 0.842 during external validation, demonstrating\na significant performance enhancement. Key LLM-structured features, such as\ndisease extent, general condition, and RT purpose, showed high predictive\nimportance and aligned closely with statistically significant predictors\nidentified through conventional statistical analyses, thereby improving model\ninterpretability. Furthermore, the framework enhanced risk stratification,\nenabling more distinct differentiation among low-, intermediate-, and high-risk\ngroups (p < 0.001) using LLM-structured clinical features. These findings\nhighlight the potential of LLMs to convert unstructured data into actionable\ninsights, improving predictive modeling and patient outcomes in clinics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate survival prediction in radiotherapy (RT) is critical for optimizing\ntreatment decisions. This study developed and validated the RT-Surv framework,\nwhich integrates general-domain, open-source large language models (LLMs) to\nstructure unstructured electronic health records alongside structured clinical\ndata. Using data from 34,276 patients and an external cohort of 852, the\nframework successfully transformed unstructured clinical information into\nstructured formats. Incorporating LLM-structured clinical features improved the\nconcordance index from 0.779 to 0.842 during external validation, demonstrating\na significant performance enhancement. Key LLM-structured features, such as\ndisease extent, general condition, and RT purpose, showed high predictive\nimportance and aligned closely with statistically significant predictors\nidentified through conventional statistical analyses, thereby improving model\ninterpretability. Furthermore, the framework enhanced risk stratification,\nenabling more distinct differentiation among low-, intermediate-, and high-risk\ngroups (p < 0.001) using LLM-structured clinical features. These findings\nhighlight the potential of LLMs to convert unstructured data into actionable\ninsights, improving predictive modeling and patient outcomes in clinics."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Chan Woo Wee"
                    },
                    {
                        "name": "Seo Hee Choi"
                    },
                    {
                        "name": "Kyung Hwan Kim"
                    },
                    {
                        "name": "Jee Suk Chang"
                    },
                    {
                        "name": "Hong In Yoon"
                    },
                    {
                        "name": "Ik Jae Lee"
                    },
                    {
                        "name": "Yong Bae Kim"
                    },
                    {
                        "name": "Jaeho Cho"
                    },
                    {
                        "name": "Ki Chang Keum"
                    },
                    {
                        "name": "Chang Geol Lee"
                    },
                    {
                        "name": "Hwa Kyung Byun"
                    },
                    {
                        "name": "Woong Sub Koom"
                    }
                ],
                "author_detail": {
                    "name": "Woong Sub Koom"
                },
                "author": "Woong Sub Koom",
                "arxiv_comment": "23 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05074v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05074v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.16316v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.16316v6",
                "updated": "2024-12-11T10:14:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    14,
                    4,
                    2,
                    346,
                    0
                ],
                "published": "2024-11-25T12:09:43Z",
                "published_parsed": [
                    2024,
                    11,
                    25,
                    12,
                    9,
                    43,
                    0,
                    330,
                    0
                ],
                "title": "Monocular Lane Detection Based on Deep Learning: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monocular Lane Detection Based on Deep Learning: A Survey"
                },
                "summary": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lane detection plays an important role in autonomous driving perception\nsystems. As deep learning algorithms gain popularity, monocular lane detection\nmethods based on them have demonstrated superior performance and emerged as a\nkey research direction in autonomous driving perception. The core designs of\nthese algorithmic frameworks can be summarized as follows: (1) Task paradigm,\nfocusing on lane instance-level discrimination; (2) Lane modeling, representing\nlanes as a set of learnable parameters in the neural network; (3) Global\ncontext supplementation, enhancing inference on the obscure lanes; (4)\nPerspective effect elimination, providing accurate 3D lanes for downstream\napplications. From these perspectives, this paper presents a comprehensive\noverview of existing methods, encompassing both the increasingly mature 2D lane\ndetection approaches and the developing 3D lane detection works. Besides, this\npaper compares the performance of mainstream methods on different benchmarks\nand investigates their inference speed under a unified setting for fair\ncomparison. Moreover, we present some extended works on lane detection,\nincluding multi-task perception, video lane detection, online high-definition\nmap construction, and lane topology reasoning, to offer readers a comprehensive\nroadmap for the evolution of lane detection. Finally, we point out some\npotential future research directions in this field. We exhaustively collect the\npapers and codes of existing works at\nhttps://github.com/Core9724/Awesome-Lane-Detection and will keep tracing the\nresearch."
                },
                "authors": [
                    {
                        "name": "Xin He"
                    },
                    {
                        "name": "Haiyun Guo"
                    },
                    {
                        "name": "Kuan Zhu"
                    },
                    {
                        "name": "Bingke Zhu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Jianwu Fang"
                    },
                    {
                        "name": "Jinqiao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinqiao Wang"
                },
                "author": "Jinqiao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.16316v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.16316v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18193v2",
                "updated": "2024-12-11T10:13:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    13,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-26T18:10:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    10,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge"
                },
                "summary": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Rishu Kumar"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Long paper, preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15696v3",
                "updated": "2024-12-11T10:13:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    13,
                    3,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-28T10:51:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen"
                },
                "summary": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Carlo Alberto Bono"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08258v1",
                "updated": "2024-12-11T10:11:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    11,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:11:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    11,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "Large Language Models for Scholarly Ontology Generation: An Extensive\n  Analysis in the Engineering Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Scholarly Ontology Generation: An Extensive\n  Analysis in the Engineering Field"
                },
                "summary": "Ontologies of research topics are crucial for structuring scientific\nknowledge, enabling scientists to navigate vast amounts of research, and\nforming the backbone of intelligent systems such as search engines and\nrecommendation systems. However, manual creation of these ontologies is\nexpensive, slow, and often results in outdated and overly general\nrepresentations. As a solution, researchers have been investigating ways to\nautomate or semi-automate the process of generating these ontologies. This\npaper offers a comprehensive analysis of the ability of large language models\n(LLMs) to identify semantic relationships between different research topics,\nwhich is a critical step in the development of such ontologies. To this end, we\ndeveloped a gold standard based on the IEEE Thesaurus to evaluate the task of\nidentifying four types of relationships between pairs of topics: broader,\nnarrower, same-as, and other. Our study evaluates the performance of seventeen\nLLMs, which differ in scale, accessibility (open vs. proprietary), and model\ntype (full vs. quantised), while also assessing four zero-shot reasoning\nstrategies. Several models have achieved outstanding results, including\nMixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,\n0.920, and 0.967, respectively. Furthermore, our findings demonstrate that\nsmaller, quantised models, when optimised through prompt engineering, can\ndeliver performance comparable to much larger proprietary models, while\nrequiring significantly fewer computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies of research topics are crucial for structuring scientific\nknowledge, enabling scientists to navigate vast amounts of research, and\nforming the backbone of intelligent systems such as search engines and\nrecommendation systems. However, manual creation of these ontologies is\nexpensive, slow, and often results in outdated and overly general\nrepresentations. As a solution, researchers have been investigating ways to\nautomate or semi-automate the process of generating these ontologies. This\npaper offers a comprehensive analysis of the ability of large language models\n(LLMs) to identify semantic relationships between different research topics,\nwhich is a critical step in the development of such ontologies. To this end, we\ndeveloped a gold standard based on the IEEE Thesaurus to evaluate the task of\nidentifying four types of relationships between pairs of topics: broader,\nnarrower, same-as, and other. Our study evaluates the performance of seventeen\nLLMs, which differ in scale, accessibility (open vs. proprietary), and model\ntype (full vs. quantised), while also assessing four zero-shot reasoning\nstrategies. Several models have achieved outstanding results, including\nMixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,\n0.920, and 0.967, respectively. Furthermore, our findings demonstrate that\nsmaller, quantised models, when optimised through prompt engineering, can\ndeliver performance comparable to much larger proprietary models, while\nrequiring significantly fewer computational resources."
                },
                "authors": [
                    {
                        "name": "Tanay Aggarwal"
                    },
                    {
                        "name": "Angelo Salatino"
                    },
                    {
                        "name": "Francesco Osborne"
                    },
                    {
                        "name": "Enrico Motta"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Motta"
                },
                "author": "Enrico Motta",
                "arxiv_comment": "submitted to Information Processing & Management",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08247v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08247v1",
                "updated": "2024-12-11T09:55:09Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    55,
                    9,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T09:55:09Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    55,
                    9,
                    2,
                    346,
                    0
                ],
                "title": "MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time\n  Scenarios with Impaired Visual Cues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoMuSE: Momentum Multi-modal Target Speaker Extraction for Real-time\n  Scenarios with Impaired Visual Cues"
                },
                "summary": "Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate the speech of\na specific target speaker from an audio mixture using time-synchronized visual\ncues. In real-world scenarios, visual cues are not always available due to\nvarious impairments, which undermines the stability of AV-TSE. Despite this\nchallenge, humans can maintain attentional momentum over time, even when the\ntarget speaker is not visible. In this paper, we introduce the Momentum\nMulti-modal target Speaker Extraction (MoMuSE), which retains a speaker\nidentity momentum in memory, enabling the model to continuously track the\ntarget speaker. Designed for real-time inference, MoMuSE extracts the current\nspeech window with guidance from both visual cues and dynamically updated\nspeaker momentum. Experimental results demonstrate that MoMuSE exhibits\nsignificant improvement, particularly in scenarios with severe impairment of\nvisual cues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio-visual Target Speaker Extraction (AV-TSE) aims to isolate the speech of\na specific target speaker from an audio mixture using time-synchronized visual\ncues. In real-world scenarios, visual cues are not always available due to\nvarious impairments, which undermines the stability of AV-TSE. Despite this\nchallenge, humans can maintain attentional momentum over time, even when the\ntarget speaker is not visible. In this paper, we introduce the Momentum\nMulti-modal target Speaker Extraction (MoMuSE), which retains a speaker\nidentity momentum in memory, enabling the model to continuously track the\ntarget speaker. Designed for real-time inference, MoMuSE extracts the current\nspeech window with guidance from both visual cues and dynamically updated\nspeaker momentum. Experimental results demonstrate that MoMuSE exhibits\nsignificant improvement, particularly in scenarios with severe impairment of\nvisual cues."
                },
                "authors": [
                    {
                        "name": "Junjie Li"
                    },
                    {
                        "name": "Ke Zhang"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Kong Aik Lee"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08247v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08247v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08245v1",
                "updated": "2024-12-11T09:53:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    53,
                    43,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T09:53:43Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    53,
                    43,
                    2,
                    346,
                    0
                ],
                "title": "Mirror Symmetry Breaking Disclosed in the Decay of Three-Proton Emitter\n  20Al",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mirror Symmetry Breaking Disclosed in the Decay of Three-Proton Emitter\n  20Al"
                },
                "summary": "The previously-unknown nucleus 20Al has been observed for the first time by\ndetecting its in-flight decays. Tracking trajectories of all decay products\nwith silicon micro-strip detectors allowed for a conclusion that 20Al is\nunbound with respect to three-proton (3p) emission. The 3p-decay energy of 20Al\nground state has been determined to be 1.93(+0.11,-0.09) MeV through a detailed\nstudy of angular correlations of its decay products, 17Ne+p+p+p. This value is\nmuch smaller in comparison with the predictions inferred from the isospin\nsymmetry by using the known energy of its mirror nucleus 20N, which indicates a\npossible mirror symmetry violation in the structure of 3p emitters. Such an\nisospin symmetry breaking is supported by the calculations of the continuum\nembedded theoretical frameworks, describing the observed 20Al ground state as\nan 1p s-wave state with a spin-parity of 1-, which contradicts to the\nspin-parity (2-) of the 20N ground state. The 20Al ground state decays by\nsequential 1p-2p emission via intermediate ground state of 19Mg, which is the\nfirst observed case of daughter two-proton radioactivity following 1p decay of\nthe parent state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The previously-unknown nucleus 20Al has been observed for the first time by\ndetecting its in-flight decays. Tracking trajectories of all decay products\nwith silicon micro-strip detectors allowed for a conclusion that 20Al is\nunbound with respect to three-proton (3p) emission. The 3p-decay energy of 20Al\nground state has been determined to be 1.93(+0.11,-0.09) MeV through a detailed\nstudy of angular correlations of its decay products, 17Ne+p+p+p. This value is\nmuch smaller in comparison with the predictions inferred from the isospin\nsymmetry by using the known energy of its mirror nucleus 20N, which indicates a\npossible mirror symmetry violation in the structure of 3p emitters. Such an\nisospin symmetry breaking is supported by the calculations of the continuum\nembedded theoretical frameworks, describing the observed 20Al ground state as\nan 1p s-wave state with a spin-parity of 1-, which contradicts to the\nspin-parity (2-) of the 20N ground state. The 20Al ground state decays by\nsequential 1p-2p emission via intermediate ground state of 19Mg, which is the\nfirst observed case of daughter two-proton radioactivity following 1p decay of\nthe parent state."
                },
                "authors": [
                    {
                        "name": "X. -D. Xu"
                    },
                    {
                        "name": "I. Mukha"
                    },
                    {
                        "name": "J. G. Li"
                    },
                    {
                        "name": "S. M. Wang"
                    },
                    {
                        "name": "L. Acosta"
                    },
                    {
                        "name": "M. Bajzek"
                    },
                    {
                        "name": "E. Casarejos"
                    },
                    {
                        "name": "D. Cortina-Gil"
                    },
                    {
                        "name": "J. M. Espino"
                    },
                    {
                        "name": "A. Fomichev"
                    },
                    {
                        "name": "H. Geissel"
                    },
                    {
                        "name": "J. Gomez-Camacho"
                    },
                    {
                        "name": "L. V. Grigorenko"
                    },
                    {
                        "name": "O. Kiselev"
                    },
                    {
                        "name": "A. A. Korsheninnikov"
                    },
                    {
                        "name": "D. Kostyleva"
                    },
                    {
                        "name": "N. Kurz"
                    },
                    {
                        "name": "Yu. A. Litvinov"
                    },
                    {
                        "name": "I. Martel"
                    },
                    {
                        "name": "C. Nociforo"
                    },
                    {
                        "name": "M. Pfutzner"
                    },
                    {
                        "name": "C. RodrÄ±guez-Tajes"
                    },
                    {
                        "name": "C. Scheidenberger"
                    },
                    {
                        "name": "M. Stanoiu"
                    },
                    {
                        "name": "K. Suemmerer"
                    },
                    {
                        "name": "H. Weick"
                    },
                    {
                        "name": "P. J. Woods"
                    },
                    {
                        "name": "M. V. Zhukov"
                    }
                ],
                "author_detail": {
                    "name": "M. V. Zhukov"
                },
                "author": "M. V. Zhukov",
                "arxiv_comment": "9 pages, 6 figures, to be submitted to APS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "nucl-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08237v1",
                "updated": "2024-12-11T09:38:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    38,
                    50,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T09:38:50Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    38,
                    50,
                    2,
                    346,
                    0
                ],
                "title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"
                },
                "summary": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data."
                },
                "authors": [
                    {
                        "name": "Xingchen Song"
                    },
                    {
                        "name": "Mengtao Xing"
                    },
                    {
                        "name": "Changwei Ma"
                    },
                    {
                        "name": "Shengqiang Li"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Binbin Zhang"
                    },
                    {
                        "name": "Fuping Pan"
                    },
                    {
                        "name": "Dinghao Zhou"
                    },
                    {
                        "name": "Yuekai Zhang"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Zhendong Peng"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02965v2",
                "updated": "2024-12-11T09:30:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    30,
                    19,
                    2,
                    346,
                    0
                ],
                "published": "2024-03-05T13:41:25Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    13,
                    41,
                    25,
                    1,
                    65,
                    0
                ],
                "title": "ChatGPT and biometrics: an assessment of face recognition, gender\n  detection, and age estimation capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT and biometrics: an assessment of face recognition, gender\n  detection, and age estimation capabilities"
                },
                "summary": "This paper explores the application of large language models (LLMs), like\nChatGPT, for biometric tasks. We specifically examine the capabilities of\nChatGPT in performing biometric-related tasks, with an emphasis on face\nrecognition, gender detection, and age estimation. Since biometrics are\nconsidered as sensitive information, ChatGPT avoids answering direct prompts,\nand thus we crafted a prompting strategy to bypass its safeguard and evaluate\nthe capabilities for biometrics tasks. Our study reveals that ChatGPT\nrecognizes facial identities and differentiates between two facial images with\nconsiderable accuracy. Additionally, experimental results demonstrate\nremarkable performance in gender detection and reasonable accuracy for the age\nestimation tasks. Our findings shed light on the promising potentials in the\napplication of LLMs and foundation models for biometrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of large language models (LLMs), like\nChatGPT, for biometric tasks. We specifically examine the capabilities of\nChatGPT in performing biometric-related tasks, with an emphasis on face\nrecognition, gender detection, and age estimation. Since biometrics are\nconsidered as sensitive information, ChatGPT avoids answering direct prompts,\nand thus we crafted a prompting strategy to bypass its safeguard and evaluate\nthe capabilities for biometrics tasks. Our study reveals that ChatGPT\nrecognizes facial identities and differentiates between two facial images with\nconsiderable accuracy. Additionally, experimental results demonstrate\nremarkable performance in gender detection and reasonable accuracy for the age\nestimation tasks. Our findings shed light on the promising potentials in the\napplication of LLMs and foundation models for biometrics."
                },
                "authors": [
                    {
                        "name": "Ahmad Hassanpour"
                    },
                    {
                        "name": "Yasamin Kowsari"
                    },
                    {
                        "name": "Hatef Otroshi Shahreza"
                    },
                    {
                        "name": "Bian Yang"
                    },
                    {
                        "name": "Sebastien Marcel"
                    }
                ],
                "author_detail": {
                    "name": "Sebastien Marcel"
                },
                "author": "Sebastien Marcel",
                "arxiv_doi": "10.1109/ICIP51287.2024.10647924",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICIP51287.2024.10647924",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.02965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a conference paper at IEEE International Conference on\n  Image Processing (ICIP) 2024",
                "arxiv_journal_ref": "Chatgpt and Biometrics: an Assessment of Face Recognition, Gender\n  Detection, and Age Estimation Capabilities,\"2024 IEEE International\n  Conference on Image Processing (ICIP)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.14806v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.14806v3",
                "updated": "2024-12-11T09:23:37Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    23,
                    37,
                    2,
                    346,
                    0
                ],
                "published": "2024-04-23T07:37:41Z",
                "published_parsed": [
                    2024,
                    4,
                    23,
                    7,
                    37,
                    41,
                    1,
                    114,
                    0
                ],
                "title": "Variational Dynamic Programming for Stochastic Optimal Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Dynamic Programming for Stochastic Optimal Control"
                },
                "summary": "We consider the problem of stochastic optimal control, where the\nstate-feedback control policies take the form of a probability distribution and\nwhere a penalty on the entropy is added. By viewing the cost function as a\nKullback- Leibler (KL) divergence between two joint distributions, we bring the\ntools from variational inference to bear on our optimal control problem. This\nallows for deriving a dynamic programming principle, where the value function\nis defined as a KL divergence again. We then resort to Gaussian distributions\nto approximate the control policies and apply the theory to control affine\nnonlinear systems with quadratic costs. This results in closed-form recursive\nupdates, which generalize LQR control and the backward Riccati equation. We\nillustrate this novel method on the simple problem of stabilizing an inverted\npendulum.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the problem of stochastic optimal control, where the\nstate-feedback control policies take the form of a probability distribution and\nwhere a penalty on the entropy is added. By viewing the cost function as a\nKullback- Leibler (KL) divergence between two joint distributions, we bring the\ntools from variational inference to bear on our optimal control problem. This\nallows for deriving a dynamic programming principle, where the value function\nis defined as a KL divergence again. We then resort to Gaussian distributions\nto approximate the control policies and apply the theory to control affine\nnonlinear systems with quadratic costs. This results in closed-form recursive\nupdates, which generalize LQR control and the backward Riccati equation. We\nillustrate this novel method on the simple problem of stabilizing an inverted\npendulum."
                },
                "authors": [
                    {
                        "name": "Marc Lambert"
                    },
                    {
                        "name": "Francis Bach"
                    },
                    {
                        "name": "SilvÃ¨re Bonnabel"
                    }
                ],
                "author_detail": {
                    "name": "SilvÃ¨re Bonnabel"
                },
                "arxiv_affiliation": "CAOR",
                "author": "SilvÃ¨re Bonnabel",
                "arxiv_journal_ref": "2024 Conference on Decision and Control, Dec 2024, Milano, Italy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.14806v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.14806v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.OC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08223v1",
                "updated": "2024-12-11T09:18:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    18,
                    20,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T09:18:20Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    18,
                    20,
                    2,
                    346,
                    0
                ],
                "title": "Zeitgebers-Based User Time Perception Analysis and Data-Driven Modeling\n  via Transformer in VR",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Zeitgebers-Based User Time Perception Analysis and Data-Driven Modeling\n  via Transformer in VR"
                },
                "summary": "Virtual Reality (VR) creates a highly realistic and controllable simulation\nenvironment that can manipulate users' sense of space and time. While the\nsensation of \"losing track of time\" is often associated with enjoyable\nexperiences, the link between time perception and user experience in VR and its\nunderlying mechanisms remains largely unexplored. This study investigates how\ndifferent zeitgebers-light color, music tempo, and task factor-influence time\nperception. We introduced the Relative Subjective Time Change (RSTC) method to\nexplore the relationship between time perception and user experience.\nAdditionally, we applied a data-driven approach called the Time Perception\nModeling Network (TPM-Net), which integrates Convolutional Neural Network (CNN)\nand Transformer architectures to model time perception based on multimodal\nphysiological and zeitgebers data. With 56 participants in a between-subject\nexperiment, our results show that task factors significantly influence time\nperception, with red light and slow-tempo music further contributing to time\nunderestimation. The RSTC method reveals that underestimating time in VR is\nstrongly associated with improved user experience, presence, and engagement.\nFurthermore, TPM-Net shows potential for modeling time perception in VR,\nenabling inference of relative changes in users' time perception and\ncorresponding changes in user experience. This study provides insights into the\nrelationship between time perception and user experience in VR, with\napplications in VR-based therapy and specialized training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual Reality (VR) creates a highly realistic and controllable simulation\nenvironment that can manipulate users' sense of space and time. While the\nsensation of \"losing track of time\" is often associated with enjoyable\nexperiences, the link between time perception and user experience in VR and its\nunderlying mechanisms remains largely unexplored. This study investigates how\ndifferent zeitgebers-light color, music tempo, and task factor-influence time\nperception. We introduced the Relative Subjective Time Change (RSTC) method to\nexplore the relationship between time perception and user experience.\nAdditionally, we applied a data-driven approach called the Time Perception\nModeling Network (TPM-Net), which integrates Convolutional Neural Network (CNN)\nand Transformer architectures to model time perception based on multimodal\nphysiological and zeitgebers data. With 56 participants in a between-subject\nexperiment, our results show that task factors significantly influence time\nperception, with red light and slow-tempo music further contributing to time\nunderestimation. The RSTC method reveals that underestimating time in VR is\nstrongly associated with improved user experience, presence, and engagement.\nFurthermore, TPM-Net shows potential for modeling time perception in VR,\nenabling inference of relative changes in users' time perception and\ncorresponding changes in user experience. This study provides insights into the\nrelationship between time perception and user experience in VR, with\napplications in VR-based therapy and specialized training."
                },
                "authors": [
                    {
                        "name": "Yi Li"
                    },
                    {
                        "name": "Zengyu Liu"
                    },
                    {
                        "name": "Xiandi Zhu"
                    },
                    {
                        "name": "Ning Xie"
                    }
                ],
                "author_detail": {
                    "name": "Ning Xie"
                },
                "author": "Ning Xie",
                "arxiv_comment": "12pages,7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02819v3",
                "updated": "2024-12-11T09:15:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    15,
                    6,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-03T20:35:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    35,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels"
                },
                "summary": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering 16k to 128k,\n695 samples in total, the annotations are human-driven. We evaluate commercial\nand open-source models on CNNSum and conduct a detailed analysis. Based on the\nobservations, we further conduct fine-tuning exploration with short-context\nsummary data. In our study: (1) GPT-4o underperformed, due to excessive\nsubjective commentary. (2) Currently, long-context summarization mainly relies\non memory ability, small LLMs with stable longer context lengths are the most\ncost-effective. Using long data concatenated from short-context summaries makes\na significant improvement. (3) Prompt templates may cause a large performance\ngap but can be mitigated through fine-tuning. (4) Fine-tuned Chat or\nInstruction versions may harm the Base model and further fine-tuning cannot\nbridge performance gap. (5) while models with RoPE base scaling exhibit strong\nextrapolation potential, their performance may vary significantly when combined\nwith other interpolation methods and need careful selection. (6) CNNSum\nprovides more reliable and insightful evaluation results than other benchmarks.\nWe release CNNSum to advance research in this field\n(https://github.com/CxsGhost/CNNSum).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering 16k to 128k,\n695 samples in total, the annotations are human-driven. We evaluate commercial\nand open-source models on CNNSum and conduct a detailed analysis. Based on the\nobservations, we further conduct fine-tuning exploration with short-context\nsummary data. In our study: (1) GPT-4o underperformed, due to excessive\nsubjective commentary. (2) Currently, long-context summarization mainly relies\non memory ability, small LLMs with stable longer context lengths are the most\ncost-effective. Using long data concatenated from short-context summaries makes\na significant improvement. (3) Prompt templates may cause a large performance\ngap but can be mitigated through fine-tuning. (4) Fine-tuned Chat or\nInstruction versions may harm the Base model and further fine-tuning cannot\nbridge performance gap. (5) while models with RoPE base scaling exhibit strong\nextrapolation potential, their performance may vary significantly when combined\nwith other interpolation methods and need careful selection. (6) CNNSum\nprovides more reliable and insightful evaluation results than other benchmarks.\nWe release CNNSum to advance research in this field\n(https://github.com/CxsGhost/CNNSum)."
                },
                "authors": [
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11553v3",
                "updated": "2024-12-11T09:04:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    4,
                    18,
                    2,
                    346,
                    0
                ],
                "published": "2024-04-17T16:53:16Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    16,
                    53,
                    16,
                    2,
                    108,
                    0
                ],
                "title": "Language Ranker: A Metric for Quantifying LLM Performance Across High\n  and Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Ranker: A Metric for Quantifying LLM Performance Across High\n  and Low-Resource Languages"
                },
                "summary": "The development of Large Language Models (LLMs) relies on extensive text\ncorpora, which are often unevenly distributed across languages. This imbalance\nresults in LLMs performing significantly better on high-resource languages like\nEnglish, German, and French, while their capabilities in low-resource languages\nremain inadequate. Currently, there is a lack of quantitative methods to\nevaluate the performance of LLMs in these low-resource languages. To address\nthis gap, we propose the Language Ranker, an intrinsic metric designed to\nbenchmark and rank languages based on LLM performance using internal\nrepresentations. By comparing the LLM's internal representation of various\nlanguages against a baseline derived from English, we can assess the model's\nmultilingual capabilities in a robust and language-agnostic manner. Our\nanalysis reveals that high-resource languages exhibit higher similarity scores\nwith English, demonstrating superior performance, while low-resource languages\nshow lower similarity scores, underscoring the effectiveness of our metric in\nassessing language-specific capabilities. Besides, the experiments show that\nthere is a strong correlation between the LLM's performance in different\nlanguages and the proportion of those languages in its pre-training corpus.\nThese insights underscore the efficacy of the Language Ranker as a tool for\nevaluating LLM performance across different languages, particularly those with\nlimited resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) relies on extensive text\ncorpora, which are often unevenly distributed across languages. This imbalance\nresults in LLMs performing significantly better on high-resource languages like\nEnglish, German, and French, while their capabilities in low-resource languages\nremain inadequate. Currently, there is a lack of quantitative methods to\nevaluate the performance of LLMs in these low-resource languages. To address\nthis gap, we propose the Language Ranker, an intrinsic metric designed to\nbenchmark and rank languages based on LLM performance using internal\nrepresentations. By comparing the LLM's internal representation of various\nlanguages against a baseline derived from English, we can assess the model's\nmultilingual capabilities in a robust and language-agnostic manner. Our\nanalysis reveals that high-resource languages exhibit higher similarity scores\nwith English, demonstrating superior performance, while low-resource languages\nshow lower similarity scores, underscoring the effectiveness of our metric in\nassessing language-specific capabilities. Besides, the experiments show that\nthere is a strong correlation between the LLM's performance in different\nlanguages and the proportion of those languages in its pre-training corpus.\nThese insights underscore the efficacy of the Language Ranker as a tool for\nevaluating LLM performance across different languages, particularly those with\nlimited resources."
                },
                "authors": [
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "Accepted by AAAI 2025 (Social Impact Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.09271v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.09271v2",
                "updated": "2024-12-11T08:45:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    45,
                    47,
                    2,
                    346,
                    0
                ],
                "published": "2023-12-14T18:49:07Z",
                "published_parsed": [
                    2023,
                    12,
                    14,
                    18,
                    49,
                    7,
                    3,
                    348,
                    0
                ],
                "title": "Bayesian Inference of Initial Conditions from Non-Linear Cosmic\n  Structures using Field-Level Emulators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Inference of Initial Conditions from Non-Linear Cosmic\n  Structures using Field-Level Emulators"
                },
                "summary": "Analysing next-generation cosmological data requires balancing accurate\nmodeling of non-linear gravitational structure formation and computational\ndemands. We propose a solution by introducing a machine learning-based\nfield-level emulator, within the Hamiltonian Monte Carlo-based Bayesian Origin\nReconstruction from Galaxies (BORG) inference algorithm. Built on a V-net\nneural network architecture, the emulator enhances the predictions by\nfirst-order Lagrangian perturbation theory to be accurately aligned with full\n$N$-body simulations while significantly reducing evaluation time. We test its\nincorporation in BORG for sampling cosmic initial conditions using mock data\nbased on non-linear large-scale structures from $N$-body simulations and\nGaussian noise. The method efficiently and accurately explores the\nhigh-dimensional parameter space of initial conditions, fully extracting the\ncross-correlation information of the data field binned at a resolution of\n$1.95h^{-1}$ Mpc. Percent-level agreement with the ground truth in the power\nspectrum and bispectrum is achieved up to the Nyquist frequency $k_\\mathrm{N}\n\\approx 2.79h \\; \\mathrm{Mpc}^{-1}$. Posterior resimulations - using the\ninferred initial conditions for $N$-body simulations - show that the recovery\nof information in the initial conditions is sufficient to accurately reproduce\nhalo properties. In particular, we show highly accurate $M_{200\\mathrm{c}}$\nhalo mass function and stacked density profiles of haloes in different mass\nbins $[0.853,16]\\times 10^{14}M_{\\odot}h^{-1}$. As all available\ncross-correlation information is extracted, we acknowledge that limitations in\nrecovering the initial conditions stem from the noise level and data grid\nresolution. This is promising as it underscores the significance of accurate\nnon-linear modeling, indicating the potential for extracting additional\ninformation at smaller scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysing next-generation cosmological data requires balancing accurate\nmodeling of non-linear gravitational structure formation and computational\ndemands. We propose a solution by introducing a machine learning-based\nfield-level emulator, within the Hamiltonian Monte Carlo-based Bayesian Origin\nReconstruction from Galaxies (BORG) inference algorithm. Built on a V-net\nneural network architecture, the emulator enhances the predictions by\nfirst-order Lagrangian perturbation theory to be accurately aligned with full\n$N$-body simulations while significantly reducing evaluation time. We test its\nincorporation in BORG for sampling cosmic initial conditions using mock data\nbased on non-linear large-scale structures from $N$-body simulations and\nGaussian noise. The method efficiently and accurately explores the\nhigh-dimensional parameter space of initial conditions, fully extracting the\ncross-correlation information of the data field binned at a resolution of\n$1.95h^{-1}$ Mpc. Percent-level agreement with the ground truth in the power\nspectrum and bispectrum is achieved up to the Nyquist frequency $k_\\mathrm{N}\n\\approx 2.79h \\; \\mathrm{Mpc}^{-1}$. Posterior resimulations - using the\ninferred initial conditions for $N$-body simulations - show that the recovery\nof information in the initial conditions is sufficient to accurately reproduce\nhalo properties. In particular, we show highly accurate $M_{200\\mathrm{c}}$\nhalo mass function and stacked density profiles of haloes in different mass\nbins $[0.853,16]\\times 10^{14}M_{\\odot}h^{-1}$. As all available\ncross-correlation information is extracted, we acknowledge that limitations in\nrecovering the initial conditions stem from the noise level and data grid\nresolution. This is promising as it underscores the significance of accurate\nnon-linear modeling, indicating the potential for extracting additional\ninformation at smaller scales."
                },
                "authors": [
                    {
                        "name": "Ludvig Doeser"
                    },
                    {
                        "name": "Drew Jamieson"
                    },
                    {
                        "name": "Stephen Stopyra"
                    },
                    {
                        "name": "Guilhem Lavaux"
                    },
                    {
                        "name": "Florent Leclercq"
                    },
                    {
                        "name": "Jens Jasche"
                    }
                ],
                "author_detail": {
                    "name": "Jens Jasche"
                },
                "author": "Jens Jasche",
                "arxiv_doi": "10.1093/mnras/stae2429",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1093/mnras/stae2429",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2312.09271v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.09271v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "19 pages, 15 figures. Updated to match version accepted by MNRAS\n  (published 2024/11/27)",
                "arxiv_journal_ref": "Monthly Notices of the Royal Astronomical Society, Volume 535,\n  Issue 2, December 2024, Pages 1258-1277",
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08201v1",
                "updated": "2024-12-11T08:44:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    44,
                    15,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:44:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    44,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Model-Editing-Based Jailbreak against Safety-aligned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Editing-Based Jailbreak against Safety-aligned Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have transformed numerous fields by enabling\nadvanced natural language interactions but remain susceptible to critical\nvulnerabilities, particularly jailbreak attacks. Current jailbreak techniques,\nwhile effective, often depend on input modifications, making them detectable\nand limiting their stealth and scalability. This paper presents Targeted Model\nEditing (TME), a novel white-box approach that bypasses safety filters by\nminimally altering internal model structures while preserving the model's\nintended functionalities. TME identifies and removes safety-critical\ntransformations (SCTs) embedded in model matrices, enabling malicious queries\nto bypass restrictions without input modifications. By analyzing distinct\nactivation patterns between safe and unsafe queries, TME isolates and\napproximates SCTs through an optimization process. Implemented in the D-LLM\nframework, our method achieves an average Attack Success Rate (ASR) of 84.86%\non four mainstream open-source LLMs, maintaining high performance. Unlike\nexisting methods, D-LLM eliminates the need for specific triggers or harmful\nresponse collections, offering a stealthier and more effective jailbreak\nstrategy. This work reveals a covert and robust threat vector in LLM security\nand emphasizes the need for stronger safeguards in model safety alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed numerous fields by enabling\nadvanced natural language interactions but remain susceptible to critical\nvulnerabilities, particularly jailbreak attacks. Current jailbreak techniques,\nwhile effective, often depend on input modifications, making them detectable\nand limiting their stealth and scalability. This paper presents Targeted Model\nEditing (TME), a novel white-box approach that bypasses safety filters by\nminimally altering internal model structures while preserving the model's\nintended functionalities. TME identifies and removes safety-critical\ntransformations (SCTs) embedded in model matrices, enabling malicious queries\nto bypass restrictions without input modifications. By analyzing distinct\nactivation patterns between safe and unsafe queries, TME isolates and\napproximates SCTs through an optimization process. Implemented in the D-LLM\nframework, our method achieves an average Attack Success Rate (ASR) of 84.86%\non four mainstream open-source LLMs, maintaining high performance. Unlike\nexisting methods, D-LLM eliminates the need for specific triggers or harmful\nresponse collections, offering a stealthier and more effective jailbreak\nstrategy. This work reveals a covert and robust threat vector in LLM security\nand emphasizes the need for stronger safeguards in model safety alignment."
                },
                "authors": [
                    {
                        "name": "Yuxi Li"
                    },
                    {
                        "name": "Zhibo Zhang"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.08642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08642v1",
                "updated": "2024-12-11T18:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    50,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:59:50Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    59,
                    50,
                    2,
                    346,
                    0
                ],
                "title": "Generative Semantic Communication: Architectures, Technologies, and\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Semantic Communication: Architectures, Technologies, and\n  Applications"
                },
                "summary": "This paper delves into the applications of generative artificial intelligence\n(GAI) in semantic communication (SemCom) and presents a thorough study. Three\npopular SemCom systems enabled by classical GAI models are first introduced,\nincluding variational autoencoders, generative adversarial networks, and\ndiffusion models. For each system, the fundamental concept of the GAI model,\nthe corresponding SemCom architecture, and the associated literature review of\nrecent efforts are elucidated. Then, a novel generative SemCom system is\nproposed by incorporating the cutting-edge GAI technology-large language models\n(LLMs). This system features two LLM-based AI agents at both the transmitter\nand receiver, serving as \"brains\" to enable powerful information understanding\nand content regeneration capabilities, respectively. This innovative design\nallows the receiver to directly generate the desired content, instead of\nrecovering the bit stream, based on the coded semantic information conveyed by\nthe transmitter. Therefore, it shifts the communication mindset from\n\"information recovery\" to \"information regeneration\" and thus ushers in a new\nera of generative SemCom. A case study on point-to-point video retrieval is\npresented to demonstrate the superiority of the proposed generative SemCom\nsystem, showcasing a 99.98% reduction in communication overhead and a 53%\nimprovement in retrieval accuracy compared to the traditional communication\nsystem. Furthermore, four typical application scenarios for generative SemCom\nare delineated, followed by a discussion of three open issues warranting future\ninvestigation. In a nutshell, this paper provides a holistic set of guidelines\nfor applying GAI in SemCom, paving the way for the efficient implementation of\ngenerative SemCom in future wireless networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper delves into the applications of generative artificial intelligence\n(GAI) in semantic communication (SemCom) and presents a thorough study. Three\npopular SemCom systems enabled by classical GAI models are first introduced,\nincluding variational autoencoders, generative adversarial networks, and\ndiffusion models. For each system, the fundamental concept of the GAI model,\nthe corresponding SemCom architecture, and the associated literature review of\nrecent efforts are elucidated. Then, a novel generative SemCom system is\nproposed by incorporating the cutting-edge GAI technology-large language models\n(LLMs). This system features two LLM-based AI agents at both the transmitter\nand receiver, serving as \"brains\" to enable powerful information understanding\nand content regeneration capabilities, respectively. This innovative design\nallows the receiver to directly generate the desired content, instead of\nrecovering the bit stream, based on the coded semantic information conveyed by\nthe transmitter. Therefore, it shifts the communication mindset from\n\"information recovery\" to \"information regeneration\" and thus ushers in a new\nera of generative SemCom. A case study on point-to-point video retrieval is\npresented to demonstrate the superiority of the proposed generative SemCom\nsystem, showcasing a 99.98% reduction in communication overhead and a 53%\nimprovement in retrieval accuracy compared to the traditional communication\nsystem. Furthermore, four typical application scenarios for generative SemCom\nare delineated, followed by a discussion of three open issues warranting future\ninvestigation. In a nutshell, this paper provides a holistic set of guidelines\nfor applying GAI in SemCom, paving the way for the efficient implementation of\ngenerative SemCom in future wireless networks."
                },
                "authors": [
                    {
                        "name": "Jinke Ren"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Weiwen Yuan"
                    },
                    {
                        "name": "Chongjie Wang"
                    },
                    {
                        "name": "Xianda Wang"
                    },
                    {
                        "name": "Yingbin Zhou"
                    },
                    {
                        "name": "Ziwei Zhu"
                    },
                    {
                        "name": "Fangxin Wang"
                    },
                    {
                        "name": "Shuguang Cui"
                    }
                ],
                "author_detail": {
                    "name": "Shuguang Cui"
                },
                "author": "Shuguang Cui",
                "arxiv_comment": "18 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08639v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08639v1",
                "updated": "2024-12-11T18:58:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    58,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:58:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    58,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "Fast Prompt Alignment for Text-to-Image Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast Prompt Alignment for Text-to-Image Generation"
                },
                "summary": "Text-to-image generation has advanced rapidly, yet aligning complex textual\nprompts with generated visuals remains challenging, especially with intricate\nobject relationships and fine-grained details. This paper introduces Fast\nPrompt Alignment (FPA), a prompt optimization framework that leverages a\none-pass approach, enhancing text-to-image alignment efficiency without the\niterative overhead typical of current methods like OPT2I. FPA uses large\nlanguage models (LLMs) for single-iteration prompt paraphrasing, followed by\nfine-tuning or in-context learning with optimized prompts to enable real-time\ninference, reducing computational demands while preserving alignment fidelity.\nExtensive evaluations on the COCO Captions and PartiPrompts datasets\ndemonstrate that FPA achieves competitive text-image alignment scores at a\nfraction of the processing time, as validated through both automated metrics\n(TIFA, VQA) and human evaluation. A human study with expert annotators further\nreveals a strong correlation between human alignment judgments and automated\nscores, underscoring the robustness of FPA's improvements. The proposed method\nshowcases a scalable, efficient alternative to iterative prompt optimization,\nenabling broader applicability in real-time, high-demand settings. The codebase\nis provided to facilitate further research:\nhttps://github.com/tiktok/fast_prompt_alignment",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image generation has advanced rapidly, yet aligning complex textual\nprompts with generated visuals remains challenging, especially with intricate\nobject relationships and fine-grained details. This paper introduces Fast\nPrompt Alignment (FPA), a prompt optimization framework that leverages a\none-pass approach, enhancing text-to-image alignment efficiency without the\niterative overhead typical of current methods like OPT2I. FPA uses large\nlanguage models (LLMs) for single-iteration prompt paraphrasing, followed by\nfine-tuning or in-context learning with optimized prompts to enable real-time\ninference, reducing computational demands while preserving alignment fidelity.\nExtensive evaluations on the COCO Captions and PartiPrompts datasets\ndemonstrate that FPA achieves competitive text-image alignment scores at a\nfraction of the processing time, as validated through both automated metrics\n(TIFA, VQA) and human evaluation. A human study with expert annotators further\nreveals a strong correlation between human alignment judgments and automated\nscores, underscoring the robustness of FPA's improvements. The proposed method\nshowcases a scalable, efficient alternative to iterative prompt optimization,\nenabling broader applicability in real-time, high-demand settings. The codebase\nis provided to facilitate further research:\nhttps://github.com/tiktok/fast_prompt_alignment"
                },
                "authors": [
                    {
                        "name": "Khalil Mrini"
                    },
                    {
                        "name": "Hanlin Lu"
                    },
                    {
                        "name": "Linjie Yang"
                    },
                    {
                        "name": "Weilin Huang"
                    },
                    {
                        "name": "Heng Wang"
                    }
                ],
                "author_detail": {
                    "name": "Heng Wang"
                },
                "author": "Heng Wang",
                "arxiv_comment": "TikTok Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08639v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08639v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08619v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08619v1",
                "updated": "2024-12-11T18:40:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    40,
                    16,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:40:16Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    40,
                    16,
                    2,
                    346,
                    0
                ],
                "title": "Synthetic Vision: Training Vision-Language Models to Understand Physics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Synthetic Vision: Training Vision-Language Models to Understand Physics"
                },
                "summary": "Physical reasoning, which involves the interpretation, understanding, and\nprediction of object behavior in dynamic environments, remains a significant\nchallenge for current Vision-Language Models (VLMs). In this work, we propose\ntwo methods to enhance VLMs' physical reasoning capabilities using simulated\ndata. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs\ngenerated from simulations relevant to physical reasoning tasks. Second, we\nintroduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to\ncreate scene descriptions enriched with physical properties and processes.\nDuring physical reasoning tasks, these PCBs can be leveraged as context to\nassist a Large Language Model (LLM) to improve its performance. We evaluate\nboth of our approaches using multiple benchmarks, including a new stability\ndetection QA dataset called Falling Tower, which includes both simulated and\nreal-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM\ncan significantly outperform larger state-of-the-art foundational models. We\nalso show that integrating PCBs boosts the performance of foundational LLMs on\nphysical reasoning tasks. Using the real-world scenes from the Falling Tower\ndataset, we also validate the robustness of both approaches in Sim2Real\ntransfer. Our results highlight the utility that simulated data can have in the\ncreation of learning systems capable of advanced physical reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physical reasoning, which involves the interpretation, understanding, and\nprediction of object behavior in dynamic environments, remains a significant\nchallenge for current Vision-Language Models (VLMs). In this work, we propose\ntwo methods to enhance VLMs' physical reasoning capabilities using simulated\ndata. First, we fine-tune a pre-trained VLM using question-answer (QA) pairs\ngenerated from simulations relevant to physical reasoning tasks. Second, we\nintroduce Physics Context Builders (PCBs), specialized VLMs fine-tuned to\ncreate scene descriptions enriched with physical properties and processes.\nDuring physical reasoning tasks, these PCBs can be leveraged as context to\nassist a Large Language Model (LLM) to improve its performance. We evaluate\nboth of our approaches using multiple benchmarks, including a new stability\ndetection QA dataset called Falling Tower, which includes both simulated and\nreal-world scenes, and CLEVRER. We demonstrate that a small QA fine-tuned VLM\ncan significantly outperform larger state-of-the-art foundational models. We\nalso show that integrating PCBs boosts the performance of foundational LLMs on\nphysical reasoning tasks. Using the real-world scenes from the Falling Tower\ndataset, we also validate the robustness of both approaches in Sim2Real\ntransfer. Our results highlight the utility that simulated data can have in the\ncreation of learning systems capable of advanced physical reasoning."
                },
                "authors": [
                    {
                        "name": "Vahid Balazadeh"
                    },
                    {
                        "name": "Mohammadmehdi Ataei"
                    },
                    {
                        "name": "Hyunmin Cheong"
                    },
                    {
                        "name": "Amir Hosein Khasahmadi"
                    },
                    {
                        "name": "Rahul G. Krishnan"
                    }
                ],
                "author_detail": {
                    "name": "Rahul G. Krishnan"
                },
                "author": "Rahul G. Krishnan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08619v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08619v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08615v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08615v1",
                "updated": "2024-12-11T18:37:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    37,
                    56,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:37:56Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    37,
                    56,
                    2,
                    346,
                    0
                ],
                "title": "Exploiting the Index Gradients for Optimization-Based Jailbreaking on\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploiting the Index Gradients for Optimization-Based Jailbreaking on\n  Large Language Models"
                },
                "summary": "Despite the advancements in training Large Language Models (LLMs) with\nalignment techniques to enhance the safety of generated content, these models\nremain susceptible to jailbreak, an adversarial attack method that exposes\nsecurity vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG)\nmethod has demonstrated the ability to automatically generate adversarial\nsuffixes that jailbreak state-of-the-art LLMs. However, the optimization\nprocess involved in GCG is highly time-consuming, rendering the jailbreaking\npipeline inefficient. In this paper, we investigate the process of GCG and\nidentify an issue of Indirect Effect, the key bottleneck of the GCG\noptimization. To this end, we propose the Model Attack Gradient Index GCG\n(MAGIC), that addresses the Indirect Effect by exploiting the gradient\ninformation of the suffix tokens, thereby accelerating the procedure by having\nless computation and fewer iterations. Our experiments on AdvBench show that\nMAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates\n(ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of\n74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on\nGPT-3.5. Code is available at https://github.com/jiah-li/magic.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the advancements in training Large Language Models (LLMs) with\nalignment techniques to enhance the safety of generated content, these models\nremain susceptible to jailbreak, an adversarial attack method that exposes\nsecurity vulnerabilities in LLMs. Notably, the Greedy Coordinate Gradient (GCG)\nmethod has demonstrated the ability to automatically generate adversarial\nsuffixes that jailbreak state-of-the-art LLMs. However, the optimization\nprocess involved in GCG is highly time-consuming, rendering the jailbreaking\npipeline inefficient. In this paper, we investigate the process of GCG and\nidentify an issue of Indirect Effect, the key bottleneck of the GCG\noptimization. To this end, we propose the Model Attack Gradient Index GCG\n(MAGIC), that addresses the Indirect Effect by exploiting the gradient\ninformation of the suffix tokens, thereby accelerating the procedure by having\nless computation and fewer iterations. Our experiments on AdvBench show that\nMAGIC achieves up to a 1.5x speedup, while maintaining Attack Success Rates\n(ASR) on par or even higher than other baselines. Our MAGIC achieved an ASR of\n74% on the Llama-2 and an ASR of 54% when conducting transfer attacks on\nGPT-3.5. Code is available at https://github.com/jiah-li/magic."
                },
                "authors": [
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Yongchang Hao"
                    },
                    {
                        "name": "Haoyu Xu"
                    },
                    {
                        "name": "Xing Wang"
                    },
                    {
                        "name": "Yu Hong"
                    }
                ],
                "author_detail": {
                    "name": "Yu Hong"
                },
                "author": "Yu Hong",
                "arxiv_comment": "13 pages,2 figures, accepted by The 31st International Conference on\n  Computational Linguistics",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08615v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08615v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08608v1",
                "updated": "2024-12-11T18:30:57Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    30,
                    57,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:30:57Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    30,
                    57,
                    2,
                    346,
                    0
                ],
                "title": "AdvWave: Stealthy Adversarial Jailbreak Attack against Large\n  Audio-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AdvWave: Stealthy Adversarial Jailbreak Attack against Large\n  Audio-Language Models"
                },
                "summary": "Recent advancements in large audio-language models (LALMs) have enabled\nspeech-based user interactions, significantly enhancing user experience and\naccelerating the deployment of LALMs in real-world applications. However,\nensuring the safety of LALMs is crucial to prevent risky outputs that may raise\nsocietal concerns or violate AI regulations. Despite the importance of this\nissue, research on jailbreaking LALMs remains limited due to their recent\nemergence and the additional technical challenges they present compared to\nattacks on DNN-based audio models. Specifically, the audio encoders in LALMs,\nwhich involve discretization operations, often lead to gradient shattering,\nhindering the effectiveness of attacks relying on gradient-based optimizations.\nThe behavioral variability of LALMs further complicates the identification of\neffective (adversarial) optimization targets. Moreover, enforcing stealthiness\nconstraints on adversarial audio waveforms introduces a reduced, non-convex\nfeasible solution space, further intensifying the challenges of the\noptimization process. To overcome these challenges, we develop AdvWave, the\nfirst jailbreak framework against LALMs. We propose a dual-phase optimization\nmethod that addresses gradient shattering, enabling effective end-to-end\ngradient-based optimization. Additionally, we develop an adaptive adversarial\ntarget search algorithm that dynamically adjusts the adversarial optimization\ntarget based on the response patterns of LALMs for specific queries. To ensure\nthat adversarial audio remains perceptually natural to human listeners, we\ndesign a classifier-guided optimization approach that generates adversarial\nnoise resembling common urban sounds. Extensive evaluations on multiple\nadvanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving\na 40% higher average jailbreak attack success rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large audio-language models (LALMs) have enabled\nspeech-based user interactions, significantly enhancing user experience and\naccelerating the deployment of LALMs in real-world applications. However,\nensuring the safety of LALMs is crucial to prevent risky outputs that may raise\nsocietal concerns or violate AI regulations. Despite the importance of this\nissue, research on jailbreaking LALMs remains limited due to their recent\nemergence and the additional technical challenges they present compared to\nattacks on DNN-based audio models. Specifically, the audio encoders in LALMs,\nwhich involve discretization operations, often lead to gradient shattering,\nhindering the effectiveness of attacks relying on gradient-based optimizations.\nThe behavioral variability of LALMs further complicates the identification of\neffective (adversarial) optimization targets. Moreover, enforcing stealthiness\nconstraints on adversarial audio waveforms introduces a reduced, non-convex\nfeasible solution space, further intensifying the challenges of the\noptimization process. To overcome these challenges, we develop AdvWave, the\nfirst jailbreak framework against LALMs. We propose a dual-phase optimization\nmethod that addresses gradient shattering, enabling effective end-to-end\ngradient-based optimization. Additionally, we develop an adaptive adversarial\ntarget search algorithm that dynamically adjusts the adversarial optimization\ntarget based on the response patterns of LALMs for specific queries. To ensure\nthat adversarial audio remains perceptually natural to human listeners, we\ndesign a classifier-guided optimization approach that generates adversarial\nnoise resembling common urban sounds. Extensive evaluations on multiple\nadvanced LALMs demonstrate that AdvWave outperforms baseline methods, achieving\na 40% higher average jailbreak attack success rate."
                },
                "authors": [
                    {
                        "name": "Mintong Kang"
                    },
                    {
                        "name": "Chejian Xu"
                    },
                    {
                        "name": "Bo Li"
                    }
                ],
                "author_detail": {
                    "name": "Bo Li"
                },
                "author": "Bo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07012v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07012v2",
                "updated": "2024-12-11T18:28:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    28,
                    0,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-09T21:44:02Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    21,
                    44,
                    2,
                    0,
                    344,
                    0
                ],
                "title": "ProVision: Programmatically Scaling Vision-centric Instruction Data for\n  Multimodal Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProVision: Programmatically Scaling Vision-centric Instruction Data for\n  Multimodal Language Models"
                },
                "summary": "With the rise of multimodal applications, instruction data has become\ncritical for training multimodal language models capable of understanding\ncomplex image-based queries. Existing practices rely on powerful but costly\nlarge language models (LLMs) or multimodal language models (MLMs) to produce\ninstruction data. These are often prone to hallucinations, licensing issues and\nthe generation process is often hard to scale and interpret. In this work, we\npresent a programmatic approach that employs scene graphs as symbolic\nrepresentations of images and human-written programs to systematically\nsynthesize vision-centric instruction data. Our approach ensures the\ninterpretability and controllability of the data generation process and scales\nefficiently while maintaining factual accuracy. By implementing a suite of 24\nsingle-image, 14 multi-image instruction generators, and a scene graph\ngeneration pipeline, we build a scalable, cost-effective system: ProVision\nwhich produces diverse question-answer pairs concerning objects, attributes,\nrelations, depth, etc., for any given image. Applied to Visual Genome and\nDataComp datasets, we generate over 10 million instruction data points,\nProVision-10M, and leverage them in both pretraining and instruction tuning\nstages of MLMs. When adopted in the instruction tuning stage, our single-image\ninstruction data yields up to a 7% improvement on the 2D split and 8% on the 3D\nsplit of CVBench, along with a 3% increase in performance on QBench2,\nRealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%\nimprovement on Mantis-Eval. Incorporation of our data in both pre-training and\nfine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%\nacross 11 benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rise of multimodal applications, instruction data has become\ncritical for training multimodal language models capable of understanding\ncomplex image-based queries. Existing practices rely on powerful but costly\nlarge language models (LLMs) or multimodal language models (MLMs) to produce\ninstruction data. These are often prone to hallucinations, licensing issues and\nthe generation process is often hard to scale and interpret. In this work, we\npresent a programmatic approach that employs scene graphs as symbolic\nrepresentations of images and human-written programs to systematically\nsynthesize vision-centric instruction data. Our approach ensures the\ninterpretability and controllability of the data generation process and scales\nefficiently while maintaining factual accuracy. By implementing a suite of 24\nsingle-image, 14 multi-image instruction generators, and a scene graph\ngeneration pipeline, we build a scalable, cost-effective system: ProVision\nwhich produces diverse question-answer pairs concerning objects, attributes,\nrelations, depth, etc., for any given image. Applied to Visual Genome and\nDataComp datasets, we generate over 10 million instruction data points,\nProVision-10M, and leverage them in both pretraining and instruction tuning\nstages of MLMs. When adopted in the instruction tuning stage, our single-image\ninstruction data yields up to a 7% improvement on the 2D split and 8% on the 3D\nsplit of CVBench, along with a 3% increase in performance on QBench2,\nRealWorldQA, and MMMU. Our multi-image instruction data leads to an 8%\nimprovement on Mantis-Eval. Incorporation of our data in both pre-training and\nfine-tuning stages of xGen-MM-4B leads to an averaged improvement of 1.6%\nacross 11 benchmarks."
                },
                "authors": [
                    {
                        "name": "Jieyu Zhang"
                    },
                    {
                        "name": "Le Xue"
                    },
                    {
                        "name": "Linxin Song"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Weikai Huang"
                    },
                    {
                        "name": "Manli Shu"
                    },
                    {
                        "name": "An Yan"
                    },
                    {
                        "name": "Zixian Ma"
                    },
                    {
                        "name": "Juan Carlos Niebles"
                    },
                    {
                        "name": "silvio savarese"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Zeyuan Chen"
                    },
                    {
                        "name": "Ranjay Krishna"
                    },
                    {
                        "name": "Ran Xu"
                    }
                ],
                "author_detail": {
                    "name": "Ran Xu"
                },
                "author": "Ran Xu",
                "arxiv_comment": "code: https://github.com/JieyuZ2/ProVision dataset:\n  https://huggingface.co/datasets/Salesforce/ProVision-10M",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07012v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07012v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08604v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08604v1",
                "updated": "2024-12-11T18:26:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    26,
                    55,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:26:55Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    26,
                    55,
                    2,
                    346,
                    0
                ],
                "title": "Preference Discerning with LLM-Enhanced Generative Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Preference Discerning with LLM-Enhanced Generative Retrieval"
                },
                "summary": "Sequential recommendation systems aim to provide personalized recommendations\nfor users based on their interaction history. To achieve this, they often\nincorporate auxiliary information, such as textual descriptions of items and\nauxiliary tasks, like predicting user preferences and intent. Despite numerous\nefforts to enhance these models, they still suffer from limited\npersonalization. To address this issue, we propose a new paradigm, which we\nterm preference discerning. In preference dscerning, we explicitly condition a\ngenerative sequential recommendation system on user preferences within its\ncontext. To this end, we generate user preferences using Large Language Models\n(LLMs) based on user reviews and item-specific data. To evaluate preference\ndiscerning capabilities of sequential recommendation systems, we introduce a\nnovel benchmark that provides a holistic evaluation across various scenarios,\nincluding preference steering and sentiment following. We assess current\nstate-of-the-art methods using our benchmark and show that they struggle to\naccurately discern user preferences. Therefore, we propose a new method named\nMender ($\\textbf{M}$ultimodal Prefer$\\textbf{en}$ce\n$\\textbf{d}$iscern$\\textbf{er}$), which improves upon existing methods and\nachieves state-of-the-art performance on our benchmark. Our results show that\nMender can be effectively guided by human preferences even though they have not\nbeen observed during training, paving the way toward more personalized\nsequential recommendation systems. We will open-source the code and benchmarks\nupon publication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sequential recommendation systems aim to provide personalized recommendations\nfor users based on their interaction history. To achieve this, they often\nincorporate auxiliary information, such as textual descriptions of items and\nauxiliary tasks, like predicting user preferences and intent. Despite numerous\nefforts to enhance these models, they still suffer from limited\npersonalization. To address this issue, we propose a new paradigm, which we\nterm preference discerning. In preference dscerning, we explicitly condition a\ngenerative sequential recommendation system on user preferences within its\ncontext. To this end, we generate user preferences using Large Language Models\n(LLMs) based on user reviews and item-specific data. To evaluate preference\ndiscerning capabilities of sequential recommendation systems, we introduce a\nnovel benchmark that provides a holistic evaluation across various scenarios,\nincluding preference steering and sentiment following. We assess current\nstate-of-the-art methods using our benchmark and show that they struggle to\naccurately discern user preferences. Therefore, we propose a new method named\nMender ($\\textbf{M}$ultimodal Prefer$\\textbf{en}$ce\n$\\textbf{d}$iscern$\\textbf{er}$), which improves upon existing methods and\nachieves state-of-the-art performance on our benchmark. Our results show that\nMender can be effectively guided by human preferences even though they have not\nbeen observed during training, paving the way toward more personalized\nsequential recommendation systems. We will open-source the code and benchmarks\nupon publication."
                },
                "authors": [
                    {
                        "name": "Fabian Paischer"
                    },
                    {
                        "name": "Liu Yang"
                    },
                    {
                        "name": "Linfeng Liu"
                    },
                    {
                        "name": "Shuai Shao"
                    },
                    {
                        "name": "Kaveh Hassani"
                    },
                    {
                        "name": "Jiacheng Li"
                    },
                    {
                        "name": "Ricky Chen"
                    },
                    {
                        "name": "Zhang Gabriel Li"
                    },
                    {
                        "name": "Xialo Gao"
                    },
                    {
                        "name": "Wei Shao"
                    },
                    {
                        "name": "Xue Feng"
                    },
                    {
                        "name": "Nima Noorshams"
                    },
                    {
                        "name": "Sem Park"
                    },
                    {
                        "name": "Bo Long"
                    },
                    {
                        "name": "Hamid Eghbalzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Hamid Eghbalzadeh"
                },
                "author": "Hamid Eghbalzadeh",
                "arxiv_comment": "11 pages + references and appendix",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08604v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08604v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.12151v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.12151v3",
                "updated": "2024-12-11T18:12:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    12,
                    43,
                    2,
                    346,
                    0
                ],
                "published": "2024-03-18T18:08:44Z",
                "published_parsed": [
                    2024,
                    3,
                    18,
                    18,
                    8,
                    44,
                    0,
                    78,
                    0
                ],
                "title": "Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusing Domain-Specific Content from Large Language Models into Knowledge\n  Graphs for Enhanced Zero Shot Object State Classification"
                },
                "summary": "Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Domain-specific knowledge can significantly contribute to addressing a wide\nvariety of vision tasks. However, the generation of such knowledge entails\nconsiderable human labor and time costs. This study investigates the potential\nof Large Language Models (LLMs) in generating and providing domain-specific\ninformation through semantic embeddings. To achieve this, an LLM is integrated\ninto a pipeline that utilizes Knowledge Graphs and pre-trained semantic vectors\nin the context of the Vision-based Zero-shot Object State Classification task.\nWe thoroughly examine the behavior of the LLM through an extensive ablation\nstudy. Our findings reveal that the integration of LLM-based embeddings, in\ncombination with general-purpose pre-trained embeddings, leads to substantial\nperformance improvements. Drawing insights from this ablation study, we conduct\na comparative analysis against competing models, thereby highlighting the\nstate-of-the-art performance achieved by the proposed approach."
                },
                "authors": [
                    {
                        "name": "Filippos Gouidis"
                    },
                    {
                        "name": "Katerina Papantoniou"
                    },
                    {
                        "name": "Konstantinos Papoutsakis"
                    },
                    {
                        "name": "Theodore Patkos"
                    },
                    {
                        "name": "Antonis Argyros"
                    },
                    {
                        "name": "Dimitris Plexousakis"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Plexousakis"
                },
                "author": "Dimitris Plexousakis",
                "arxiv_doi": "10.1609/aaaiss.v3i1.31190",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1609/aaaiss.v3i1.31190",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.12151v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.12151v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at the AAAI-MAKE 2024",
                "arxiv_journal_ref": "Proceedings of the AAAI Spring Symposium, 2024, pages 115-124",
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08593v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08593v1",
                "updated": "2024-12-11T18:11:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    11,
                    39,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:11:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    11,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based\n  Automated Requirement Traceability and Compliance Checks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Graph-RAG and Prompt Engineering to Enhance LLM-Based\n  Automated Requirement Traceability and Compliance Checks"
                },
                "summary": "Ensuring that Software Requirements Specifications (SRS) align with\nhigher-level organizational or national requirements is vital, particularly in\nregulated environments such as finance and aerospace. In these domains,\nmaintaining consistency, adhering to regulatory frameworks, minimizing errors,\nand meeting critical expectations are essential for the reliable functioning of\nsystems. The widespread adoption of large language models (LLMs) highlights\ntheir immense potential, yet there remains considerable scope for improvement\nin retrieving relevant information and enhancing reasoning capabilities. This\nstudy demonstrates that integrating a robust Graph-RAG framework with advanced\nprompt engineering techniques, such as Chain of Thought and Tree of Thought,\ncan significantly enhance performance. Compared to baseline RAG methods and\nsimple prompting strategies, this approach delivers more accurate and\ncontext-aware results. While this method demonstrates significant improvements\nin performance, it comes with challenges. It is both costly and more complex to\nimplement across diverse contexts, requiring careful adaptation to specific\nscenarios. Additionally, its effectiveness heavily relies on having complete\nand accurate input data, which may not always be readily available, posing\nfurther limitations to its scalability and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring that Software Requirements Specifications (SRS) align with\nhigher-level organizational or national requirements is vital, particularly in\nregulated environments such as finance and aerospace. In these domains,\nmaintaining consistency, adhering to regulatory frameworks, minimizing errors,\nand meeting critical expectations are essential for the reliable functioning of\nsystems. The widespread adoption of large language models (LLMs) highlights\ntheir immense potential, yet there remains considerable scope for improvement\nin retrieving relevant information and enhancing reasoning capabilities. This\nstudy demonstrates that integrating a robust Graph-RAG framework with advanced\nprompt engineering techniques, such as Chain of Thought and Tree of Thought,\ncan significantly enhance performance. Compared to baseline RAG methods and\nsimple prompting strategies, this approach delivers more accurate and\ncontext-aware results. While this method demonstrates significant improvements\nin performance, it comes with challenges. It is both costly and more complex to\nimplement across diverse contexts, requiring careful adaptation to specific\nscenarios. Additionally, its effectiveness heavily relies on having complete\nand accurate input data, which may not always be readily available, posing\nfurther limitations to its scalability and practicality."
                },
                "authors": [
                    {
                        "name": "Arsalan Masoudifard"
                    },
                    {
                        "name": "Mohammad Mowlavi Sorond"
                    },
                    {
                        "name": "Moein Madadi"
                    },
                    {
                        "name": "Mohammad Sabokrou"
                    },
                    {
                        "name": "Elahe Habibi"
                    }
                ],
                "author_detail": {
                    "name": "Elahe Habibi"
                },
                "author": "Elahe Habibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08593v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08593v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.16822v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.16822v3",
                "updated": "2024-12-11T18:07:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    7,
                    25,
                    2,
                    346,
                    0
                ],
                "published": "2024-02-26T18:47:27Z",
                "published_parsed": [
                    2024,
                    2,
                    26,
                    18,
                    47,
                    27,
                    0,
                    57,
                    0
                ],
                "title": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts"
                },
                "summary": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nprompts generated by Rainbow Teaming are highly transferable and that\nfine-tuning models with synthetic data generated by our method significantly\nenhances their safety without sacrificing general performance or helpfulness.\nWe additionally explore the versatility of Rainbow Teaming by applying it to\nquestion answering and cybersecurity, showcasing its potential to drive robust\nopen-ended self-improvement in a wide range of applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) become increasingly prevalent across many\nreal-world applications, understanding and enhancing their robustness to\nadversarial attacks is of paramount importance. Existing methods for\nidentifying adversarial prompts tend to focus on specific domains, lack\ndiversity, or require extensive human annotations. To address these\nlimitations, we present Rainbow Teaming, a novel black-box approach for\nproducing a diverse collection of adversarial prompts. Rainbow Teaming casts\nadversarial prompt generation as a quality-diversity problem and uses\nopen-ended search to generate prompts that are both effective and diverse.\nFocusing on the safety domain, we use Rainbow Teaming to target various\nstate-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach\nreveals hundreds of effective adversarial prompts, with an attack success rate\nexceeding 90% across all tested models. Furthermore, we demonstrate that\nprompts generated by Rainbow Teaming are highly transferable and that\nfine-tuning models with synthetic data generated by our method significantly\nenhances their safety without sacrificing general performance or helpfulness.\nWe additionally explore the versatility of Rainbow Teaming by applying it to\nquestion answering and cybersecurity, showcasing its potential to drive robust\nopen-ended self-improvement in a wide range of applications."
                },
                "authors": [
                    {
                        "name": "Mikayel Samvelyan"
                    },
                    {
                        "name": "Sharath Chandra Raparthy"
                    },
                    {
                        "name": "Andrei Lupu"
                    },
                    {
                        "name": "Eric Hambro"
                    },
                    {
                        "name": "Aram H. Markosyan"
                    },
                    {
                        "name": "Manish Bhatt"
                    },
                    {
                        "name": "Yuning Mao"
                    },
                    {
                        "name": "Minqi Jiang"
                    },
                    {
                        "name": "Jack Parker-Holder"
                    },
                    {
                        "name": "Jakob Foerster"
                    },
                    {
                        "name": "Tim RocktÃ¤schel"
                    },
                    {
                        "name": "Roberta Raileanu"
                    }
                ],
                "author_detail": {
                    "name": "Roberta Raileanu"
                },
                "author": "Roberta Raileanu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.16822v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.16822v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08587v1",
                "updated": "2024-12-11T18:06:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    6,
                    44,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:06:44Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    6,
                    44,
                    2,
                    346,
                    0
                ],
                "title": "Advancing Single- and Multi-task Text Classification through Large\n  Language Model Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancing Single- and Multi-task Text Classification through Large\n  Language Model Fine-tuning"
                },
                "summary": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Both encoder-only models (e.g., BERT, RoBERTa) and large language models\n(LLMs, e.g., Llama3) have been widely used for text classification tasks.\nHowever, there is a lack of systematic studies comparing the performance of\nencoder-based models and LLMs in text classification, particularly when\nfine-tuning is involved. This study employed a diverse range of models and\nmethods, varying in size and architecture, and including both fine-tuned and\npre-trained approaches. We first assessed the performances of these LLMs on the\n20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only\nRoBERTa models. Additionally, we explored the multi-task capabilities of both\nmodel types by combining multiple classification tasks, including intent\ndetection and slot-filling, into a single model using data from both datasets.\nOur results indicate that fully fine-tuned Llama3-70B models outperform\nRoBERTa-large and other decoder LLMs across various classification tasks and\ndatasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the\nperformance of dual-model setups in both tasks across both datasets. Overall,\nour study provides a comprehensive benchmark of encoder-only and LLM models on\ntext classification tasks and demonstrates a method to combine two or more\nfully fine-tuned decoder LLMs for reduced latency and equivalent performance."
                },
                "authors": [
                    {
                        "name": "Hang Zhao"
                    },
                    {
                        "name": "Qile P. Chen"
                    },
                    {
                        "name": "Yijing Barry Zhang"
                    },
                    {
                        "name": "Gang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Yang"
                },
                "author": "Gang Yang",
                "arxiv_comment": "9 pages, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08585v1",
                "updated": "2024-12-11T18:03:05Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T18:03:05Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    18,
                    3,
                    5,
                    2,
                    346,
                    0
                ],
                "title": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TURBOATTENTION: Efficient Attention Approximation For High Throughputs\n  LLMs"
                },
                "summary": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference demands significant amount of\ncomputation and memory, especially in the key attention mechanism. While\ntechniques, such as quantization and acceleration algorithms, like\nFlashAttention, have improved efficiency of the overall inference, they address\ndifferent aspects of the problem: quantization focuses on weight-activation\noperations, while FlashAttention improves execution but requires high-precision\nformats. Recent Key-value (KV) cache quantization reduces memory bandwidth but\nstill needs floating-point dequantization for attention operation.\n  We present TurboAttention, a comprehensive approach to enable quantized\nexecution of attention that simultaneously addresses both memory and\ncomputational efficiency. Our solution introduces two key innovations: FlashQ,\na headwise attention quantization technique that enables both compression of KV\ncache and quantized execution of activation-activation multiplication, and\nSparsity-based Softmax Approximation (SAS), which eliminates the need for\ndequantization to FP32 during exponentiation operation in attention.\nExperimental results demonstrate that TurboAttention achieves 1.2-1.8x speedup\nin attention, reduces the KV cache size by over 4.4x, and enables up to 2.37x\nmaximum throughput over the FP16 baseline while outperforming state-of-the-art\nquantization and compression techniques across various datasets and models."
                },
                "authors": [
                    {
                        "name": "Hao Kang"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "James Hensman"
                    },
                    {
                        "name": "Tushar Krishna"
                    },
                    {
                        "name": "Victor Ruhle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08581v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08581v1",
                "updated": "2024-12-11T17:57:23Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    57,
                    23,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:57:23Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    57,
                    23,
                    2,
                    346,
                    0
                ],
                "title": "Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge:\n  Feasibility, Challenges, and Road Ahead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated Soap Opera Testing Directed by LLMs and Scenario Knowledge:\n  Feasibility, Challenges, and Road Ahead"
                },
                "summary": "Exploratory testing (ET) harnesses tester's knowledge, creativity, and\nexperience to create varying tests that uncover unexpected bugs from the\nend-user's perspective. Although ET has proven effective in system-level\ntesting of interactive systems, the need for manual execution has hindered\nlarge-scale adoption. In this work, we explore the feasibility, challenges and\nroad ahead of automated scenario-based ET (a.k.a soap opera testing). We\nconduct a formative study, identifying key insights for effective manual soap\nopera testing and challenges in automating the process. We then develop a\nmulti-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to\nautomate soap opera testing. The system consists of three multi-modal agents,\nPlanner, Player, and Detector that collaborate to execute tests and identify\npotential bugs. Experimental results demonstrate the potential of automated\nsoap opera testing, but there remains a significant gap compared to manual\nexecution, especially under-explored scenario boundaries and incorrectly\nidentified bugs. Based on the observation, we envision road ahead for the\nfuture of automated soap opera testing, focusing on three key aspects: the\nsynergy of neural and symbolic approaches, human-AI co-learning, and the\nintegration of soap opera testing with broader software engineering practices.\nThese insights aim to guide and inspire the future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploratory testing (ET) harnesses tester's knowledge, creativity, and\nexperience to create varying tests that uncover unexpected bugs from the\nend-user's perspective. Although ET has proven effective in system-level\ntesting of interactive systems, the need for manual execution has hindered\nlarge-scale adoption. In this work, we explore the feasibility, challenges and\nroad ahead of automated scenario-based ET (a.k.a soap opera testing). We\nconduct a formative study, identifying key insights for effective manual soap\nopera testing and challenges in automating the process. We then develop a\nmulti-agent system leveraging LLMs and a Scenario Knowledge Graph (SKG) to\nautomate soap opera testing. The system consists of three multi-modal agents,\nPlanner, Player, and Detector that collaborate to execute tests and identify\npotential bugs. Experimental results demonstrate the potential of automated\nsoap opera testing, but there remains a significant gap compared to manual\nexecution, especially under-explored scenario boundaries and incorrectly\nidentified bugs. Based on the observation, we envision road ahead for the\nfuture of automated soap opera testing, focusing on three key aspects: the\nsynergy of neural and symbolic approaches, human-AI co-learning, and the\nintegration of soap opera testing with broader software engineering practices.\nThese insights aim to guide and inspire the future research."
                },
                "authors": [
                    {
                        "name": "Yanqi Su"
                    },
                    {
                        "name": "Zhenchang Xing"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Chunyang Chen"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Qinghua Lu"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "arxiv_comment": "20 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08581v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08581v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08568v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08568v1",
                "updated": "2024-12-11T17:33:51Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    33,
                    51,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:33:51Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    33,
                    51,
                    2,
                    346,
                    0
                ],
                "title": "Real-Time Trajectory Generation for Soft Robot Manipulators Using\n  Differential Flatness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Trajectory Generation for Soft Robot Manipulators Using\n  Differential Flatness"
                },
                "summary": "Soft robots have the potential to interact with sensitive environments and\nperform complex tasks effectively. However, motion plans and trajectories for\nsoft manipulators are challenging to calculate due to their deformable nature\nand nonlinear dynamics. This article introduces a fast real-time trajectory\ngeneration approach for soft robot manipulators, which creates\ndynamically-feasible motions for arbitrary kinematically-feasible paths of the\nrobot's end effector. Our insight is that piecewise constant curvature (PCC)\ndynamics models of soft robots can be differentially flat, therefore control\ninputs can be calculated algebraically rather than through a nonlinear\ndifferential equation. We prove this flatness under certain conditions, with\nthe curvatures of the robot as the flat outputs. Our two-step trajectory\ngeneration approach uses an inverse kinematics procedure to calculate a motion\nplan of robot curvatures per end-effector position, then, our flatness\ndiffeomorphism generates corresponding control inputs that respect velocity. We\nvalidate our approach through simulations of our representative soft robot\nmanipulator along three different trajectories, demonstrating a margin of 23x\nfaster than real-time at a frequency of 100 Hz. This approach could allow fast\nverifiable replanning of soft robots' motions in safety-critical physical\nenvironments, crucial for deployment in the real world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft robots have the potential to interact with sensitive environments and\nperform complex tasks effectively. However, motion plans and trajectories for\nsoft manipulators are challenging to calculate due to their deformable nature\nand nonlinear dynamics. This article introduces a fast real-time trajectory\ngeneration approach for soft robot manipulators, which creates\ndynamically-feasible motions for arbitrary kinematically-feasible paths of the\nrobot's end effector. Our insight is that piecewise constant curvature (PCC)\ndynamics models of soft robots can be differentially flat, therefore control\ninputs can be calculated algebraically rather than through a nonlinear\ndifferential equation. We prove this flatness under certain conditions, with\nthe curvatures of the robot as the flat outputs. Our two-step trajectory\ngeneration approach uses an inverse kinematics procedure to calculate a motion\nplan of robot curvatures per end-effector position, then, our flatness\ndiffeomorphism generates corresponding control inputs that respect velocity. We\nvalidate our approach through simulations of our representative soft robot\nmanipulator along three different trajectories, demonstrating a margin of 23x\nfaster than real-time at a frequency of 100 Hz. This approach could allow fast\nverifiable replanning of soft robots' motions in safety-critical physical\nenvironments, crucial for deployment in the real world."
                },
                "authors": [
                    {
                        "name": "Akua Dickson"
                    },
                    {
                        "name": "Juan C. Pacheco Garcia"
                    },
                    {
                        "name": "Ran Jing"
                    },
                    {
                        "name": "Meredith L. Anderson"
                    },
                    {
                        "name": "Andrew P. Sabelhaus"
                    }
                ],
                "author_detail": {
                    "name": "Andrew P. Sabelhaus"
                },
                "author": "Andrew P. Sabelhaus",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08568v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08568v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08564v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08564v1",
                "updated": "2024-12-11T17:32:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    32,
                    21,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:32:21Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    32,
                    21,
                    2,
                    346,
                    0
                ],
                "title": "Can We Generate Visual Programs Without Prompting LLMs?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can We Generate Visual Programs Without Prompting LLMs?"
                },
                "summary": "Visual programming prompts LLMs (large language mod-els) to generate\nexecutable code for visual tasks like visual question answering (VQA).\nPrompt-based methods are difficult to improve while also being unreliable and\ncostly in both time and money. Our goal is to develop an efficient visual\nprogramming system without 1) using prompt-based LLMs at inference time and 2)\na large set of program and answer annotations. We develop a synthetic data\naugmentation approach and alternative program generation method based on\ndecoupling programs into higher-level skills called templates and the\ncorresponding arguments. Our results show that with data augmentation,\nprompt-free smaller LLMs ($\\approx$ 1B parameters) are competitive with\nstate-of-the art models with the added benefit of much faster inference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual programming prompts LLMs (large language mod-els) to generate\nexecutable code for visual tasks like visual question answering (VQA).\nPrompt-based methods are difficult to improve while also being unreliable and\ncostly in both time and money. Our goal is to develop an efficient visual\nprogramming system without 1) using prompt-based LLMs at inference time and 2)\na large set of program and answer annotations. We develop a synthetic data\naugmentation approach and alternative program generation method based on\ndecoupling programs into higher-level skills called templates and the\ncorresponding arguments. Our results show that with data augmentation,\nprompt-free smaller LLMs ($\\approx$ 1B parameters) are competitive with\nstate-of-the art models with the added benefit of much faster inference"
                },
                "authors": [
                    {
                        "name": "Michal Shlapentokh-Rothman"
                    },
                    {
                        "name": "Yu-Xiong Wang"
                    },
                    {
                        "name": "Derek Hoiem"
                    }
                ],
                "author_detail": {
                    "name": "Derek Hoiem"
                },
                "author": "Derek Hoiem",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08564v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08564v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01999v2",
                "updated": "2024-12-11T17:31:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    31,
                    19,
                    2,
                    346,
                    0
                ],
                "published": "2024-10-02T20:04:02Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    20,
                    4,
                    2,
                    2,
                    276,
                    0
                ],
                "title": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding\n  Capabilities of CodeLLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding\n  Capabilities of CodeLLMs"
                },
                "summary": "Recent advancements in Code Large Language Models (CodeLLMs) have\npredominantly focused on open-ended code generation tasks, often neglecting the\ncritical aspect of code understanding and comprehension. To bridge this gap, we\npresent CodeMMLU, a comprehensive multiple-choice question-answer benchmark\ndesigned to evaluate the depth of software and code understanding in LLMs.\nCodeMMLU includes over 10,000 questions sourced from diverse domains,\nencompassing tasks such as code analysis, defect detection, and software\nengineering principles across multiple programming languages. Unlike\ntraditional benchmarks, CodeMMLU assesses models's ability to reason about code\nrather than merely generate it, providing deeper insights into their grasp of\ncomplex software concepts and systems. Our extensive evaluation reveals that\neven state-of-the-art models face significant challenges with CodeMMLU,\nhighlighting deficiencies in comprehension beyond code generation. By\nunderscoring the crucial relationship between code understanding and effective\ngeneration, CodeMMLU serves as a vital resource for advancing AI-assisted\nsoftware development, ultimately aiming to create more reliable and capable\ncoding assistants.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Code Large Language Models (CodeLLMs) have\npredominantly focused on open-ended code generation tasks, often neglecting the\ncritical aspect of code understanding and comprehension. To bridge this gap, we\npresent CodeMMLU, a comprehensive multiple-choice question-answer benchmark\ndesigned to evaluate the depth of software and code understanding in LLMs.\nCodeMMLU includes over 10,000 questions sourced from diverse domains,\nencompassing tasks such as code analysis, defect detection, and software\nengineering principles across multiple programming languages. Unlike\ntraditional benchmarks, CodeMMLU assesses models's ability to reason about code\nrather than merely generate it, providing deeper insights into their grasp of\ncomplex software concepts and systems. Our extensive evaluation reveals that\neven state-of-the-art models face significant challenges with CodeMMLU,\nhighlighting deficiencies in comprehension beyond code generation. By\nunderscoring the crucial relationship between code understanding and effective\ngeneration, CodeMMLU serves as a vital resource for advancing AI-assisted\nsoftware development, ultimately aiming to create more reliable and capable\ncoding assistants."
                },
                "authors": [
                    {
                        "name": "Dung Nguyen Manh"
                    },
                    {
                        "name": "Thang Phan Chau"
                    },
                    {
                        "name": "Nam Le Hai"
                    },
                    {
                        "name": "Thong T. Doan"
                    },
                    {
                        "name": "Nam V. Nguyen"
                    },
                    {
                        "name": "Quang Pham"
                    },
                    {
                        "name": "Nghi D. Q. Bui"
                    }
                ],
                "author_detail": {
                    "name": "Nghi D. Q. Bui"
                },
                "author": "Nghi D. Q. Bui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08559v1",
                "updated": "2024-12-11T17:22:07Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    22,
                    7,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T17:22:07Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    17,
                    22,
                    7,
                    2,
                    346,
                    0
                ],
                "title": "Underestimated Privacy Risks for Minority Populations in Large Language\n  Model Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Underestimated Privacy Risks for Minority Populations in Large Language\n  Model Unlearning"
                },
                "summary": "Large Language Models are trained on extensive datasets that often contain\nsensitive, human-generated information, raising significant concerns about\nprivacy breaches. While certified unlearning approaches offer strong privacy\nguarantees, they rely on restrictive model assumptions that are not applicable\nto LLMs. As a result, various unlearning heuristics have been proposed, with\nthe associated privacy risks assessed only empirically. The standard evaluation\npipelines typically randomly select data for removal from the training set,\napply unlearning techniques, and use membership inference attacks to compare\nthe unlearned models against models retrained without the to-be-unlearned data.\nHowever, since every data point is subject to the right to be forgotten,\nunlearning should be considered in the worst-case scenario from the privacy\nperspective. Prior work shows that data outliers may exhibit higher\nmemorization effects. Intuitively, they are harder to be unlearn and thus the\nprivacy risk of unlearning them is underestimated in the current evaluation. In\nthis paper, we leverage minority data to identify such a critical flaw in\npreviously widely adopted evaluations. We substantiate this claim through\ncarefully designed experiments, including unlearning canaries related to\nminority groups, inspired by privacy auditing literature. Using personally\nidentifiable information as a representative minority identifier, we\ndemonstrate that minority groups experience at least 20% more privacy leakage\nin most cases across six unlearning approaches, three MIAs, three benchmark\ndatasets, and two LLMs of different scales. Given that the right to be\nforgotten should be upheld for every individual, we advocate for a more\nrigorous evaluation of LLM unlearning methods. Our minority-aware evaluation\nframework represents an initial step toward ensuring more equitable assessments\nof LLM unlearning efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are trained on extensive datasets that often contain\nsensitive, human-generated information, raising significant concerns about\nprivacy breaches. While certified unlearning approaches offer strong privacy\nguarantees, they rely on restrictive model assumptions that are not applicable\nto LLMs. As a result, various unlearning heuristics have been proposed, with\nthe associated privacy risks assessed only empirically. The standard evaluation\npipelines typically randomly select data for removal from the training set,\napply unlearning techniques, and use membership inference attacks to compare\nthe unlearned models against models retrained without the to-be-unlearned data.\nHowever, since every data point is subject to the right to be forgotten,\nunlearning should be considered in the worst-case scenario from the privacy\nperspective. Prior work shows that data outliers may exhibit higher\nmemorization effects. Intuitively, they are harder to be unlearn and thus the\nprivacy risk of unlearning them is underestimated in the current evaluation. In\nthis paper, we leverage minority data to identify such a critical flaw in\npreviously widely adopted evaluations. We substantiate this claim through\ncarefully designed experiments, including unlearning canaries related to\nminority groups, inspired by privacy auditing literature. Using personally\nidentifiable information as a representative minority identifier, we\ndemonstrate that minority groups experience at least 20% more privacy leakage\nin most cases across six unlearning approaches, three MIAs, three benchmark\ndatasets, and two LLMs of different scales. Given that the right to be\nforgotten should be upheld for every individual, we advocate for a more\nrigorous evaluation of LLM unlearning methods. Our minority-aware evaluation\nframework represents an initial step toward ensuring more equitable assessments\nof LLM unlearning efficacy."
                },
                "authors": [
                    {
                        "name": "Rongzhe Wei"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Mohsen Ghassemi"
                    },
                    {
                        "name": "Eleonora KreaÄiÄ"
                    },
                    {
                        "name": "Yifan Li"
                    },
                    {
                        "name": "Xiang Yue"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Vamsi K. Potluru"
                    },
                    {
                        "name": "Pan Li"
                    },
                    {
                        "name": "Eli Chien"
                    }
                ],
                "author_detail": {
                    "name": "Eli Chien"
                },
                "author": "Eli Chien",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.10715v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.10715v5",
                "updated": "2024-12-11T16:59:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    59,
                    50,
                    2,
                    346,
                    0
                ],
                "published": "2023-06-19T06:22:02Z",
                "published_parsed": [
                    2023,
                    6,
                    19,
                    6,
                    22,
                    2,
                    0,
                    170,
                    0
                ],
                "title": "Robust Multi-Agent Control via Maximum Entropy Heterogeneous-Agent\n  Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Multi-Agent Control via Maximum Entropy Heterogeneous-Agent\n  Reinforcement Learning"
                },
                "summary": "In multi-agent reinforcement learning, optimal control with robustness\nguarantees are critical for its deployment in real world. However, existing\nmethods face challenges related to sample complexity, training instability,\npotential suboptimal Nash Equilibrium convergence and non-robustness to\nmultiple perturbations. In this paper, we propose a unified framework for\nlearning \\emph{stochastic} policies to resolve these issues. We embed\ncooperative MARL problems into probabilistic graphical models, from which we\nderive the maximum entropy (MaxEnt) objective optimal for MARL. Based on the\nMaxEnt framework, we propose \\emph{Heterogeneous-Agent Soft Actor-Critic}\n(HASAC) algorithm. Theoretically, we prove the monotonic improvement and\nconvergence to \\emph{quantal response equilibrium} (QRE) properties of HASAC.\nFurthermore, HASAC is provably robust against a wide range of real-world\nuncertainties, including perturbations in rewards, environment dynamics,\nstates, and actions. Finally, we generalize a unified template for MaxEnt\nalgorithmic design named \\emph{Maximum Entropy Heterogeneous-Agent Mirror\nLearning} (MEHAML), which provides any induced method with the same guarantees\nas HASAC. We evaluate HASAC on seven benchmarks: Bi-DexHands, Multi-Agent\nMuJoCo, Pursuit-Evade, StarCraft Multi-Agent Challenge, Google Research\nFootball, Multi-Agent Particle Environment, Light Aircraft Game. Results show\nthat HASAC consistently outperforms strong baselines in 34 out of 38 tasks,\nexhibiting improved training stability, better sample efficiency and sufficient\nexploration. The robustness of HASAC was further validated when encountering\nuncertainties in rewards, dynamics, states, and actions of 14 magnitudes, and\nreal-world deployment in a multi-robot arena against these four types of\nuncertainties. See our page at \\url{https://sites.google.com/view/meharl}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In multi-agent reinforcement learning, optimal control with robustness\nguarantees are critical for its deployment in real world. However, existing\nmethods face challenges related to sample complexity, training instability,\npotential suboptimal Nash Equilibrium convergence and non-robustness to\nmultiple perturbations. In this paper, we propose a unified framework for\nlearning \\emph{stochastic} policies to resolve these issues. We embed\ncooperative MARL problems into probabilistic graphical models, from which we\nderive the maximum entropy (MaxEnt) objective optimal for MARL. Based on the\nMaxEnt framework, we propose \\emph{Heterogeneous-Agent Soft Actor-Critic}\n(HASAC) algorithm. Theoretically, we prove the monotonic improvement and\nconvergence to \\emph{quantal response equilibrium} (QRE) properties of HASAC.\nFurthermore, HASAC is provably robust against a wide range of real-world\nuncertainties, including perturbations in rewards, environment dynamics,\nstates, and actions. Finally, we generalize a unified template for MaxEnt\nalgorithmic design named \\emph{Maximum Entropy Heterogeneous-Agent Mirror\nLearning} (MEHAML), which provides any induced method with the same guarantees\nas HASAC. We evaluate HASAC on seven benchmarks: Bi-DexHands, Multi-Agent\nMuJoCo, Pursuit-Evade, StarCraft Multi-Agent Challenge, Google Research\nFootball, Multi-Agent Particle Environment, Light Aircraft Game. Results show\nthat HASAC consistently outperforms strong baselines in 34 out of 38 tasks,\nexhibiting improved training stability, better sample efficiency and sufficient\nexploration. The robustness of HASAC was further validated when encountering\nuncertainties in rewards, dynamics, states, and actions of 14 magnitudes, and\nreal-world deployment in a multi-robot arena against these four types of\nuncertainties. See our page at \\url{https://sites.google.com/view/meharl}."
                },
                "authors": [
                    {
                        "name": "Simin Li"
                    },
                    {
                        "name": "Yifan Zhong"
                    },
                    {
                        "name": "Jiarong Liu"
                    },
                    {
                        "name": "Jianing Guo"
                    },
                    {
                        "name": "Siyuan Qi"
                    },
                    {
                        "name": "Ruixiao Xu"
                    },
                    {
                        "name": "Xin Yu"
                    },
                    {
                        "name": "Siyi Hu"
                    },
                    {
                        "name": "Haobo Fu"
                    },
                    {
                        "name": "Qiang Fu"
                    },
                    {
                        "name": "Xiaojun Chang"
                    },
                    {
                        "name": "Yujing Hu"
                    },
                    {
                        "name": "Bo An"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Yaodong Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yaodong Yang"
                },
                "author": "Yaodong Yang",
                "arxiv_comment": "Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.10715v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.10715v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08542v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08542v1",
                "updated": "2024-12-11T16:59:31Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    59,
                    31,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:59:31Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    59,
                    31,
                    2,
                    346,
                    0
                ],
                "title": "MaestroMotif: Skill Design from Artificial Intelligence Feedback",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MaestroMotif: Skill Design from Artificial Intelligence Feedback"
                },
                "summary": "Describing skills in natural language has the potential to provide an\naccessible way to inject human knowledge about decision-making into an AI\nsystem. We present MaestroMotif, a method for AI-assisted skill design, which\nyields high-performing and adaptable agents. MaestroMotif leverages the\ncapabilities of Large Language Models (LLMs) to effectively create and reuse\nskills. It first uses an LLM's feedback to automatically design rewards\ncorresponding to each skill, starting from their natural language description.\nThen, it employs an LLM's code generation abilities, together with\nreinforcement learning, for training the skills and combining them to implement\ncomplex behaviors specified in language. We evaluate MaestroMotif using a suite\nof complex tasks in the NetHack Learning Environment (NLE), demonstrating that\nit surpasses existing approaches in both performance and usability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Describing skills in natural language has the potential to provide an\naccessible way to inject human knowledge about decision-making into an AI\nsystem. We present MaestroMotif, a method for AI-assisted skill design, which\nyields high-performing and adaptable agents. MaestroMotif leverages the\ncapabilities of Large Language Models (LLMs) to effectively create and reuse\nskills. It first uses an LLM's feedback to automatically design rewards\ncorresponding to each skill, starting from their natural language description.\nThen, it employs an LLM's code generation abilities, together with\nreinforcement learning, for training the skills and combining them to implement\ncomplex behaviors specified in language. We evaluate MaestroMotif using a suite\nof complex tasks in the NetHack Learning Environment (NLE), demonstrating that\nit surpasses existing approaches in both performance and usability."
                },
                "authors": [
                    {
                        "name": "Martin Klissarov"
                    },
                    {
                        "name": "Mikael Henaff"
                    },
                    {
                        "name": "Roberta Raileanu"
                    },
                    {
                        "name": "Shagun Sodhani"
                    },
                    {
                        "name": "Pascal Vincent"
                    },
                    {
                        "name": "Amy Zhang"
                    },
                    {
                        "name": "Pierre-Luc Bacon"
                    },
                    {
                        "name": "Doina Precup"
                    },
                    {
                        "name": "Marlos C. Machado"
                    },
                    {
                        "name": "Pierluca D'Oro"
                    }
                ],
                "author_detail": {
                    "name": "Pierluca D'Oro"
                },
                "author": "Pierluca D'Oro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08542v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08542v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08540v1",
                "updated": "2024-12-11T16:58:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    58,
                    29,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:58:29Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    58,
                    29,
                    2,
                    346,
                    0
                ],
                "title": "Orderly Management of Packets in RDMA by Eunomia",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Orderly Management of Packets in RDMA by Eunomia"
                },
                "summary": "To fulfill the low latency requirements of today's applications, deployment\nof RDMA in datacenters has become prevalent over the recent years. However, the\nin-order delivery requirement of RDMAs prevents them from leveraging powerful\ntechniques that help improve the performance of datacenters, ranging from\nfine-grained load balancers to throughput-optimal expander topologies. We\ndemonstrate experimentally that these techniques significantly deteriorate the\nperformance in an RDMA network because they induce packet reordering.\nFurthermore, lifting the in-order delivery constraint enhances the flexibility\nof RDMA networks and enables them to employ these performance-enhancing\ntechniques. To realize this, we propose an ordering layer, Eunomia, to equip\nRDMA NICs to handle packet reordering. Eunomia employs a hybrid-dynamic bitmap\nstructure that efficiently uses the limited on-chip memory with the help of a\ncustomized memory controller and handles high degrees of packet reordering. We\nevaluate the feasibility of Eunomia through an FPGA-based implementation and\nits performance through large-scale simulations. We show that Eunomia enables a\nwide range of applications in RDMA datacenter networks, such as fine-grained\nload balancers which improve performance by reducing average flow completion\ntimes by 85% and 52% compared to ECMP and Conweave, respectively, or employment\nof RDMA in expander topologies like Jellyfish which allows up to 60% lower flow\ncompletion times and higher throughput gains compared to Fat tree.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To fulfill the low latency requirements of today's applications, deployment\nof RDMA in datacenters has become prevalent over the recent years. However, the\nin-order delivery requirement of RDMAs prevents them from leveraging powerful\ntechniques that help improve the performance of datacenters, ranging from\nfine-grained load balancers to throughput-optimal expander topologies. We\ndemonstrate experimentally that these techniques significantly deteriorate the\nperformance in an RDMA network because they induce packet reordering.\nFurthermore, lifting the in-order delivery constraint enhances the flexibility\nof RDMA networks and enables them to employ these performance-enhancing\ntechniques. To realize this, we propose an ordering layer, Eunomia, to equip\nRDMA NICs to handle packet reordering. Eunomia employs a hybrid-dynamic bitmap\nstructure that efficiently uses the limited on-chip memory with the help of a\ncustomized memory controller and handles high degrees of packet reordering. We\nevaluate the feasibility of Eunomia through an FPGA-based implementation and\nits performance through large-scale simulations. We show that Eunomia enables a\nwide range of applications in RDMA datacenter networks, such as fine-grained\nload balancers which improve performance by reducing average flow completion\ntimes by 85% and 52% compared to ECMP and Conweave, respectively, or employment\nof RDMA in expander topologies like Jellyfish which allows up to 60% lower flow\ncompletion times and higher throughput gains compared to Fat tree."
                },
                "authors": [
                    {
                        "name": "Sana Mahmood"
                    },
                    {
                        "name": "Jinqi Lu"
                    },
                    {
                        "name": "Soudeh Ghorbani"
                    }
                ],
                "author_detail": {
                    "name": "Soudeh Ghorbani"
                },
                "author": "Soudeh Ghorbani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05467v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05467v3",
                "updated": "2024-12-11T16:49:22Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    49,
                    22,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-06T23:43:59Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    23,
                    43,
                    59,
                    4,
                    341,
                    0
                ],
                "title": "The BrowserGym Ecosystem for Web Agent Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym Ecosystem for Web Agent Research"
                },
                "summary": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The BrowserGym ecosystem addresses the growing need for efficient evaluation\nand benchmarking of web agents, particularly those leveraging automation and\nLarge Language Models (LLMs) for web interaction tasks. Many existing\nbenchmarks suffer from fragmentation and inconsistent evaluation methodologies,\nmaking it challenging to achieve reliable comparisons and reproducible results.\nBrowserGym aims to solve this by providing a unified, gym-like environment with\nwell-defined observation and action spaces, facilitating standardized\nevaluation across diverse benchmarks. Combined with AgentLab, a complementary\nframework that aids in agent creation, testing, and analysis, BrowserGym offers\nflexibility for integrating new benchmarks while ensuring consistent evaluation\nand comprehensive experiment management. This standardized approach seeks to\nreduce the time and complexity of developing web agents, supporting more\nreliable comparisons and facilitating in-depth analysis of agent behaviors, and\ncould result in more adaptable, capable agents, ultimately accelerating\ninnovation in LLM-driven automation. As a supporting evidence, we conduct the\nfirst large-scale, multi-benchmark web agent experiment and compare the\nperformance of 6 state-of-the-art LLMs across all benchmarks currently\navailable in BrowserGym. Among other findings, our results highlight a large\ndiscrepancy between OpenAI and Anthropic's latests models, with\nClaude-3.5-Sonnet leading the way on almost all benchmarks, except on\nvision-related tasks where GPT-4o is superior. Despite these advancements, our\nresults emphasize that building robust and efficient web agents remains a\nsignificant challenge, due to the inherent complexity of real-world web\nenvironments and the limitations of current models."
                },
                "authors": [
                    {
                        "name": "Thibault Le Sellier De Chezelles"
                    },
                    {
                        "name": "Maxime Gasse"
                    },
                    {
                        "name": "Alexandre Drouin"
                    },
                    {
                        "name": "Massimo Caccia"
                    },
                    {
                        "name": "LÃ©o Boisvert"
                    },
                    {
                        "name": "Megh Thakkar"
                    },
                    {
                        "name": "Tom Marty"
                    },
                    {
                        "name": "Rim Assouel"
                    },
                    {
                        "name": "Sahar Omidi Shayegan"
                    },
                    {
                        "name": "Lawrence Keunho Jang"
                    },
                    {
                        "name": "Xing Han LÃ¹"
                    },
                    {
                        "name": "Ori Yoran"
                    },
                    {
                        "name": "Dehan Kong"
                    },
                    {
                        "name": "Frank F. Xu"
                    },
                    {
                        "name": "Siva Reddy"
                    },
                    {
                        "name": "Quentin Cappart"
                    },
                    {
                        "name": "Graham Neubig"
                    },
                    {
                        "name": "Ruslan Salakhutdinov"
                    },
                    {
                        "name": "Nicolas Chapados"
                    },
                    {
                        "name": "Alexandre Lacoste"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre Lacoste"
                },
                "author": "Alexandre Lacoste",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05467v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05467v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14654v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14654v2",
                "updated": "2024-12-11T16:38:01Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    38,
                    1,
                    2,
                    346,
                    0
                ],
                "published": "2024-11-22T00:59:25Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    59,
                    25,
                    4,
                    327,
                    0
                ],
                "title": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jinming Xing"
                    },
                    {
                        "name": "Ruilin Xing"
                    },
                    {
                        "name": "Yan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Yan Sun"
                },
                "author": "Yan Sun",
                "arxiv_comment": "4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14654v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14654v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08521v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08521v1",
                "updated": "2024-12-11T16:35:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:35:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    35,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EMS: Adaptive Evict-then-Merge Strategy for Head-wise KV Cache\n  Compression Based on Global-Local Importance"
                },
                "summary": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) continue to advance, the demand for higher\nquality and faster processing of long contexts across various applications is\ngrowing. KV cache is widely adopted as it stores previously generated key and\nvalue tokens, effectively reducing redundant computations during inference.\nHowever, as memory overhead becomes a significant concern, efficient\ncompression of KV cache has gained increasing attention. Most existing methods\nperform compression from two perspectives: identifying important tokens and\ndesigning compression strategies. However, these approaches often produce\nbiased distributions of important tokens due to the influence of accumulated\nattention scores or positional encoding. Furthermore, they overlook the\nsparsity and redundancy across different heads, which leads to difficulties in\npreserving the most effective information at the head level. To this end, we\npropose EMS to overcome these limitations, while achieving better KV cache\ncompression under extreme compression ratios. Specifically, we introduce a\nGlobal-Local score that combines accumulated attention scores from both global\nand local KV tokens to better identify the token importance. For the\ncompression strategy, we design an adaptive and unified Evict-then-Merge\nframework that accounts for the sparsity and redundancy of KV tokens across\ndifferent heads. Additionally, we implement the head-wise parallel compression\nthrough a zero-class mechanism to enhance efficiency. Extensive experiments\ndemonstrate our SOTA performance even under extreme compression ratios. EMS\nconsistently achieves the lowest perplexity, improves scores by over 1.28\npoints across four LLMs on LongBench under a 256 cache budget, and preserves\n95% retrieval accuracy with a cache budget less than 2% of the context length\nin the Needle-in-a-Haystack task."
                },
                "authors": [
                    {
                        "name": "Yingxin Li"
                    },
                    {
                        "name": "Ye Li"
                    },
                    {
                        "name": "Yuan Meng"
                    },
                    {
                        "name": "Xinzhu Ma"
                    },
                    {
                        "name": "Zihan Geng"
                    },
                    {
                        "name": "Shutao Xia"
                    },
                    {
                        "name": "Zhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Wang"
                },
                "author": "Zhi Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08521v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08521v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08519v1",
                "updated": "2024-12-11T16:32:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    32,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:32:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    32,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "Bridging Relevance and Reasoning: Rationale Distillation in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Relevance and Reasoning: Rationale Distillation in\n  Retrieval-Augmented Generation"
                },
                "summary": "The reranker and generator are two critical components in the\nRetrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking\nrelevant documents and generating responses. However, due to differences in\npre-training data and objectives, there is an inevitable gap between the\ndocuments ranked as relevant by the reranker and those required by the\ngenerator to support answering the query. To address this gap, we propose\nRADIO, a novel and practical preference alignment framework with RAtionale\nDIstillatiOn. Specifically, We first propose a rationale extraction method that\nleverages the reasoning capabilities of Large Language Models (LLMs) to extract\nthe rationales necessary for answering the query. Subsequently, a\nrationale-based alignment process is designed to rerank the documents based on\nthe extracted rationales, and fine-tune the reranker to align the preferences.\nWe conduct extensive experiments on two tasks across three datasets to\ndemonstrate the effectiveness of our approach compared to baseline methods. Our\ncode is released online to ease reproduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reranker and generator are two critical components in the\nRetrieval-Augmented Generation (i.e., RAG) pipeline, responsible for ranking\nrelevant documents and generating responses. However, due to differences in\npre-training data and objectives, there is an inevitable gap between the\ndocuments ranked as relevant by the reranker and those required by the\ngenerator to support answering the query. To address this gap, we propose\nRADIO, a novel and practical preference alignment framework with RAtionale\nDIstillatiOn. Specifically, We first propose a rationale extraction method that\nleverages the reasoning capabilities of Large Language Models (LLMs) to extract\nthe rationales necessary for answering the query. Subsequently, a\nrationale-based alignment process is designed to rerank the documents based on\nthe extracted rationales, and fine-tune the reranker to align the preferences.\nWe conduct extensive experiments on two tasks across three datasets to\ndemonstrate the effectiveness of our approach compared to baseline methods. Our\ncode is released online to ease reproduction."
                },
                "authors": [
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Derong Xu"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08516v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08516v1",
                "updated": "2024-12-11T16:28:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    28,
                    18,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T16:28:18Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    16,
                    28,
                    18,
                    2,
                    346,
                    0
                ],
                "title": "AltFS: Agency-light Feature Selection with Large Language Models in Deep\n  Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AltFS: Agency-light Feature Selection with Large Language Models in Deep\n  Recommender Systems"
                },
                "summary": "Feature selection is crucial in recommender systems for improving model\nefficiency and predictive performance. Traditional methods rely on agency\nmodels, such as decision trees or neural networks, to estimate feature\nimportance. However, this approach is inherently limited, as the agency models\nmay fail to learn effectively in all scenarios due to suboptimal training\nconditions (e.g., feature collinearity, high-dimensional sparsity, and data\ninsufficiency). In this paper, we propose AltFS, an Agency-light Feature\nSelection method for deep recommender systems. AltFS integrates semantic\nreasoning from Large Language Models (LLMs) with task-specific learning from\nagency models. Initially, LLMs will generate a semantic ranking of feature\nimportance, which is then refined by an agency model, combining world knowledge\nwith task-specific insights. Extensive experiments on three public datasets\nfrom real-world recommender platforms demonstrate the effectiveness of AltFS.\nOur code is publicly available for reproducibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature selection is crucial in recommender systems for improving model\nefficiency and predictive performance. Traditional methods rely on agency\nmodels, such as decision trees or neural networks, to estimate feature\nimportance. However, this approach is inherently limited, as the agency models\nmay fail to learn effectively in all scenarios due to suboptimal training\nconditions (e.g., feature collinearity, high-dimensional sparsity, and data\ninsufficiency). In this paper, we propose AltFS, an Agency-light Feature\nSelection method for deep recommender systems. AltFS integrates semantic\nreasoning from Large Language Models (LLMs) with task-specific learning from\nagency models. Initially, LLMs will generate a semantic ranking of feature\nimportance, which is then refined by an agency model, combining world knowledge\nwith task-specific insights. Extensive experiments on three public datasets\nfrom real-world recommender platforms demonstrate the effectiveness of AltFS.\nOur code is publicly available for reproducibility."
                },
                "authors": [
                    {
                        "name": "Pengyue Jia"
                    },
                    {
                        "name": "Zhaocheng Du"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    },
                    {
                        "name": "Xiaopeng Li"
                    },
                    {
                        "name": "Yuhao Wang"
                    },
                    {
                        "name": "Qidong Liu"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    }
                ],
                "author_detail": {
                    "name": "Ruiming Tang"
                },
                "author": "Ruiming Tang",
                "arxiv_comment": "under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08516v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08516v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08478v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08478v1",
                "updated": "2024-12-11T15:45:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    45,
                    44,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:45:44Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    45,
                    44,
                    2,
                    346,
                    0
                ],
                "title": "ECSeptional DNS Data: Evaluating Nameserver ECS Deployments with\n  Response-Aware Scanning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ECSeptional DNS Data: Evaluating Nameserver ECS Deployments with\n  Response-Aware Scanning"
                },
                "summary": "DNS is one of the cornerstones of the Internet. Nowadays, a substantial\nfraction of DNS queries are handled by public resolvers (e.g., Google Public\nDNS and Cisco's OpenDNS) rather than ISP nameservers. This behavior makes it\ndifficult for authoritative nameservers to provide answers based on the\nrequesting resolver. The impact is especially important for entities that make\nclient origin inferences to perform DNS-based load balancing (e.g., CDNS). The\nEDNS0 Client Subnet (ECS) option adds the client's IP prefix to DNS queries,\nwhich allows authoritative nameservers to provide prefix-based responses. In\nthis study, we introduce a new method for conducting ECS scans, which provides\ninsights into ECS behavior and significantly reduces the required number of\nqueries by up to 97% compared to state-of-the-art techniques. Our approach is\nalso the first to facilitate ECS scans for IPv6. We conduct a comprehensive\nevaluation of the ECS landscape, examining the usage and implementation of ECS\nacross various services. Overall, 53% of all nameservers support prefix-based\nresponses. Furthermore, we find that Google nameservers do not comply with the\nGoogle Public DNS guidelines. Lastly, we plan to make our tool, and data\npublicly available to foster further research in the area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DNS is one of the cornerstones of the Internet. Nowadays, a substantial\nfraction of DNS queries are handled by public resolvers (e.g., Google Public\nDNS and Cisco's OpenDNS) rather than ISP nameservers. This behavior makes it\ndifficult for authoritative nameservers to provide answers based on the\nrequesting resolver. The impact is especially important for entities that make\nclient origin inferences to perform DNS-based load balancing (e.g., CDNS). The\nEDNS0 Client Subnet (ECS) option adds the client's IP prefix to DNS queries,\nwhich allows authoritative nameservers to provide prefix-based responses. In\nthis study, we introduce a new method for conducting ECS scans, which provides\ninsights into ECS behavior and significantly reduces the required number of\nqueries by up to 97% compared to state-of-the-art techniques. Our approach is\nalso the first to facilitate ECS scans for IPv6. We conduct a comprehensive\nevaluation of the ECS landscape, examining the usage and implementation of ECS\nacross various services. Overall, 53% of all nameservers support prefix-based\nresponses. Furthermore, we find that Google nameservers do not comply with the\nGoogle Public DNS guidelines. Lastly, we plan to make our tool, and data\npublicly available to foster further research in the area."
                },
                "authors": [
                    {
                        "name": "Patrick Sattler"
                    },
                    {
                        "name": "Johannes Zirngibl"
                    },
                    {
                        "name": "Fahad Hilal"
                    },
                    {
                        "name": "Oliver Gasser"
                    },
                    {
                        "name": "Kevin Vermeulen"
                    },
                    {
                        "name": "Georg Carle"
                    },
                    {
                        "name": "Mattijs Jonker"
                    }
                ],
                "author_detail": {
                    "name": "Mattijs Jonker"
                },
                "author": "Mattijs Jonker",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08478v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08478v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17196v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17196v3",
                "updated": "2024-12-11T15:45:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    45,
                    21,
                    2,
                    346,
                    0
                ],
                "published": "2024-10-22T17:15:20Z",
                "published_parsed": [
                    2024,
                    10,
                    22,
                    17,
                    15,
                    20,
                    1,
                    296,
                    0
                ],
                "title": "VoiceBench: Benchmarking LLM-Based Voice Assistants",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VoiceBench: Benchmarking LLM-Based Voice Assistants"
                },
                "summary": "Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building on the success of large language models (LLMs), recent advancements\nsuch as GPT-4o have enabled real-time speech interactions through LLM-based\nvoice assistants, offering a significantly improved user experience compared to\ntraditional text-based interactions. However, the absence of benchmarks\ndesigned to evaluate these speech interaction capabilities has hindered\nprogress of LLM-based voice assistants development. Current evaluations focus\nprimarily on automatic speech recognition (ASR) or general knowledge evaluation\nwith clean speeches, neglecting the more intricate, real-world scenarios that\ninvolve diverse speaker characteristics, environmental and content factors. To\naddress this, we introduce VoiceBench, the first benchmark designed to provide\na multi-faceted evaluation of LLM-based voice assistants. VoiceBench also\nincludes both real and synthetic spoken instructions that incorporate the above\nthree key real-world variations. Extensive experiments reveal the limitations\nof current LLM-based voice assistant models and offer valuable insights for\nfuture research and development in this field."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Xianghu Yue"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Xiaoxue Gao"
                    },
                    {
                        "name": "Robby T. Tan"
                    },
                    {
                        "name": "Haizhou Li"
                    }
                ],
                "author_detail": {
                    "name": "Haizhou Li"
                },
                "author": "Haizhou Li",
                "arxiv_comment": "Work in progress. Data is available at\n  https://github.com/MatthewCYM/VoiceBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17196v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17196v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08468v1",
                "updated": "2024-12-11T15:33:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    33,
                    35,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:33:35Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    33,
                    35,
                    2,
                    346,
                    0
                ],
                "title": "Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-GraspLLM: A Multimodal LLM for Multi-Hand Semantic Guided Grasp\n  Generation"
                },
                "summary": "Multi-hand semantic grasp generation aims to generate feasible and\nsemantically appropriate grasp poses for different robotic hands based on\nnatural language instructions. Although the task is highly valuable, due to the\nlack of multi-hand grasp datasets with fine-grained contact description between\nrobotic hands and objects, it is still a long-standing difficult task. In this\npaper, we present Multi-GraspSet, the first large-scale multi-hand grasp\ndataset with automatically contact annotations. Based on Multi-GraspSet, we\npropose Multi-GraspLLM, a unified language-guided grasp generation framework.\nIt leverages large language models (LLM) to handle variable-length sequences,\ngenerating grasp poses for diverse robotic hands in a single unified\narchitecture. Multi-GraspLLM first aligns the encoded point cloud features and\ntext features into a unified semantic space. It then generates grasp bin tokens\nwhich are subsequently converted into grasp pose for each robotic hand via\nhand-aware linear mapping. The experimental results demonstrate that our\napproach significantly outperforms existing methods on Multi-GraspSet. More\ninformation can be found on our project page https://multi-graspllm.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-hand semantic grasp generation aims to generate feasible and\nsemantically appropriate grasp poses for different robotic hands based on\nnatural language instructions. Although the task is highly valuable, due to the\nlack of multi-hand grasp datasets with fine-grained contact description between\nrobotic hands and objects, it is still a long-standing difficult task. In this\npaper, we present Multi-GraspSet, the first large-scale multi-hand grasp\ndataset with automatically contact annotations. Based on Multi-GraspSet, we\npropose Multi-GraspLLM, a unified language-guided grasp generation framework.\nIt leverages large language models (LLM) to handle variable-length sequences,\ngenerating grasp poses for diverse robotic hands in a single unified\narchitecture. Multi-GraspLLM first aligns the encoded point cloud features and\ntext features into a unified semantic space. It then generates grasp bin tokens\nwhich are subsequently converted into grasp pose for each robotic hand via\nhand-aware linear mapping. The experimental results demonstrate that our\napproach significantly outperforms existing methods on Multi-GraspSet. More\ninformation can be found on our project page https://multi-graspllm.github.io."
                },
                "authors": [
                    {
                        "name": "Haosheng Li"
                    },
                    {
                        "name": "Weixin Mao"
                    },
                    {
                        "name": "Weipeng Deng"
                    },
                    {
                        "name": "Chenyu Meng"
                    },
                    {
                        "name": "Haoqiang Fan"
                    },
                    {
                        "name": "Tiancai Wang"
                    },
                    {
                        "name": "Ping Tan"
                    },
                    {
                        "name": "Hongan Wang"
                    },
                    {
                        "name": "Xiaoming Deng"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoming Deng"
                },
                "author": "Xiaoming Deng",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08445v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08445v1",
                "updated": "2024-12-11T15:09:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    9,
                    54,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:09:54Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    9,
                    54,
                    2,
                    346,
                    0
                ],
                "title": "TapeAgents: a Holistic Framework for Agent Development and Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TapeAgents: a Holistic Framework for Agent Development and Optimization"
                },
                "summary": "We present TapeAgents, an agent framework built around a granular, structured\nlog tape of the agent session that also plays the role of the session's\nresumable state. In TapeAgents we leverage tapes to facilitate all stages of\nthe LLM Agent development lifecycle. The agent reasons by processing the tape\nand the LLM output to produce new thought and action steps and append them to\nthe tape. The environment then reacts to the agent's actions by likewise\nappending observation steps to the tape. By virtue of this tape-centred design,\nTapeAgents can provide AI practitioners with holistic end-to-end support. At\nthe development stage, tapes facilitate session persistence, agent auditing,\nand step-by-step debugging. Post-deployment, one can reuse tapes for\nevaluation, fine-tuning, and prompt-tuning; crucially, one can adapt tapes from\nother agents or use revised historical tapes. In this report, we explain the\nTapeAgents design in detail. We demonstrate possible applications of TapeAgents\nwith several concrete examples of building monolithic agents and multi-agent\nteams, of optimizing agent prompts and finetuning the agent's LLM. We present\ntooling prototypes and report a case study where we use TapeAgents to finetune\na Llama-3.1-8B form-filling assistant to perform as well as GPT-4o while being\norders of magnitude cheaper. Lastly, our comparative analysis shows that\nTapeAgents's advantages over prior frameworks stem from our novel design of the\nLLM agent as a resumable, modular state machine with a structured\nconfiguration, that generates granular, structured logs and that can transform\nthese logs into training text -- a unique combination of features absent in\nprevious work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present TapeAgents, an agent framework built around a granular, structured\nlog tape of the agent session that also plays the role of the session's\nresumable state. In TapeAgents we leverage tapes to facilitate all stages of\nthe LLM Agent development lifecycle. The agent reasons by processing the tape\nand the LLM output to produce new thought and action steps and append them to\nthe tape. The environment then reacts to the agent's actions by likewise\nappending observation steps to the tape. By virtue of this tape-centred design,\nTapeAgents can provide AI practitioners with holistic end-to-end support. At\nthe development stage, tapes facilitate session persistence, agent auditing,\nand step-by-step debugging. Post-deployment, one can reuse tapes for\nevaluation, fine-tuning, and prompt-tuning; crucially, one can adapt tapes from\nother agents or use revised historical tapes. In this report, we explain the\nTapeAgents design in detail. We demonstrate possible applications of TapeAgents\nwith several concrete examples of building monolithic agents and multi-agent\nteams, of optimizing agent prompts and finetuning the agent's LLM. We present\ntooling prototypes and report a case study where we use TapeAgents to finetune\na Llama-3.1-8B form-filling assistant to perform as well as GPT-4o while being\norders of magnitude cheaper. Lastly, our comparative analysis shows that\nTapeAgents's advantages over prior frameworks stem from our novel design of the\nLLM agent as a resumable, modular state machine with a structured\nconfiguration, that generates granular, structured logs and that can transform\nthese logs into training text -- a unique combination of features absent in\nprevious work."
                },
                "authors": [
                    {
                        "name": "Dzmitry Bahdanau"
                    },
                    {
                        "name": "Nicolas Gontier"
                    },
                    {
                        "name": "Gabriel Huang"
                    },
                    {
                        "name": "Ehsan Kamalloo"
                    },
                    {
                        "name": "Rafael Pardinas"
                    },
                    {
                        "name": "Alex PichÃ©"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Oleh Shliazhko"
                    },
                    {
                        "name": "Jordan Prince Tremblay"
                    },
                    {
                        "name": "Karam Ghanem"
                    },
                    {
                        "name": "Soham Parikh"
                    },
                    {
                        "name": "Mitul Tiwari"
                    },
                    {
                        "name": "Quaizar Vohra"
                    }
                ],
                "author_detail": {
                    "name": "Quaizar Vohra"
                },
                "author": "Quaizar Vohra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08445v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08445v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08442v1",
                "updated": "2024-12-11T15:06:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    6,
                    25,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T15:06:25Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    15,
                    6,
                    25,
                    2,
                    346,
                    0
                ],
                "title": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Multimodal LLMs to Generalist Embodied Agents: Methods and Lessons"
                },
                "summary": "We examine the capability of Multimodal Large Language Models (MLLMs) to\ntackle diverse domains that extend beyond the traditional language and vision\ntasks these models are typically trained on. Specifically, our focus lies in\nareas such as Embodied AI, Games, UI Control, and Planning. To this end, we\nintroduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).\nGEA is a single unified model capable of grounding itself across these varied\ndomains through a multi-embodiment action tokenizer. GEA is trained with\nsupervised learning on a large dataset of embodied experiences and with online\nRL in interactive simulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal the importance of\ntraining with cross-domain data and online RL for building generalist agents.\nThe final GEA model achieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist models and\nbenchmark-specific approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the capability of Multimodal Large Language Models (MLLMs) to\ntackle diverse domains that extend beyond the traditional language and vision\ntasks these models are typically trained on. Specifically, our focus lies in\nareas such as Embodied AI, Games, UI Control, and Planning. To this end, we\nintroduce a process of adapting an MLLM to a Generalist Embodied Agent (GEA).\nGEA is a single unified model capable of grounding itself across these varied\ndomains through a multi-embodiment action tokenizer. GEA is trained with\nsupervised learning on a large dataset of embodied experiences and with online\nRL in interactive simulators. We explore the data and algorithmic choices\nnecessary to develop such a model. Our findings reveal the importance of\ntraining with cross-domain data and online RL for building generalist agents.\nThe final GEA model achieves strong generalization performance to unseen tasks\nacross diverse benchmarks compared to other generalist models and\nbenchmark-specific approaches."
                },
                "authors": [
                    {
                        "name": "Andrew Szot"
                    },
                    {
                        "name": "Bogdan Mazoure"
                    },
                    {
                        "name": "Omar Attia"
                    },
                    {
                        "name": "Aleksei Timofeev"
                    },
                    {
                        "name": "Harsh Agrawal"
                    },
                    {
                        "name": "Devon Hjelm"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Zsolt Kira"
                    },
                    {
                        "name": "Alexander Toshev"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Toshev"
                },
                "author": "Alexander Toshev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08775v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08775v2",
                "updated": "2024-12-11T14:58:53Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    58,
                    53,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-13T12:34:14Z",
                "published_parsed": [
                    2024,
                    9,
                    13,
                    12,
                    34,
                    14,
                    4,
                    257,
                    0
                ],
                "title": "What Should We Engineer in Prompts? Training Humans in\n  Requirement-Driven LLM Use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Should We Engineer in Prompts? Training Humans in\n  Requirement-Driven LLM Use"
                },
                "summary": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot)\nneeds humans to clearly articulate customized requirements (e.g., \"start the\nresponse with a tl;dr\"). However, existing prompt engineering instructions\noften lack focused training on requirement articulation and instead tend to\nemphasize increasingly automatable strategies (e.g., tricks like adding\nrole-plays and \"think step-by-step\"). To address the gap, we introduce\nRequirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human\nattention on generating clear, complete requirements during prompting. We\nimplement ROPE through an assessment and training suite that provides\ndeliberate practice with LLM-generated feedback. In a randomized controlled\nexperiment with 30 novices, ROPE significantly outperforms conventional prompt\nengineering training (20% vs. 1% gains), a gap that automatic prompt\noptimization cannot close. Furthermore, we demonstrate a direct correlation\nbetween the quality of input requirements and LLM outputs. Our work paves the\nway to empower more end-users to build complex LLM applications."
                },
                "authors": [
                    {
                        "name": "Qianou Ma"
                    },
                    {
                        "name": "Weirui Peng"
                    },
                    {
                        "name": "Chenyang Yang"
                    },
                    {
                        "name": "Hua Shen"
                    },
                    {
                        "name": "Kenneth Koedinger"
                    },
                    {
                        "name": "Tongshuang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tongshuang Wu"
                },
                "author": "Tongshuang Wu",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08775v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08775v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08430v1",
                "updated": "2024-12-11T14:51:13Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    51,
                    13,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:51:13Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    51,
                    13,
                    2,
                    346,
                    0
                ],
                "title": "Assessing Personalized AI Mentoring with Large Language Models in the\n  Computing Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Personalized AI Mentoring with Large Language Models in the\n  Computing Field"
                },
                "summary": "This paper provides an in-depth evaluation of three state-of-the-art Large\nLanguage Models (LLMs) for personalized career mentoring in the computing\nfield, using three distinct student profiles that consider gender, race, and\nprofessional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2\nusing a zero-shot learning approach without human intervention. A quantitative\nevaluation was conducted through a custom natural language processing analytics\npipeline to highlight the uniqueness of the responses and to identify words\nreflecting each student's profile, including race, gender, or professional\nlevel. The analysis of frequently used words in the responses indicates that\nGPT-4 offers more personalized mentoring compared to the other two LLMs.\nAdditionally, a qualitative evaluation was performed to see if human experts\nreached similar conclusions. The analysis of survey responses shows that GPT-4\noutperformed the other two LLMs in delivering more accurate and useful\nmentoring while addressing specific challenges with encouragement languages.\nOur work establishes a foundation for developing personalized mentoring tools\nbased on LLMs, incorporating human mentors in the process to deliver a more\nimpactful and tailored mentoring experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper provides an in-depth evaluation of three state-of-the-art Large\nLanguage Models (LLMs) for personalized career mentoring in the computing\nfield, using three distinct student profiles that consider gender, race, and\nprofessional levels. We evaluated the performance of GPT-4, LLaMA 3, and Palm 2\nusing a zero-shot learning approach without human intervention. A quantitative\nevaluation was conducted through a custom natural language processing analytics\npipeline to highlight the uniqueness of the responses and to identify words\nreflecting each student's profile, including race, gender, or professional\nlevel. The analysis of frequently used words in the responses indicates that\nGPT-4 offers more personalized mentoring compared to the other two LLMs.\nAdditionally, a qualitative evaluation was performed to see if human experts\nreached similar conclusions. The analysis of survey responses shows that GPT-4\noutperformed the other two LLMs in delivering more accurate and useful\nmentoring while addressing specific challenges with encouragement languages.\nOur work establishes a foundation for developing personalized mentoring tools\nbased on LLMs, incorporating human mentors in the process to deliver a more\nimpactful and tailored mentoring experience."
                },
                "authors": [
                    {
                        "name": "Xiao Luo"
                    },
                    {
                        "name": "Sean O'Connell"
                    },
                    {
                        "name": "Shamima Mithun"
                    }
                ],
                "author_detail": {
                    "name": "Shamima Mithun"
                },
                "author": "Shamima Mithun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08428v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08428v1",
                "updated": "2024-12-11T14:48:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    48,
                    19,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:48:19Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    48,
                    19,
                    2,
                    346,
                    0
                ],
                "title": "SwarmGPT-Primitive: A Language-Driven Choreographer for Drone Swarms\n  Using Safe Motion Primitive Composition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SwarmGPT-Primitive: A Language-Driven Choreographer for Drone Swarms\n  Using Safe Motion Primitive Composition"
                },
                "summary": "Catalyzed by advancements in hardware and software, drone performances are\nincreasingly making their mark in the entertainment industry. However,\ndesigning smooth and safe choreographies for drone swarms is complex and often\nrequires expert domain knowledge. In this work, we introduce\nSwarmGPT-Primitive, a language-based choreographer that integrates the\nreasoning capabilities of large language models (LLMs) with safe motion\nplanning to facilitate deployable drone swarm choreographies. The LLM composes\nchoreographies for a given piece of music by utilizing a library of motion\nprimitives; the language-based choreographer is augmented with an\noptimization-based safety filter, which certifies the choreography for\nreal-world deployment by making minimal adjustments when feasibility and safety\nconstraints are violated. The overall SwarmGPT-Primitive framework decouples\nchoreographic design from safe motion planning, which allows non-expert users\nto re-prompt and refine compositions without concerns about compliance with\nconstraints such as avoiding collisions or downwash effects or satisfying\nactuation limits. We demonstrate our approach through simulations and\nexperiments with swarms of up to 20 drones performing choreographies designed\nbased on various songs, highlighting the system's ability to generate effective\nand synchronized drone choreographies for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Catalyzed by advancements in hardware and software, drone performances are\nincreasingly making their mark in the entertainment industry. However,\ndesigning smooth and safe choreographies for drone swarms is complex and often\nrequires expert domain knowledge. In this work, we introduce\nSwarmGPT-Primitive, a language-based choreographer that integrates the\nreasoning capabilities of large language models (LLMs) with safe motion\nplanning to facilitate deployable drone swarm choreographies. The LLM composes\nchoreographies for a given piece of music by utilizing a library of motion\nprimitives; the language-based choreographer is augmented with an\noptimization-based safety filter, which certifies the choreography for\nreal-world deployment by making minimal adjustments when feasibility and safety\nconstraints are violated. The overall SwarmGPT-Primitive framework decouples\nchoreographic design from safe motion planning, which allows non-expert users\nto re-prompt and refine compositions without concerns about compliance with\nconstraints such as avoiding collisions or downwash effects or satisfying\nactuation limits. We demonstrate our approach through simulations and\nexperiments with swarms of up to 20 drones performing choreographies designed\nbased on various songs, highlighting the system's ability to generate effective\nand synchronized drone choreographies for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Vedant Vyas"
                    },
                    {
                        "name": "Martin Schuck"
                    },
                    {
                        "name": "Dinushka O. Dahanaggamaarachchi"
                    },
                    {
                        "name": "Siqi Zhou"
                    },
                    {
                        "name": "Angela P. Schoellig"
                    }
                ],
                "author_detail": {
                    "name": "Angela P. Schoellig"
                },
                "author": "Angela P. Schoellig",
                "arxiv_comment": "Submitted to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08428v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08428v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05185v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05185v2",
                "updated": "2024-12-11T14:43:02Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    43,
                    2,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-06T17:04:42Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    17,
                    4,
                    42,
                    4,
                    341,
                    0
                ],
                "title": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LinVT: Empower Your Image-level Large Language Model to Understand\n  Videos"
                },
                "summary": "Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been widely used in various tasks,\nmotivating us to develop an LLM-based assistant for videos. Instead of training\nfrom scratch, we propose a module to transform arbitrary well-trained\nimage-based LLMs into video-LLMs (after being trained on video data). To better\nadapt image-LLMs for processing videos, we introduce two design principles:\nlinear transformation to preserve the original visual-language alignment and\nrepresentative information condensation from redundant video content. Guided by\nthese principles, we propose a plug-and-play Linear Video Tokenizer(LinVT),\nwhich enables existing image-LLMs to understand videos. We benchmark LinVT with\nsix recent visual LLMs: Aquila, Blip-3, InternVL2, Mipha, Molmo and Qwen2-VL,\nshowcasing the high compatibility of LinVT. LinVT-based LLMs achieve\nstate-of-the-art performance across various video benchmarks, illustrating the\neffectiveness of LinVT in multi-modal video understanding."
                },
                "authors": [
                    {
                        "name": "Lishuai Gao"
                    },
                    {
                        "name": "Yujie Zhong"
                    },
                    {
                        "name": "Yingsen Zeng"
                    },
                    {
                        "name": "Haoxian Tan"
                    },
                    {
                        "name": "Dengjie Li"
                    },
                    {
                        "name": "Zheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Zhao"
                },
                "author": "Zheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05185v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05185v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08414v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08414v1",
                "updated": "2024-12-11T14:31:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    31,
                    39,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:31:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    31,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Detecting Conversational Mental Manipulation with Intent-Aware Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting Conversational Mental Manipulation with Intent-Aware Prompting"
                },
                "summary": "Mental manipulation severely undermines mental wellness by covertly and\nnegatively distorting decision-making. While there is an increasing interest in\nmental health care within the natural language processing community, progress\nin tackling manipulation remains limited due to the complexity of detecting\nsubtle, covert tactics in conversations. In this paper, we propose Intent-Aware\nPrompting (IAP), a novel approach for detecting mental manipulations using\nlarge language models (LLMs), providing a deeper understanding of manipulative\ntactics by capturing the underlying intents of participants. Experimental\nresults on the MentalManip dataset demonstrate superior effectiveness of IAP\nagainst other advanced prompting strategies. Notably, our approach\nsubstantially reduces false negatives, helping detect more instances of mental\nmanipulation with minimal misjudgment of positive cases. The code of this paper\nis available at https://github.com/Anton-Jiayuan-MA/Manip-IAP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mental manipulation severely undermines mental wellness by covertly and\nnegatively distorting decision-making. While there is an increasing interest in\nmental health care within the natural language processing community, progress\nin tackling manipulation remains limited due to the complexity of detecting\nsubtle, covert tactics in conversations. In this paper, we propose Intent-Aware\nPrompting (IAP), a novel approach for detecting mental manipulations using\nlarge language models (LLMs), providing a deeper understanding of manipulative\ntactics by capturing the underlying intents of participants. Experimental\nresults on the MentalManip dataset demonstrate superior effectiveness of IAP\nagainst other advanced prompting strategies. Notably, our approach\nsubstantially reduces false negatives, helping detect more instances of mental\nmanipulation with minimal misjudgment of positive cases. The code of this paper\nis available at https://github.com/Anton-Jiayuan-MA/Manip-IAP."
                },
                "authors": [
                    {
                        "name": "Jiayuan Ma"
                    },
                    {
                        "name": "Hongbin Na"
                    },
                    {
                        "name": "Zimu Wang"
                    },
                    {
                        "name": "Yining Hua"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Ling Chen"
                    }
                ],
                "author_detail": {
                    "name": "Ling Chen"
                },
                "author": "Ling Chen",
                "arxiv_journal_ref": "COLING2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08414v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08414v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08398v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08398v1",
                "updated": "2024-12-11T14:17:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    17,
                    17,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:17:17Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    17,
                    17,
                    2,
                    346,
                    0
                ],
                "title": "Grasp Diffusion Network: Learning Grasp Generators from Partial Point\n  Clouds with Diffusion Models in SO(3)xR3",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasp Diffusion Network: Learning Grasp Generators from Partial Point\n  Clouds with Diffusion Models in SO(3)xR3"
                },
                "summary": "Grasping objects successfully from a single-view camera is crucial in many\nrobot manipulation tasks. An approach to solve this problem is to leverage\nsimulation to create large datasets of pairs of objects and grasp poses, and\nthen learn a conditional generative model that can be prompted quickly during\ndeployment. However, the grasp pose data is highly multimodal since there are\nseveral ways to grasp an object. Hence, in this work, we learn a grasp\ngenerative model with diffusion models to sample candidate grasp poses given a\npartial point cloud of an object. A novel aspect of our method is to consider\ndiffusion in the manifold space of rotations and to propose a\ncollision-avoidance cost guidance to improve the grasp success rate during\ninference. To accelerate grasp sampling we use recent techniques from the\ndiffusion literature to achieve faster inference times. We show in simulation\nand real-world experiments that our approach can grasp several objects from raw\ndepth images with $90\\%$ success rate and benchmark it against several\nbaselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasping objects successfully from a single-view camera is crucial in many\nrobot manipulation tasks. An approach to solve this problem is to leverage\nsimulation to create large datasets of pairs of objects and grasp poses, and\nthen learn a conditional generative model that can be prompted quickly during\ndeployment. However, the grasp pose data is highly multimodal since there are\nseveral ways to grasp an object. Hence, in this work, we learn a grasp\ngenerative model with diffusion models to sample candidate grasp poses given a\npartial point cloud of an object. A novel aspect of our method is to consider\ndiffusion in the manifold space of rotations and to propose a\ncollision-avoidance cost guidance to improve the grasp success rate during\ninference. To accelerate grasp sampling we use recent techniques from the\ndiffusion literature to achieve faster inference times. We show in simulation\nand real-world experiments that our approach can grasp several objects from raw\ndepth images with $90\\%$ success rate and benchmark it against several\nbaselines."
                },
                "authors": [
                    {
                        "name": "Joao Carvalho"
                    },
                    {
                        "name": "An T. Le"
                    },
                    {
                        "name": "Philipp Jahr"
                    },
                    {
                        "name": "Qiao Sun"
                    },
                    {
                        "name": "Julen Urain"
                    },
                    {
                        "name": "Dorothea Koert"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08398v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08398v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08393v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08393v1",
                "updated": "2024-12-11T14:05:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    5,
                    4,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T14:05:04Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    14,
                    5,
                    4,
                    2,
                    346,
                    0
                ],
                "title": "Learning to Reason via Self-Iterative Process Feedback for Small\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Reason via Self-Iterative Process Feedback for Small\n  Language Models"
                },
                "summary": "Small language models (SLMs) are more efficient, cost-effective, and\ncustomizable than large language models (LLMs), though they often underperform\nin specific areas like reasoning. Past methods for enhancing SLMs' reasoning,\nsuch as supervised fine-tuning and distillation, often depend on costly\nexternal signals, resulting in SLMs being overly confident with limited\nsupervision signals, thus limiting their abilities. Therefore, this study\nenables SLMs to learn to reason from self-iterative feedback. By combining odds\nratio preference optimization (ORPO), we fine-tune and align SLMs using\npositive and negative signals generated by themselves. Additionally, we\nintroduce process supervision for rewards in preference alignment by\nsampling-based inference simulation and process reward models. Compared to\nSupervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B\nby 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed\nmethod also demonstrated superior out-of-domain generalization capabilities on\nMMLU_Math and HumanEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small language models (SLMs) are more efficient, cost-effective, and\ncustomizable than large language models (LLMs), though they often underperform\nin specific areas like reasoning. Past methods for enhancing SLMs' reasoning,\nsuch as supervised fine-tuning and distillation, often depend on costly\nexternal signals, resulting in SLMs being overly confident with limited\nsupervision signals, thus limiting their abilities. Therefore, this study\nenables SLMs to learn to reason from self-iterative feedback. By combining odds\nratio preference optimization (ORPO), we fine-tune and align SLMs using\npositive and negative signals generated by themselves. Additionally, we\nintroduce process supervision for rewards in preference alignment by\nsampling-based inference simulation and process reward models. Compared to\nSupervised Fine-Tuning (SFT), our method improves the performance of Gemma-2B\nby 12.43 (Acc) on GSM8K and 3.95 (Pass@1) on MBPP. Furthermore, the proposed\nmethod also demonstrated superior out-of-domain generalization capabilities on\nMMLU_Math and HumanEval."
                },
                "authors": [
                    {
                        "name": "Kaiyuan Chen"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Xuejie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xuejie Zhang"
                },
                "author": "Xuejie Zhang",
                "arxiv_comment": "Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08393v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08393v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05587v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05587v2",
                "updated": "2024-12-11T13:56:40Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    56,
                    40,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-07T08:50:24Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    8,
                    50,
                    24,
                    5,
                    342,
                    0
                ],
                "title": "GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on\n  the Google Earth Engine Platform Powered by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GEE-OPs: An Operator Knowledge Base for Geospatial Code Generation on\n  the Google Earth Engine Platform Powered by Large Language Models"
                },
                "summary": "As the scale and complexity of spatiotemporal data continue to grow rapidly,\nthe use of geospatial modeling on the Google Earth Engine (GEE) platform\npresents dual challenges: improving the coding efficiency of domain experts and\nenhancing the coding capabilities of interdisciplinary users. To address these\nchallenges and improve the performance of large language models (LLMs) in\ngeospatial code generation tasks, we propose a framework for building a\ngeospatial operator knowledge base tailored to the GEE JavaScript API. This\nframework consists of an operator syntax knowledge table, an operator\nrelationship frequency table, an operator frequent pattern knowledge table, and\nan operator relationship chain knowledge table. By leveraging Abstract Syntax\nTree (AST) techniques and frequent itemset mining, we systematically extract\noperator knowledge from 185,236 real GEE scripts and syntax documentation,\nforming a structured knowledge base. Experimental results demonstrate that the\nframework achieves over 90% accuracy, recall, and F1 score in operator\nknowledge extraction. When integrated with the Retrieval-Augmented Generation\n(RAG) strategy for LLM-based geospatial code generation tasks, the knowledge\nbase improves performance by 20-30%. Ablation studies further quantify the\nnecessity of each knowledge table in the knowledge base construction. This work\nprovides robust support for the advancement and application of geospatial code\nmodeling techniques, offering an innovative approach to constructing\ndomain-specific knowledge bases that enhance the code generation capabilities\nof LLMs, and fostering the deeper integration of generative AI technologies\nwithin the field of geoinformatics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the scale and complexity of spatiotemporal data continue to grow rapidly,\nthe use of geospatial modeling on the Google Earth Engine (GEE) platform\npresents dual challenges: improving the coding efficiency of domain experts and\nenhancing the coding capabilities of interdisciplinary users. To address these\nchallenges and improve the performance of large language models (LLMs) in\ngeospatial code generation tasks, we propose a framework for building a\ngeospatial operator knowledge base tailored to the GEE JavaScript API. This\nframework consists of an operator syntax knowledge table, an operator\nrelationship frequency table, an operator frequent pattern knowledge table, and\nan operator relationship chain knowledge table. By leveraging Abstract Syntax\nTree (AST) techniques and frequent itemset mining, we systematically extract\noperator knowledge from 185,236 real GEE scripts and syntax documentation,\nforming a structured knowledge base. Experimental results demonstrate that the\nframework achieves over 90% accuracy, recall, and F1 score in operator\nknowledge extraction. When integrated with the Retrieval-Augmented Generation\n(RAG) strategy for LLM-based geospatial code generation tasks, the knowledge\nbase improves performance by 20-30%. Ablation studies further quantify the\nnecessity of each knowledge table in the knowledge base construction. This work\nprovides robust support for the advancement and application of geospatial code\nmodeling techniques, offering an innovative approach to constructing\ndomain-specific knowledge bases that enhance the code generation capabilities\nof LLMs, and fostering the deeper integration of generative AI technologies\nwithin the field of geoinformatics."
                },
                "authors": [
                    {
                        "name": "Shuyang Hou"
                    },
                    {
                        "name": "Jianyuan Liang"
                    },
                    {
                        "name": "Anqi Zhao"
                    },
                    {
                        "name": "Huayi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Huayi Wu"
                },
                "author": "Huayi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05587v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05587v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08389v1",
                "updated": "2024-12-11T13:56:04Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    56,
                    4,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T13:56:04Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    56,
                    4,
                    2,
                    346,
                    0
                ],
                "title": "SweetieChat: A Strategy-Enhanced Role-playing Framework for Diverse\n  Scenarios Handling Emotional Support Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SweetieChat: A Strategy-Enhanced Role-playing Framework for Diverse\n  Scenarios Handling Emotional Support Agent"
                },
                "summary": "Large Language Models (LLMs) have demonstrated promising potential in\nproviding empathetic support during interactions. However, their responses\noften become verbose or overly formulaic, failing to adequately address the\ndiverse emotional support needs of real-world scenarios. To tackle this\nchallenge, we propose an innovative strategy-enhanced role-playing framework,\ndesigned to simulate authentic emotional support conversations. Specifically,\nour approach unfolds in two steps: (1) Strategy-Enhanced Role-Playing\nInteractions, which involve three pivotal roles -- Seeker, Strategy Counselor,\nand Supporter -- engaging in diverse scenarios to emulate real-world\ninteractions and promote a broader range of dialogues; and (2) Emotional\nSupport Agent Training, achieved through fine-tuning LLMs using our specially\nconstructed dataset. Within this framework, we develop the \\textbf{ServeForEmo}\ndataset, comprising an extensive collection of 3.7K+ multi-turn dialogues and\n62.8K+ utterances. We further present \\textbf{SweetieChat}, an emotional\nsupport agent capable of handling diverse open-domain scenarios. Extensive\nexperiments and human evaluations confirm the framework's effectiveness in\nenhancing emotional support, highlighting its unique ability to provide more\nnuanced and tailored assistance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated promising potential in\nproviding empathetic support during interactions. However, their responses\noften become verbose or overly formulaic, failing to adequately address the\ndiverse emotional support needs of real-world scenarios. To tackle this\nchallenge, we propose an innovative strategy-enhanced role-playing framework,\ndesigned to simulate authentic emotional support conversations. Specifically,\nour approach unfolds in two steps: (1) Strategy-Enhanced Role-Playing\nInteractions, which involve three pivotal roles -- Seeker, Strategy Counselor,\nand Supporter -- engaging in diverse scenarios to emulate real-world\ninteractions and promote a broader range of dialogues; and (2) Emotional\nSupport Agent Training, achieved through fine-tuning LLMs using our specially\nconstructed dataset. Within this framework, we develop the \\textbf{ServeForEmo}\ndataset, comprising an extensive collection of 3.7K+ multi-turn dialogues and\n62.8K+ utterances. We further present \\textbf{SweetieChat}, an emotional\nsupport agent capable of handling diverse open-domain scenarios. Extensive\nexperiments and human evaluations confirm the framework's effectiveness in\nenhancing emotional support, highlighting its unique ability to provide more\nnuanced and tailored assistance."
                },
                "authors": [
                    {
                        "name": "Jing Ye"
                    },
                    {
                        "name": "Lu Xiang"
                    },
                    {
                        "name": "Yaping Zhang"
                    },
                    {
                        "name": "Chengqing Zong"
                    }
                ],
                "author_detail": {
                    "name": "Chengqing Zong"
                },
                "author": "Chengqing Zong",
                "arxiv_comment": "24 pages. Accepted by COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08385v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08385v1",
                "updated": "2024-12-11T13:50:17Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    50,
                    17,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T13:50:17Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    50,
                    17,
                    2,
                    346,
                    0
                ],
                "title": "NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment\n  Prediction Dataset and Specialized Language Model for Enhanced Decision\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NyayaAnumana & INLegalLlama: The Largest Indian Legal Judgment\n  Prediction Dataset and Specialized Language Model for Enhanced Decision\n  Analysis"
                },
                "summary": "The integration of artificial intelligence (AI) in legal judgment prediction\n(LJP) has the potential to transform the legal landscape, particularly in\njurisdictions like India, where a significant backlog of cases burdens the\nlegal system. This paper introduces NyayaAnumana, the largest and most diverse\ncorpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945\npreprocessed cases. NyayaAnumana, which combines the words \"Nyay\" (judgment)\nand \"Anuman\" (prediction or inference) respectively for most major Indian\nlanguages, includes a wide range of cases from the Supreme Court, High Courts,\nTribunal Courts, District Courts, and Daily Orders and, thus, provides\nunparalleled diversity and coverage. Our dataset surpasses existing datasets\nlike PredEx and ILDC, offering a comprehensive foundation for advanced AI\nresearch in the legal domain.\n  In addition to the dataset, we present INLegalLlama, a domain-specific\ngenerative large language model (LLM) tailored to the intricacies of the Indian\nlegal system. It is developed through a two-phase training approach over a base\nLLaMa model. First, Indian legal documents are injected using continual\npretraining. Second, task-specific supervised finetuning is done. This method\nallows the model to achieve a deeper understanding of legal contexts.\n  Our experiments demonstrate that incorporating diverse court data\nsignificantly boosts model accuracy, achieving approximately 90% F1-score in\nprediction tasks. INLegalLlama not only improves prediction accuracy but also\noffers comprehensible explanations, addressing the need for explainability in\nAI-assisted legal decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of artificial intelligence (AI) in legal judgment prediction\n(LJP) has the potential to transform the legal landscape, particularly in\njurisdictions like India, where a significant backlog of cases burdens the\nlegal system. This paper introduces NyayaAnumana, the largest and most diverse\ncorpus of Indian legal cases compiled for LJP, encompassing a total of 7,02,945\npreprocessed cases. NyayaAnumana, which combines the words \"Nyay\" (judgment)\nand \"Anuman\" (prediction or inference) respectively for most major Indian\nlanguages, includes a wide range of cases from the Supreme Court, High Courts,\nTribunal Courts, District Courts, and Daily Orders and, thus, provides\nunparalleled diversity and coverage. Our dataset surpasses existing datasets\nlike PredEx and ILDC, offering a comprehensive foundation for advanced AI\nresearch in the legal domain.\n  In addition to the dataset, we present INLegalLlama, a domain-specific\ngenerative large language model (LLM) tailored to the intricacies of the Indian\nlegal system. It is developed through a two-phase training approach over a base\nLLaMa model. First, Indian legal documents are injected using continual\npretraining. Second, task-specific supervised finetuning is done. This method\nallows the model to achieve a deeper understanding of legal contexts.\n  Our experiments demonstrate that incorporating diverse court data\nsignificantly boosts model accuracy, achieving approximately 90% F1-score in\nprediction tasks. INLegalLlama not only improves prediction accuracy but also\noffers comprehensible explanations, addressing the need for explainability in\nAI-assisted legal decisions."
                },
                "authors": [
                    {
                        "name": "Shubham Kumar Nigam"
                    },
                    {
                        "name": "Balaramamahanthi Deepak Patnaik"
                    },
                    {
                        "name": "Shivam Mishra"
                    },
                    {
                        "name": "Noel Shallum"
                    },
                    {
                        "name": "Kripabandhu Ghosh"
                    },
                    {
                        "name": "Arnab Bhattacharya"
                    }
                ],
                "author_detail": {
                    "name": "Arnab Bhattacharya"
                },
                "author": "Arnab Bhattacharya",
                "arxiv_comment": "Accepted on COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08385v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08385v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.04964v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.04964v2",
                "updated": "2024-12-11T13:27:00Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    27,
                    0,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-06T11:29:32Z",
                "published_parsed": [
                    2024,
                    12,
                    6,
                    11,
                    29,
                    32,
                    4,
                    341,
                    0
                ],
                "title": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flash Communication: Reducing Tensor Parallelization Bottleneck for Fast\n  Large Language Model Inference"
                },
                "summary": "The ever-increasing sizes of large language models necessitate distributed\nsolutions for fast inference that exploit multi-dimensional parallelism, where\ncomputational loads are split across various accelerators such as GPU clusters.\nHowever, this approach often introduces significant communication overhead,\nespecially on devices with limited bandwidth. In this paper, we introduce Flash\nCommunication, a novel low-bit compression technique designed to alleviate the\ntensor-parallelism communication bottleneck during inference. Our method\nsubstantially boosts intra-node communication speed by more than 3x and reduces\nthe time-to-first-token by 2x, with nearly no sacrifice in model accuracy.\nExtensive experiments on various up-to-date LLMs demonstrate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ever-increasing sizes of large language models necessitate distributed\nsolutions for fast inference that exploit multi-dimensional parallelism, where\ncomputational loads are split across various accelerators such as GPU clusters.\nHowever, this approach often introduces significant communication overhead,\nespecially on devices with limited bandwidth. In this paper, we introduce Flash\nCommunication, a novel low-bit compression technique designed to alleviate the\ntensor-parallelism communication bottleneck during inference. Our method\nsubstantially boosts intra-node communication speed by more than 3x and reduces\nthe time-to-first-token by 2x, with nearly no sacrifice in model accuracy.\nExtensive experiments on various up-to-date LLMs demonstrate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Qingyuan Li"
                    },
                    {
                        "name": "Bo Zhang"
                    },
                    {
                        "name": "Liang Ye"
                    },
                    {
                        "name": "Yifan Zhang"
                    },
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Yerui Sun"
                    },
                    {
                        "name": "Lin Ma"
                    },
                    {
                        "name": "Yuchen Xie"
                    }
                ],
                "author_detail": {
                    "name": "Yuchen Xie"
                },
                "author": "Yuchen Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.04964v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.04964v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18353v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18353v2",
                "updated": "2024-12-11T13:02:30Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    13,
                    2,
                    30,
                    2,
                    346,
                    0
                ],
                "published": "2024-04-29T01:24:14Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    1,
                    24,
                    14,
                    0,
                    120,
                    0
                ],
                "title": "How secure is AI-generated Code: A Large-Scale Comparison of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How secure is AI-generated Code: A Large-Scale Comparison of Large\n  Language Models"
                },
                "summary": "This study compares state-of-the-art Large Language Models (LLMs) on their\ntendency to generate vulnerabilities when writing C programs using a neutral\nzero-shot prompt. Tihanyi et al. introduced the FormAI dataset at PROMISE'23,\nfeaturing 112,000 C programs generated by GPT-3.5-turbo, with over 51.24%\nidentified as vulnerable. We extended that research with a large-scale study\ninvolving 9 state-of-the-art models such as OpenAI's GPT-4o-mini, Google's\nGemini Pro 1.0, TII's 180 billion-parameter Falcon, Meta's 13 billion-parameter\nCode Llama, and several other compact models. Additionally, we introduce the\nFormAI-v2 dataset, which comprises 331 000 compilable C programs generated by\nthese LLMs. Each program in the dataset is labeled based on the vulnerabilities\ndetected in its source code through formal verification, using the Efficient\nSMT-based Context-Bounded Model Checker (ESBMC). This technique minimizes false\npositives by providing a counterexample for the specific vulnerability and\nreduces false negatives by thoroughly completing the verification process. Our\nstudy reveals that at least 62.07% of the generated programs are vulnerable.\nThe differences between the models are minor, as they all show similar coding\nerrors with slight variations. Our research highlights that while LLMs offer\npromising capabilities for code generation, deploying their output in a\nproduction environment requires proper risk assessment and validation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study compares state-of-the-art Large Language Models (LLMs) on their\ntendency to generate vulnerabilities when writing C programs using a neutral\nzero-shot prompt. Tihanyi et al. introduced the FormAI dataset at PROMISE'23,\nfeaturing 112,000 C programs generated by GPT-3.5-turbo, with over 51.24%\nidentified as vulnerable. We extended that research with a large-scale study\ninvolving 9 state-of-the-art models such as OpenAI's GPT-4o-mini, Google's\nGemini Pro 1.0, TII's 180 billion-parameter Falcon, Meta's 13 billion-parameter\nCode Llama, and several other compact models. Additionally, we introduce the\nFormAI-v2 dataset, which comprises 331 000 compilable C programs generated by\nthese LLMs. Each program in the dataset is labeled based on the vulnerabilities\ndetected in its source code through formal verification, using the Efficient\nSMT-based Context-Bounded Model Checker (ESBMC). This technique minimizes false\npositives by providing a counterexample for the specific vulnerability and\nreduces false negatives by thoroughly completing the verification process. Our\nstudy reveals that at least 62.07% of the generated programs are vulnerable.\nThe differences between the models are minor, as they all show similar coding\nerrors with slight variations. Our research highlights that while LLMs offer\npromising capabilities for code generation, deploying their output in a\nproduction environment requires proper risk assessment and validation."
                },
                "authors": [
                    {
                        "name": "Norbert Tihanyi"
                    },
                    {
                        "name": "Tamas Bisztray"
                    },
                    {
                        "name": "Mohamed Amine Ferrag"
                    },
                    {
                        "name": "Ridhi Jain"
                    },
                    {
                        "name": "Lucas C. Cordeiro"
                    }
                ],
                "author_detail": {
                    "name": "Lucas C. Cordeiro"
                },
                "author": "Lucas C. Cordeiro",
                "arxiv_doi": "10.1007/s10664-024-10590-1.",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10664-024-10590-1.",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2404.18353v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18353v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted and will be shortly published at Empirical Software\n  Engineering (EMSE). Journal Impact Factor: 3.5 (2023)",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07646v2",
                "updated": "2024-12-11T12:50:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    50,
                    3,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-10T16:32:19Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    16,
                    32,
                    19,
                    1,
                    345,
                    0
                ],
                "title": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Searching for Structure: Investigating Emergent Communication with Large\n  Language Models"
                },
                "summary": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human languages have evolved to be structured through repeated language\nlearning and use. These processes introduce biases that operate during language\nacquisition and shape linguistic systems toward communicative efficiency. In\nthis paper, we investigate whether the same happens if artificial languages are\noptimised for implicit biases of Large Language Models (LLMs). To this end, we\nsimulate a classical referential game in which LLMs learn and use artificial\nlanguages. Our results show that initially unstructured holistic languages are\nindeed shaped to have some structural properties that allow two LLM agents to\ncommunicate successfully. Similar to observations in human experiments,\ngenerational transmission increases the learnability of languages, but can at\nthe same time result in non-humanlike degenerate vocabularies. Taken together,\nthis work extends experimental findings, shows that LLMs can be used as tools\nin simulations of language evolution, and opens possibilities for future\nhuman-machine experiments in this field."
                },
                "authors": [
                    {
                        "name": "Tom Kouwenhoven"
                    },
                    {
                        "name": "Max Peeperkorn"
                    },
                    {
                        "name": "Tessa Verhoef"
                    }
                ],
                "author_detail": {
                    "name": "Tessa Verhoef"
                },
                "author": "Tessa Verhoef",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06843v2",
                "updated": "2024-12-11T12:35:25Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    12,
                    35,
                    25,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-07T16:35:14Z",
                "published_parsed": [
                    2024,
                    12,
                    7,
                    16,
                    35,
                    14,
                    5,
                    342,
                    0
                ],
                "title": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe\n  Responses in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Semantic Loss Guided Data Efficient Supervised Fine Tuning for Safe\n  Responses in LLMs"
                },
                "summary": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is\na significant issue in their applications. While various efforts aim to address\nthis safety concern, previous approaches often demand substantial human data\ncollection or rely on the less dependable option of using another LLM to\ngenerate corrective data. In this paper, we aim to take this problem and\novercome limitations of requiring significant high-quality human data. Our\nmethod requires only a small set of unsafe responses to toxic prompts, easily\nobtained from the unsafe LLM itself. By employing a semantic cost combined with\na negative Earth Mover Distance (EMD) loss, we guide the LLM away from\ngenerating unsafe responses. Additionally, we propose a novel lower bound for\nEMD loss, enabling more efficient optimization. Our results demonstrate\nsuperior performance and data efficiency compared to baselines, and we further\nexamine the nuanced effects of over-alignment and potential degradation of\nlanguage capabilities when using contrastive data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) generating unsafe responses to toxic prompts is\na significant issue in their applications. While various efforts aim to address\nthis safety concern, previous approaches often demand substantial human data\ncollection or rely on the less dependable option of using another LLM to\ngenerate corrective data. In this paper, we aim to take this problem and\novercome limitations of requiring significant high-quality human data. Our\nmethod requires only a small set of unsafe responses to toxic prompts, easily\nobtained from the unsafe LLM itself. By employing a semantic cost combined with\na negative Earth Mover Distance (EMD) loss, we guide the LLM away from\ngenerating unsafe responses. Additionally, we propose a novel lower bound for\nEMD loss, enabling more efficient optimization. Our results demonstrate\nsuperior performance and data efficiency compared to baselines, and we further\nexamine the nuanced effects of over-alignment and potential degradation of\nlanguage capabilities when using contrastive data."
                },
                "authors": [
                    {
                        "name": "Yuxiao Lu"
                    },
                    {
                        "name": "Arunesh Sinha"
                    },
                    {
                        "name": "Pradeep Varakantham"
                    }
                ],
                "author_detail": {
                    "name": "Pradeep Varakantham"
                },
                "author": "Pradeep Varakantham",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.00380v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.00380v3",
                "updated": "2024-12-11T11:52:58Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    52,
                    58,
                    2,
                    346,
                    0
                ],
                "published": "2024-06-01T09:36:16Z",
                "published_parsed": [
                    2024,
                    6,
                    1,
                    9,
                    36,
                    16,
                    5,
                    153,
                    0
                ],
                "title": "HonestLLM: Toward an Honest and Helpful Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HonestLLM: Toward an Honest and Helpful Large Language Model"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\nindustries due to their exceptional generative capabilities. However, for safe\nand effective real-world deployments, ensuring honesty and helpfulness is\ncritical. This paper addresses the question: Can we prioritize the helpfulness\nof LLMs while preserving their honesty? To begin with, we establish exhaustive\nprinciples aimed at guaranteeing the honesty of LLM. Additionally, we introduce\na novel dataset, referred to as HoneSet, comprising 930 queries spanning six\ncategories meticulously crafted to assess an LLM's capacity for maintaining\nhonesty. Subsequently, we present two approaches to augmenting honesty and\nhelpfulness in LLMs: a training-free enhancement and a fine-tuning-based\nimprovement. The training-free approach, which is based on curiosity-driven\nprompting, empowers LLMs to articulate internal confusion and uncertainty\nregarding queries, thereby optimizing their responses. Conversely, the\nfine-tuning-based method employs a two-stage process inspired by curriculum\nlearning: initially instructing LLMs to discern between honest and dishonest\nresponses, then refining their training to enhance helpfulness. Experiments\nconducted on nine prominent LLMs demonstrate a significant improvement in\nalignment with honesty across all models through the implementation of our\nproposed enhancements. Particularly noteworthy is the 65.3% enhancement\nobserved in Llama3-8b and the remarkable 124.7% improvement in Mistral-7b, as\nmeasured by the H$^{2}$ (honest and helpful) assessment. We believe that our\nwork can pave the way for developing more trustworthy LLMs for real-world\napplications."
                },
                "authors": [
                    {
                        "name": "Chujie Gao"
                    },
                    {
                        "name": "Siyuan Wu"
                    },
                    {
                        "name": "Yue Huang"
                    },
                    {
                        "name": "Dongping Chen"
                    },
                    {
                        "name": "Qihui Zhang"
                    },
                    {
                        "name": "Zhengyan Fu"
                    },
                    {
                        "name": "Yao Wan"
                    },
                    {
                        "name": "Lichao Sun"
                    },
                    {
                        "name": "Xiangliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangliang Zhang"
                },
                "author": "Xiangliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.00380v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.00380v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08315v1",
                "updated": "2024-12-11T11:52:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    52,
                    16,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T11:52:16Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    52,
                    16,
                    2,
                    346,
                    0
                ],
                "title": "Lightweight Method for Interactive 3D Medical Image Segmentation with\n  Multi-Round Result Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lightweight Method for Interactive 3D Medical Image Segmentation with\n  Multi-Round Result Fusion"
                },
                "summary": "In medical imaging, precise annotation of lesions or organs is often\nrequired. However, 3D volumetric images typically consist of hundreds or\nthousands of slices, making the annotation process extremely time-consuming and\nlaborious. Recently, the Segment Anything Model (SAM) has drawn widespread\nattention due to its remarkable zero-shot generalization capabilities in\ninteractive segmentation. While researchers have explored adapting SAM for\nmedical applications, such as using SAM adapters or constructing 3D SAM models,\na key question remains: Can traditional CNN networks achieve the same strong\nzero-shot generalization in this task? In this paper, we propose the\nLightweight Interactive Network for 3D Medical Image Segmentation (LIM-Net), a\nnovel approach demonstrating the potential of compact CNN-based models. Built\nupon a 2D CNN backbone, LIM-Net initiates segmentation by generating a 2D\nprompt mask from user hints. This mask is then propagated through the 3D\nsequence via the Memory Module. To refine and stabilize results during\ninteraction, the Multi-Round Result Fusion (MRF) Module selects and merges\noptimal masks from multiple rounds. Our extensive experiments across multiple\ndatasets and modalities demonstrate LIM-Net's competitive performance. It\nexhibits stronger generalization to unseen data compared to SAM-based models,\nwith competitive accuracy while requiring fewer interactions. Notably,\nLIM-Net's lightweight design offers significant advantages in deployment and\ninference efficiency, with low GPU memory consumption suitable for\nresource-constrained environments. These promising results demonstrate LIM-Net\ncan serve as a strong baseline, complementing and contrasting with popular SAM\nmodels to further boost effective interactive medical image segmentation. The\ncode will be released at \\url{https://github.com/goodtime-123/LIM-Net}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In medical imaging, precise annotation of lesions or organs is often\nrequired. However, 3D volumetric images typically consist of hundreds or\nthousands of slices, making the annotation process extremely time-consuming and\nlaborious. Recently, the Segment Anything Model (SAM) has drawn widespread\nattention due to its remarkable zero-shot generalization capabilities in\ninteractive segmentation. While researchers have explored adapting SAM for\nmedical applications, such as using SAM adapters or constructing 3D SAM models,\na key question remains: Can traditional CNN networks achieve the same strong\nzero-shot generalization in this task? In this paper, we propose the\nLightweight Interactive Network for 3D Medical Image Segmentation (LIM-Net), a\nnovel approach demonstrating the potential of compact CNN-based models. Built\nupon a 2D CNN backbone, LIM-Net initiates segmentation by generating a 2D\nprompt mask from user hints. This mask is then propagated through the 3D\nsequence via the Memory Module. To refine and stabilize results during\ninteraction, the Multi-Round Result Fusion (MRF) Module selects and merges\noptimal masks from multiple rounds. Our extensive experiments across multiple\ndatasets and modalities demonstrate LIM-Net's competitive performance. It\nexhibits stronger generalization to unseen data compared to SAM-based models,\nwith competitive accuracy while requiring fewer interactions. Notably,\nLIM-Net's lightweight design offers significant advantages in deployment and\ninference efficiency, with low GPU memory consumption suitable for\nresource-constrained environments. These promising results demonstrate LIM-Net\ncan serve as a strong baseline, complementing and contrasting with popular SAM\nmodels to further boost effective interactive medical image segmentation. The\ncode will be released at \\url{https://github.com/goodtime-123/LIM-Net}."
                },
                "authors": [
                    {
                        "name": "Bingzhi Shen"
                    },
                    {
                        "name": "Lufan Chang"
                    },
                    {
                        "name": "Siqi Chen"
                    },
                    {
                        "name": "Shuxiang Guo"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.01990v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.01990v2",
                "updated": "2024-12-11T11:39:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    39,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-03T15:35:01Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    15,
                    35,
                    1,
                    1,
                    247,
                    0
                ],
                "title": "Efficient Large Foundation Model Inference: A Perspective From Model and\n  System Co-Design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Large Foundation Model Inference: A Perspective From Model and\n  System Co-Design"
                },
                "summary": "As Large Language Models (LLMs) become popular, the need for efficient design\nfor ML models on LLMs grows. We are amazed by the excellent output by the LLMs,\nyet we are still troubled with slow inference speed and large memory\nconsumption of contemporary LLMs. This paper focuses on modern efficient\ninference technologies on LLMs and illustrates them from two perspectives:\nmodel and system design. These methodologies optimize LLM inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become popular, the need for efficient design\nfor ML models on LLMs grows. We are amazed by the excellent output by the LLMs,\nyet we are still troubled with slow inference speed and large memory\nconsumption of contemporary LLMs. This paper focuses on modern efficient\ninference technologies on LLMs and illustrates them from two perspectives:\nmodel and system design. These methodologies optimize LLM inference from\ndifferent aspects to save computational resources, making LLMs more efficient,\naffordable, and more accessible."
                },
                "authors": [
                    {
                        "name": "Dong Liu"
                    },
                    {
                        "name": "Zhixin Lai"
                    },
                    {
                        "name": "Yite Wang"
                    },
                    {
                        "name": "Jing Wu"
                    },
                    {
                        "name": "Yanxuan Yu"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Benjamin Lengerich"
                    },
                    {
                        "name": "Ying Nian Wu"
                    }
                ],
                "author_detail": {
                    "name": "Ying Nian Wu"
                },
                "author": "Ying Nian Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.01990v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.01990v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.02232v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.02232v4",
                "updated": "2024-12-11T11:18:54Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    18,
                    54,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-05T04:53:01Z",
                "published_parsed": [
                    2024,
                    8,
                    5,
                    4,
                    53,
                    1,
                    0,
                    218,
                    0
                ],
                "title": "SpecRover: Code Intent Extraction via LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRover: Code Intent Extraction via LLMs"
                },
                "summary": "Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous program improvement typically involves automatically producing bug\nfixes and feature additions. Such program improvement can be accomplished by a\ncombination of large language model (LLM) and program analysis capabilities, in\nthe form of an LLM agent. Since program repair or program improvement typically\nrequires a specification of intended behavior - specification inference can be\nuseful for producing high quality program patches. In this work, we examine\nefficient and low-cost workflows for iterative specification inference within\nan LLM agent. Given a GitHub issue to be resolved in a software project, our\ngoal is to conduct iterative code search accompanied by specification inference\n- thereby inferring intent from both the project structure and behavior. The\nintent thus captured is examined by a reviewer agent with the goal of vetting\nthe patches as well as providing a measure of confidence in the vetted patches.\nOur approach SpecRover (AutoCodeRover-v2) is built on the open-source LLM agent\nAutoCodeRover. In an evaluation on the full SWE-Bench consisting of 2294 GitHub\nissues, it shows more than 50% improvement in efficacy over AutoCodeRover.\nCompared to the open-source agents available, our work shows modest cost ($0.65\nper issue) in resolving an average GitHub issue in SWE-Bench lite. The\nproduction of explanation by SpecRover allows for a better \"signal\" to be given\nto the developer, on when the suggested patches can be accepted with\nconfidence. SpecRover also seeks to demonstrate the continued importance of\nspecification inference in automated program repair, even as program repair\ntechnologies enter the LLM era."
                },
                "authors": [
                    {
                        "name": "Haifeng Ruan"
                    },
                    {
                        "name": "Yuntong Zhang"
                    },
                    {
                        "name": "Abhik Roychoudhury"
                    }
                ],
                "author_detail": {
                    "name": "Abhik Roychoudhury"
                },
                "author": "Abhik Roychoudhury",
                "arxiv_comment": "Haifeng Ruan and Yuntong Zhang contributed equally to this work. To\n  appear in ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.02232v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.02232v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08291v1",
                "updated": "2024-12-11T11:07:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    7,
                    50,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T11:07:50Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    11,
                    7,
                    50,
                    2,
                    346,
                    0
                ],
                "title": "Code LLMs: A Taxonomy-based Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code LLMs: A Taxonomy-based Survey"
                },
                "summary": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks and have recently expanded their impact to coding tasks,\nbridging the gap between natural languages (NL) and programming languages (PL).\nThis taxonomy-based survey provides a comprehensive analysis of LLMs in the\nNL-PL domain, investigating how these models are utilized in coding tasks and\nexamining their methodologies, architectures, and training processes. We\npropose a taxonomy-based framework that categorizes relevant concepts,\nproviding a unified classification system to facilitate a deeper understanding\nof this rapidly evolving field. This survey offers insights into the current\nstate and future directions of LLMs in coding tasks, including their\napplications and limitations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated remarkable capabilities across\nvarious NLP tasks and have recently expanded their impact to coding tasks,\nbridging the gap between natural languages (NL) and programming languages (PL).\nThis taxonomy-based survey provides a comprehensive analysis of LLMs in the\nNL-PL domain, investigating how these models are utilized in coding tasks and\nexamining their methodologies, architectures, and training processes. We\npropose a taxonomy-based framework that categorizes relevant concepts,\nproviding a unified classification system to facilitate a deeper understanding\nof this rapidly evolving field. This survey offers insights into the current\nstate and future directions of LLMs in coding tasks, including their\napplications and limitations."
                },
                "authors": [
                    {
                        "name": "Nishat Raihan"
                    },
                    {
                        "name": "Christian Newman"
                    },
                    {
                        "name": "Marcos Zampieri"
                    }
                ],
                "author_detail": {
                    "name": "Marcos Zampieri"
                },
                "author": "Marcos Zampieri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08281v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08281v1",
                "updated": "2024-12-11T10:56:47Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    56,
                    47,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:56:47Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    56,
                    47,
                    2,
                    346,
                    0
                ],
                "title": "Lachesis: Predicting LLM Inference Accuracy using Structural Properties\n  of Reasoning Paths",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lachesis: Predicting LLM Inference Accuracy using Structural Properties\n  of Reasoning Paths"
                },
                "summary": "Large Language Models are increasingly used to build agents to perform more\ncomplex tasks. As LLMs perform more complicated reasoning through longer\ninteractions, self-consistency, i.e., the idea that the answer obtained from\nsampling and marginalising a number of multiple independent inferences is more\nlikely to be correct, has received much attention as a simple validation\ntechnique. This paper aims to empirically verify this intuitive hypothesis by\npredicting the correctness of answers obtained using self-consistency from\nproperties of the samples of reasoning paths. We introduce Lachesis, a\npredictive model for self-consistency based LLM inferences, and empirically\nevaluate it using AutoFL, a recently proposed LLM-based fault localisation\ntechnique, as the target technique that uses self-consistency. Lachesis\nconverts collected reasoning paths from AutoFL using specifically designed\nreasoning path representations, and trains LSTM and GCN models to predict\nwhether a given set of reasoning paths would result in a correct answer. The\nresults suggest that Lachesis can predict the correctness of answers with a\nprecision of up to 0.8136, highlighting the possibility of training a\npredictive model that can allow early termination of inferences that are not\nlikely to be successful.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are increasingly used to build agents to perform more\ncomplex tasks. As LLMs perform more complicated reasoning through longer\ninteractions, self-consistency, i.e., the idea that the answer obtained from\nsampling and marginalising a number of multiple independent inferences is more\nlikely to be correct, has received much attention as a simple validation\ntechnique. This paper aims to empirically verify this intuitive hypothesis by\npredicting the correctness of answers obtained using self-consistency from\nproperties of the samples of reasoning paths. We introduce Lachesis, a\npredictive model for self-consistency based LLM inferences, and empirically\nevaluate it using AutoFL, a recently proposed LLM-based fault localisation\ntechnique, as the target technique that uses self-consistency. Lachesis\nconverts collected reasoning paths from AutoFL using specifically designed\nreasoning path representations, and trains LSTM and GCN models to predict\nwhether a given set of reasoning paths would result in a correct answer. The\nresults suggest that Lachesis can predict the correctness of answers with a\nprecision of up to 0.8136, highlighting the possibility of training a\npredictive model that can allow early termination of inferences that are not\nlikely to be successful."
                },
                "authors": [
                    {
                        "name": "Naryeong Kim"
                    },
                    {
                        "name": "Sungmin Kang"
                    },
                    {
                        "name": "Gabin An"
                    },
                    {
                        "name": "Shin Yoo"
                    }
                ],
                "author_detail": {
                    "name": "Shin Yoo"
                },
                "author": "Shin Yoo",
                "arxiv_comment": "To appear at DeepTest 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08281v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08281v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.19670v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.19670v4",
                "updated": "2024-12-11T10:56:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    56,
                    3,
                    2,
                    346,
                    0
                ],
                "published": "2024-05-30T03:44:54Z",
                "published_parsed": [
                    2024,
                    5,
                    30,
                    3,
                    44,
                    54,
                    3,
                    151,
                    0
                ],
                "title": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for\n  Retrieval-Augmented Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One Token Can Help! Learning Scalable and Pluggable Virtual Tokens for\n  Retrieval-Augmented Large Language Models"
                },
                "summary": "Retrieval-augmented generation (RAG) is a promising way to improve large\nlanguage models (LLMs) for generating more factual, accurate, and up-to-date\ncontent. Existing methods either optimize prompts to guide LLMs in leveraging\nretrieved information or directly fine-tune LLMs to adapt to RAG scenarios.\nAlthough fine-tuning can yield better performance, it often compromises the\nLLMs' general generation capabilities by modifying their parameters. This\nlimitation poses challenges in practical applications, especially when LLMs are\nalready deployed, as parameter adjustments may affect their original\nfunctionality. To address this, we propose a novel method that involves\nlearning scalable and pluggable virtual tokens for RAG. By maintaining the\nLLMs' original parameters and fine-tuning only the embeddings of these\npluggable tokens, our approach not only enhances LLMs' performance but also\npreserves their general generation capabilities. Furthermore, we design several\ntraining strategies to improve the scalability, flexibility, and\ngeneralizability of our method. Comprehensive experiments across 12\nquestion-answering tasks demonstrate the superiority of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is a promising way to improve large\nlanguage models (LLMs) for generating more factual, accurate, and up-to-date\ncontent. Existing methods either optimize prompts to guide LLMs in leveraging\nretrieved information or directly fine-tune LLMs to adapt to RAG scenarios.\nAlthough fine-tuning can yield better performance, it often compromises the\nLLMs' general generation capabilities by modifying their parameters. This\nlimitation poses challenges in practical applications, especially when LLMs are\nalready deployed, as parameter adjustments may affect their original\nfunctionality. To address this, we propose a novel method that involves\nlearning scalable and pluggable virtual tokens for RAG. By maintaining the\nLLMs' original parameters and fine-tuning only the embeddings of these\npluggable tokens, our approach not only enhances LLMs' performance but also\npreserves their general generation capabilities. Furthermore, we design several\ntraining strategies to improve the scalability, flexibility, and\ngeneralizability of our method. Comprehensive experiments across 12\nquestion-answering tasks demonstrate the superiority of our approach."
                },
                "authors": [
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Zhaoheng Huang"
                    },
                    {
                        "name": "Zhicheng Dou"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "Accepted by AAAI 2025, repo: https://github.com/DaoD/SPRING/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.19670v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.19670v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08279v1",
                "updated": "2024-12-11T10:52:29Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    52,
                    29,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:52:29Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    52,
                    29,
                    2,
                    346,
                    0
                ],
                "title": "Y-NQ: English-YorÃ¹bÃ¡ Evaluation dataset for Open-Book Reading\n  Comprehension and Text Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Y-NQ: English-YorÃ¹bÃ¡ Evaluation dataset for Open-Book Reading\n  Comprehension and Text Generation"
                },
                "summary": "The purpose of this work is to share an English-Yor\\`ub\\'a evaluation dataset\nfor open-book reading comprehension and text generation to assess the\nperformance of models both in a high- and a low- resource language. The dataset\ncontains 358 questions and answers on 338 English documents and 208 Yor\\`ub\\'a\ndocuments. The average document length is ~ 10k words for English and 430 words\nfor Yor\\`ub\\'a. Experiments show a consistent disparity in performance between\nthe two languages, with Yor\\`ub\\'a falling behind English for automatic metrics\neven if documents are much shorter for this language. For a small set of\ndocuments with comparable length, performance of Yor\\`ub\\'a drops by x2.5\ntimes. When analyzing performance by length, we observe that Yor\\`ub\\'a\ndecreases performance dramatically for documents that reach 1500 words while\nEnglish performance is barely affected at that length. Our dataset opens the\ndoor to showcasing if English LLM reading comprehension capabilities extend to\nYor\\`ub\\'a, which for the evaluated LLMs is not the case.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The purpose of this work is to share an English-Yor\\`ub\\'a evaluation dataset\nfor open-book reading comprehension and text generation to assess the\nperformance of models both in a high- and a low- resource language. The dataset\ncontains 358 questions and answers on 338 English documents and 208 Yor\\`ub\\'a\ndocuments. The average document length is ~ 10k words for English and 430 words\nfor Yor\\`ub\\'a. Experiments show a consistent disparity in performance between\nthe two languages, with Yor\\`ub\\'a falling behind English for automatic metrics\neven if documents are much shorter for this language. For a small set of\ndocuments with comparable length, performance of Yor\\`ub\\'a drops by x2.5\ntimes. When analyzing performance by length, we observe that Yor\\`ub\\'a\ndecreases performance dramatically for documents that reach 1500 words while\nEnglish performance is barely affected at that length. Our dataset opens the\ndoor to showcasing if English LLM reading comprehension capabilities extend to\nYor\\`ub\\'a, which for the evaluated LLMs is not the case."
                },
                "authors": [
                    {
                        "name": "Marta R. Costa-jussÃ "
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Ifeoluwanimi Adebara"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Eduardo SÃ¡nchez"
                    }
                ],
                "author_detail": {
                    "name": "Eduardo SÃ¡nchez"
                },
                "author": "Eduardo SÃ¡nchez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08268v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08268v1",
                "updated": "2024-12-11T10:35:45Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    35,
                    45,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:35:45Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    35,
                    45,
                    2,
                    346,
                    0
                ],
                "title": "LCFO: Long Context and Long Form Output Dataset and Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LCFO: Long Context and Long Form Output Dataset and Benchmarking"
                },
                "summary": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents the Long Context and Form Output (LCFO) benchmark, a\nnovel evaluation framework for assessing gradual summarization and summary\nexpansion capabilities across diverse domains. LCFO consists of long input\ndocuments (5k words average length), each of which comes with three summaries\nof different lengths (20%, 10%, and 5% of the input text), as well as\napproximately 15 questions and answers (QA) related to the input content.\nNotably, LCFO also provides alignments between specific QA pairs and\ncorresponding summaries in 7 domains. The primary motivation behind providing\nsummaries of different lengths is to establish a controllable framework for\ngenerating long texts from shorter inputs, i.e. summary expansion. To establish\nan evaluation metric framework for summarization and summary expansion, we\nprovide human evaluation scores for human-generated outputs, as well as results\nfrom various state-of-the-art large language models (LLMs). GPT-4o-mini\nachieves best human scores among automatic systems in both summarization and\nsummary expansion tasks (~ +10% and +20%, respectively). It even surpasses\nhuman output quality in the case of short summaries (~ +7%). Overall automatic\nmetrics achieve low correlations with human evaluation scores (~ 0.4) but\nmoderate correlation on specific evaluation aspects such as fluency and\nattribution (~ 0.6). The LCFO benchmark offers a standardized platform for\nevaluating summarization and summary expansion performance, as well as\ncorresponding automatic metrics, thereby providing an important evaluation\nframework to advance generative AI."
                },
                "authors": [
                    {
                        "name": "Marta R. Costa-jussÃ "
                    },
                    {
                        "name": "Pierre Andrews"
                    },
                    {
                        "name": "Mariano Coria Meglioli"
                    },
                    {
                        "name": "Joy Chen"
                    },
                    {
                        "name": "Joe Chuang"
                    },
                    {
                        "name": "David Dale"
                    },
                    {
                        "name": "Christophe Ropers"
                    },
                    {
                        "name": "Alexandre Mourachko"
                    },
                    {
                        "name": "Eduardo SÃ¡nchez"
                    },
                    {
                        "name": "Holger Schwenk"
                    },
                    {
                        "name": "Tuan Tran"
                    },
                    {
                        "name": "Arina Turkatenko"
                    },
                    {
                        "name": "Carleigh Wood"
                    }
                ],
                "author_detail": {
                    "name": "Carleigh Wood"
                },
                "author": "Carleigh Wood",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08268v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08268v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.05074v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.05074v5",
                "updated": "2024-12-11T10:14:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    14,
                    32,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-09T14:02:24Z",
                "published_parsed": [
                    2024,
                    8,
                    9,
                    14,
                    2,
                    24,
                    4,
                    222,
                    0
                ],
                "title": "Improving Mortality Prediction After Radiotherapy with Large Language\n  Model Structuring of Large-Scale Unstructured Electronic Health Records",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving Mortality Prediction After Radiotherapy with Large Language\n  Model Structuring of Large-Scale Unstructured Electronic Health Records"
                },
                "summary": "Accurate survival prediction in radiotherapy (RT) is critical for optimizing\ntreatment decisions. This study developed and validated the RT-Surv framework,\nwhich integrates general-domain, open-source large language models (LLMs) to\nstructure unstructured electronic health records alongside structured clinical\ndata. Using data from 34,276 patients and an external cohort of 852, the\nframework successfully transformed unstructured clinical information into\nstructured formats. Incorporating LLM-structured clinical features improved the\nconcordance index from 0.779 to 0.842 during external validation, demonstrating\na significant performance enhancement. Key LLM-structured features, such as\ndisease extent, general condition, and RT purpose, showed high predictive\nimportance and aligned closely with statistically significant predictors\nidentified through conventional statistical analyses, thereby improving model\ninterpretability. Furthermore, the framework enhanced risk stratification,\nenabling more distinct differentiation among low-, intermediate-, and high-risk\ngroups (p < 0.001) using LLM-structured clinical features. These findings\nhighlight the potential of LLMs to convert unstructured data into actionable\ninsights, improving predictive modeling and patient outcomes in clinics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate survival prediction in radiotherapy (RT) is critical for optimizing\ntreatment decisions. This study developed and validated the RT-Surv framework,\nwhich integrates general-domain, open-source large language models (LLMs) to\nstructure unstructured electronic health records alongside structured clinical\ndata. Using data from 34,276 patients and an external cohort of 852, the\nframework successfully transformed unstructured clinical information into\nstructured formats. Incorporating LLM-structured clinical features improved the\nconcordance index from 0.779 to 0.842 during external validation, demonstrating\na significant performance enhancement. Key LLM-structured features, such as\ndisease extent, general condition, and RT purpose, showed high predictive\nimportance and aligned closely with statistically significant predictors\nidentified through conventional statistical analyses, thereby improving model\ninterpretability. Furthermore, the framework enhanced risk stratification,\nenabling more distinct differentiation among low-, intermediate-, and high-risk\ngroups (p < 0.001) using LLM-structured clinical features. These findings\nhighlight the potential of LLMs to convert unstructured data into actionable\ninsights, improving predictive modeling and patient outcomes in clinics."
                },
                "authors": [
                    {
                        "name": "Sangjoon Park"
                    },
                    {
                        "name": "Chan Woo Wee"
                    },
                    {
                        "name": "Seo Hee Choi"
                    },
                    {
                        "name": "Kyung Hwan Kim"
                    },
                    {
                        "name": "Jee Suk Chang"
                    },
                    {
                        "name": "Hong In Yoon"
                    },
                    {
                        "name": "Ik Jae Lee"
                    },
                    {
                        "name": "Yong Bae Kim"
                    },
                    {
                        "name": "Jaeho Cho"
                    },
                    {
                        "name": "Ki Chang Keum"
                    },
                    {
                        "name": "Chang Geol Lee"
                    },
                    {
                        "name": "Hwa Kyung Byun"
                    },
                    {
                        "name": "Woong Sub Koom"
                    }
                ],
                "author_detail": {
                    "name": "Woong Sub Koom"
                },
                "author": "Woong Sub Koom",
                "arxiv_comment": "23 pages, 2 tables, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.05074v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.05074v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.18193v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.18193v2",
                "updated": "2024-12-11T10:13:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    13,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-26T18:10:26Z",
                "published_parsed": [
                    2024,
                    9,
                    26,
                    18,
                    10,
                    26,
                    3,
                    270,
                    0
                ],
                "title": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrEmLIn: A Repository of Green Baseline Embeddings for 87 Low-Resource\n  Languages Injected with Multilingual Graph Knowledge"
                },
                "summary": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contextualized embeddings based on large language models (LLMs) are available\nfor various languages, but their coverage is often limited for lower resourced\nlanguages. Using LLMs for such languages is often difficult due to a high\ncomputational cost; not only during training, but also during inference. Static\nword embeddings are much more resource-efficient (\"green\"), and thus still\nprovide value, particularly for very low-resource languages. There is, however,\na notable lack of comprehensive repositories with such embeddings for diverse\nlanguages. To address this gap, we present GrEmLIn, a centralized repository of\ngreen, static baseline embeddings for 87 mid- and low-resource languages. We\ncompute GrEmLIn embeddings with a novel method that enhances GloVe embeddings\nby integrating multilingual graph knowledge, which makes our static embeddings\ncompetitive with LLM representations, while being parameter-free at inference\ntime. Our experiments demonstrate that GrEmLIn embeddings outperform\nstate-of-the-art contextualized embeddings from E5 on the task of lexical\nsimilarity. They remain competitive in extrinsic evaluation tasks like\nsentiment analysis and natural language inference, with average performance\ngaps of just 5-10\\% or less compared to state-of-the-art models, given a\nsufficient vocabulary overlap with the target task, and underperform only on\ntopic classification. Our code and embeddings are publicly available at\nhttps://huggingface.co/DFKI."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Rishu Kumar"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "Long paper, preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.18193v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.18193v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15696v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15696v3",
                "updated": "2024-12-11T10:13:03Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    13,
                    3,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-28T10:51:18Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    10,
                    51,
                    18,
                    2,
                    241,
                    0
                ],
                "title": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing diversity, negativity, and stereotypes in Chinese-language AI\n  technologies: a case study on Baidu, Ernie and Qwen"
                },
                "summary": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) and search engines have the potential to\nperpetuate biases and stereotypes by amplifying existing prejudices in their\ntraining data and algorithmic processes, thereby influencing public perception\nand decision-making. While most work has focused on Western-centric AI\ntechnologies, we study Chinese-based tools by investigating social biases\nembedded in the major Chinese search engine, Baidu, and two leading LLMs, Ernie\nand Qwen. Leveraging a dataset of 240 social groups across 13 categories\ndescribing Chinese society, we collect over 30k views encoded in the\naforementioned tools by prompting them for candidate words describing such\ngroups. We find that language models exhibit a larger variety of embedded views\ncompared to the search engine, although Baidu and Qwen generate negative\ncontent more often than Ernie. We also find a moderate prevalence of\nstereotypes embedded in the language models, many of which potentially promote\noffensive and derogatory views. Our work highlights the importance of promoting\nfairness and inclusivity in AI technologies with a global perspective."
                },
                "authors": [
                    {
                        "name": "Geng Liu"
                    },
                    {
                        "name": "Carlo Alberto Bono"
                    },
                    {
                        "name": "Francesco Pierri"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Pierri"
                },
                "author": "Francesco Pierri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15696v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15696v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08258v1",
                "updated": "2024-12-11T10:11:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    11,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:11:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    11,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "Large Language Models for Scholarly Ontology Generation: An Extensive\n  Analysis in the Engineering Field",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Scholarly Ontology Generation: An Extensive\n  Analysis in the Engineering Field"
                },
                "summary": "Ontologies of research topics are crucial for structuring scientific\nknowledge, enabling scientists to navigate vast amounts of research, and\nforming the backbone of intelligent systems such as search engines and\nrecommendation systems. However, manual creation of these ontologies is\nexpensive, slow, and often results in outdated and overly general\nrepresentations. As a solution, researchers have been investigating ways to\nautomate or semi-automate the process of generating these ontologies. This\npaper offers a comprehensive analysis of the ability of large language models\n(LLMs) to identify semantic relationships between different research topics,\nwhich is a critical step in the development of such ontologies. To this end, we\ndeveloped a gold standard based on the IEEE Thesaurus to evaluate the task of\nidentifying four types of relationships between pairs of topics: broader,\nnarrower, same-as, and other. Our study evaluates the performance of seventeen\nLLMs, which differ in scale, accessibility (open vs. proprietary), and model\ntype (full vs. quantised), while also assessing four zero-shot reasoning\nstrategies. Several models have achieved outstanding results, including\nMixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,\n0.920, and 0.967, respectively. Furthermore, our findings demonstrate that\nsmaller, quantised models, when optimised through prompt engineering, can\ndeliver performance comparable to much larger proprietary models, while\nrequiring significantly fewer computational resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ontologies of research topics are crucial for structuring scientific\nknowledge, enabling scientists to navigate vast amounts of research, and\nforming the backbone of intelligent systems such as search engines and\nrecommendation systems. However, manual creation of these ontologies is\nexpensive, slow, and often results in outdated and overly general\nrepresentations. As a solution, researchers have been investigating ways to\nautomate or semi-automate the process of generating these ontologies. This\npaper offers a comprehensive analysis of the ability of large language models\n(LLMs) to identify semantic relationships between different research topics,\nwhich is a critical step in the development of such ontologies. To this end, we\ndeveloped a gold standard based on the IEEE Thesaurus to evaluate the task of\nidentifying four types of relationships between pairs of topics: broader,\nnarrower, same-as, and other. Our study evaluates the performance of seventeen\nLLMs, which differ in scale, accessibility (open vs. proprietary), and model\ntype (full vs. quantised), while also assessing four zero-shot reasoning\nstrategies. Several models have achieved outstanding results, including\nMixtral-8x7B, Dolphin-Mistral-7B, and Claude 3 Sonnet, with F1-scores of 0.847,\n0.920, and 0.967, respectively. Furthermore, our findings demonstrate that\nsmaller, quantised models, when optimised through prompt engineering, can\ndeliver performance comparable to much larger proprietary models, while\nrequiring significantly fewer computational resources."
                },
                "authors": [
                    {
                        "name": "Tanay Aggarwal"
                    },
                    {
                        "name": "Angelo Salatino"
                    },
                    {
                        "name": "Francesco Osborne"
                    },
                    {
                        "name": "Enrico Motta"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Motta"
                },
                "author": "Enrico Motta",
                "arxiv_comment": "submitted to Information Processing & Management",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08251v1",
                "updated": "2024-12-11T10:03:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    3,
                    6,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T10:03:06Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    10,
                    3,
                    6,
                    2,
                    346,
                    0
                ],
                "title": "Parameter Estimation based Automatic Modulation Recognition for Radio\n  Frequency Signal",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parameter Estimation based Automatic Modulation Recognition for Radio\n  Frequency Signal"
                },
                "summary": "Automatic modulation recognition (AMR) critically contributes to spectrum\nsensing, dynamic spectrum access, and intelligent communications in cognitive\nradio systems. The introduction of deep learning has greatly improved the\naccuracy of AMR. However, current automatic identification methods require the\ninput of key parameters such as the carrier frequency, which is necessary to\nconvert the radio frequency (RF) to a base-band signal before it can be used\nfor identification. In addition, the high complexity of deep learning models\nleads to high computational effort and long recognition times of existing\nmethods, which are difficult to implement in demodulation system deployments.\nTo address the above issues, in this paper, we first use power spectrum\nanalysis to estimate the carrier frequency and signal bandwidth, which realizes\nthe effective conversion from RF signals to base-band signals. This paper\nchooses the long short-term memory (LSTM) network as the model for automatic\nidentification, which has low implementation complexity while maintaining high\naccuracy. Finally, by training the LSTM with actual sampling data combined with\nparameter estimation (PE), the method proposed in this paper can guarantee more\nthan 90% format recognition accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic modulation recognition (AMR) critically contributes to spectrum\nsensing, dynamic spectrum access, and intelligent communications in cognitive\nradio systems. The introduction of deep learning has greatly improved the\naccuracy of AMR. However, current automatic identification methods require the\ninput of key parameters such as the carrier frequency, which is necessary to\nconvert the radio frequency (RF) to a base-band signal before it can be used\nfor identification. In addition, the high complexity of deep learning models\nleads to high computational effort and long recognition times of existing\nmethods, which are difficult to implement in demodulation system deployments.\nTo address the above issues, in this paper, we first use power spectrum\nanalysis to estimate the carrier frequency and signal bandwidth, which realizes\nthe effective conversion from RF signals to base-band signals. This paper\nchooses the long short-term memory (LSTM) network as the model for automatic\nidentification, which has low implementation complexity while maintaining high\naccuracy. Finally, by training the LSTM with actual sampling data combined with\nparameter estimation (PE), the method proposed in this paper can guarantee more\nthan 90% format recognition accuracy."
                },
                "authors": [
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Kuojun Yang"
                    },
                    {
                        "name": "Zelin Ji"
                    },
                    {
                        "name": "Qinchuan Zhang"
                    },
                    {
                        "name": "Huiqing Pan"
                    }
                ],
                "author_detail": {
                    "name": "Huiqing Pan"
                },
                "author": "Huiqing Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08244v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08244v1",
                "updated": "2024-12-11T09:53:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    53,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T09:53:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    53,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "Call to Protect the Dark and Quiet Sky from Harmful Interference by\n  Satellite Constellations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Call to Protect the Dark and Quiet Sky from Harmful Interference by\n  Satellite Constellations"
                },
                "summary": "The growing number of satellite constellations in low Earth orbit (LEO)\nenhances global communications and Earth observation, and support of space\ncommerce is a high priority of many governments. At the same time, the\nproliferation of satellites in LEO has negative effects on astronomical\nobservations and research, and the preservation of the dark and quiet sky.\nThese satellite constellations reflect sunlight onto optical telescopes, and\ntheir radio emission impacts radio observatories, jeopardising our access to\nessential scientific discoveries through astronomy. The changing visual\nappearance of the sky also impacts our cultural heritage and environment. Both\nground-based observatories and space-based telescopes in LEO are affected, and\nthere are no places on Earth that can escape the effects of satellite\nconstellations given their global nature. The minimally disturbed dark and\nradio-quiet sky is crucial for conducting fundamental research in astronomy and\nimportant public services such as planetary defence, technology development,\nand high-precision geolocation. Some aspects of satellite deployment and\noperation are regulated by States and intergovernmental organisations. While\nregulatory agencies in some States have started to require operators to\ncoordinate with their national astronomy agencies over impacts, mitigation of\nthe impact of space objects on astronomical activities is not sufficiently\nregulated. To address this issue, the CPS urges States and the international\ncommunity to take steps to protect the dark and quiet sky as specified in this\npaper.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing number of satellite constellations in low Earth orbit (LEO)\nenhances global communications and Earth observation, and support of space\ncommerce is a high priority of many governments. At the same time, the\nproliferation of satellites in LEO has negative effects on astronomical\nobservations and research, and the preservation of the dark and quiet sky.\nThese satellite constellations reflect sunlight onto optical telescopes, and\ntheir radio emission impacts radio observatories, jeopardising our access to\nessential scientific discoveries through astronomy. The changing visual\nappearance of the sky also impacts our cultural heritage and environment. Both\nground-based observatories and space-based telescopes in LEO are affected, and\nthere are no places on Earth that can escape the effects of satellite\nconstellations given their global nature. The minimally disturbed dark and\nradio-quiet sky is crucial for conducting fundamental research in astronomy and\nimportant public services such as planetary defence, technology development,\nand high-precision geolocation. Some aspects of satellite deployment and\noperation are regulated by States and intergovernmental organisations. While\nregulatory agencies in some States have started to require operators to\ncoordinate with their national astronomy agencies over impacts, mitigation of\nthe impact of space objects on astronomical activities is not sufficiently\nregulated. To address this issue, the CPS urges States and the international\ncommunity to take steps to protect the dark and quiet sky as specified in this\npaper."
                },
                "authors": [
                    {
                        "name": "IAU Centre for the Protection of the Dark"
                    },
                    {
                        "name": "Quiet Sky from Satellite Constellation Interference"
                    },
                    {
                        "name": "Gyula I. G. JÃ³zsa"
                    },
                    {
                        "name": "Andrew Williams"
                    },
                    {
                        "name": "Richard Green"
                    },
                    {
                        "name": "Isabel Marsh"
                    },
                    {
                        "name": "John Antoniadis"
                    },
                    {
                        "name": "Domingos Barbosa"
                    },
                    {
                        "name": "John Barentine"
                    },
                    {
                        "name": "Guillermo Blanc"
                    },
                    {
                        "name": "Bruno Coelho"
                    },
                    {
                        "name": "Patricia Cooper"
                    },
                    {
                        "name": "Sara Dalledonne"
                    },
                    {
                        "name": "Federico Di Vruno"
                    },
                    {
                        "name": "Joe Diamond"
                    },
                    {
                        "name": "Adam Dong"
                    },
                    {
                        "name": "Ronald Drimmel"
                    },
                    {
                        "name": "Siegfried Eggl"
                    },
                    {
                        "name": "Nusrin Habeeb"
                    },
                    {
                        "name": "Jessica Heim"
                    },
                    {
                        "name": "Chris Hofer"
                    },
                    {
                        "name": "Narae Hwang"
                    },
                    {
                        "name": "Mathieu Isidro"
                    },
                    {
                        "name": "David Koplow"
                    },
                    {
                        "name": "James Lowenthal"
                    },
                    {
                        "name": "Sara Lucatello"
                    },
                    {
                        "name": "Mariya Lyubenova"
                    },
                    {
                        "name": "Robert Massey"
                    },
                    {
                        "name": "Mike Peel"
                    },
                    {
                        "name": "Meredith Rawls"
                    },
                    {
                        "name": "Adrien Saada"
                    },
                    {
                        "name": "Alejandro Sanchez"
                    },
                    {
                        "name": "Pedro Sanhueza"
                    },
                    {
                        "name": "Warren Skidmore"
                    },
                    {
                        "name": "Boris Sorokin"
                    },
                    {
                        "name": "P. Sreekumar"
                    },
                    {
                        "name": "Tim Stevenson"
                    },
                    {
                        "name": "Paula Tartari"
                    },
                    {
                        "name": "Vincenza Tornatore"
                    },
                    {
                        "name": "Connie Walker"
                    },
                    {
                        "name": "Benjamin Winkel"
                    },
                    {
                        "name": "Yana Yakushina"
                    }
                ],
                "author_detail": {
                    "name": "Yana Yakushina"
                },
                "arxiv_affiliation": "CPS",
                "author": "Yana Yakushina",
                "arxiv_comment": "This position paper was developed by the IAU Centre for the\n  Protection of the Dark and Quiet Sky from Satellite Constellation\n  Interference (CPS). It can also be downloaded at the CPS website at\n  https://cps.iau.org/news/cps-urges-action-in-first-recommendations-paper/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08244v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08244v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08237v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08237v1",
                "updated": "2024-12-11T09:38:50Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    38,
                    50,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T09:38:50Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    38,
                    50,
                    2,
                    346,
                    0
                ],
                "title": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TouchTTS: An Embarrassingly Simple TTS Framework that Everyone Can Touch"
                },
                "summary": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is well known that LLM-based systems are data-hungry. Recent LLM-based TTS\nworks typically employ complex data processing pipelines to obtain high-quality\ntraining data. These sophisticated pipelines require excellent models at each\nstage (e.g., speech denoising, speech enhancement, speaker diarization, and\npunctuation models), which themselves demand high-quality training data and are\nrarely open-sourced. Even with state-of-the-art models, issues persist, such as\nincomplete background noise removal and misalignment between punctuation and\nactual speech pauses. Moreover, the stringent filtering strategies often retain\nonly 10-30\\% of the original data, significantly impeding data scaling efforts.\nIn this work, we leverage a noise-robust audio tokenizer (S3Tokenizer) to\ndesign a simplified yet effective TTS data processing pipeline that maintains\ndata quality while substantially reducing data acquisition costs, achieving a\ndata retention rate of over 50\\%. Beyond data scaling challenges, LLM-based TTS\nsystems also incur higher deployment costs compared to conventional approaches.\nCurrent systems typically use LLMs solely for text-to-token generation, while\nrequiring separate models (e.g., flow matching models) for token-to-waveform\ngeneration, which cannot be directly executed by LLM inference engines, further\ncomplicating deployment. To address these challenges, we eliminate redundant\nmodules in both LLM and flow components, replacing the flow model backbone with\nan LLM architecture. Building upon this simplified flow backbone, we propose a\nunified architecture for both streaming and non-streaming inference,\nsignificantly reducing deployment costs. Finally, we explore the feasibility of\nunifying TTS and ASR tasks using the same data for training, thanks to the\nsimplified pipeline and the S3Tokenizer that reduces the quality requirements\nfor TTS training data."
                },
                "authors": [
                    {
                        "name": "Xingchen Song"
                    },
                    {
                        "name": "Mengtao Xing"
                    },
                    {
                        "name": "Changwei Ma"
                    },
                    {
                        "name": "Shengqiang Li"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Binbin Zhang"
                    },
                    {
                        "name": "Fuping Pan"
                    },
                    {
                        "name": "Dinghao Zhou"
                    },
                    {
                        "name": "Yuekai Zhang"
                    },
                    {
                        "name": "Shun Lei"
                    },
                    {
                        "name": "Zhendong Peng"
                    },
                    {
                        "name": "Zhiyong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyong Wu"
                },
                "author": "Zhiyong Wu",
                "arxiv_comment": "Technical Report",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08237v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08237v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.02965v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.02965v2",
                "updated": "2024-12-11T09:30:19Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    30,
                    19,
                    2,
                    346,
                    0
                ],
                "published": "2024-03-05T13:41:25Z",
                "published_parsed": [
                    2024,
                    3,
                    5,
                    13,
                    41,
                    25,
                    1,
                    65,
                    0
                ],
                "title": "ChatGPT and biometrics: an assessment of face recognition, gender\n  detection, and age estimation capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChatGPT and biometrics: an assessment of face recognition, gender\n  detection, and age estimation capabilities"
                },
                "summary": "This paper explores the application of large language models (LLMs), like\nChatGPT, for biometric tasks. We specifically examine the capabilities of\nChatGPT in performing biometric-related tasks, with an emphasis on face\nrecognition, gender detection, and age estimation. Since biometrics are\nconsidered as sensitive information, ChatGPT avoids answering direct prompts,\nand thus we crafted a prompting strategy to bypass its safeguard and evaluate\nthe capabilities for biometrics tasks. Our study reveals that ChatGPT\nrecognizes facial identities and differentiates between two facial images with\nconsiderable accuracy. Additionally, experimental results demonstrate\nremarkable performance in gender detection and reasonable accuracy for the age\nestimation tasks. Our findings shed light on the promising potentials in the\napplication of LLMs and foundation models for biometrics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the application of large language models (LLMs), like\nChatGPT, for biometric tasks. We specifically examine the capabilities of\nChatGPT in performing biometric-related tasks, with an emphasis on face\nrecognition, gender detection, and age estimation. Since biometrics are\nconsidered as sensitive information, ChatGPT avoids answering direct prompts,\nand thus we crafted a prompting strategy to bypass its safeguard and evaluate\nthe capabilities for biometrics tasks. Our study reveals that ChatGPT\nrecognizes facial identities and differentiates between two facial images with\nconsiderable accuracy. Additionally, experimental results demonstrate\nremarkable performance in gender detection and reasonable accuracy for the age\nestimation tasks. Our findings shed light on the promising potentials in the\napplication of LLMs and foundation models for biometrics."
                },
                "authors": [
                    {
                        "name": "Ahmad Hassanpour"
                    },
                    {
                        "name": "Yasamin Kowsari"
                    },
                    {
                        "name": "Hatef Otroshi Shahreza"
                    },
                    {
                        "name": "Bian Yang"
                    },
                    {
                        "name": "Sebastien Marcel"
                    }
                ],
                "author_detail": {
                    "name": "Sebastien Marcel"
                },
                "author": "Sebastien Marcel",
                "arxiv_doi": "10.1109/ICIP51287.2024.10647924",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/ICIP51287.2024.10647924",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2403.02965v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.02965v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a conference paper at IEEE International Conference on\n  Image Processing (ICIP) 2024",
                "arxiv_journal_ref": "Chatgpt and Biometrics: an Assessment of Face Recognition, Gender\n  Detection, and Age Estimation Capabilities,\"2024 IEEE International\n  Conference on Image Processing (ICIP)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02819v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02819v3",
                "updated": "2024-12-11T09:15:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    15,
                    6,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-03T20:35:57Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    20,
                    35,
                    57,
                    1,
                    338,
                    0
                ],
                "title": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CNNSum: Exploring Long-Context Summarization with Large Language Models\n  in Chinese Novels"
                },
                "summary": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering 16k to 128k,\n695 samples in total, the annotations are human-driven. We evaluate commercial\nand open-source models on CNNSum and conduct a detailed analysis. Based on the\nobservations, we further conduct fine-tuning exploration with short-context\nsummary data. In our study: (1) GPT-4o underperformed, due to excessive\nsubjective commentary. (2) Currently, long-context summarization mainly relies\non memory ability, small LLMs with stable longer context lengths are the most\ncost-effective. Using long data concatenated from short-context summaries makes\na significant improvement. (3) Prompt templates may cause a large performance\ngap but can be mitigated through fine-tuning. (4) Fine-tuned Chat or\nInstruction versions may harm the Base model and further fine-tuning cannot\nbridge performance gap. (5) while models with RoPE base scaling exhibit strong\nextrapolation potential, their performance may vary significantly when combined\nwith other interpolation methods and need careful selection. (6) CNNSum\nprovides more reliable and insightful evaluation results than other benchmarks.\nWe release CNNSum to advance research in this field\n(https://github.com/CxsGhost/CNNSum).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have been well-researched in many long-context\ntasks. However, due to high annotation costs, high-quality long-context summary\ndatasets for training or evaluation are scarce, limiting further research. In\nthis work, we introduce CNNSum, a new multi-scale Chinese long-context novel\nsummarization benchmark, including four subsets, length covering 16k to 128k,\n695 samples in total, the annotations are human-driven. We evaluate commercial\nand open-source models on CNNSum and conduct a detailed analysis. Based on the\nobservations, we further conduct fine-tuning exploration with short-context\nsummary data. In our study: (1) GPT-4o underperformed, due to excessive\nsubjective commentary. (2) Currently, long-context summarization mainly relies\non memory ability, small LLMs with stable longer context lengths are the most\ncost-effective. Using long data concatenated from short-context summaries makes\na significant improvement. (3) Prompt templates may cause a large performance\ngap but can be mitigated through fine-tuning. (4) Fine-tuned Chat or\nInstruction versions may harm the Base model and further fine-tuning cannot\nbridge performance gap. (5) while models with RoPE base scaling exhibit strong\nextrapolation potential, their performance may vary significantly when combined\nwith other interpolation methods and need careful selection. (6) CNNSum\nprovides more reliable and insightful evaluation results than other benchmarks.\nWe release CNNSum to advance research in this field\n(https://github.com/CxsGhost/CNNSum)."
                },
                "authors": [
                    {
                        "name": "Lingxiao Wei"
                    },
                    {
                        "name": "He Yan"
                    },
                    {
                        "name": "Xiangju Lu"
                    },
                    {
                        "name": "Junmin Zhu"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Wei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Zhang"
                },
                "author": "Wei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02819v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02819v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.11553v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.11553v3",
                "updated": "2024-12-11T09:04:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    9,
                    4,
                    18,
                    2,
                    346,
                    0
                ],
                "published": "2024-04-17T16:53:16Z",
                "published_parsed": [
                    2024,
                    4,
                    17,
                    16,
                    53,
                    16,
                    2,
                    108,
                    0
                ],
                "title": "Language Ranker: A Metric for Quantifying LLM Performance Across High\n  and Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Ranker: A Metric for Quantifying LLM Performance Across High\n  and Low-Resource Languages"
                },
                "summary": "The development of Large Language Models (LLMs) relies on extensive text\ncorpora, which are often unevenly distributed across languages. This imbalance\nresults in LLMs performing significantly better on high-resource languages like\nEnglish, German, and French, while their capabilities in low-resource languages\nremain inadequate. Currently, there is a lack of quantitative methods to\nevaluate the performance of LLMs in these low-resource languages. To address\nthis gap, we propose the Language Ranker, an intrinsic metric designed to\nbenchmark and rank languages based on LLM performance using internal\nrepresentations. By comparing the LLM's internal representation of various\nlanguages against a baseline derived from English, we can assess the model's\nmultilingual capabilities in a robust and language-agnostic manner. Our\nanalysis reveals that high-resource languages exhibit higher similarity scores\nwith English, demonstrating superior performance, while low-resource languages\nshow lower similarity scores, underscoring the effectiveness of our metric in\nassessing language-specific capabilities. Besides, the experiments show that\nthere is a strong correlation between the LLM's performance in different\nlanguages and the proportion of those languages in its pre-training corpus.\nThese insights underscore the efficacy of the Language Ranker as a tool for\nevaluating LLM performance across different languages, particularly those with\nlimited resources.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of Large Language Models (LLMs) relies on extensive text\ncorpora, which are often unevenly distributed across languages. This imbalance\nresults in LLMs performing significantly better on high-resource languages like\nEnglish, German, and French, while their capabilities in low-resource languages\nremain inadequate. Currently, there is a lack of quantitative methods to\nevaluate the performance of LLMs in these low-resource languages. To address\nthis gap, we propose the Language Ranker, an intrinsic metric designed to\nbenchmark and rank languages based on LLM performance using internal\nrepresentations. By comparing the LLM's internal representation of various\nlanguages against a baseline derived from English, we can assess the model's\nmultilingual capabilities in a robust and language-agnostic manner. Our\nanalysis reveals that high-resource languages exhibit higher similarity scores\nwith English, demonstrating superior performance, while low-resource languages\nshow lower similarity scores, underscoring the effectiveness of our metric in\nassessing language-specific capabilities. Besides, the experiments show that\nthere is a strong correlation between the LLM's performance in different\nlanguages and the proportion of those languages in its pre-training corpus.\nThese insights underscore the efficacy of the Language Ranker as a tool for\nevaluating LLM performance across different languages, particularly those with\nlimited resources."
                },
                "authors": [
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Yucheng Shi"
                    },
                    {
                        "name": "Zirui Liu"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Ali Payani"
                    },
                    {
                        "name": "Ninghao Liu"
                    },
                    {
                        "name": "Mengnan Du"
                    }
                ],
                "author_detail": {
                    "name": "Mengnan Du"
                },
                "author": "Mengnan Du",
                "arxiv_comment": "Accepted by AAAI 2025 (Social Impact Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.11553v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.11553v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08201v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08201v1",
                "updated": "2024-12-11T08:44:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    44,
                    15,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:44:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    44,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Model-Editing-Based Jailbreak against Safety-aligned Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model-Editing-Based Jailbreak against Safety-aligned Large Language\n  Models"
                },
                "summary": "Large Language Models (LLMs) have transformed numerous fields by enabling\nadvanced natural language interactions but remain susceptible to critical\nvulnerabilities, particularly jailbreak attacks. Current jailbreak techniques,\nwhile effective, often depend on input modifications, making them detectable\nand limiting their stealth and scalability. This paper presents Targeted Model\nEditing (TME), a novel white-box approach that bypasses safety filters by\nminimally altering internal model structures while preserving the model's\nintended functionalities. TME identifies and removes safety-critical\ntransformations (SCTs) embedded in model matrices, enabling malicious queries\nto bypass restrictions without input modifications. By analyzing distinct\nactivation patterns between safe and unsafe queries, TME isolates and\napproximates SCTs through an optimization process. Implemented in the D-LLM\nframework, our method achieves an average Attack Success Rate (ASR) of 84.86%\non four mainstream open-source LLMs, maintaining high performance. Unlike\nexisting methods, D-LLM eliminates the need for specific triggers or harmful\nresponse collections, offering a stealthier and more effective jailbreak\nstrategy. This work reveals a covert and robust threat vector in LLM security\nand emphasizes the need for stronger safeguards in model safety alignment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have transformed numerous fields by enabling\nadvanced natural language interactions but remain susceptible to critical\nvulnerabilities, particularly jailbreak attacks. Current jailbreak techniques,\nwhile effective, often depend on input modifications, making them detectable\nand limiting their stealth and scalability. This paper presents Targeted Model\nEditing (TME), a novel white-box approach that bypasses safety filters by\nminimally altering internal model structures while preserving the model's\nintended functionalities. TME identifies and removes safety-critical\ntransformations (SCTs) embedded in model matrices, enabling malicious queries\nto bypass restrictions without input modifications. By analyzing distinct\nactivation patterns between safe and unsafe queries, TME isolates and\napproximates SCTs through an optimization process. Implemented in the D-LLM\nframework, our method achieves an average Attack Success Rate (ASR) of 84.86%\non four mainstream open-source LLMs, maintaining high performance. Unlike\nexisting methods, D-LLM eliminates the need for specific triggers or harmful\nresponse collections, offering a stealthier and more effective jailbreak\nstrategy. This work reveals a covert and robust threat vector in LLM security\nand emphasizes the need for stronger safeguards in model safety alignment."
                },
                "authors": [
                    {
                        "name": "Yuxi Li"
                    },
                    {
                        "name": "Zhibo Zhang"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Ling Shi"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08201v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08201v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.15950v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.15950v5",
                "updated": "2024-12-11T08:40:48Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    40,
                    48,
                    2,
                    346,
                    0
                ],
                "published": "2023-10-24T15:51:13Z",
                "published_parsed": [
                    2023,
                    10,
                    24,
                    15,
                    51,
                    13,
                    1,
                    297,
                    0
                ],
                "title": "Representation Learning with Large Language Models for Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Learning with Large Language Models for Recommendation"
                },
                "summary": "Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommender systems have seen significant advancements with the influence of\ndeep learning and graph neural networks, particularly in capturing complex\nuser-item relationships. However, these graph-based recommenders heavily depend\non ID-based data, potentially disregarding valuable textual information\nassociated with users and items, resulting in less informative learned\nrepresentations. Moreover, the utilization of implicit feedback data introduces\npotential noise and bias, posing challenges for the effectiveness of user\npreference learning. While the integration of large language models (LLMs) into\ntraditional ID-based recommenders has gained attention, challenges such as\nscalability issues, limitations in text-only reliance, and prompt input\nconstraints need to be addressed for effective implementation in practical\nrecommender systems. To address these challenges, we propose a model-agnostic\nframework RLMRec that aims to enhance existing recommenders with LLM-empowered\nrepresentation learning. It proposes a recommendation paradigm that integrates\nrepresentation learning with LLMs to capture intricate semantic aspects of user\nbehaviors and preferences. RLMRec incorporates auxiliary textual signals,\ndevelops a user/item profiling paradigm empowered by LLMs, and aligns the\nsemantic space of LLMs with the representation space of collaborative\nrelational signals through a cross-view alignment framework. This work further\nestablish a theoretical foundation demonstrating that incorporating textual\nsignals through mutual information maximization enhances the quality of\nrepresentations. In our evaluation, we integrate RLMRec with state-of-the-art\nrecommender models, while also analyzing its efficiency and robustness to noise\ndata. Our implementation codes are available at\nhttps://github.com/HKUDS/RLMRec."
                },
                "authors": [
                    {
                        "name": "Xubin Ren"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Lianghao Xia"
                    },
                    {
                        "name": "Lixin Su"
                    },
                    {
                        "name": "Suqi Cheng"
                    },
                    {
                        "name": "Junfeng Wang"
                    },
                    {
                        "name": "Dawei Yin"
                    },
                    {
                        "name": "Chao Huang"
                    }
                ],
                "author_detail": {
                    "name": "Chao Huang"
                },
                "author": "Chao Huang",
                "arxiv_doi": "10.1145/3589334.3645458",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3589334.3645458",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.15950v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.15950v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published as a WWW'24 full paper",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08194v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08194v1",
                "updated": "2024-12-11T08:35:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    35,
                    56,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:35:56Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    35,
                    56,
                    2,
                    346,
                    0
                ],
                "title": "Magneto: Combining Small and Large Language Models for Schema Matching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magneto: Combining Small and Large Language Models for Schema Matching"
                },
                "summary": "Recent advances in language models opened new opportunities to address\ncomplex schema matching tasks. Schema matching approaches have been proposed\nthat demonstrate the usefulness of language models, but they have also\nuncovered important limitations: Small language models (SLMs) require training\ndata (which can be both expensive and challenging to obtain), and large\nlanguage models (LLMs) often incur high computational costs and must deal with\nconstraints imposed by context windows. We present Magneto, a cost-effective\nand accurate solution for schema matching that combines the advantages of SLMs\nand LLMs to address their limitations. By structuring the schema matching\npipeline in two phases, retrieval and reranking, Magneto can use\ncomputationally efficient SLM-based strategies to derive candidate matches\nwhich can then be reranked by LLMs, thus making it possible to reduce runtime\nwithout compromising matching accuracy. We propose a self-supervised approach\nto fine-tune SLMs which uses LLMs to generate syntactically diverse training\ndata, and prompting strategies that are effective for reranking. We also\nintroduce a new benchmark, developed in collaboration with domain experts,\nwhich includes real biomedical datasets and presents new challenges to schema\nmatching methods. Through a detailed experimental evaluation, using both our\nnew and existing benchmarks, we show that Magneto is scalable and attains high\naccuracy for datasets from different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in language models opened new opportunities to address\ncomplex schema matching tasks. Schema matching approaches have been proposed\nthat demonstrate the usefulness of language models, but they have also\nuncovered important limitations: Small language models (SLMs) require training\ndata (which can be both expensive and challenging to obtain), and large\nlanguage models (LLMs) often incur high computational costs and must deal with\nconstraints imposed by context windows. We present Magneto, a cost-effective\nand accurate solution for schema matching that combines the advantages of SLMs\nand LLMs to address their limitations. By structuring the schema matching\npipeline in two phases, retrieval and reranking, Magneto can use\ncomputationally efficient SLM-based strategies to derive candidate matches\nwhich can then be reranked by LLMs, thus making it possible to reduce runtime\nwithout compromising matching accuracy. We propose a self-supervised approach\nto fine-tune SLMs which uses LLMs to generate syntactically diverse training\ndata, and prompting strategies that are effective for reranking. We also\nintroduce a new benchmark, developed in collaboration with domain experts,\nwhich includes real biomedical datasets and presents new challenges to schema\nmatching methods. Through a detailed experimental evaluation, using both our\nnew and existing benchmarks, we show that Magneto is scalable and attains high\naccuracy for datasets from different domains."
                },
                "authors": [
                    {
                        "name": "Yurong Liu"
                    },
                    {
                        "name": "Eduardo Pena"
                    },
                    {
                        "name": "Aecio Santos"
                    },
                    {
                        "name": "Eden Wu"
                    },
                    {
                        "name": "Juliana Freire"
                    }
                ],
                "author_detail": {
                    "name": "Juliana Freire"
                },
                "author": "Juliana Freire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08194v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08194v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01317v2",
                "updated": "2024-12-11T08:35:08Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    35,
                    8,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-02T09:33:28Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    33,
                    28,
                    0,
                    337,
                    0
                ],
                "title": "The Seeds of the FUTURE Sprout from History: Fuzzing for Unveiling\n  Vulnerabilities in Prospective Deep-Learning Libraries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Seeds of the FUTURE Sprout from History: Fuzzing for Unveiling\n  Vulnerabilities in Prospective Deep-Learning Libraries"
                },
                "summary": "The widespread application of large language models (LLMs) underscores the\nimportance of deep learning (DL) technologies that rely on foundational DL\nlibraries such as PyTorch and TensorFlow. Despite their robust features, these\nlibraries face challenges with scalability and adaptation to rapid advancements\nin the LLM community. In response, tech giants like Apple and Huawei are\ndeveloping their own DL libraries to enhance performance, increase scalability,\nand safeguard intellectual property. Ensuring the security of these libraries\nis crucial, with fuzzing being a vital solution. However, existing fuzzing\nframeworks struggle with target flexibility, effectively testing bug-prone API\nsequences, and leveraging the limited available information in new libraries.\nTo address these limitations, we propose FUTURE, the first universal fuzzing\nframework tailored for newly introduced and prospective DL libraries. FUTURE\nleverages historical bug information from existing libraries and fine-tunes\nLLMs for specialized code generation. This strategy helps identify bugs in new\nlibraries and uses insights from these libraries to enhance security in\nexisting ones, creating a cycle from history to future and back. To evaluate\nFUTURE's effectiveness, we conduct comprehensive evaluations on three newly\nintroduced DL libraries. Evaluation results demonstrate that FUTURE\nsignificantly outperforms existing fuzzers in bug detection, success rate of\nbug reproduction, validity rate of code generation, and API coverage. Notably,\nFUTURE has detected 148 bugs across 452 targeted APIs, including 142 previously\nunknown bugs. Among these, 10 have been assigned CVE IDs. Additionally, FUTURE\ndetects 7 bugs in PyTorch, demonstrating its ability to enhance security in\nexisting libraries in reverse.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread application of large language models (LLMs) underscores the\nimportance of deep learning (DL) technologies that rely on foundational DL\nlibraries such as PyTorch and TensorFlow. Despite their robust features, these\nlibraries face challenges with scalability and adaptation to rapid advancements\nin the LLM community. In response, tech giants like Apple and Huawei are\ndeveloping their own DL libraries to enhance performance, increase scalability,\nand safeguard intellectual property. Ensuring the security of these libraries\nis crucial, with fuzzing being a vital solution. However, existing fuzzing\nframeworks struggle with target flexibility, effectively testing bug-prone API\nsequences, and leveraging the limited available information in new libraries.\nTo address these limitations, we propose FUTURE, the first universal fuzzing\nframework tailored for newly introduced and prospective DL libraries. FUTURE\nleverages historical bug information from existing libraries and fine-tunes\nLLMs for specialized code generation. This strategy helps identify bugs in new\nlibraries and uses insights from these libraries to enhance security in\nexisting ones, creating a cycle from history to future and back. To evaluate\nFUTURE's effectiveness, we conduct comprehensive evaluations on three newly\nintroduced DL libraries. Evaluation results demonstrate that FUTURE\nsignificantly outperforms existing fuzzers in bug detection, success rate of\nbug reproduction, validity rate of code generation, and API coverage. Notably,\nFUTURE has detected 148 bugs across 452 targeted APIs, including 142 previously\nunknown bugs. Among these, 10 have been assigned CVE IDs. Additionally, FUTURE\ndetects 7 bugs in PyTorch, demonstrating its ability to enhance security in\nexisting libraries in reverse."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Li"
                    },
                    {
                        "name": "Jingzheng Wu"
                    },
                    {
                        "name": "Xiang Ling"
                    },
                    {
                        "name": "Tianyue Luo"
                    },
                    {
                        "name": "Zhiqing Rui"
                    },
                    {
                        "name": "Yanjun Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yanjun Wu"
                },
                "author": "Yanjun Wu",
                "arxiv_comment": "This paper has been accepted by 47th International Conference on\n  Software Engineering (ICSE 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08185v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08185v1",
                "updated": "2024-12-11T08:24:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    24,
                    15,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:24:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    24,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim\n  Prioritization for Human Fact-checkers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Multidimensional Checkworthiness: Designing AI-assisted Claim\n  Prioritization for Human Fact-checkers"
                },
                "summary": "Given the massive volume of potentially false claims circulating online,\nclaim prioritization is essential in allocating limited human resources\navailable for fact-checking. In this study, we perceive claim prioritization as\nan information retrieval (IR) task: just as multidimensional IR relevance, with\nmany factors influencing which search results a user deems relevant,\ncheckworthiness is also multi-faceted, subjective, and even personal, with many\nfactors influencing how fact-checkers triage and select which claims to check.\nOur study investigates both the multidimensional nature of checkworthiness and\neffective tool support to assist fact-checkers in claim prioritization.\nMethodologically, we pursue Research through Design combined with mixed-method\nevaluation. We develop an AI-assisted claim prioritization prototype as a probe\nto explore how fact-checkers use multidimensional checkworthiness factors in\nclaim prioritization, simultaneously probing fact-checker needs while also\nexploring the design space to meet those needs.\n  Our study with 16 professional fact-checkers investigates: 1) how\nparticipants assessed the relative importance of different checkworthy\ndimensions and apply different priorities in claim selection; 2) how they\ncreated customized GPT-based search filters and the corresponding benefits and\nlimitations; and 3) their overall user experiences with our prototype. Our work\nmakes a conceptual contribution between multidimensional IR relevance and\nfact-checking checkworthiness, with findings demonstrating the value of\ncorresponding tooling support. Specifically, we uncovered a hierarchical\nprioritization strategy fact-checkers implicitly use, revealing an\nunderexplored aspect of their workflow, with actionable design recommendations\nfor improving claim triage across multi-dimensional checkworthiness and\ntailoring this process with LLM integration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Given the massive volume of potentially false claims circulating online,\nclaim prioritization is essential in allocating limited human resources\navailable for fact-checking. In this study, we perceive claim prioritization as\nan information retrieval (IR) task: just as multidimensional IR relevance, with\nmany factors influencing which search results a user deems relevant,\ncheckworthiness is also multi-faceted, subjective, and even personal, with many\nfactors influencing how fact-checkers triage and select which claims to check.\nOur study investigates both the multidimensional nature of checkworthiness and\neffective tool support to assist fact-checkers in claim prioritization.\nMethodologically, we pursue Research through Design combined with mixed-method\nevaluation. We develop an AI-assisted claim prioritization prototype as a probe\nto explore how fact-checkers use multidimensional checkworthiness factors in\nclaim prioritization, simultaneously probing fact-checker needs while also\nexploring the design space to meet those needs.\n  Our study with 16 professional fact-checkers investigates: 1) how\nparticipants assessed the relative importance of different checkworthy\ndimensions and apply different priorities in claim selection; 2) how they\ncreated customized GPT-based search filters and the corresponding benefits and\nlimitations; and 3) their overall user experiences with our prototype. Our work\nmakes a conceptual contribution between multidimensional IR relevance and\nfact-checking checkworthiness, with findings demonstrating the value of\ncorresponding tooling support. Specifically, we uncovered a hierarchical\nprioritization strategy fact-checkers implicitly use, revealing an\nunderexplored aspect of their workflow, with actionable design recommendations\nfor improving claim triage across multi-dimensional checkworthiness and\ntailoring this process with LLM integration."
                },
                "authors": [
                    {
                        "name": "Houjiang Liu"
                    },
                    {
                        "name": "Jacek Gwizdka"
                    },
                    {
                        "name": "Matthew Lease"
                    }
                ],
                "author_detail": {
                    "name": "Matthew Lease"
                },
                "author": "Matthew Lease",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08185v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08185v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08179v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08179v1",
                "updated": "2024-12-11T08:09:42Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    9,
                    42,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:09:42Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    9,
                    42,
                    2,
                    346,
                    0
                ],
                "title": "Auto-Generating Earnings Report Analysis via a Financial-Augmented LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-Generating Earnings Report Analysis via a Financial-Augmented LLM"
                },
                "summary": "Financial analysis heavily relies on the evaluation of earnings reports to\ngain insights into company performance. Traditional generation of these reports\nrequires extensive financial expertise and is time-consuming. With the\nimpressive progress in Large Language Models (LLMs), a wide variety of\nfinancially focused LLMs has emerged, addressing tasks like sentiment analysis\nand entity recognition in the financial domain. This paper presents a novel\nchallenge: developing an LLM specifically for automating the generation of\nearnings reports analysis. Our methodology involves an in-depth analysis of\nexisting earnings reports followed by a unique approach to fine-tune an LLM for\nthis purpose. This approach combines retrieval augmentation and the generation\nof instruction-based data, specifically tailored for the financial sector, to\nenhance the LLM's performance. With extensive financial documents, we construct\nfinancial instruction data, enabling the refined adaptation of our LLM to\nfinancial contexts. Preliminary results indicate that our augmented LLM\noutperforms general open-source models and rivals commercial counterparts like\nGPT-3.5 in financial applications. Our research paves the way for streamlined\nand insightful automation in financial report generation, marking a significant\nstride in the field of financial analysis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial analysis heavily relies on the evaluation of earnings reports to\ngain insights into company performance. Traditional generation of these reports\nrequires extensive financial expertise and is time-consuming. With the\nimpressive progress in Large Language Models (LLMs), a wide variety of\nfinancially focused LLMs has emerged, addressing tasks like sentiment analysis\nand entity recognition in the financial domain. This paper presents a novel\nchallenge: developing an LLM specifically for automating the generation of\nearnings reports analysis. Our methodology involves an in-depth analysis of\nexisting earnings reports followed by a unique approach to fine-tune an LLM for\nthis purpose. This approach combines retrieval augmentation and the generation\nof instruction-based data, specifically tailored for the financial sector, to\nenhance the LLM's performance. With extensive financial documents, we construct\nfinancial instruction data, enabling the refined adaptation of our LLM to\nfinancial contexts. Preliminary results indicate that our augmented LLM\noutperforms general open-source models and rivals commercial counterparts like\nGPT-3.5 in financial applications. Our research paves the way for streamlined\nand insightful automation in financial report generation, marking a significant\nstride in the field of financial analysis."
                },
                "authors": [
                    {
                        "name": "Van-Duc Le"
                    }
                ],
                "author_detail": {
                    "name": "Van-Duc Le"
                },
                "author": "Van-Duc Le",
                "arxiv_comment": "8 pages, 1 figure, 4 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08179v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08179v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08176v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08176v1",
                "updated": "2024-12-11T08:07:12Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:07:12Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    7,
                    12,
                    2,
                    346,
                    0
                ],
                "title": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TextRefiner: Internal Visual Feature as Efficient Refiner for\n  Vision-Language Models Prompt Tuning"
                },
                "summary": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the efficiency of prompt learning in transferring vision-language\nmodels (VLMs) to downstream tasks, existing methods mainly learn the prompts in\na coarse-grained manner where the learned prompt vectors are shared across all\ncategories. Consequently, the tailored prompts often fail to discern\nclass-specific visual concepts, thereby hindering the transferred performance\nfor classes that share similar or complex visual attributes. Recent advances\nmitigate this challenge by leveraging external knowledge from Large Language\nModels (LLMs) to furnish class descriptions, yet incurring notable inference\ncosts. In this paper, we introduce TextRefiner, a plug-and-play method to\nrefine the text prompts of existing methods by leveraging the internal\nknowledge of VLMs. Particularly, TextRefiner builds a novel local cache module\nto encapsulate fine-grained visual concepts derivedfrom local tokens within the\nimage branch. By aggregating and aligning the cached visual descriptions with\nthe original output of the text branch, TextRefiner can efficiently refine and\nenrich the learned prompts from existing methods without relying on any\nexternal expertise. For example, it improves the performance of CoOp from 71.66\n% to 76.94 % on 11 benchmarks, surpassing CoCoOp which introduces instance-wise\nfeatures for text prompts. Equipped with TextRefiner, PromptKD achieves\nstate-of-the-art performance and is efficient in inference. Our code is relesed\nat https://github.com/xjjxmu/TextRefiner"
                },
                "authors": [
                    {
                        "name": "Jingjing Xie"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Jun Peng"
                    },
                    {
                        "name": "Zhaohong Huang"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Accepted by AAAI2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08176v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08176v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08174v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08174v1",
                "updated": "2024-12-11T08:03:35Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    3,
                    35,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T08:03:35Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    8,
                    3,
                    35,
                    2,
                    346,
                    0
                ],
                "title": "Can Graph Neural Networks Learn Language with Extremely Weak Text\n  Supervision?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can Graph Neural Networks Learn Language with Extremely Weak Text\n  Supervision?"
                },
                "summary": "While great success has been achieved in building vision models with\nContrastive Language-Image Pre-training (CLIP) over Internet-scale image-text\npairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is\nchallenging because of three fundamental issues: the scarcity of labeled data\nand text supervision, different levels of downstream tasks, and the conceptual\ngaps between domains. In this work, to address these issues, we leverage\nmulti-modal prompt learning to effectively adapt pre-trained GNN to downstream\ntasks and data, given only a few semantically labeled samples, each with\nextremely weak text supervision. Our new paradigm embeds the graphs directly in\nthe same space as the Large Language Models (LLMs) by learning both graph\nprompts and text prompts simultaneously. To accomplish this, we improve\nstate-of-the-art graph prompt method, and then propose the first graph-language\nmulti-modal prompt learning approach for exploiting the knowledge in\npre-trained models. Notably, due to the insufficient supervision for\nfine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen,\nso the learnable parameters are much fewer than fine-tuning any pre-trained\nmodel. Through extensive experiments on real-world datasets, we demonstrate the\nsuperior performance of our paradigm in few-shot, multi-task-level, and\ncross-domain settings. Moreover, we build the first CLIP-style zero-shot\nclassification prototype that can generalize GNNs to unseen classes with\nextremely weak text supervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While great success has been achieved in building vision models with\nContrastive Language-Image Pre-training (CLIP) over Internet-scale image-text\npairs, building transferable Graph Neural Networks (GNNs) with CLIP pipeline is\nchallenging because of three fundamental issues: the scarcity of labeled data\nand text supervision, different levels of downstream tasks, and the conceptual\ngaps between domains. In this work, to address these issues, we leverage\nmulti-modal prompt learning to effectively adapt pre-trained GNN to downstream\ntasks and data, given only a few semantically labeled samples, each with\nextremely weak text supervision. Our new paradigm embeds the graphs directly in\nthe same space as the Large Language Models (LLMs) by learning both graph\nprompts and text prompts simultaneously. To accomplish this, we improve\nstate-of-the-art graph prompt method, and then propose the first graph-language\nmulti-modal prompt learning approach for exploiting the knowledge in\npre-trained models. Notably, due to the insufficient supervision for\nfine-tuning, in our paradigm, the pre-trained GNN and the LLM are kept frozen,\nso the learnable parameters are much fewer than fine-tuning any pre-trained\nmodel. Through extensive experiments on real-world datasets, we demonstrate the\nsuperior performance of our paradigm in few-shot, multi-task-level, and\ncross-domain settings. Moreover, we build the first CLIP-style zero-shot\nclassification prototype that can generalize GNNs to unseen classes with\nextremely weak text supervision."
                },
                "authors": [
                    {
                        "name": "Zihao Li"
                    },
                    {
                        "name": "Lecheng Zheng"
                    },
                    {
                        "name": "Bowen Jin"
                    },
                    {
                        "name": "Dongqi Fu"
                    },
                    {
                        "name": "Baoyu Jing"
                    },
                    {
                        "name": "Yikun Ban"
                    },
                    {
                        "name": "Jingrui He"
                    },
                    {
                        "name": "Jiawei Han"
                    }
                ],
                "author_detail": {
                    "name": "Jiawei Han"
                },
                "author": "Jiawei Han",
                "arxiv_comment": "Preprint, 26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08174v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08174v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06394v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06394v2",
                "updated": "2024-12-11T07:52:06Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    7,
                    52,
                    6,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-09T11:22:59Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    11,
                    22,
                    59,
                    0,
                    344,
                    0
                ],
                "title": "GameArena: Evaluating LLM Reasoning through Live Computer Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GameArena: Evaluating LLM Reasoning through Live Computer Games"
                },
                "summary": "Evaluating the reasoning abilities of large language models (LLMs) is\nchallenging. Existing benchmarks often depend on static datasets, which are\nvulnerable to data contamination and may get saturated over time, or on binary\nlive human feedback that conflates reasoning with other abilities. As the most\nprominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in\nreal-world settings, but lacks the granularity in assessing specific reasoning\ncapabilities. We introduce GameArena, a dynamic benchmark designed to evaluate\nLLM reasoning capabilities through interactive gameplay with humans. GameArena\nconsists of three games designed to test specific reasoning capabilities (e.g.,\ndeductive and inductive reasoning), while keeping participants entertained and\nengaged. We analyze the gaming data retrospectively to uncover the underlying\nreasoning processes of LLMs and measure their fine-grained reasoning\ncapabilities. We collect over 2000 game sessions and provide detailed\nassessments of various reasoning capabilities for five state-of-the-art LLMs.\nOur user study with 100 participants suggests that GameArena improves user\nengagement compared to Chatbot Arena. For the first time, GameArena enables the\ncollection of step-by-step LLM reasoning data in the wild.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating the reasoning abilities of large language models (LLMs) is\nchallenging. Existing benchmarks often depend on static datasets, which are\nvulnerable to data contamination and may get saturated over time, or on binary\nlive human feedback that conflates reasoning with other abilities. As the most\nprominent dynamic benchmark, Chatbot Arena evaluates open-ended questions in\nreal-world settings, but lacks the granularity in assessing specific reasoning\ncapabilities. We introduce GameArena, a dynamic benchmark designed to evaluate\nLLM reasoning capabilities through interactive gameplay with humans. GameArena\nconsists of three games designed to test specific reasoning capabilities (e.g.,\ndeductive and inductive reasoning), while keeping participants entertained and\nengaged. We analyze the gaming data retrospectively to uncover the underlying\nreasoning processes of LLMs and measure their fine-grained reasoning\ncapabilities. We collect over 2000 game sessions and provide detailed\nassessments of various reasoning capabilities for five state-of-the-art LLMs.\nOur user study with 100 participants suggests that GameArena improves user\nengagement compared to Chatbot Arena. For the first time, GameArena enables the\ncollection of step-by-step LLM reasoning data in the wild."
                },
                "authors": [
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Qiyu Li"
                    },
                    {
                        "name": "Anze Xie"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Haojian Jin"
                    },
                    {
                        "name": "Hao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Zhang"
                },
                "author": "Hao Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06394v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06394v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19453v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19453v4",
                "updated": "2024-12-11T07:41:18Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    7,
                    41,
                    18,
                    2,
                    346,
                    0
                ],
                "published": "2024-10-25T10:28:59Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    10,
                    28,
                    59,
                    4,
                    299,
                    0
                ],
                "title": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShifCon: Enhancing Non-Dominant Language Capabilities with a Shift-based\n  Contrastive Framework"
                },
                "summary": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although fine-tuning Large Language Models (LLMs) with multilingual data can\nrapidly enhance the multilingual capabilities of LLMs, they still exhibit a\nperformance gap between the dominant language (e.g., English) and non-dominant\nones due to the imbalance of training data across languages. To further enhance\nthe performance of non-dominant languages, we propose ShifCon, a Shift-based\nContrastive framework that aligns the internal forward process of other\nlanguages toward that of the dominant one. Specifically, it shifts the\nrepresentations of non-dominant languages into the dominant language subspace,\nallowing them to access relatively rich information encoded in the model\nparameters. The enriched representations are then shifted back into their\noriginal language subspace before generation. Moreover, we introduce a subspace\ndistance metric to pinpoint the optimal layer area for shifting representations\nand employ multilingual contrastive learning to further enhance the alignment\nof representations within this area. Experiments demonstrate that our ShifCon\nframework significantly enhances the performance of non-dominant languages,\nparticularly for low-resource ones. Further analysis offers extra insights to\nverify the effectiveness of ShifCon and propel future research"
                },
                "authors": [
                    {
                        "name": "Hengyuan Zhang"
                    },
                    {
                        "name": "Chenming Shang"
                    },
                    {
                        "name": "Sizhe Wang"
                    },
                    {
                        "name": "Dongdong Zhang"
                    },
                    {
                        "name": "Feng Yao"
                    },
                    {
                        "name": "Renliang Sun"
                    },
                    {
                        "name": "Yiyao Yu"
                    },
                    {
                        "name": "Yujiu Yang"
                    },
                    {
                        "name": "Furu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Furu Wei"
                },
                "author": "Furu Wei",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.19453v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19453v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.00218v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.00218v3",
                "updated": "2024-12-11T07:18:10Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    7,
                    18,
                    10,
                    2,
                    346,
                    0
                ],
                "published": "2024-11-29T19:25:00Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    19,
                    25,
                    0,
                    4,
                    334,
                    0
                ],
                "title": "NushuRescue: Revitalization of the Endangered Nushu Language with AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NushuRescue: Revitalization of the Endangered Nushu Language with AI"
                },
                "summary": "The preservation and revitalization of endangered and extinct languages is a\nmeaningful endeavor, conserving cultural heritage while enriching fields like\nlinguistics and anthropology. However, these languages are typically\nlow-resource, making their reconstruction labor-intensive and costly. This\nchallenge is exemplified by Nushu, a rare script historically used by Yao women\nin China for self-expression within a patriarchal society. To address this\nchallenge, we introduce NushuRescue, an AI-driven framework designed to train\nlarge language models (LLMs) on endangered languages with minimal data.\nNushuRescue automates evaluation and expands target corpora to accelerate\nlinguistic revitalization. As a foundational component, we developed NCGold, a\n500-sentence Nushu-Chinese parallel corpus, the first publicly available\ndataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to Nushu\nand only 35 short examples from NCGold, NushuRescue achieved 48.69% translation\naccuracy on 50 withheld sentences and generated NCSilver, a set of 98 newly\ntranslated modern Chinese sentences of varying lengths. A sample of both NCGold\nand NCSilver is included in the Supplementary Materials. Additionally, we\ndeveloped FastText-based and Seq2Seq models to further support research on\nNushu. NushuRescue provides a versatile and scalable tool for the\nrevitalization of endangered languages, minimizing the need for extensive human\ninput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The preservation and revitalization of endangered and extinct languages is a\nmeaningful endeavor, conserving cultural heritage while enriching fields like\nlinguistics and anthropology. However, these languages are typically\nlow-resource, making their reconstruction labor-intensive and costly. This\nchallenge is exemplified by Nushu, a rare script historically used by Yao women\nin China for self-expression within a patriarchal society. To address this\nchallenge, we introduce NushuRescue, an AI-driven framework designed to train\nlarge language models (LLMs) on endangered languages with minimal data.\nNushuRescue automates evaluation and expands target corpora to accelerate\nlinguistic revitalization. As a foundational component, we developed NCGold, a\n500-sentence Nushu-Chinese parallel corpus, the first publicly available\ndataset of its kind. Leveraging GPT-4-Turbo, with no prior exposure to Nushu\nand only 35 short examples from NCGold, NushuRescue achieved 48.69% translation\naccuracy on 50 withheld sentences and generated NCSilver, a set of 98 newly\ntranslated modern Chinese sentences of varying lengths. A sample of both NCGold\nand NCSilver is included in the Supplementary Materials. Additionally, we\ndeveloped FastText-based and Seq2Seq models to further support research on\nNushu. NushuRescue provides a versatile and scalable tool for the\nrevitalization of endangered languages, minimizing the need for extensive human\ninput."
                },
                "authors": [
                    {
                        "name": "Ivory Yang"
                    },
                    {
                        "name": "Weicheng Ma"
                    },
                    {
                        "name": "Soroush Vosoughi"
                    }
                ],
                "author_detail": {
                    "name": "Soroush Vosoughi"
                },
                "author": "Soroush Vosoughi",
                "arxiv_comment": "Accepted to COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.00218v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.00218v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.15827v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.15827v2",
                "updated": "2024-12-11T07:05:21Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    7,
                    5,
                    21,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-24T07:40:33Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    7,
                    40,
                    33,
                    1,
                    268,
                    0
                ],
                "title": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to\n  Model Interpretability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unveiling Language Competence Neurons: A Psycholinguistic Approach to\n  Model Interpretability"
                },
                "summary": "As large language models (LLMs) advance in their linguistic capacity,\nunderstanding how they capture aspects of language competence remains a\nsignificant challenge. This study therefore employs psycholinguistic paradigms\nin English, which are well-suited for probing deeper cognitive aspects of\nlanguage processing, to explore neuron-level representations in language model\nacross three tasks: sound-shape association, sound-gender association, and\nimplicit causality. Our findings indicate that while GPT-2-XL struggles with\nthe sound-shape task, it demonstrates human-like abilities in both sound-gender\nassociation and implicit causality. Targeted neuron ablation and activation\nmanipulation reveal a crucial relationship: When GPT-2-XL displays a linguistic\nability, specific neurons correspond to that competence; conversely, the\nabsence of such an ability indicates a lack of specialized neurons. This study\nis the first to utilize psycholinguistic experiments to investigate deep\nlanguage competence at the neuron level, providing a new level of granularity\nin model interpretability and insights into the internal mechanisms driving\nlanguage ability in the transformer-based LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) advance in their linguistic capacity,\nunderstanding how they capture aspects of language competence remains a\nsignificant challenge. This study therefore employs psycholinguistic paradigms\nin English, which are well-suited for probing deeper cognitive aspects of\nlanguage processing, to explore neuron-level representations in language model\nacross three tasks: sound-shape association, sound-gender association, and\nimplicit causality. Our findings indicate that while GPT-2-XL struggles with\nthe sound-shape task, it demonstrates human-like abilities in both sound-gender\nassociation and implicit causality. Targeted neuron ablation and activation\nmanipulation reveal a crucial relationship: When GPT-2-XL displays a linguistic\nability, specific neurons correspond to that competence; conversely, the\nabsence of such an ability indicates a lack of specialized neurons. This study\nis the first to utilize psycholinguistic experiments to investigate deep\nlanguage competence at the neuron level, providing a new level of granularity\nin model interpretability and insights into the internal mechanisms driving\nlanguage ability in the transformer-based LLM."
                },
                "authors": [
                    {
                        "name": "Xufeng Duan"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Bei Xiao"
                    },
                    {
                        "name": "Zhenguang G. Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguang G. Cai"
                },
                "author": "Zhenguang G. Cai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.15827v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.15827v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17972v2",
                "updated": "2024-12-11T06:39:43Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    6,
                    39,
                    43,
                    2,
                    346,
                    0
                ],
                "published": "2024-06-25T23:07:18Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    23,
                    7,
                    18,
                    1,
                    177,
                    0
                ],
                "title": "LABOR-LLM: Language-Based Occupational Representations with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LABOR-LLM: Language-Based Occupational Representations with Large\n  Language Models"
                },
                "summary": "Vafa et al. (2024) introduced a transformer-based econometric model, CAREER,\nthat predicts a worker's next job as a function of career history (an\n\"occupation model\"). CAREER was initially estimated (\"pre-trained\") using a\nlarge, unrepresentative resume dataset, which served as a \"foundation model,\"\nand parameter estimation was continued (\"fine-tuned\") using data from a\nrepresentative survey. CAREER had better predictive performance than\nbenchmarks. This paper considers an alternative where the resume-based\nfoundation model is replaced by a large language model (LLM). We convert\ntabular data from the survey into text files that resemble resumes and\nfine-tune the LLMs using these text files with the objective to predict the\nnext token (word). The resulting fine-tuned LLM is used as an input to an\noccupation model. Its predictive performance surpasses all prior models. We\ndemonstrate the value of fine-tuning and further show that by adding more\ncareer data from a different population, fine-tuning smaller LLMs surpasses the\nperformance of fine-tuning larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vafa et al. (2024) introduced a transformer-based econometric model, CAREER,\nthat predicts a worker's next job as a function of career history (an\n\"occupation model\"). CAREER was initially estimated (\"pre-trained\") using a\nlarge, unrepresentative resume dataset, which served as a \"foundation model,\"\nand parameter estimation was continued (\"fine-tuned\") using data from a\nrepresentative survey. CAREER had better predictive performance than\nbenchmarks. This paper considers an alternative where the resume-based\nfoundation model is replaced by a large language model (LLM). We convert\ntabular data from the survey into text files that resemble resumes and\nfine-tune the LLMs using these text files with the objective to predict the\nnext token (word). The resulting fine-tuned LLM is used as an input to an\noccupation model. Its predictive performance surpasses all prior models. We\ndemonstrate the value of fine-tuning and further show that by adding more\ncareer data from a different population, fine-tuning smaller LLMs surpasses the\nperformance of fine-tuning larger models."
                },
                "authors": [
                    {
                        "name": "Susan Athey"
                    },
                    {
                        "name": "Herman Brunborg"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Ayush Kanodia"
                    },
                    {
                        "name": "Keyon Vafa"
                    }
                ],
                "author_detail": {
                    "name": "Keyon Vafa"
                },
                "author": "Keyon Vafa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07322v2",
                "updated": "2024-12-11T06:33:55Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    6,
                    33,
                    55,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-10T09:10:11Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    9,
                    10,
                    11,
                    1,
                    345,
                    0
                ],
                "title": "ConceptSearch: Towards Efficient Program Search Using LLMs for\n  Abstraction and Reasoning Corpus (ARC)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConceptSearch: Towards Efficient Program Search Using LLMs for\n  Abstraction and Reasoning Corpus (ARC)"
                },
                "summary": "The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to\nartificial intelligence, demanding broad generalization and few-shot learning\ncapabilities that remain elusive for current deep learning methods, including\nlarge language models (LLMs). While LLMs excel in program synthesis, their\ndirect application to ARC yields limited success. To address this, we introduce\nConceptSearch, a novel function-search algorithm that leverages LLMs for\nprogram generation and employs a concept-based scoring method to guide the\nsearch efficiently. Unlike simplistic pixel-based metrics like Hamming\ndistance, ConceptSearch evaluates programs on their ability to capture the\nunderlying transformation concept reflected in the input-output examples. We\nexplore three scoring functions: Hamming distance, a CNN-based scoring\nfunction, and an LLM-based natural language scoring function. Experimental\nresults demonstrate the effectiveness of ConceptSearch, achieving a significant\nperformance improvement over direct prompting with GPT-4. Moreover, our novel\nconcept-based scoring exhibits up to 30% greater efficiency compared to Hamming\ndistance, measured in terms of the number of iterations required to reach the\ncorrect solution. These findings highlight the potential of LLM-driven program\nsearch when integrated with concept-based guidance for tackling challenging\ngeneralization problems like ARC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Abstraction and Reasoning Corpus (ARC) poses a significant challenge to\nartificial intelligence, demanding broad generalization and few-shot learning\ncapabilities that remain elusive for current deep learning methods, including\nlarge language models (LLMs). While LLMs excel in program synthesis, their\ndirect application to ARC yields limited success. To address this, we introduce\nConceptSearch, a novel function-search algorithm that leverages LLMs for\nprogram generation and employs a concept-based scoring method to guide the\nsearch efficiently. Unlike simplistic pixel-based metrics like Hamming\ndistance, ConceptSearch evaluates programs on their ability to capture the\nunderlying transformation concept reflected in the input-output examples. We\nexplore three scoring functions: Hamming distance, a CNN-based scoring\nfunction, and an LLM-based natural language scoring function. Experimental\nresults demonstrate the effectiveness of ConceptSearch, achieving a significant\nperformance improvement over direct prompting with GPT-4. Moreover, our novel\nconcept-based scoring exhibits up to 30% greater efficiency compared to Hamming\ndistance, measured in terms of the number of iterations required to reach the\ncorrect solution. These findings highlight the potential of LLM-driven program\nsearch when integrated with concept-based guidance for tackling challenging\ngeneralization problems like ARC."
                },
                "authors": [
                    {
                        "name": "Kartik Singhal"
                    },
                    {
                        "name": "Gautam Shroff"
                    }
                ],
                "author_detail": {
                    "name": "Gautam Shroff"
                },
                "author": "Gautam Shroff",
                "arxiv_comment": "Pre-print of paper accepted at AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.15867v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.15867v2",
                "updated": "2024-12-11T06:21:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    6,
                    21,
                    28,
                    2,
                    346,
                    0
                ],
                "published": "2024-08-28T15:35:05Z",
                "published_parsed": [
                    2024,
                    8,
                    28,
                    15,
                    35,
                    5,
                    2,
                    241,
                    0
                ],
                "title": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Practical Challenges for Reliable RIS Deployment in Heterogeneous\n  Multi-Operator Multi-Band Networks"
                },
                "summary": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconfigurable intelligent surfaces (RISs) have been introduced as arrays of\nnearly passive elements with software-tunable electromagnetic properties to\ndynamically manipulate the reflection/transmission of radio signals. Research\nworks in this area are focused on two applications, namely {\\it user-assist}\nRIS aiming at tuning the RIS to enhance the quality-of-service (QoS) of target\nusers, and the {\\it malicious} RIS aiming for an attacker to degrade the QoS at\nvictim receivers through generating {\\it intended} destructive interference.\nWhile both user-assist and malicious RIS applications have been explored\nextensively, the impact of RIS deployments on imposing {\\it unintended}\ninterference on various wireless user-equipments (EUs) remains underexplored.\nThis paper investigates the challenges of integrating RISs into multi-carrier,\nmulti-user, and multi-operator networks. We discuss how RIS deployments\nintended to benefit specific users can negatively impact other users served at\nvarious carrier frequencies through different network operators. While not an\nideal solution, we discuss how ultra-narrowband metasurfaces can be\nincorporated into the manufacturing of RISs to mitigate some challenges of RIS\ndeployment in wireless networks. We also present a simulation scenario to\nilluminate some practical challenges associated with the deployment of RISs in\nshared public environments."
                },
                "authors": [
                    {
                        "name": "Mehdi Monemi"
                    },
                    {
                        "name": "Mehdi Rasti"
                    },
                    {
                        "name": "Arthur S. de Sena"
                    },
                    {
                        "name": "Mohammad Amir Fallah"
                    },
                    {
                        "name": "Matti Latva-Aho"
                    },
                    {
                        "name": "Marco Di Renzo"
                    }
                ],
                "author_detail": {
                    "name": "Marco Di Renzo"
                },
                "author": "Marco Di Renzo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.15867v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.15867v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00958v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00958v5",
                "updated": "2024-12-11T06:01:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    6,
                    1,
                    38,
                    2,
                    346,
                    0
                ],
                "published": "2024-07-01T04:29:35Z",
                "published_parsed": [
                    2024,
                    7,
                    1,
                    4,
                    29,
                    35,
                    0,
                    183,
                    0
                ],
                "title": "Dynamic Universal Approximation Theory: The Basic Theory for\n  Transformer-based Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Universal Approximation Theory: The Basic Theory for\n  Transformer-based Large Language Models"
                },
                "summary": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language models have emerged as a critical area of focus in artificial\nintelligence, particularly with the introduction of groundbreaking innovations\nlike ChatGPT. Large-scale Transformer networks have quickly become the leading\napproach for advancing natural language processing algorithms. Built on the\nTransformer architecture, these models enable interactions that closely mimic\nhuman communication and, equipped with extensive knowledge, can even assist in\nguiding human tasks. Despite their impressive capabilities and growing\ncomplexity, a key question remains-the theoretical foundations of large\nlanguage models (LLMs). What makes Transformer so effective for powering\nintelligent language applications, such as translation and coding? What\nunderlies LLMs' ability for In-Context Learning (ICL)? How does the LoRA scheme\nenhance the fine-tuning of LLMs? And what supports the practicality of pruning\nLLMs? To address these critical questions and explore the technological\nstrategies within LLMs, we leverage the Universal Approximation Theory (UAT) to\noffer a theoretical backdrop, shedding light on the mechanisms that underpin\nthese advancements."
                },
                "authors": [
                    {
                        "name": "Wei Wang"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00958v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00958v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08109v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08109v1",
                "updated": "2024-12-11T05:31:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    31,
                    39,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T05:31:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    31,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation\n  Beyond the Familiar",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unseen Horizons: Unveiling the Real Capability of LLM Code Generation\n  Beyond the Familiar"
                },
                "summary": "Recently, large language models (LLMs) have shown strong potential in code\ngeneration tasks. However, there are still gaps before they can be fully\napplied in actual software development processes. Accurately assessing the code\ngeneration capabilities of large language models has become an important basis\nfor evaluating and improving the models. Some existing works have constructed\ndatasets to evaluate the capabilities of these models. However, the current\nevaluation process may encounter the illusion of \"Specialist in Familiarity\",\nprimarily due to three gaps: the exposure of target code, case timeliness, and\ndependency availability. The fundamental reason for these gaps is that the code\nin current datasets may have been extensively exposed and exercised during the\ntraining phase, and due to the continuous training and development of LLM,\ntheir timeliness has been severely compromised. The key to solve the problem is\nto, as much as possible, evaluate the LLMs using code that they have not\nencountered before. Thus, the fundamental idea in this paper is to draw on the\nconcept of code obfuscation, changing code at different levels while ensuring\nthe functionality and output. To this end, we build a code-obfuscation based\nbenchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world\nprojects, including function description and code. Then we use three-level\nstrategy (symbol, structure and semantic) to obfuscate descriptions, code and\ncontext dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the\neffectiveness of different obfuscation strategy. We use official test suites of\nthese projects to evaluate the generated code. The results show that after\nobfuscation, the average decrease ratio of test pass rate can up to 62.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, large language models (LLMs) have shown strong potential in code\ngeneration tasks. However, there are still gaps before they can be fully\napplied in actual software development processes. Accurately assessing the code\ngeneration capabilities of large language models has become an important basis\nfor evaluating and improving the models. Some existing works have constructed\ndatasets to evaluate the capabilities of these models. However, the current\nevaluation process may encounter the illusion of \"Specialist in Familiarity\",\nprimarily due to three gaps: the exposure of target code, case timeliness, and\ndependency availability. The fundamental reason for these gaps is that the code\nin current datasets may have been extensively exposed and exercised during the\ntraining phase, and due to the continuous training and development of LLM,\ntheir timeliness has been severely compromised. The key to solve the problem is\nto, as much as possible, evaluate the LLMs using code that they have not\nencountered before. Thus, the fundamental idea in this paper is to draw on the\nconcept of code obfuscation, changing code at different levels while ensuring\nthe functionality and output. To this end, we build a code-obfuscation based\nbenchmark OBFUSEVAL. We first collect 1,354 raw cases from five real-world\nprojects, including function description and code. Then we use three-level\nstrategy (symbol, structure and semantic) to obfuscate descriptions, code and\ncontext dependencies. We evaluate four LLMs on OBFU- SEVAL and compared the\neffectiveness of different obfuscation strategy. We use official test suites of\nthese projects to evaluate the generated code. The results show that after\nobfuscation, the average decrease ratio of test pass rate can up to 62.5%."
                },
                "authors": [
                    {
                        "name": "Yuanliang Zhang"
                    },
                    {
                        "name": "Yifan Xie"
                    },
                    {
                        "name": "Shanshan Li"
                    },
                    {
                        "name": "Ke Liu"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Zhouyang Jia"
                    },
                    {
                        "name": "Xiangbing Huang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Chaopeng Luo"
                    },
                    {
                        "name": "Zhizheng Zheng"
                    },
                    {
                        "name": "Rulin Xu"
                    },
                    {
                        "name": "Yitong Liu"
                    },
                    {
                        "name": "Si Zheng"
                    },
                    {
                        "name": "Xiangke Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangke Liao"
                },
                "author": "Xiangke Liao",
                "arxiv_comment": "Large Language Model,Code Generation Capability,Code Dataset",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08109v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08109v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08108v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08108v1",
                "updated": "2024-12-11T05:23:34Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    23,
                    34,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T05:23:34Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    23,
                    34,
                    2,
                    346,
                    0
                ],
                "title": "Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language\n  Models Across Both Images and Text with a Single Perturbation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly-Universal Adversarial Perturbations: Deceiving Vision-Language\n  Models Across Both Images and Text with a Single Perturbation"
                },
                "summary": "Large Vision-Language Models (VLMs) have demonstrated remarkable performance\nacross multimodal tasks by integrating vision encoders with large language\nmodels (LLMs). However, these models remain vulnerable to adversarial attacks.\nAmong such attacks, Universal Adversarial Perturbations (UAPs) are especially\npowerful, as a single optimized perturbation can mislead the model across\nvarious input images. In this work, we introduce a novel UAP specifically\ndesigned for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),\ncapable of universally deceiving VLMs across both image and text inputs. To\nsuccessfully disrupt the vision encoder's fundamental process, we analyze the\ncore components of the attention mechanism. After identifying value vectors in\nthe middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a\nlabel-free manner with a frozen model. Despite being developed as a black-box\nto the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently\noutperforming baseline methods across vision-language tasks. Extensive ablation\nstudies and analyses further demonstrate the robustness of Doubly-UAP and\nprovide insights into how it influences internal attention mechanisms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language Models (VLMs) have demonstrated remarkable performance\nacross multimodal tasks by integrating vision encoders with large language\nmodels (LLMs). However, these models remain vulnerable to adversarial attacks.\nAmong such attacks, Universal Adversarial Perturbations (UAPs) are especially\npowerful, as a single optimized perturbation can mislead the model across\nvarious input images. In this work, we introduce a novel UAP specifically\ndesigned for VLMs: the Doubly-Universal Adversarial Perturbation (Doubly-UAP),\ncapable of universally deceiving VLMs across both image and text inputs. To\nsuccessfully disrupt the vision encoder's fundamental process, we analyze the\ncore components of the attention mechanism. After identifying value vectors in\nthe middle-to-late layers as the most vulnerable, we optimize Doubly-UAP in a\nlabel-free manner with a frozen model. Despite being developed as a black-box\nto the LLM, Doubly-UAP achieves high attack success rates on VLMs, consistently\noutperforming baseline methods across vision-language tasks. Extensive ablation\nstudies and analyses further demonstrate the robustness of Doubly-UAP and\nprovide insights into how it influences internal attention mechanisms."
                },
                "authors": [
                    {
                        "name": "Hee-Seon Kim"
                    },
                    {
                        "name": "Minbeom Kim"
                    },
                    {
                        "name": "Changick Kim"
                    }
                ],
                "author_detail": {
                    "name": "Changick Kim"
                },
                "author": "Changick Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08108v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08108v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.00385v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.00385v2",
                "updated": "2024-12-11T05:14:28Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    14,
                    28,
                    2,
                    346,
                    0
                ],
                "published": "2023-09-30T14:04:22Z",
                "published_parsed": [
                    2023,
                    9,
                    30,
                    14,
                    4,
                    22,
                    5,
                    273,
                    0
                ],
                "title": "Dynamic Demonstrations Controller for In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Demonstrations Controller for In-Context Learning"
                },
                "summary": "In-context learning (ICL) is a new paradigm for natural language processing\n(NLP), where a large language model (LLM) observes a small number of\ndemonstrations and a test instance as its input, and directly makes predictions\nwithout updating model parameters. Previous studies have revealed that ICL is\nsensitive to the selection and the ordering of demonstrations. However, there\nare few studies regarding the impact of the demonstration number on the ICL\nperformance within a limited input length of LLM, because it is commonly\nbelieved that the number of demonstrations is positively correlated with model\nperformance. In this paper, we found this conclusion does not always hold true.\nThrough pilot experiments, we discover that increasing the number of\ndemonstrations does not necessarily lead to improved performance. Building upon\nthis insight, we propose a Dynamic Demonstrations Controller (D$^2$Controller),\nwhich can improve the ICL performance by adjusting the number of demonstrations\ndynamically. The experimental results show that D$^2$Controller yields a 4.6%\nrelative improvement on ten different sizes of LLMs across ten datasets.\nMoreover, we also extend our method to previous ICL models and achieve\ncompetitive results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-context learning (ICL) is a new paradigm for natural language processing\n(NLP), where a large language model (LLM) observes a small number of\ndemonstrations and a test instance as its input, and directly makes predictions\nwithout updating model parameters. Previous studies have revealed that ICL is\nsensitive to the selection and the ordering of demonstrations. However, there\nare few studies regarding the impact of the demonstration number on the ICL\nperformance within a limited input length of LLM, because it is commonly\nbelieved that the number of demonstrations is positively correlated with model\nperformance. In this paper, we found this conclusion does not always hold true.\nThrough pilot experiments, we discover that increasing the number of\ndemonstrations does not necessarily lead to improved performance. Building upon\nthis insight, we propose a Dynamic Demonstrations Controller (D$^2$Controller),\nwhich can improve the ICL performance by adjusting the number of demonstrations\ndynamically. The experimental results show that D$^2$Controller yields a 4.6%\nrelative improvement on ten different sizes of LLMs across ten datasets.\nMoreover, we also extend our method to previous ICL models and achieve\ncompetitive results."
                },
                "authors": [
                    {
                        "name": "Fei Zhao"
                    },
                    {
                        "name": "Taotian Pang"
                    },
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Zheng Ma"
                    },
                    {
                        "name": "Shujian Huang"
                    },
                    {
                        "name": "Xinyu Dai"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Dai"
                },
                "author": "Xinyu Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.00385v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.00385v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08102v1",
                "updated": "2024-12-11T05:05:16Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    5,
                    16,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T05:05:16Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    5,
                    5,
                    16,
                    2,
                    346,
                    0
                ],
                "title": "Verification and Validation of a Vision-Based Landing System for\n  Autonomous VTOL Air Taxis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verification and Validation of a Vision-Based Landing System for\n  Autonomous VTOL Air Taxis"
                },
                "summary": "Autonomous air taxis are poised to revolutionize urban mass transportation,\nhowever, ensuring their safety and reliability remains an open challenge.\nValidating autonomy solutions on air taxis in the real world presents\ncomplexities, risks, and costs that further convolute this challenge.\nVerification and Validation (V&V) frameworks play a crucial role in the design\nand development of highly reliable systems by formally verifying safety\nproperties and validating algorithm behavior across diverse operational\nscenarios. Advancements in high-fidelity simulators have significantly enhanced\ntheir capability to emulate real-world conditions, encouraging their use for\nvalidating autonomous air taxi solutions, especially during early development\nstages. This evolution underscores the growing importance of simulation\nenvironments, not only as complementary tools to real-world testing but as\nessential platforms for evaluating algorithms in a controlled, reproducible,\nand scalable manner.\n  This work presents a V&V framework for a vision-based landing system for air\ntaxis with vertical take-off and landing (VTOL) capabilities. Specifically, we\nuse Verse, a tool for formal verification, to model and verify the safety of\nthe system by obtaining and analyzing the reachable sets. To conduct this\nanalysis, we utilize a photorealistic simulation environment. The simulation\nenvironment, built on Unreal Engine, provides realistic terrain, weather, and\nsensor characteristics to emulate real-world conditions with high fidelity. To\nvalidate the safety analysis results, we conduct extensive scenario-based\ntesting to assess the reachability set and robustness of the landing algorithm\nin various conditions. This approach showcases the representativeness of\nhigh-fidelity simulators, offering an effective means to analyze and refine\nalgorithms before real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous air taxis are poised to revolutionize urban mass transportation,\nhowever, ensuring their safety and reliability remains an open challenge.\nValidating autonomy solutions on air taxis in the real world presents\ncomplexities, risks, and costs that further convolute this challenge.\nVerification and Validation (V&V) frameworks play a crucial role in the design\nand development of highly reliable systems by formally verifying safety\nproperties and validating algorithm behavior across diverse operational\nscenarios. Advancements in high-fidelity simulators have significantly enhanced\ntheir capability to emulate real-world conditions, encouraging their use for\nvalidating autonomous air taxi solutions, especially during early development\nstages. This evolution underscores the growing importance of simulation\nenvironments, not only as complementary tools to real-world testing but as\nessential platforms for evaluating algorithms in a controlled, reproducible,\nand scalable manner.\n  This work presents a V&V framework for a vision-based landing system for air\ntaxis with vertical take-off and landing (VTOL) capabilities. Specifically, we\nuse Verse, a tool for formal verification, to model and verify the safety of\nthe system by obtaining and analyzing the reachable sets. To conduct this\nanalysis, we utilize a photorealistic simulation environment. The simulation\nenvironment, built on Unreal Engine, provides realistic terrain, weather, and\nsensor characteristics to emulate real-world conditions with high fidelity. To\nvalidate the safety analysis results, we conduct extensive scenario-based\ntesting to assess the reachability set and robustness of the landing algorithm\nin various conditions. This approach showcases the representativeness of\nhigh-fidelity simulators, offering an effective means to analyze and refine\nalgorithms before real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ayoosh Bansal"
                    },
                    {
                        "name": "Duo Wang"
                    },
                    {
                        "name": "Mikael Yeghiazaryan"
                    },
                    {
                        "name": "Yangge Li"
                    },
                    {
                        "name": "Chuyuan Tao"
                    },
                    {
                        "name": "Hyung-Jin Yoon"
                    },
                    {
                        "name": "Prateek Arora"
                    },
                    {
                        "name": "Christos Papachristos"
                    },
                    {
                        "name": "Petros Voulgaris"
                    },
                    {
                        "name": "Sayan Mitra"
                    },
                    {
                        "name": "Lui Sha"
                    },
                    {
                        "name": "Naira Hovakimyan"
                    }
                ],
                "author_detail": {
                    "name": "Naira Hovakimyan"
                },
                "author": "Naira Hovakimyan",
                "arxiv_comment": "To be published in AIAA SciTech 2025 Forum",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08099v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08099v1",
                "updated": "2024-12-11T04:53:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    53,
                    15,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T04:53:15Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    53,
                    15,
                    2,
                    346,
                    0
                ],
                "title": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adversarial Vulnerabilities in Large Language Models for Time Series\n  Forecasting"
                },
                "summary": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have recently demonstrated significant potential\nin the field of time series forecasting, offering impressive capabilities in\nhandling complex temporal data. However, their robustness and reliability in\nreal-world applications remain under-explored, particularly concerning their\nsusceptibility to adversarial attacks. In this paper, we introduce a targeted\nadversarial attack framework for LLM-based time series forecasting. By\nemploying both gradient-free and black-box optimization methods, we generate\nminimal yet highly effective perturbations that significantly degrade the\nforecasting accuracy across multiple datasets and LLM architectures. Our\nexperiments, which include models like TimeGPT and LLM-Time with GPT-3.5,\nGPT-4, LLaMa, and Mistral, show that adversarial attacks lead to much more\nsevere performance degradation than random noise, and demonstrate the broad\neffectiveness of our attacks across different LLMs. The results underscore the\ncritical vulnerabilities of LLMs in time series forecasting, highlighting the\nneed for robust defense mechanisms to ensure their reliable deployment in\npractical applications."
                },
                "authors": [
                    {
                        "name": "Fuqiang Liu"
                    },
                    {
                        "name": "Sicong Jiang"
                    },
                    {
                        "name": "Luis Miranda-Moreno"
                    },
                    {
                        "name": "Seongjin Choi"
                    },
                    {
                        "name": "Lijun Sun"
                    }
                ],
                "author_detail": {
                    "name": "Lijun Sun"
                },
                "author": "Lijun Sun",
                "arxiv_comment": "11 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08099v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08099v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06769v2",
                "updated": "2024-12-11T04:52:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    52,
                    56,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-09T18:55:56Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    18,
                    55,
                    56,
                    0,
                    344,
                    0
                ],
                "title": "Training Large Language Models to Reason in a Continuous Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training Large Language Models to Reason in a Continuous Latent Space"
                },
                "summary": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are restricted to reason in the \"language\nspace\", where they typically express the reasoning process with a\nchain-of-thought (CoT) to solve a complex reasoning problem. However, we argue\nthat language space may not always be optimal for reasoning. For example, most\nword tokens are primarily for textual coherence and not essential for\nreasoning, while some critical tokens require complex planning and pose huge\nchallenges to LLMs. To explore the potential of LLM reasoning in an\nunrestricted latent space instead of using natural language, we introduce a new\nparadigm Coconut (Chain of Continuous Thought). We utilize the last hidden\nstate of the LLM as a representation of the reasoning state (termed \"continuous\nthought\"). Rather than decoding this into a word token, we feed it back to the\nLLM as the subsequent input embedding directly in the continuous space.\nExperiments show that Coconut can effectively augment the LLM on several\nreasoning tasks. This novel latent reasoning paradigm leads to emergent\nadvanced reasoning patterns: the continuous thought can encode multiple\nalternative next reasoning steps, allowing the model to perform a breadth-first\nsearch (BFS) to solve the problem, rather than prematurely committing to a\nsingle deterministic path like CoT. Coconut outperforms CoT in certain logical\nreasoning tasks that require substantial backtracking during planning, with\nfewer thinking tokens during inference. These findings demonstrate the promise\nof latent reasoning and offer valuable insights for future research."
                },
                "authors": [
                    {
                        "name": "Shibo Hao"
                    },
                    {
                        "name": "Sainbayar Sukhbaatar"
                    },
                    {
                        "name": "DiJia Su"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Zhiting Hu"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Yuandong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yuandong Tian"
                },
                "author": "Yuandong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08098v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08098v1",
                "updated": "2024-12-11T04:52:41Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    52,
                    41,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T04:52:41Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    52,
                    41,
                    2,
                    346,
                    0
                ],
                "title": "What You See Is Not Always What You Get: An Empirical Study of Code\n  Comprehension by Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What You See Is Not Always What You Get: An Empirical Study of Code\n  Comprehension by Large Language Models"
                },
                "summary": "Recent studies have demonstrated outstanding capabilities of large language\nmodels (LLMs) in software engineering domain, covering numerous tasks such as\ncode generation and comprehension. While the benefit of LLMs for coding task is\nwell noted, it is perceived that LLMs are vulnerable to adversarial attacks. In\nthis paper, we study the specific LLM vulnerability to imperceptible character\nattacks, a type of prompt-injection attack that uses special characters to\nbefuddle an LLM whilst keeping the attack hidden to human eyes. We devise four\ncategories of attacks and investigate their effects on the performance outcomes\nof tasks relating to code analysis and code comprehension. Two generations of\nChatGPT are included to evaluate the impact of advancements made to\ncontemporary models. Our experimental design consisted of comparing perturbed\nand unperturbed code snippets and evaluating two performance outcomes, which\nare model confidence using log probabilities of response, and correctness of\nresponse. We conclude that earlier version of ChatGPT exhibits a strong\nnegative linear correlation between the amount of perturbation and the\nperformance outcomes, while the recent ChatGPT presents a strong negative\ncorrelation between the presence of perturbation and performance outcomes, but\nno valid correlational relationship between perturbation budget and performance\noutcomes. We anticipate this work contributes to an in-depth understanding of\nleveraging LLMs for coding tasks. It is suggested future research should delve\ninto how to create LLMs that can return a correct response even if the prompt\nexhibits perturbations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent studies have demonstrated outstanding capabilities of large language\nmodels (LLMs) in software engineering domain, covering numerous tasks such as\ncode generation and comprehension. While the benefit of LLMs for coding task is\nwell noted, it is perceived that LLMs are vulnerable to adversarial attacks. In\nthis paper, we study the specific LLM vulnerability to imperceptible character\nattacks, a type of prompt-injection attack that uses special characters to\nbefuddle an LLM whilst keeping the attack hidden to human eyes. We devise four\ncategories of attacks and investigate their effects on the performance outcomes\nof tasks relating to code analysis and code comprehension. Two generations of\nChatGPT are included to evaluate the impact of advancements made to\ncontemporary models. Our experimental design consisted of comparing perturbed\nand unperturbed code snippets and evaluating two performance outcomes, which\nare model confidence using log probabilities of response, and correctness of\nresponse. We conclude that earlier version of ChatGPT exhibits a strong\nnegative linear correlation between the amount of perturbation and the\nperformance outcomes, while the recent ChatGPT presents a strong negative\ncorrelation between the presence of perturbation and performance outcomes, but\nno valid correlational relationship between perturbation budget and performance\noutcomes. We anticipate this work contributes to an in-depth understanding of\nleveraging LLMs for coding tasks. It is suggested future research should delve\ninto how to create LLMs that can return a correct response even if the prompt\nexhibits perturbations."
                },
                "authors": [
                    {
                        "name": "Bangshuo Zhu"
                    },
                    {
                        "name": "Jiawen Wen"
                    },
                    {
                        "name": "Huaming Chen"
                    }
                ],
                "author_detail": {
                    "name": "Huaming Chen"
                },
                "author": "Huaming Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08098v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08098v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.17840v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.17840v2",
                "updated": "2024-12-11T04:37:15Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    37,
                    15,
                    2,
                    346,
                    0
                ],
                "published": "2024-06-25T17:46:28Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    46,
                    28,
                    1,
                    177,
                    0
                ],
                "title": "Human-Object Interaction from Human-Level Instructions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human-Object Interaction from Human-Level Instructions"
                },
                "summary": "Intelligent agents must autonomously interact with the environments to\nperform daily tasks based on human-level instructions. They need a foundational\nunderstanding of the world to accurately interpret these instructions, along\nwith precise low-level movement and interaction skills to execute the derived\nactions. In this work, we propose the first complete system for synthesizing\nphysically plausible, long-horizon human-object interactions for object\nmanipulation in contextual environments, driven by human-level instructions. We\nleverage large language models (LLMs) to interpret the input instructions into\ndetailed execution plans. Unlike prior work, our system is capable of\ngenerating detailed finger-object interactions, in seamless coordination with\nfull-body movements. We also train a policy to track generated motions in\nphysics simulation via reinforcement learning (RL) to ensure physical\nplausibility of the motion. Our experiments demonstrate the effectiveness of\nour system in synthesizing realistic interactions with diverse objects in\ncomplex environments, highlighting its potential for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Intelligent agents must autonomously interact with the environments to\nperform daily tasks based on human-level instructions. They need a foundational\nunderstanding of the world to accurately interpret these instructions, along\nwith precise low-level movement and interaction skills to execute the derived\nactions. In this work, we propose the first complete system for synthesizing\nphysically plausible, long-horizon human-object interactions for object\nmanipulation in contextual environments, driven by human-level instructions. We\nleverage large language models (LLMs) to interpret the input instructions into\ndetailed execution plans. Unlike prior work, our system is capable of\ngenerating detailed finger-object interactions, in seamless coordination with\nfull-body movements. We also train a policy to track generated motions in\nphysics simulation via reinforcement learning (RL) to ensure physical\nplausibility of the motion. Our experiments demonstrate the effectiveness of\nour system in synthesizing realistic interactions with diverse objects in\ncomplex environments, highlighting its potential for real-world applications."
                },
                "authors": [
                    {
                        "name": "Zhen Wu"
                    },
                    {
                        "name": "Jiaman Li"
                    },
                    {
                        "name": "Pei Xu"
                    },
                    {
                        "name": "C. Karen Liu"
                    }
                ],
                "author_detail": {
                    "name": "C. Karen Liu"
                },
                "author": "C. Karen Liu",
                "arxiv_comment": "project page: https://hoifhli.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.17840v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.17840v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08090v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08090v1",
                "updated": "2024-12-11T04:16:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    16,
                    39,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T04:16:39Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    16,
                    39,
                    2,
                    346,
                    0
                ],
                "title": "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual LLMs Inherently Reward In-Language Time-Sensitive Semantic\n  Alignment for Low-Resource Languages"
                },
                "summary": "The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON a\ntemporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages - Romanian, German, and French, encompassing three temporal tasks and\nincluding a diverse set of four contemporaneous LLMs. This marks a significant\nstep forward in addressing resource disparity in the context of temporal\nreasoning across languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The unwavering disparity in labeled resources between resource-rich languages\nand those considered low-resource remains a significant impediment for Large\nLanguage Models (LLMs). Recent strides in cross-lingual in-context learning\n(X-ICL), mainly through semantically aligned examples retrieved from\nmultilingual pre-trained transformers, have shown promise in mitigating this\nissue. However, our investigation reveals that LLMs intrinsically reward\nin-language semantically aligned cross-lingual instances over direct\ncross-lingual semantic alignments, with a pronounced disparity in handling\ntime-sensitive queries in the X-ICL setup. Such queries demand sound temporal\nreasoning ability from LLMs, yet the advancements have predominantly focused on\nEnglish. This study aims to bridge this gap by improving temporal reasoning\ncapabilities in low-resource languages. To this end, we introduce mTEMPREASON a\ntemporal reasoning dataset aimed at the varied degrees of low-resource\nlanguages and propose Cross-Lingual Time-Sensitive Semantic Alignment\n(CLiTSSA), a novel method to improve temporal reasoning in these contexts. To\nfacilitate this, we construct an extension of mTEMPREASON comprising pairs of\nparallel cross-language temporal queries along with their anticipated\nin-language semantic similarity scores. Our empirical evidence underscores the\nsuperior performance of CLiTSSA compared to established baselines across three\nlanguages - Romanian, German, and French, encompassing three temporal tasks and\nincluding a diverse set of four contemporaneous LLMs. This marks a significant\nstep forward in addressing resource disparity in the context of temporal\nreasoning across languages."
                },
                "authors": [
                    {
                        "name": "Ashutosh Bajpai"
                    },
                    {
                        "name": "Tanmoy Chakraborty"
                    }
                ],
                "author_detail": {
                    "name": "Tanmoy Chakraborty"
                },
                "author": "Tanmoy Chakraborty",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08090v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08090v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02387v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02387v6",
                "updated": "2024-12-11T04:12:39Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    4,
                    12,
                    39,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-04T02:30:12Z",
                "published_parsed": [
                    2024,
                    9,
                    4,
                    2,
                    30,
                    12,
                    2,
                    248,
                    0
                ],
                "title": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models and Cognitive Science: A Comprehensive Review of\n  Similarities, Differences, and Challenges"
                },
                "summary": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review explores the intersection of Large Language Models\n(LLMs) and cognitive science, examining similarities and differences between\nLLMs and human cognitive processes. We analyze methods for evaluating LLMs\ncognitive abilities and discuss their potential as cognitive models. The review\ncovers applications of LLMs in various cognitive fields, highlighting insights\ngained for cognitive science research. We assess cognitive biases and\nlimitations of LLMs, along with proposed methods for improving their\nperformance. The integration of LLMs with cognitive architectures is examined,\nrevealing promising avenues for enhancing artificial intelligence (AI)\ncapabilities. Key challenges and future research directions are identified,\nemphasizing the need for continued refinement of LLMs to better align with\nhuman cognition. This review provides a balanced perspective on the current\nstate and future potential of LLMs in advancing our understanding of both\nartificial and human intelligence."
                },
                "authors": [
                    {
                        "name": "Qian Niu"
                    },
                    {
                        "name": "Junyu Liu"
                    },
                    {
                        "name": "Ziqian Bi"
                    },
                    {
                        "name": "Pohsun Feng"
                    },
                    {
                        "name": "Benji Peng"
                    },
                    {
                        "name": "Keyu Chen"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Lawrence KQ Yan"
                    },
                    {
                        "name": "Yichao Zhang"
                    },
                    {
                        "name": "Caitlyn Heqi Yin"
                    },
                    {
                        "name": "Cheng Fei"
                    },
                    {
                        "name": "Tianyang Wang"
                    },
                    {
                        "name": "Yunze Wang"
                    },
                    {
                        "name": "Silin Chen"
                    },
                    {
                        "name": "Ming Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ming Liu"
                },
                "author": "Ming Liu",
                "arxiv_comment": "10 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02387v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02387v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08072v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08072v1",
                "updated": "2024-12-11T03:35:38Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    35,
                    38,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:35:38Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    35,
                    38,
                    2,
                    346,
                    0
                ],
                "title": "Using Large Language Models for Parametric Shape Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using Large Language Models for Parametric Shape Optimization"
                },
                "summary": "Recent advanced large language models (LLMs) have showcased their emergent\ncapability of in-context learning, facilitating intelligent decision-making\nthrough natural language prompts without retraining. This new machine learning\nparadigm has shown promise in various fields, including general control and\noptimization problems. Inspired by these advancements, we explore the potential\nof LLMs for a specific and essential engineering task: parametric shape\noptimization (PSO). We develop an optimization framework, LLM-PSO, that\nleverages an LLM to determine the optimal shape of parameterized engineering\ndesigns in the spirit of evolutionary strategies. Utilizing the ``Claude 3.5\nSonnet'' LLM, we evaluate LLM-PSO on two benchmark flow optimization problems,\nspecifically aiming to identify drag-minimizing profiles for 1) a\ntwo-dimensional airfoil in laminar flow, and 2) a three-dimensional\naxisymmetric body in Stokes flow. In both cases, LLM-PSO successfully\nidentifies optimal shapes in agreement with benchmark solutions. Besides, it\ngenerally converges faster than other classical optimization algorithms. Our\npreliminary exploration may inspire further investigations into harnessing LLMs\nfor shape optimization and engineering design more broadly.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advanced large language models (LLMs) have showcased their emergent\ncapability of in-context learning, facilitating intelligent decision-making\nthrough natural language prompts without retraining. This new machine learning\nparadigm has shown promise in various fields, including general control and\noptimization problems. Inspired by these advancements, we explore the potential\nof LLMs for a specific and essential engineering task: parametric shape\noptimization (PSO). We develop an optimization framework, LLM-PSO, that\nleverages an LLM to determine the optimal shape of parameterized engineering\ndesigns in the spirit of evolutionary strategies. Utilizing the ``Claude 3.5\nSonnet'' LLM, we evaluate LLM-PSO on two benchmark flow optimization problems,\nspecifically aiming to identify drag-minimizing profiles for 1) a\ntwo-dimensional airfoil in laminar flow, and 2) a three-dimensional\naxisymmetric body in Stokes flow. In both cases, LLM-PSO successfully\nidentifies optimal shapes in agreement with benchmark solutions. Besides, it\ngenerally converges faster than other classical optimization algorithms. Our\npreliminary exploration may inspire further investigations into harnessing LLMs\nfor shape optimization and engineering design more broadly."
                },
                "authors": [
                    {
                        "name": "Xinxin Zhang"
                    },
                    {
                        "name": "Zhuoqun Xu"
                    },
                    {
                        "name": "Guangpu Zhu"
                    },
                    {
                        "name": "Chien Ming Jonathan Tay"
                    },
                    {
                        "name": "Yongdong Cui"
                    },
                    {
                        "name": "Boo Cheong Khoo"
                    },
                    {
                        "name": "Lailai Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Lailai Zhu"
                },
                "author": "Lailai Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08072v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08072v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08069v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08069v1",
                "updated": "2024-12-11T03:31:36Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    31,
                    36,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:31:36Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    31,
                    36,
                    2,
                    346,
                    0
                ],
                "title": "DialogAgent: An Auto-engagement Agent for Code Question Answering Data\n  Production",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DialogAgent: An Auto-engagement Agent for Code Question Answering Data\n  Production"
                },
                "summary": "Large Language Models (LLMs) have become increasingly integral to enhancing\ndeveloper productivity, particularly in code generation, comprehension, and\nrepair tasks. However, fine-tuning these models with high-quality, real-world\ndata is challenging due to privacy concerns and the lack of accessible, labeled\ndatasets. In this paper, we present DialogAgent, an automated tool for\ngenerating synthetic training data that closely mimics real developer\ninteractions within Integrated Development Environments (IDEs). DialogAgent\nenables the production of diverse, high-fidelity query-response pairs by\nsimulating multi-turn dialogues and contextual behaviors observed in real-world\nprogramming scenarios. The tool significantly reduces the reliance on manual\ndata generation, increasing efficiency by 4.8 times compared to traditional\nmethods. Our experiments and online deployment demonstrate substantial\nimprovements in model performance for code-related question-answering tasks:\nthe acceptance rate of responses generated by our in-house model is improved by\n33%, after training on synthesized data generated by DialogAgent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly integral to enhancing\ndeveloper productivity, particularly in code generation, comprehension, and\nrepair tasks. However, fine-tuning these models with high-quality, real-world\ndata is challenging due to privacy concerns and the lack of accessible, labeled\ndatasets. In this paper, we present DialogAgent, an automated tool for\ngenerating synthetic training data that closely mimics real developer\ninteractions within Integrated Development Environments (IDEs). DialogAgent\nenables the production of diverse, high-fidelity query-response pairs by\nsimulating multi-turn dialogues and contextual behaviors observed in real-world\nprogramming scenarios. The tool significantly reduces the reliance on manual\ndata generation, increasing efficiency by 4.8 times compared to traditional\nmethods. Our experiments and online deployment demonstrate substantial\nimprovements in model performance for code-related question-answering tasks:\nthe acceptance rate of responses generated by our in-house model is improved by\n33%, after training on synthesized data generated by DialogAgent."
                },
                "authors": [
                    {
                        "name": "Xiaoyun Liang"
                    },
                    {
                        "name": "Jingyi Ren"
                    },
                    {
                        "name": "Jiayi Qi"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08069v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08069v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01084v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01084v3",
                "updated": "2024-12-11T03:23:44Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    23,
                    44,
                    2,
                    346,
                    0
                ],
                "published": "2024-11-01T23:53:00Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    23,
                    53,
                    0,
                    4,
                    306,
                    0
                ],
                "title": "Plentiful Jailbreaks with String Compositions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plentiful Jailbreaks with String Compositions"
                },
                "summary": "Large language models (LLMs) remain vulnerable to a slew of adversarial\nattacks and jailbreaking methods. One common approach employed by white-hat\nattackers, or red-teamers, is to process model inputs and outputs using\nstring-level obfuscations, which can include leetspeak, rotary ciphers, Base64,\nASCII, and more. Our work extends these encoding-based attacks by unifying them\nin a framework of invertible string transformations. With invertibility, we can\ndevise arbitrary string compositions, defined as sequences of transformations,\nthat we can encode and decode end-to-end programmatically. We devise a\nautomated best-of-n attack that samples from a combinatorially large number of\nstring compositions. Our jailbreaks obtain competitive attack success rates on\nseveral leading frontier models when evaluated on HarmBench, highlighting that\nencoding-based attacks remain a persistent vulnerability even in advanced LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) remain vulnerable to a slew of adversarial\nattacks and jailbreaking methods. One common approach employed by white-hat\nattackers, or red-teamers, is to process model inputs and outputs using\nstring-level obfuscations, which can include leetspeak, rotary ciphers, Base64,\nASCII, and more. Our work extends these encoding-based attacks by unifying them\nin a framework of invertible string transformations. With invertibility, we can\ndevise arbitrary string compositions, defined as sequences of transformations,\nthat we can encode and decode end-to-end programmatically. We devise a\nautomated best-of-n attack that samples from a combinatorially large number of\nstring compositions. Our jailbreaks obtain competitive attack success rates on\nseveral leading frontier models when evaluated on HarmBench, highlighting that\nencoding-based attacks remain a persistent vulnerability even in advanced LLMs."
                },
                "authors": [
                    {
                        "name": "Brian R. Y. Huang"
                    }
                ],
                "author_detail": {
                    "name": "Brian R. Y. Huang"
                },
                "author": "Brian R. Y. Huang",
                "arxiv_comment": "NeurIPS SoLaR Workshop 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01084v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01084v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08063v1",
                "updated": "2024-12-11T03:15:49Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:15:49Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    15,
                    49,
                    2,
                    346,
                    0
                ],
                "title": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ContextModule: Improving Code Completion via Repository-level Contextual\n  Information"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive capabilities in\ncode completion tasks, where they assist developers by predicting and\ngenerating new code in real-time. However, existing LLM-based code completion\nsystems primarily rely on the immediate context of the file being edited, often\nmissing valuable repository-level information, user behaviour and edit history\nthat could improve suggestion accuracy. Additionally, challenges such as\nefficiently retrieving relevant code snippets from large repositories,\nincorporating user behavior, and balancing accuracy with low-latency\nrequirements in production environments remain unresolved. In this paper, we\npropose ContextModule, a framework designed to enhance LLM-based code\ncompletion by retrieving and integrating three types of contextual information\nfrom the repository: user behavior-based code, similar code snippets, and\ncritical symbol definitions. By capturing user interactions across files and\nleveraging repository-wide static analysis, ContextModule improves the\nrelevance and precision of generated code. We implement performance\noptimizations, such as index caching, to ensure the system meets the latency\nconstraints of real-world coding environments. Experimental results and\nindustrial practise demonstrate that ContextModule significantly improves code\ncompletion accuracy and user acceptance rates."
                },
                "authors": [
                    {
                        "name": "Zhanming Guan"
                    },
                    {
                        "name": "Junlin Liu"
                    },
                    {
                        "name": "Jierui Liu"
                    },
                    {
                        "name": "Chao Peng"
                    },
                    {
                        "name": "Dexin Liu"
                    },
                    {
                        "name": "Ningyuan Sun"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Wenchao Li"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Hang Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Zhu"
                },
                "author": "Hang Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07768v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07768v2",
                "updated": "2024-12-11T03:04:20Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    4,
                    20,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-10T18:59:32Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    59,
                    32,
                    1,
                    345,
                    0
                ],
                "title": "Test-time Correction with Human Feedback: An Online 3D Detection System\n  via Visual Prompting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Correction with Human Feedback: An Online 3D Detection System\n  via Visual Prompting"
                },
                "summary": "This paper introduces Test-time Correction (TTC) system, a novel online 3D\ndetection system designated for online correction of test-time errors via human\nfeedback, to guarantee the safety of deployed autonomous driving systems.\nUnlike well-studied offline 3D detectors frozen at inference, TTC explores the\ncapability of instant online error rectification. By leveraging user feedback\nwith interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC\ncould immediately update the corresponding detection results for future\nstreaming inputs, even though the model is deployed with fixed parameters. This\nenables autonomous driving systems to adapt to new scenarios immediately and\ndecrease deployment risks reliably without additional expensive training. To\nachieve such TTC system, we equip existing 3D detectors with Online Adapter\n(OA) module, a prompt-driven query generator for online correction. At the core\nof OA module are visual prompts, images of missed object-of-interest for\nguiding the corresponding detection and subsequent tracking. Those visual\nprompts, belonging to missed objects through online inference, are maintained\nby the visual prompt buffer for continuous error correction in subsequent\nframes. By doing so, TTC consistently detects online missed objects and\nimmediately lowers driving risks. It achieves reliable, versatile, and adaptive\ndriving autonomy. Extensive experiments demonstrate significant gain on instant\nerror rectification over pre-trained 3D detectors, even in challenging\nscenarios with limited labels, zero-shot detection, and adverse conditions. We\nhope this work would inspire the community to investigate online rectification\nsystems for autonomous driving post-deployment. Code would be publicly shared.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Test-time Correction (TTC) system, a novel online 3D\ndetection system designated for online correction of test-time errors via human\nfeedback, to guarantee the safety of deployed autonomous driving systems.\nUnlike well-studied offline 3D detectors frozen at inference, TTC explores the\ncapability of instant online error rectification. By leveraging user feedback\nwith interactive prompts at a frame, e.g., a simple click or draw of boxes, TTC\ncould immediately update the corresponding detection results for future\nstreaming inputs, even though the model is deployed with fixed parameters. This\nenables autonomous driving systems to adapt to new scenarios immediately and\ndecrease deployment risks reliably without additional expensive training. To\nachieve such TTC system, we equip existing 3D detectors with Online Adapter\n(OA) module, a prompt-driven query generator for online correction. At the core\nof OA module are visual prompts, images of missed object-of-interest for\nguiding the corresponding detection and subsequent tracking. Those visual\nprompts, belonging to missed objects through online inference, are maintained\nby the visual prompt buffer for continuous error correction in subsequent\nframes. By doing so, TTC consistently detects online missed objects and\nimmediately lowers driving risks. It achieves reliable, versatile, and adaptive\ndriving autonomy. Extensive experiments demonstrate significant gain on instant\nerror rectification over pre-trained 3D detectors, even in challenging\nscenarios with limited labels, zero-shot detection, and adverse conditions. We\nhope this work would inspire the community to investigate online rectification\nsystems for autonomous driving post-deployment. Code would be publicly shared."
                },
                "authors": [
                    {
                        "name": "Zetong Yang"
                    },
                    {
                        "name": "Hanxue Zhang"
                    },
                    {
                        "name": "Yanan Sun"
                    },
                    {
                        "name": "Li Chen"
                    },
                    {
                        "name": "Fei Xia"
                    },
                    {
                        "name": "Fatma GÃ¼ney"
                    },
                    {
                        "name": "Hongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Li"
                },
                "author": "Hongyang Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07768v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07768v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08054v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08054v1",
                "updated": "2024-12-11T03:00:24Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    0,
                    24,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T03:00:24Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    3,
                    0,
                    24,
                    2,
                    346,
                    0
                ],
                "title": "Federated In-Context LLM Agent Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated In-Context LLM Agent Learning"
                },
                "summary": "Large Language Models (LLMs) have revolutionized intelligent services by\nenabling logical reasoning, tool use, and interaction with external systems as\nagents. The advancement of LLMs is frequently hindered by the scarcity of\nhigh-quality data, much of which is inherently sensitive. Federated learning\n(FL) offers a potential solution by facilitating the collaborative training of\ndistributed LLMs while safeguarding private data. However, FL frameworks face\nsignificant bandwidth and computational demands, along with challenges from\nheterogeneous data distributions. The emerging in-context learning capability\nof LLMs offers a promising approach by aggregating natural language rather than\nbulky model parameters. Yet, this method risks privacy leakage, as it\nnecessitates the collection and presentation of data samples from various\nclients during aggregation. In this paper, we propose a novel\nprivacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,\nwhich to our best knowledge for the first work unleashes the power of\nin-context learning to train diverse LLM agents through FL. In our design,\nknowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums\nGeneration (KCG) module are transmitted between clients and the server instead\nof model parameters in previous FL methods. Apart from that, an incredible\nRetrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU)\nmodule is designed and we incorporate the aggregated global knowledge\ncompendium as a teacher to teach LLM agents the usage of tools. We conducted\nextensive experiments and the results show that FICAL has competitive\nperformance compared to other SOTA baselines with a significant communication\ncost decrease of $\\mathbf{3.33\\times10^5}$ times.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized intelligent services by\nenabling logical reasoning, tool use, and interaction with external systems as\nagents. The advancement of LLMs is frequently hindered by the scarcity of\nhigh-quality data, much of which is inherently sensitive. Federated learning\n(FL) offers a potential solution by facilitating the collaborative training of\ndistributed LLMs while safeguarding private data. However, FL frameworks face\nsignificant bandwidth and computational demands, along with challenges from\nheterogeneous data distributions. The emerging in-context learning capability\nof LLMs offers a promising approach by aggregating natural language rather than\nbulky model parameters. Yet, this method risks privacy leakage, as it\nnecessitates the collection and presentation of data samples from various\nclients during aggregation. In this paper, we propose a novel\nprivacy-preserving Federated In-Context LLM Agent Learning (FICAL) algorithm,\nwhich to our best knowledge for the first work unleashes the power of\nin-context learning to train diverse LLM agents through FL. In our design,\nknowledge compendiums generated by a novel LLM-enhanced Knowledge Compendiums\nGeneration (KCG) module are transmitted between clients and the server instead\nof model parameters in previous FL methods. Apart from that, an incredible\nRetrieval Augmented Generation (RAG) based Tool Learning and Utilizing (TLU)\nmodule is designed and we incorporate the aggregated global knowledge\ncompendium as a teacher to teach LLM agents the usage of tools. We conducted\nextensive experiments and the results show that FICAL has competitive\nperformance compared to other SOTA baselines with a significant communication\ncost decrease of $\\mathbf{3.33\\times10^5}$ times."
                },
                "authors": [
                    {
                        "name": "Panlong Wu"
                    },
                    {
                        "name": "Kangshuo Li"
                    },
                    {
                        "name": "Junbao Nan"
                    },
                    {
                        "name": "Fangxin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Fangxin Wang"
                },
                "author": "Fangxin Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08054v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08054v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08052v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08052v1",
                "updated": "2024-12-11T02:59:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    59,
                    46,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T02:59:46Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    59,
                    46,
                    2,
                    346,
                    0
                ],
                "title": "CANDOR: Counterfactual ANnotated DOubly Robust Off-Policy Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CANDOR: Counterfactual ANnotated DOubly Robust Off-Policy Evaluation"
                },
                "summary": "Off-policy evaluation (OPE) provides safety guarantees by estimating the\nperformance of a policy before deployment. Recent work introduced IS+, an\nimportance sampling (IS) estimator that uses expert-annotated counterfactual\nsamples to improve behavior dataset coverage. However, IS estimators are known\nto have high variance; furthermore, the performance of IS+ deteriorates when\nannotations are imperfect. In this work, we propose a family of OPE estimators\ninspired by the doubly robust (DR) principle. A DR estimator combines IS with a\nreward model estimate, known as the direct method (DM), and offers favorable\nstatistical guarantees. We propose three strategies for incorporating\ncounterfactual annotations into a DR-inspired estimator and analyze their\nproperties under various realistic settings. We prove that using imperfect\nannotations in the DM part of the estimator best leverages the annotations, as\nopposed to using them in the IS part. To support our theoretical findings, we\nevaluate the proposed estimators in three contextual bandit environments. Our\nempirical results show that when the reward model is misspecified and the\nannotations are imperfect, it is most beneficial to use the annotations only in\nthe DM portion of a DR estimator. Based on these theoretical and empirical\ninsights, we provide a practical guide for using counterfactual annotations in\ndifferent realistic settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Off-policy evaluation (OPE) provides safety guarantees by estimating the\nperformance of a policy before deployment. Recent work introduced IS+, an\nimportance sampling (IS) estimator that uses expert-annotated counterfactual\nsamples to improve behavior dataset coverage. However, IS estimators are known\nto have high variance; furthermore, the performance of IS+ deteriorates when\nannotations are imperfect. In this work, we propose a family of OPE estimators\ninspired by the doubly robust (DR) principle. A DR estimator combines IS with a\nreward model estimate, known as the direct method (DM), and offers favorable\nstatistical guarantees. We propose three strategies for incorporating\ncounterfactual annotations into a DR-inspired estimator and analyze their\nproperties under various realistic settings. We prove that using imperfect\nannotations in the DM part of the estimator best leverages the annotations, as\nopposed to using them in the IS part. To support our theoretical findings, we\nevaluate the proposed estimators in three contextual bandit environments. Our\nempirical results show that when the reward model is misspecified and the\nannotations are imperfect, it is most beneficial to use the annotations only in\nthe DM portion of a DR estimator. Based on these theoretical and empirical\ninsights, we provide a practical guide for using counterfactual annotations in\ndifferent realistic settings."
                },
                "authors": [
                    {
                        "name": "Aishwarya Mandyam"
                    },
                    {
                        "name": "Shengpu Tang"
                    },
                    {
                        "name": "Jiayu Yao"
                    },
                    {
                        "name": "Jenna Wiens"
                    },
                    {
                        "name": "Barbara E. Engelhardt"
                    }
                ],
                "author_detail": {
                    "name": "Barbara E. Engelhardt"
                },
                "author": "Barbara E. Engelhardt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08052v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08052v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08041v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08041v1",
                "updated": "2024-12-11T02:44:14Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    44,
                    14,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T02:44:14Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    44,
                    14,
                    2,
                    346,
                    0
                ],
                "title": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the benefits of code hints for refactoring deprecated Java\n  APIs"
                },
                "summary": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When done manually, refactoring legacy code in order to eliminate uses of\ndeprecated APIs is an error-prone and time-consuming process. In this paper, we\ninvestigate to which degree refactorings for deprecated Java APIs can be\nautomated, and quantify the benefit of Javadoc code hints for this task. To\nthis end, we build a symbolic and a neural engine for the automatic refactoring\nof deprecated APIs. The former is based on type-directed and component-based\nprogram synthesis, whereas the latter uses LLMs. We applied our engines to\nrefactor the deprecated methods in the Oracle JDK 15. Our experiments show that\ncode hints are enabling for the automation of this task: even the worst engine\ncorrectly refactors 71% of the tasks with code hints, which drops to at best\n14% on tasks without. Adding more code hints to Javadoc can hence boost the\nrefactoring of code that uses deprecated APIs."
                },
                "authors": [
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Pascal Kesseli"
                    },
                    {
                        "name": "Daniel Kroening"
                    },
                    {
                        "name": "Hanliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Hanliang Zhang"
                },
                "author": "Hanliang Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08041v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08041v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08038v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08038v1",
                "updated": "2024-12-11T02:37:32Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    37,
                    32,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T02:37:32Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    37,
                    32,
                    2,
                    346,
                    0
                ],
                "title": "Bootstrapping Heterogeneous Graph Representation Learning via Large\n  Language Models: A Generalized Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bootstrapping Heterogeneous Graph Representation Learning via Large\n  Language Models: A Generalized Approach"
                },
                "summary": "Graph representation learning methods are highly effective in handling\ncomplex non-Euclidean data by capturing intricate relationships and features\nwithin graph structures. However, traditional methods face challenges when\ndealing with heterogeneous graphs that contain various types of nodes and edges\ndue to the diverse sources and complex nature of the data. Existing\nHeterogeneous Graph Neural Networks (HGNNs) have shown promising results but\nrequire prior knowledge of node and edge types and unified node feature\nformats, which limits their applicability. Recent advancements in graph\nrepresentation learning using Large Language Models (LLMs) offer new solutions\nby integrating LLMs' data processing capabilities, enabling the alignment of\nvarious graph representations. Nevertheless, these methods often overlook\nheterogeneous graph data and require extensive preprocessing. To address these\nlimitations, we propose a novel method that leverages the strengths of both LLM\nand GNN, allowing for the processing of graph data with any format and type of\nnodes and edges without the need for type information or special preprocessing.\nOur method employs LLM to automatically summarize and classify different data\nformats and types, aligns node features, and uses a specialized GNN for\ntargeted learning, thus obtaining effective graph representations for\ndownstream tasks. Theoretical analysis and experimental validation have\ndemonstrated the effectiveness of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph representation learning methods are highly effective in handling\ncomplex non-Euclidean data by capturing intricate relationships and features\nwithin graph structures. However, traditional methods face challenges when\ndealing with heterogeneous graphs that contain various types of nodes and edges\ndue to the diverse sources and complex nature of the data. Existing\nHeterogeneous Graph Neural Networks (HGNNs) have shown promising results but\nrequire prior knowledge of node and edge types and unified node feature\nformats, which limits their applicability. Recent advancements in graph\nrepresentation learning using Large Language Models (LLMs) offer new solutions\nby integrating LLMs' data processing capabilities, enabling the alignment of\nvarious graph representations. Nevertheless, these methods often overlook\nheterogeneous graph data and require extensive preprocessing. To address these\nlimitations, we propose a novel method that leverages the strengths of both LLM\nand GNN, allowing for the processing of graph data with any format and type of\nnodes and edges without the need for type information or special preprocessing.\nOur method employs LLM to automatically summarize and classify different data\nformats and types, aligns node features, and uses a specialized GNN for\ntargeted learning, thus obtaining effective graph representations for\ndownstream tasks. Theoretical analysis and experimental validation have\ndemonstrated the effectiveness of our method."
                },
                "authors": [
                    {
                        "name": "Hang Gao"
                    },
                    {
                        "name": "Chenhao Zhang"
                    },
                    {
                        "name": "Fengge Wu"
                    },
                    {
                        "name": "Junsuo Zhao"
                    },
                    {
                        "name": "Changwen Zheng"
                    },
                    {
                        "name": "Huaping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huaping Liu"
                },
                "author": "Huaping Liu",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08038v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08038v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05493v2",
                "updated": "2024-12-11T02:31:56Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    31,
                    56,
                    2,
                    346,
                    0
                ],
                "published": "2024-09-09T10:43:18Z",
                "published_parsed": [
                    2024,
                    9,
                    9,
                    10,
                    43,
                    18,
                    0,
                    253,
                    0
                ],
                "title": "DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects\n  in Unrestricted Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DexDiff: Towards Extrinsic Dexterity Manipulation of Ungraspable Objects\n  in Unrestricted Environments"
                },
                "summary": "Grasping large and flat objects (e.g. a book or a pan) is often regarded as\nan ungraspable task, which poses significant challenges due to the unreachable\ngrasping poses. Previous works leverage Extrinsic Dexterity like walls or table\nedges to grasp such objects. However, they are limited to task-specific\npolicies and lack task planning to find pre-grasp conditions. This makes it\ndifficult to adapt to various environments and extrinsic dexterity constraints.\nTherefore, we present DexDiff, a robust robotic manipulation method for\nlong-horizon planning with extrinsic dexterity. Specifically, we utilize a\nvision-language model (VLM) to perceive the environmental state and generate\nhigh-level task plans, followed by a goal-conditioned action diffusion (GCAD)\nmodel to predict the sequence of low-level actions. This model learns the\nlow-level policy from offline data with the cumulative reward guided by\nhigh-level planning as the goal condition, which allows for improved prediction\nof robot actions. Experimental results demonstrate that our method not only\neffectively performs ungraspable tasks but also generalizes to previously\nunseen objects. It outperforms baselines by a 47% higher success rate in\nsimulation and facilitates efficient deployment and manipulation in real-world\nscenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grasping large and flat objects (e.g. a book or a pan) is often regarded as\nan ungraspable task, which poses significant challenges due to the unreachable\ngrasping poses. Previous works leverage Extrinsic Dexterity like walls or table\nedges to grasp such objects. However, they are limited to task-specific\npolicies and lack task planning to find pre-grasp conditions. This makes it\ndifficult to adapt to various environments and extrinsic dexterity constraints.\nTherefore, we present DexDiff, a robust robotic manipulation method for\nlong-horizon planning with extrinsic dexterity. Specifically, we utilize a\nvision-language model (VLM) to perceive the environmental state and generate\nhigh-level task plans, followed by a goal-conditioned action diffusion (GCAD)\nmodel to predict the sequence of low-level actions. This model learns the\nlow-level policy from offline data with the cumulative reward guided by\nhigh-level planning as the goal condition, which allows for improved prediction\nof robot actions. Experimental results demonstrate that our method not only\neffectively performs ungraspable tasks but also generalizes to previously\nunseen objects. It outperforms baselines by a 47% higher success rate in\nsimulation and facilitates efficient deployment and manipulation in real-world\nscenarios."
                },
                "authors": [
                    {
                        "name": "Chengzhong Ma"
                    },
                    {
                        "name": "Houxue Yang"
                    },
                    {
                        "name": "Hanbo Zhang"
                    },
                    {
                        "name": "Zeyang Liu"
                    },
                    {
                        "name": "Chao Zhao"
                    },
                    {
                        "name": "Jian Tang"
                    },
                    {
                        "name": "Xuguang Lan"
                    },
                    {
                        "name": "Nanning Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Nanning Zheng"
                },
                "author": "Nanning Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.08035v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.08035v1",
                "updated": "2024-12-11T02:31:46Z",
                "updated_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    31,
                    46,
                    2,
                    346,
                    0
                ],
                "published": "2024-12-11T02:31:46Z",
                "published_parsed": [
                    2024,
                    12,
                    11,
                    2,
                    31,
                    46,
                    2,
                    346,
                    0
                ],
                "title": "Scalable, Validated Code Translation of Entire Projects using Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable, Validated Code Translation of Entire Projects using Large\n  Language Models"
                },
                "summary": "Large language models (LLMs) show promise in code translation due to their\nability to generate idiomatic code. However, a significant limitation when\nusing LLMs for code translation is scalability: existing works have shown a\ndrop in translation success rates for code exceeding around 100 lines. We\novercome this limitation by developing a modular approach to translation, where\nwe partition the code into small code fragments which can be translated\nindependently and semantically validated (that is, checking I/O equivalence).\nWhen this approach is applied naively, we discover that LLMs are unreliable\nwhen translating features of the source language that do not have a direct\nmapping to the target language, and that the LLM often gets stuck in repair\nloops when attempting to fix errors. To address these issues, we introduce two\nkey concepts: (1) feature mapping, which integrates predefined translation\nrules with LLM-based translation to guide the LLM in navigating subtle language\ndifferences and producing semantically accurate code; and (2)\ntype-compatibility, which facilitates localized checks at the function\nsignature level to detect errors early, thereby narrowing the scope of\npotential repairs. We apply our approach to translating real-world Go codebases\nto Rust, demonstrating that we can consistently generate reliable Rust\ntranslations for projects up to 6,600 lines of code and 369 functions, with an\naverage of 73% of functions successfully validated for I/O equivalence,\nconsiderably higher than any existing work.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) show promise in code translation due to their\nability to generate idiomatic code. However, a significant limitation when\nusing LLMs for code translation is scalability: existing works have shown a\ndrop in translation success rates for code exceeding around 100 lines. We\novercome this limitation by developing a modular approach to translation, where\nwe partition the code into small code fragments which can be translated\nindependently and semantically validated (that is, checking I/O equivalence).\nWhen this approach is applied naively, we discover that LLMs are unreliable\nwhen translating features of the source language that do not have a direct\nmapping to the target language, and that the LLM often gets stuck in repair\nloops when attempting to fix errors. To address these issues, we introduce two\nkey concepts: (1) feature mapping, which integrates predefined translation\nrules with LLM-based translation to guide the LLM in navigating subtle language\ndifferences and producing semantically accurate code; and (2)\ntype-compatibility, which facilitates localized checks at the function\nsignature level to detect errors early, thereby narrowing the scope of\npotential repairs. We apply our approach to translating real-world Go codebases\nto Rust, demonstrating that we can consistently generate reliable Rust\ntranslations for projects up to 6,600 lines of code and 369 functions, with an\naverage of 73% of functions successfully validated for I/O equivalence,\nconsiderably higher than any existing work."
                },
                "authors": [
                    {
                        "name": "Hanliang Zhang"
                    },
                    {
                        "name": "Cristina David"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Brandon Paulsen"
                    },
                    {
                        "name": "Daniel Kroening"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Kroening"
                },
                "author": "Daniel Kroening",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.08035v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.08035v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]