[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.13797v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13797v1",
                "updated": "2025-10-15T17:57:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:57:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    21,
                    2,
                    288,
                    0
                ],
                "title": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breadcrumbs Reasoning: Memory-Efficient Reasoning with Compression\n  Beacons"
                },
                "summary": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The scalability of large language models for long-context reasoning is\nseverely constrained by the linear growth of their Transformer key-value cache,\nwhich incurs significant memory and computational costs. We posit that as a\nmodel generates reasoning tokens, the informational value of past generated\ntokens diminishes, creating an opportunity for compression. In this work, we\npropose to periodically compress the generation KV cache with a learned,\nspecial-purpose token and evict compressed entries. We train the model to\nperform this compression via a modified joint distillation and reinforcement\nlearning (RL) framework. Our training method minimizes overhead over the\nconventional RL process, as it leverages RL outputs for distillation.\nEmpirically, our method achieves a superior memory-accuracy Pareto frontier\ncompared to both the model without cache compression and training-free\ncompression techniques."
                },
                "authors": [
                    {
                        "name": "Giovanni Monea"
                    },
                    {
                        "name": "Yair Feldman"
                    },
                    {
                        "name": "Shankar Padmanabhan"
                    },
                    {
                        "name": "Kiant√© Brantley"
                    },
                    {
                        "name": "Yoav Artzi"
                    }
                ],
                "author_detail": {
                    "name": "Yoav Artzi"
                },
                "author": "Yoav Artzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13797v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13797v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13401v1",
                "updated": "2025-10-15T10:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs"
                },
                "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second)."
                },
                "authors": [
                    {
                        "name": "Jude Haris"
                    },
                    {
                        "name": "Jos√© Cano"
                    }
                ],
                "author_detail": {
                    "name": "Jos√© Cano"
                },
                "author": "Jos√© Cano",
                "arxiv_comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13334v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13334v1",
                "updated": "2025-10-15T09:18:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T09:18:58Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    18,
                    58,
                    2,
                    288,
                    0
                ],
                "title": "Taming the Fragility of KV Cache Eviction in LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taming the Fragility of KV Cache Eviction in LLM Inference"
                },
                "summary": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models have revolutionized natural language processing, yet\ntheir deployment remains hampered by the substantial memory and runtime\noverhead of the transformer's Key-Value cache. To mitigate this, recent methods\nemploy a scoring-aggregation framework to evict unimportant cache entries,\nbased on the stability assumption-that a fixed subset of entries remains\nconsistently important during generation. However, prior work has largely\nfocused on refining importance indicators for scoring, while defaulting to mean\naggregation due to a faithful trust in the stability assumption. In this work,\nwe argue that this underlying assumption is inherently fragile, making mean\naggregation highly vulnerable in extreme cases. To counter this, we propose a\nsimple yet elegant defensive aggregation strategy: a two-step, linear-time\napproach that controls worst-case risk, thereby defending against extreme cases\nwith negligible computational overhead. Embodying this strategy, we propose a\nnovel cache eviction method, DefensiveKV and its extension, Layer-DefensiveKV,\nwhich incorporates layer-wise budget allocation. Across seven task domains (18\ndatasets), our methods reduce generation quality loss by 2.3x and 4.3x\nrespectively, versus the strongest baseline under a 20% cache size. These\nresults set new performance benchmarks and pioneer a promising direction for\noptimizing cache eviction against underlying fragility through worst-case risk\nmanagement. Our code is available at https://github.com/FFY0/DefensiveKV."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Haoyu Guo"
                    },
                    {
                        "name": "JunLin Lv"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    },
                    {
                        "name": "Xike Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xike Xie"
                },
                "author": "Xike Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13334v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13334v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13279v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13279v1",
                "updated": "2025-10-15T08:25:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T08:25:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    8,
                    25,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Partitioned Scheduling for DAG Tasks Considering Probabilistic Execution\n  Time"
                },
                "summary": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autonomous driving systems, critical for safety, require real-time guarantees\nand can be modeled as DAGs. Their acceleration features, such as caches and\npipelining, often result in execution times below the worst-case. Thus, a\nprobabilistic approach ensuring constraint satisfaction within a probability\nthreshold is more suitable than worst-case guarantees for these systems. This\npaper considers probabilistic guarantees for DAG tasks by utilizing the results\nof probabilistic guarantees for single processors, which have been relatively\nmore advanced than those for multi-core processors. This paper proposes a task\nset partitioning method that guarantees schedulability under the partitioned\nscheduling. The evaluation on randomly generated DAG task sets demonstrates\nthat the proposed method schedules more task sets with a smaller mean analysis\ntime compared to existing probabilistic schedulability analysis for DAGs. The\nevaluation also compares four bin-packing heuristics, revealing Item-Centric\nWorst-Fit-Decreasing schedules the most task sets."
                },
                "authors": [
                    {
                        "name": "Fuma Omori"
                    },
                    {
                        "name": "Atsushi Yano"
                    },
                    {
                        "name": "Takuya Azumi"
                    }
                ],
                "author_detail": {
                    "name": "Takuya Azumi"
                },
                "author": "Takuya Azumi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13279v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13279v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13223v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13223v1",
                "updated": "2025-10-15T07:20:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T07:20:14Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    7,
                    20,
                    14,
                    2,
                    288,
                    0
                ],
                "title": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BanaServe: Unified KV Cache and Dynamic Module Migration for Balancing\n  Disaggregated LLM Serving in AI Infrastructure"
                },
                "summary": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly deployed in AI infrastructure,\ndriving the need for high throughput, resource efficient serving systems.\nDisaggregated LLM serving, which separates prompt prefill from auto-regressive\ndecode, has emerged as a promising architecture by isolating their\nheterogeneous compute and memory demands. However, current disaggregated\nsystems face three key limitations: (i) static resource allocation cannot adapt\nto highly dynamic workloads, causing over-provisioning that wastes resources or\nunder-provisioning that violates service level objectives (SLOs); (ii) inherent\nload imbalance between prefill and decode stages, where prefill is\ncompute-bound and decode is memory-bound, causes under-utilization in one tier\nwhile the other becomes a bottleneck; and (iii) prefix cache aware routing\nskews load distribution, as high cache hit rate prefill nodes attract\ndisproportionately more requests, further degrading balance and efficiency. To\naddress these issues, we present BanaServe, a dynamic orchestration framework\nthat continuously rebalances computational and memory resources across prefill\nand decode instances while eliminating hotspots induced by cache. BanaServe\nintroduces layer level weight migration, attention level Key Value Cache (KV\nCache) migration, and Global KV Cache Store sharing with layer wise overlapped\ntransmission, enabling both coarse grained (layer level) and fine grained\n(attention level) load redistribution with minimal latency overhead. These\nmechanisms allow routers to perform purely load aware scheduling, unconstrained\nby cache placement. Compared to vLLM, BanaServe achieves 1.2x-3.9x higher\nthroughput with 3.9%-78.4% lower total processing time, and outperforms\nDistServe by 1.1x-2.8x in throughput with 1.4%-70.1% latency reduction."
                },
                "authors": [
                    {
                        "name": "Yiyuan He"
                    },
                    {
                        "name": "Minxian Xu"
                    },
                    {
                        "name": "Jingfeng Wu"
                    },
                    {
                        "name": "Jianmin Hu"
                    },
                    {
                        "name": "Chong Ma"
                    },
                    {
                        "name": "Min Shen"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Lin Qu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13223v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13223v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13084v1",
                "updated": "2025-10-15T01:55:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T01:55:32Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    32,
                    2,
                    288,
                    0
                ],
                "title": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edit-Your-Interest: Efficient Video Editing via Feature Most-Similar\n  Propagation"
                },
                "summary": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-image (T2I) diffusion models have recently demonstrated significant\nprogress in video editing.\n  However, existing video editing methods are severely limited by their high\ncomputational overhead and memory consumption.\n  Furthermore, these approaches often sacrifice visual fidelity, leading to\nundesirable temporal inconsistencies and artifacts such as blurring and\npronounced mosaic-like patterns.\n  We propose Edit-Your-Interest, a lightweight, text-driven, zero-shot video\nediting method.\n  Edit-Your-Interest introduces a spatio-temporal feature memory to cache\nfeatures from previous frames, significantly reducing computational overhead\ncompared to full-sequence spatio-temporal modeling approaches.\n  Specifically, we first introduce a Spatio-Temporal Feature Memory bank (SFM),\nwhich is designed to efficiently cache and retain the crucial image tokens\nprocessed by spatial attention.\n  Second, we propose the Feature Most-Similar Propagation (FMP) method. FMP\npropagates the most relevant tokens from previous frames to subsequent ones,\npreserving temporal consistency.\n  Finally, we introduce an SFM update algorithm that continuously refreshes the\ncached features, ensuring their long-term relevance and effectiveness\nthroughout the video sequence.\n  Furthermore, we leverage cross-attention maps to automatically extract masks\nfor the instances of interest.\n  These masks are seamlessly integrated into the diffusion denoising process,\nenabling fine-grained control over target objects and allowing\nEdit-Your-Interest to perform highly accurate edits while robustly preserving\nthe background integrity.\n  Extensive experiments decisively demonstrate that the proposed\nEdit-Your-Interest outperforms state-of-the-art methods in both efficiency and\nvisual fidelity, validating its superior effectiveness and practicality."
                },
                "authors": [
                    {
                        "name": "Yi Zuo"
                    },
                    {
                        "name": "Zitao Wang"
                    },
                    {
                        "name": "Lingling Li"
                    },
                    {
                        "name": "Xu Liu"
                    },
                    {
                        "name": "Fang Liu"
                    },
                    {
                        "name": "Licheng Jiao"
                    }
                ],
                "author_detail": {
                    "name": "Licheng Jiao"
                },
                "author": "Licheng Jiao",
                "arxiv_comment": "32 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.15969v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.15969v3",
                "updated": "2025-10-15T01:55:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    1,
                    55,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-19T02:25:04Z",
                "published_parsed": [
                    2025,
                    6,
                    19,
                    2,
                    25,
                    4,
                    3,
                    170,
                    0
                ],
                "title": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LazyEviction: Lagged KV Eviction with Attention Pattern Observation for\n  Efficient Long Reasoning"
                },
                "summary": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit enhanced capabilities by\nChain-of-Thought reasoning. However, the extended reasoning sequences introduce\nsignificant GPU memory overhead due to increased key-value (KV) cache. Existing\nKV cache compression methods mitigate memory bottlenecks but struggle in long\nreasoning tasks. In this paper, we analyze attention patterns in reasoning\ntasks and reveal a Token Importance Recurrence phenomenon: a large proportion\nof tokens regain high attention after multiple decoding steps, which is failed\nto capture by existing works and may lead to unpredictable eviction on such\nperiodically critical tokens. To address this, we propose LazyEviction, an\nobservation window-based lagged eviction framework retaining latent recurring\ntokens by prioritized eviction based on tokens' recurrence patterns. Extensive\nexperiments demonstrate that LazyEviction reduces KV cache by 50%~70% while\nmaintaining comparable accuracy, outperforming existing KV cache compression\nbaselines. Our implementation code can be found at\nhttps://github.com/Halo-949/LazyEviction."
                },
                "authors": [
                    {
                        "name": "Haoyue Zhang"
                    },
                    {
                        "name": "Hualei Zhang"
                    },
                    {
                        "name": "Xiaosong Ma"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    }
                ],
                "author_detail": {
                    "name": "Song Guo"
                },
                "author": "Song Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.15969v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.15969v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12889v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12889v1",
                "updated": "2025-10-14T18:04:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:04:00Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    4,
                    0,
                    1,
                    287,
                    0
                ],
                "title": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dodoor: Efficient Randomized Decentralized Scheduling with Load Caching\n  for Heterogeneous Tasks and Clusters"
                },
                "summary": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces Dodoor, an efficient randomized decentralized scheduler\ndesigned for task scheduling in modern data centers. Dodoor leverages advanced\nresearch on the weighted balls-into-bins model with b-batched setting. Unlike\nother decentralized schedulers that rely on real-time probing of remote\nservers, Dodoor makes scheduling decisions based on cached server information,\nwhich is updated in batches, to reduce communication overheads. To schedule\ntasks with dynamic, multidimensional resource requirements in heterogeneous\ncluster, Dodoor uses a novel load score to measure servers' loads for each\nscheduled task. This score captures the anti-affinity between servers and tasks\nin contrast to the commonly used heuristic of counting pending tasks to balance\nload. On a 101-node heterogeneous cluster, Dodoor is evaluated using two\nworkloads: (i) simulated Azure virtual machines placements and (ii) real\nserverless Python functions executions in Docker. The evaluation shows that\nDodoor reduces scheduling messages by 55--66% on both workloads. Dodoor can\nalso increase throughput by up to 33.2% and 21.5%, reduce mean makespan latency\nby 12.1% and 7.2%, and improve tail latency by 21.9% and 24.6% across the two\nworkloads."
                },
                "authors": [
                    {
                        "name": "Wei Da"
                    },
                    {
                        "name": "Evangelia Kalyvianaki"
                    }
                ],
                "author_detail": {
                    "name": "Evangelia Kalyvianaki"
                },
                "author": "Evangelia Kalyvianaki",
                "arxiv_comment": "single column,20 pages and 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12889v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12889v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12872v1",
                "updated": "2025-10-14T18:00:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T18:00:01Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    18,
                    0,
                    1,
                    1,
                    287,
                    0
                ],
                "title": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCOMM: Online Cross-context KV-cache Communication for Efficient\n  LLM-based Multi-agent Systems"
                },
                "summary": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-agent large language model (LLM) systems are increasingly adopted for\ncomplex language processing tasks that require communication and coordination\namong agents. However, these systems often suffer substantial overhead from\nrepeated reprocessing of overlapping contexts across agents. In typical\npipelines, once an agent receives a message from its predecessor, the full\ncontext-including prior turns-must be reprocessed from scratch, leading to\ninefficient processing. While key-value (KV) caching is an effective solution\nfor avoiding redundant computation in single-agent settings where prefixes\nremain unchanged, it cannot be directly reused in multi-agent scenarios due to\ndiverging prefixes introduced by agent-specific context extensions. We identify\nthat the core challenge lies in the offset variance of KV-caches across agents.\nTo address this, we propose KVCOMM, a training-free framework that enables\nefficient prefilling in multi-agent inference by reusing KV-caches and aligning\ncache offsets of overlapping contexts under diverse prefix contexts. KVCOMM\nestimates and adjusts KV-caches for shared content by referencing a pool of\ncached examples-termed anchors-that store observed cache deviations under\nvarying prefixes. The anchor pool is maintained and updated online, allowing\ndynamic adaptation to distinct user requests and context structures. KVCOMM\nachieves over 70% reuse rate across diverse multi-agent workloads, including\nretrieval-augmented generation, math reasoning, and collaborative coding tasks,\nall without quality degradation. Particularly, when each fully-connected agent\nreceives 1K input tokens with 512 prefix tokens and 512 output tokens under a\nfive-agent setting, KVCOMM achieves up to 7.8x speedup compared to the standard\nprefill pipeline, reducing TTFT from ~430 ms to ~55 ms."
                },
                "authors": [
                    {
                        "name": "Hancheng Ye"
                    },
                    {
                        "name": "Zhengqi Gao"
                    },
                    {
                        "name": "Mingyuan Ma"
                    },
                    {
                        "name": "Qinsi Wang"
                    },
                    {
                        "name": "Yuzhe Fu"
                    },
                    {
                        "name": "Ming-Yu Chung"
                    },
                    {
                        "name": "Yueqian Lin"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Jianyi Zhang"
                    },
                    {
                        "name": "Danyang Zhuo"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "Accepted for publication in NeurIPS2025. Code is available at\n  \\url{https://github.com/HankYe/KVCOMM}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12705v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12705v1",
                "updated": "2025-10-14T16:39:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T16:39:29Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    39,
                    29,
                    1,
                    287,
                    0
                ],
                "title": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A GPU-resident Memory-Aware Algorithm for Accelerating Bidiagonalization\n  of Banded Matrices"
                },
                "summary": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reduction of a banded matrix to a bidiagonal form is a crucial step in\nthe Singular Value Decomposition (SVD), a cornerstone of scientific computing\nand AI. Despite being a highly parallel algorithm, it was previously believed\nto be unsuitable for GPU computation because it is memory bandwidth-bound.\nRecent developments in GPU hardware, including larger L1 memory per Streaming\nMultiprocessor/Compute Unit, have changed that. We present the first GPU\nalgorithm for reducing a banded matrix to bidiagonal form as part of the\nNextLA$.$jl open-source software package. Our algorithm is based on previous\nCPU-based multicore parallel cache-efficient bulge chasing algorithms and\nadapted to optimize for GPU throughput. We leverage Julia Language's Array\nabstractions and KernelAbstractions to implement a single hardware- and data\nprecision-agnostic function on NVIDIA, AMD, Intel, and Apple Metal GPUs for\nhalf, single, and double precision, and examine performance optimization across\nhardware architectures and data precision. We also develop a hardware-aware\nperformance model and identify key hyperparameters, such as inner tilewidth and\nblock concurrency, that govern optimal GPU execution for bandwidth-bound\nworkloads. We demonstrate highly parallel bandwidth-bound algorithm on the GPU\ncan outperform CPU-based implementations: the GPU algorithm outperforms\nmultithreaded CPU High-Performance libraries PLASMA and SLATE as of matrix size\n1024 x 1024 and by a factor over 100 for matrices of 32k x 32k. In addition,\nthe performance of the algorithm increases linearly with matrix bandwidth size,\nmaking faster reduction of larger matrix bandwidths now also possible. With\nthis work, we break memory bandwidth barriers, as well as matrix bandwidth\nbarriers, resulting in orders-of-magnitude faster algorithms for the reduction\nof banded matrices to bidiagonal form on the GPU."
                },
                "authors": [
                    {
                        "name": "Evelyne Ringoot"
                    },
                    {
                        "name": "Rabab Alomairy"
                    },
                    {
                        "name": "Alan Edelman"
                    }
                ],
                "author_detail": {
                    "name": "Alan Edelman"
                },
                "author": "Alan Edelman",
                "arxiv_comment": "13 pages, 7 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12705v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12705v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v3",
                "updated": "2025-10-14T16:05:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    16,
                    5,
                    11,
                    1,
                    287,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "arxiv_comment": "Accepted to NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.00299v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.00299v5",
                "updated": "2025-10-14T15:42:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    42,
                    41,
                    1,
                    287,
                    0
                ],
                "published": "2025-02-01T03:49:47Z",
                "published_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    49,
                    47,
                    5,
                    32,
                    0
                ],
                "title": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChunkKV: Semantic-Preserving KV Cache Compression for Efficient\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require significant GPU memory when processing\nlong texts, with the key value (KV) cache consuming up to 70\\% of total memory\nduring inference. Although existing compression methods reduce memory by\nevaluating the importance of individual tokens, they overlook critical semantic\nrelationships between tokens, resulting in fragmented context and degraded\nperformance. We introduce ChunkKV, which fundamentally reimagines KV cache\ncompression by treating semantic chunks - rather than isolated tokens - as\nbasic compression units. This approach preserves complete linguistic structures\nand contextual integrity, ensuring that essential meaning is retained even\nunder aggressive compression. Our innovation includes a novel layer-wise index\nreuse technique that exploits the higher cross-layer similarity of preserved\nindices in ChunkKV, reducing computational overhead and improving throughput by\n26.5\\%. Comprehensive evaluations on challenging benchmarks: LongBench,\nNeedle-In-A-HayStack, GSM8K, and JailbreakV demonstrate that ChunkKV\noutperforms state-of-the-art methods by up to 8.7\\% in precision while\nmaintaining the same compression ratio. These results confirm that\nsemantic-aware compression significantly enhances both efficiency and\nperformance for long-context LLM inference, providing a simple yet effective\nsolution to the memory bottleneck problem. The code is available at\n\\href{https://github.com/NVIDIA/kvpress}{link}."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Zhenheng Tang"
                    },
                    {
                        "name": "Peijie Dong"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Yue Liu"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.00299v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.00299v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12642v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12642v1",
                "updated": "2025-10-14T15:34:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:34:35Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    34,
                    35,
                    1,
                    287,
                    0
                ],
                "title": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aixel: A Unified, Adaptive and Extensible System for AI-powered Data\n  Analysis"
                },
                "summary": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A growing trend in modern data analysis is the integration of data management\nwith learning, guided by accuracy, latency, and cost requirements. In practice,\napplications draw data of different formats from many sources. In the\nmeanwhile, the objectives and budgets change over time. Existing systems handle\nthese applications across databases, analysis libraries, and tuning services.\nSuch fragmentation leads to complex user interaction, limited adaptability,\nsuboptimal performance, and poor extensibility across components. To address\nthese challenges, we present Aixel, a unified, adaptive, and extensible system\nfor AI-powered data analysis. The system organizes work across four layers:\napplication, task, model, and data. The task layer provides a declarative\ninterface to capture user intent, which is parsed into an executable operator\nplan. An optimizer compiles and schedules this plan to meet specified goals in\naccuracy, latency, and cost. The task layer coordinates the execution of data\nand model operators, with built-in support for reuse and caching to improve\nefficiency. The model layer offers versioned storage for index, metadata,\ntensors, and model artifacts. It supports adaptive construction, task-aligned\ndrift detection, and safe updates that reuse shared components. The data layer\nprovides unified data management capabilities, including indexing,\nconstraint-aware discovery, task-aligned selection, and comprehensive feature\nmanagement. With the above designed layers, Aixel delivers a user friendly,\nadaptive, efficient, and extensible system."
                },
                "authors": [
                    {
                        "name": "Meihui Zhang"
                    },
                    {
                        "name": "Liming Wang"
                    },
                    {
                        "name": "Chi Zhang"
                    },
                    {
                        "name": "Zhaojing Luo"
                    }
                ],
                "author_detail": {
                    "name": "Zhaojing Luo"
                },
                "author": "Zhaojing Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12642v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12642v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12629v1",
                "updated": "2025-10-14T15:26:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T15:26:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    15,
                    26,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noisy Neighbor: Exploiting RDMA for Resource Exhaustion Attacks in\n  Containerized Clouds"
                },
                "summary": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern containerized cloud environments, the adoption of RDMA (Remote\nDirect Memory Access) has expanded to reduce CPU overhead and enable\nhigh-performance data exchange. Achieving this requires strong performance\nisolation to ensure that one container's RDMA workload does not degrade the\nperformance of others, thereby maintaining critical security assurances.\nHowever, existing isolation techniques are difficult to apply effectively due\nto the complexity of microarchitectural resource management within RDMA NICs\n(RNICs). This paper experimentally analyzes two types of resource exhaustion\nattacks on NVIDIA BlueField-3: (i) state saturation attacks and (ii) pipeline\nsaturation attacks. Our results show that state saturation attacks can cause up\nto a 93.9% loss in bandwidth, a 1,117x increase in latency, and a 115% rise in\ncache misses for victim containers, while pipeline saturation attacks lead to\nsevere link-level congestion and significant amplification, where small verb\nrequests result in disproportionately high resource consumption. To mitigate\nthese threats and restore predictable security assurances, we propose HT-Verbs,\na threshold-driven framework based on real-time per-container RDMA verb\ntelemetry and adaptive resource classification that partitions RNIC resources\ninto hot, warm, and cold tiers and throttles abusive workloads without\nrequiring hardware modifications."
                },
                "authors": [
                    {
                        "name": "Gunwoo Kim"
                    },
                    {
                        "name": "Taejune Park"
                    },
                    {
                        "name": "Jinwoo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Jinwoo Kim"
                },
                "author": "Jinwoo Kim",
                "arxiv_comment": "20 pages, 14 figures, presented at the 4th International Workshop on\n  System Security Assurance (SecAssure 2025), co-located with ESORICS 2025, to\n  appear in Springer LNCS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12280v1",
                "updated": "2025-10-14T08:34:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T08:34:09Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    8,
                    34,
                    9,
                    1,
                    287,
                    0
                ],
                "title": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Analysis and Evaluation of Using Microsecond-Latency Memory for\n  In-Memory Indices and Caches in SSD-Based Key-Value Stores"
                },
                "summary": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When key-value (KV) stores use SSDs for storing a large number of items,\noftentimes they also require large in-memory data structures including indices\nand caches to be traversed to reduce IOs. This paper considers offloading most\nof such data structures from the costly host DRAM to secondary memory whose\nlatency is in the microsecond range, an order of magnitude longer than those of\ncurrently available DIMM-mounted or CXL memory devices. While emerging\nmicrosecond-latency memory is likely to cost much less than DRAM, it can\nsignificantly slow down SSD-based KV stores if naively employed. This paper\nanalyzes and evaluates the impact of microsecond-level memory latency on the KV\noperation throughput. Our analysis finds that a well-known latency-hiding\ntechnique of software prefetching for long-latency memory from user-level\nthreads is effective. The novelty of our analysis lies in modeling how the\ninterplay between prefetching and IO affects performance, from which we derive\nan equation that well explains the throughput degradation due to long memory\nlatency. The model tells us that the presence of IO significantly enhances the\ntolerance to memory latency, leading to a finding that SSD-based KV stores can\nbe made latency-tolerant without devising new techniques for\nmicrosecond-latency memory. To confirm this, we design a microbenchmark as well\nas modify existing SSD-based KV stores so that they issue prefetches from\nuser-level threads, and run them while placing most of in-memory data\nstructures on FPGA-based memory with adjustable microsecond latency. The\nresults demonstrate that their KV operation throughputs can be well explained\nby our model, and the modified KV stores achieve near-DRAM throughputs for up\nto a memory latency of 5 microseconds. This suggests the possibility that\nSSD-based KV stores can use microsecond-latency memory as a cost-effective\nalternative to the host DRAM."
                },
                "authors": [
                    {
                        "name": "Yosuke Bando"
                    },
                    {
                        "name": "Akinobu Mita"
                    },
                    {
                        "name": "Kazuhiro Hiwada"
                    },
                    {
                        "name": "Shintaro Sano"
                    },
                    {
                        "name": "Tomoya Suzuki"
                    },
                    {
                        "name": "Yu Nakanishi"
                    },
                    {
                        "name": "Kazutaka Tomida"
                    },
                    {
                        "name": "Hirotsugu Kajihara"
                    },
                    {
                        "name": "Akiyuki Kaneko"
                    },
                    {
                        "name": "Daisuke Taki"
                    },
                    {
                        "name": "Yukimasa Miyamoto"
                    },
                    {
                        "name": "Tomokazu Yoshida"
                    },
                    {
                        "name": "Tatsuo Shiozawa"
                    }
                ],
                "author_detail": {
                    "name": "Tatsuo Shiozawa"
                },
                "author": "Tatsuo Shiozawa",
                "arxiv_doi": "10.1145/3769759",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769759",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.12280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proc. ACM Manag. Data 3, 6 (SIGMOD), Article 294 (December 2025),\n  28 pages",
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10975v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10975v2",
                "updated": "2025-10-14T07:41:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    7,
                    41,
                    47,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T03:26:14Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    26,
                    14,
                    0,
                    286,
                    0
                ],
                "title": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RoVer: Robot Reward Model as Test-Time Verifier for\n  Vision-Language-Action Model"
                },
                "summary": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models have become a prominent paradigm for\nembodied intelligence, yet further performance improvements typically rely on\nscaling up training data and model size -- an approach that is prohibitively\nexpensive for robotics and fundamentally limited by data collection costs. We\naddress this limitation with $\\mathbf{RoVer}$, an embodied test-time scaling\nframework that uses a $\\mathbf{Ro}$bot Process Reward Model (PRM) as a\nTest-Time $\\mathbf{Ver}$ifier to enhance the capabilities of existing VLA\nmodels without modifying their architectures or weights. Specifically, RoVer\n(i) assigns scalar-based process rewards to evaluate the reliability of\ncandidate actions, and (ii) predicts an action-space direction for candidate\nexpansion/refinement. During inference, RoVer generates multiple candidate\nactions concurrently from the base policy, expands them along PRM-predicted\ndirections, and then scores all candidates with PRM to select the optimal\naction for execution. Notably, by caching shared perception features, it can\namortize perception cost and evaluate more candidates under the same test-time\ncomputational budget. Essentially, our approach effectively transforms\navailable computing resources into better action decision-making, realizing the\nbenefits of test-time scaling without extra training overhead. Our\ncontributions are threefold: (1) a general, plug-and-play test-time scaling\nframework for VLAs; (2) a PRM that jointly provides scalar process rewards and\nan action-space direction to guide exploration; and (3) an efficient\ndirection-guided sampling strategy that leverages a shared perception cache to\nenable scalable candidate generation and selection during inference."
                },
                "authors": [
                    {
                        "name": "Mingtong Dai"
                    },
                    {
                        "name": "Lingbo Liu"
                    },
                    {
                        "name": "Yongjie Bai"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhouxia Wang"
                    },
                    {
                        "name": "Rui SU"
                    },
                    {
                        "name": "Chunjie Chen"
                    },
                    {
                        "name": "Liang Lin"
                    },
                    {
                        "name": "Xinyu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Xinyu Wu"
                },
                "author": "Xinyu Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10975v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10975v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11496v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11496v2",
                "updated": "2025-10-14T05:05:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    5,
                    5,
                    14,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-13T15:04:38Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    15,
                    4,
                    38,
                    0,
                    286,
                    0
                ],
                "title": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AndesVL Technical Report: An Efficient Mobile-side Multimodal Large\n  Language Model"
                },
                "summary": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, while cloud-based MLLMs such as QwenVL, InternVL, GPT-4o,\nGemini, and Claude Sonnet have demonstrated outstanding performance with\nenormous model sizes reaching hundreds of billions of parameters, they\nsignificantly surpass the limitations in memory, power consumption, and\ncomputing capacity of edge devices such as mobile phones. This paper introduces\nAndesVL, a suite of mobile-side MLLMs with 0.6B to 4B parameters based on\nQwen3's LLM and various visual encoders. We comprehensively outline the model\narchitectures, training pipeline, and training data of AndesVL, which achieves\nfirst-tier performance across a wide range of open-source benchmarks, including\nfields such as text-rich image understanding, reasoning and math, multi-image\ncomprehension, general VQA, hallucination mitigation, multilingual\nunderstanding, and GUI-related tasks when compared with state-of-the-art models\nof a similar scale. Furthermore, we introduce a 1+N LoRA architecture alongside\na Quantization-Aware LoRA Fine-Tuning (QALFT) framework to facilitate efficient\ntask adaptation and model compression during mobile-side deployment of AndesVL.\nMoreover, utilizing our cache eviction algorithm -- OKV -- along with\ncustomized speculative decoding and compression strategies, we achieve a 6.7x\npeak decoding speedup ratio, up to 30.9% memory reduction, and 1.8\nbits-per-weight when deploying AndesVL-4B on MediaTek Dimensity 9500 chips. We\nrelease all models on https://huggingface.co/OPPOer."
                },
                "authors": [
                    {
                        "name": "Zhiwei Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Nan Wang"
                    },
                    {
                        "name": "Yafei Liu"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Ruichen Wang"
                    },
                    {
                        "name": "Zhihao Li"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Long Cheng"
                    },
                    {
                        "name": "Dongze Hao"
                    },
                    {
                        "name": "Quanlong Zheng"
                    },
                    {
                        "name": "Yanhao Zhang"
                    },
                    {
                        "name": "Haobo Ji"
                    },
                    {
                        "name": "Jian Ma"
                    },
                    {
                        "name": "Zhitong Zheng"
                    },
                    {
                        "name": "Zhenyi Lin"
                    },
                    {
                        "name": "Haolin Deng"
                    },
                    {
                        "name": "Xin Zou"
                    },
                    {
                        "name": "Xiaojie Yin"
                    },
                    {
                        "name": "Ruilin Wang"
                    },
                    {
                        "name": "Liankai Cai"
                    },
                    {
                        "name": "Haijing Liu"
                    },
                    {
                        "name": "Yuqing Qiu"
                    },
                    {
                        "name": "Ke Chen"
                    },
                    {
                        "name": "Zixian Li"
                    },
                    {
                        "name": "Chi Xie"
                    },
                    {
                        "name": "Huafei Li"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Chuangchuang Wang"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Zhiguang Zhu"
                    },
                    {
                        "name": "Kai Tang"
                    },
                    {
                        "name": "Wenmei Gao"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Jun Wu"
                    },
                    {
                        "name": "Chao Liu"
                    },
                    {
                        "name": "Qin Xie"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Haonan Lu"
                    }
                ],
                "author_detail": {
                    "name": "Haonan Lu"
                },
                "author": "Haonan Lu",
                "arxiv_comment": "Tech report of OPPO AndesVL Team",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11496v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11496v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12051v1",
                "updated": "2025-10-14T01:26:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "published": "2025-10-14T01:26:36Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    1,
                    26,
                    36,
                    1,
                    287,
                    0
                ],
                "title": "APCE: Adaptive Progressive Context Expansion for Long Context Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "APCE: Adaptive Progressive Context Expansion for Long Context Processing"
                },
                "summary": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying useful Long-Context Transformer Models (LCTMs) requires addressing\ntwo key challenges: (1) A growing memory footprint due to quadratic\nself-attention and linear KV-cache scaling in memory as sequence length\nincreases; (2) the ContextRot phenomena where empirical evidence suggests that\ntransformer architecture's performance degrades with increasing context length.\nGiven the shared dependency on the input, a natural question arises: Can we\nsurgically select the most important input chunks for processing to\nsynergistically (a) reduce the memory footprint, and (b) mitigate the\nContextRot effects? In this paper, we answer this question in the affirmative\nfor long-context summarization tasks. We propose APCE as a context-aware\nsolution to select the most important input chunks through low-dimensional\nsemantic similarity matching with the current query. By directly operating on\nthe input, APCE decouples from strict dependency on underlying hardware or CUDA\nenvironments, promising a compatible solution scalable to different deployment\nsystems. Our empirical evaluations have demonstrated superior or on-par\nsummarization performance for APCE compared to the full dense baseline using a\nfraction (50%-70%) of the input sequence resulting in KV-cache and\nself-attention memory efficiency improvements. We hope our findings inspire\nfurther research on context-aware efficiency solutions for LCTMs geared towards\nother relevant long-context tasks."
                },
                "authors": [
                    {
                        "name": "Baisub Lee"
                    },
                    {
                        "name": "Sanghyun Byun"
                    },
                    {
                        "name": "Mohanad Odema"
                    },
                    {
                        "name": "Jung Guack"
                    },
                    {
                        "name": "Jacob Song"
                    },
                    {
                        "name": "Woo Seong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Woo Seong Chung"
                },
                "author": "Woo Seong Chung",
                "arxiv_comment": "NeurIPS 2025 Workshop: ML For Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22622v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22622v2",
                "updated": "2025-10-13T22:41:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    22,
                    41,
                    26,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-26T17:48:24Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    48,
                    24,
                    4,
                    269,
                    0
                ],
                "title": "LongLive: Real-time Interactive Long Video Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LongLive: Real-time Interactive Long Video Generation"
                },
                "summary": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present LongLive, a frame-level autoregressive (AR) framework for\nreal-time and interactive long video generation. Long video generation presents\nchallenges in both efficiency and quality. Diffusion and Diffusion-Forcing\nmodels can produce high-quality videos but suffer from low efficiency due to\nbidirectional attention. Causal attention AR models support KV caching for\nfaster inference, but often degrade in quality on long videos due to memory\nchallenges during long-video training. In addition, beyond static prompt-based\ngeneration, interactive capabilities, such as streaming prompt inputs, are\ncritical for dynamic content creation, enabling users to guide narratives in\nreal time. This interactive requirement significantly increases complexity,\nespecially in ensuring visual consistency and semantic coherence during prompt\ntransitions. To address these challenges, LongLive adopts a causal, frame-level\nAR design that integrates a KV-recache mechanism that refreshes cached states\nwith new prompts for smooth, adherent switches; streaming long tuning to enable\nlong video training and to align training and inference (train-long-test-long);\nand short window attention paired with a frame-level attention sink, shorten as\nframe sink, preserving long-range consistency while enabling faster generation.\nWith these key designs, LongLive fine-tunes a 1.3B-parameter short-clip model\nto minute-long generation in just 32 GPU-days. At inference, LongLive sustains\n20.7 FPS on a single NVIDIA H100, achieves strong performance on VBench in both\nshort and long videos. LongLive supports up to 240-second videos on a single\nH100 GPU. LongLive further supports INT8-quantized inference with only marginal\nquality loss."
                },
                "authors": [
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Yicheng Xiao"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Enze Xie"
                    },
                    {
                        "name": "Yingcong Chen"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Yukang Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yukang Chen"
                },
                "author": "Yukang Chen",
                "arxiv_comment": "Code, model, and demos are available at\n  https://github.com/NVlabs/LongLive",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22622v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22622v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11938v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11938v1",
                "updated": "2025-10-13T21:01:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T21:01:40Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    21,
                    1,
                    40,
                    0,
                    286,
                    0
                ],
                "title": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexPipe: Adapting Dynamic LLM Serving Through Inflight Pipeline\n  Refactoring in Fragmented Serverless Clusters"
                },
                "summary": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Large Language Models (LLMs) in production faces significant\nchallenges from highly variable request patterns and severe resource\nfragmentation in serverless clusters. Current systems rely on static pipeline\nconfigurations that struggle to adapt to dynamic workload conditions, leading\nto substantial inefficiencies. We present FlexPipe, a novel system that\ndynamically reconfigures pipeline architectures during runtime to address these\nfundamental limitations. FlexPipe decomposes models into fine-grained stages\nand intelligently adjusts pipeline granularity based on real-time request\npattern analysis, implementing three key innovations: fine-grained model\npartitioning with preserved computational graph constraints, inflight pipeline\nrefactoring with consistent cache transitions, and topology-aware resource\nallocation that navigates GPU fragmentation. Comprehensive evaluation on an\n82-GPU cluster demonstrates that FlexPipe achieves up to 8.5x better resource\nefficiency while maintaining 38.3% lower latency compared to state-of-the-art\nsystems, reducing GPU reservation requirements from 75% to 30% of peak\ncapacity."
                },
                "authors": [
                    {
                        "name": "Yanying Lin"
                    },
                    {
                        "name": "Shijie Peng"
                    },
                    {
                        "name": "Chengzhi Lu"
                    },
                    {
                        "name": "Chengzhong Xu"
                    },
                    {
                        "name": "Kejiang Ye"
                    }
                ],
                "author_detail": {
                    "name": "Kejiang Ye"
                },
                "author": "Kejiang Ye",
                "arxiv_doi": "10.1145/3767295.3769316",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3767295.3769316",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.11938v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11938v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "EuroSys 26",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11857v2",
                "updated": "2025-10-13T20:40:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    20,
                    40,
                    32,
                    0,
                    286,
                    0
                ],
                "published": "2024-10-04T15:23:28Z",
                "published_parsed": [
                    2024,
                    10,
                    4,
                    15,
                    23,
                    28,
                    4,
                    278,
                    0
                ],
                "title": "LLMBridge: Reducing Costs in a Prompt-Centric Internet",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMBridge: Reducing Costs in a Prompt-Centric Internet"
                },
                "summary": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's Internet infrastructure is centered around content retrieval over\nHTTP, with middleboxes (e.g., HTTP proxies) playing a crucial role in\nperformance, security, and cost-effectiveness. We envision a future where\nInternet communication will be dominated by \"prompts\" sent to generative AI\nmodels. For this, we will need proxies that provide similar functions to HTTP\nproxies (e.g., caching, routing, compression) while dealing with unique\nchallenges and opportunities of prompt-based communication. As a first step\ntoward supporting prompt-based communication, we present LLMBridge, an LLM\nproxy designed for cost-conscious users, such as those in developing regions\nand education (e.g., students, instructors). LLMBridge supports three key\noptimizations: model selection (routing prompts to the most suitable model),\ncontext management (intelligently reducing the amount of context), and semantic\ncaching (serving prompts using local models and vector databases). These\noptimizations introduce trade-offs between cost and quality, which applications\nnavigate through a high-level, bidirectional interface. As case studies, we\ndeploy LLMBridge in two cost-sensitive settings: a WhatsApp-based Q&A service\nand a university classroom environment. The WhatsApp service has been live for\nover twelve months, serving 100+ users and handling more than 14.7K requests.\nIn parallel, we exposed LLMBridge to students across three computer science\ncourses over a semester, where it supported diverse LLM-powered applications -\nsuch as reasoning agents and chatbots - and handled an average of 500 requests\nper day. We report on deployment experiences across both settings and use the\ncollected workloads to benchmark the effectiveness of various cost-optimization\nstrategies, analyzing their trade-offs in cost, latency, and response quality."
                },
                "authors": [
                    {
                        "name": "Noah Martin"
                    },
                    {
                        "name": "Abdullah Bin Faisal"
                    },
                    {
                        "name": "Hiba Eltigani"
                    },
                    {
                        "name": "Rukhshan Haroon"
                    },
                    {
                        "name": "Swaminathan Lamelas"
                    },
                    {
                        "name": "Fahad Dogar"
                    }
                ],
                "author_detail": {
                    "name": "Fahad Dogar"
                },
                "author": "Fahad Dogar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v3",
                "updated": "2025-10-13T17:15:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    17,
                    15,
                    14,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Lingxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17238v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17238v2",
                "updated": "2025-10-13T16:48:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    16,
                    48,
                    37,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-21T21:05:29Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    21,
                    5,
                    29,
                    6,
                    264,
                    0
                ],
                "title": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoEs Are Stronger than You Think: Hyper-Parallel Inference Scaling with\n  RoE"
                },
                "summary": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The generation quality of large language models (LLMs) is often improved by\nutilizing inference-time sequence-level scaling methods (e.g.,\nChain-of-Thought). We introduce hyper-parallel scaling, a complementary\nframework that improves prediction quality at the token level. Hyper-parallel\nscaling computes and aggregates multiple output proposals for a single token\nfrom the model. We implement this concept in Mixture-of-Experts (MoE) models,\nwhich we refer to as Roster of Experts (RoE). RoE is a training-free inference\nalgorithm that turns a single MoE into a dynamic ensemble of MoEs. RoE injects\ncontrolled stochasticity into the expert routing mechanism, enabling it to\nsample multiple diverse experts for each token and aggregate their outputs for\na more accurate final prediction. To overcome the computational cost, we\nintroduce an efficient batching strategy and a specialized KV-caching mechanism\nthat minimizes compute and memory overhead. For example, RoE enables a 7B MoE\nmodel to match the performance of a 10.5B MoE model while using 30% less\ncompute for inference. These gains are achieved without any fine-tuning of\nmodel parameters."
                },
                "authors": [
                    {
                        "name": "Soheil Zibakhsh"
                    },
                    {
                        "name": "Mohammad Samragh"
                    },
                    {
                        "name": "Kumari Nishu"
                    },
                    {
                        "name": "Lauren Hannah"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "arxiv_comment": "Corrected typo in arxiv abstract",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17238v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17238v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11292v1",
                "updated": "2025-10-13T11:28:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T11:28:30Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    28,
                    30,
                    0,
                    286,
                    0
                ],
                "title": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LouisKV: Efficient KV Cache Retrieval for Long Input-Output Sequences"
                },
                "summary": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Key-Value (KV) cache succeeds in reducing redundant computations in\nauto-regressive models, it introduces significant memory overhead, limiting its\npractical deployment in long-sequence scenarios. Existing KV retrieval methods\nmitigate this by dynamically retaining only a subset of KV entries on the GPU.\nHowever, they still suffer from notable efficiency and accuracy bottlenecks due\nto per-token retrieval and coarse-grained page-level KV management, especially\nin long-output reasoning scenarios. With the emergence of large reasoning\nmodels, efficiently handling such scenarios has become increasingly important.\nTo address this issue, we present two key observations: (1) critical KVs\nexhibit strong temporal locality during decoding, and (2) these KVs exhibit\ndistinct distribution patterns across the input prompt and generated output.\nBuilding on these observations, we propose LouisKV, an efficient KV cache\nretrieval framework designed for various long-sequence scenarios. Specifically,\nLouisKV introduces a semantic-aware retrieval strategy leveraging temporal\nlocality to trigger retrieval only at semantic boundaries, drastically reducing\ncomputation and data transfer overhead. LouisKV also designs a decoupled,\nfine-grained management scheme that tailors differentiated strategies for input\nand output sequences to create retrieval units that better match the model's\nattention patterns, enabling precise identification of critical KVs.\nFurthermore, to boost efficiency, LouisKV incorporates several kernel-level\noptimizations, including custom Triton and CUDA kernels to accelerate the KV\nclustering and retrieval. Evaluations show that LouisKV achieves up to\n4.7$\\times$ speedup over state-of-the-art KV retrieval methods while\nmaintaining near-lossless accuracy across diverse long-sequence tasks,\nincluding long-input short-output, short-input long-output, and long-input\nlong-output scenarios."
                },
                "authors": [
                    {
                        "name": "Wenbo Wu"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Xiurui Pan"
                    },
                    {
                        "name": "Ye Wang"
                    },
                    {
                        "name": "Jie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jie Zhang"
                },
                "author": "Jie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.21725v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.21725v2",
                "updated": "2025-10-13T11:21:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    11,
                    21,
                    0,
                    0,
                    286,
                    0
                ],
                "published": "2025-03-27T17:37:12Z",
                "published_parsed": [
                    2025,
                    3,
                    27,
                    17,
                    37,
                    12,
                    3,
                    86,
                    0
                ],
                "title": "Low-noise environment for probing fundamental symmetries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-noise environment for probing fundamental symmetries"
                },
                "summary": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the design and characterization of a low-noise environment for\nmeasuring the electron's electric dipole moment (EDM) with a beam of molecules.\nTo minimize magnetic Johnson noise from metals, the design features ceramic\nelectric field plates housed in a glass vacuum chamber. To suppress external\nmagnetic noise the apparatus is enclosed within a cylindrical four-layer\nmu-metal shield with a shielding factor exceeding $10^6$ in one radial\ndirection and $10^5$ in the other. Finite element modelling shows that the\ndifference between these shielding factors is due to imperfect joints between\nsections of mu-metal. Using atomic magnetometers to monitor the magnetic field\ninside the shield, we measure noise below 40 fT/$\\sqrt{{\\rm Hz}}$ at 1 Hz and\nabove, rising to 500 fT/$\\sqrt{{\\rm Hz}}$ at 0.1 Hz. Analytical and numerical\nstudies show that residual magnetic Johnson noise contributes approximately 13\nfT/$\\sqrt{{\\rm Hz}}$. The background magnetic field averaged along the beamline\nis maintained below 3 pT, with typical gradients of a few nT/m. An electric\nfield of 20 kV/cm is applied without discharges and with leakage currents below\n1 nA. Each magnetometer measures the magnetic field correlated with the\ndirection of the applied electric field with a precision of 0.11 fT in 104\nhours of data. These results demonstrate that the apparatus is suitable for\nmeasuring the electron EDM with precision at the $10^{-31}$ e cm level. The\ndesign principles and characterization techniques presented here are broadly\napplicable to precision measurements probing fundamental symmetries in\nmolecules, atoms, and neutrons."
                },
                "authors": [
                    {
                        "name": "F. J. Collings"
                    },
                    {
                        "name": "N. J. Fitch"
                    },
                    {
                        "name": "R. A. Jenkins"
                    },
                    {
                        "name": "J. M. Dyne"
                    },
                    {
                        "name": "E. Wursten"
                    },
                    {
                        "name": "M. T. Ziemba"
                    },
                    {
                        "name": "X. S. Zheng"
                    },
                    {
                        "name": "F. Castellini"
                    },
                    {
                        "name": "J. Lim"
                    },
                    {
                        "name": "B. E. Sauer"
                    },
                    {
                        "name": "M. R. Tarbutt"
                    }
                ],
                "author_detail": {
                    "name": "M. R. Tarbutt"
                },
                "author": "M. R. Tarbutt",
                "arxiv_doi": "10.1088/1367-2630/ae0ea7",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1088/1367-2630/ae0ea7",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.21725v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.21725v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Added a figure, minor changes to text",
                "arxiv_primary_category": {
                    "term": "physics.atom-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.atom-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08666v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08666v2",
                "updated": "2025-10-13T10:39:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    39,
                    59,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-09T16:19:42Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    16,
                    19,
                    42,
                    3,
                    282,
                    0
                ],
                "title": "dInfer: An Efficient Inference Framework for Diffusion Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "dInfer: An Efficient Inference Framework for Diffusion Language Models"
                },
                "summary": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based large language models (dLLMs) have emerged as a promising\nalternative to autoregressive (AR) LLMs, leveraging denoising-based generation\nto enable inherent parallelism. Even more and more open-sourced dLLM models\nemerge, yet their widespread adoption remains constrained by the lack of a\nstandardized and efficient inference framework. We present dInfer, an efficient\nand extensible framework for dLLM inference. dInfer decomposes the inference\npipeline into four modular components--model, diffusion iteration manager,\ndecoding strategy, and KV-cache manager--and integrates novel algorithms for\neach component alongside system-level optimizations. Through this combination\nof algorithmic innovations and system enhancements, dInfer achieves substantial\nefficiency gains without compromising output quality on LLaDA-MoE. At batch\nsize 1, it surpasses 1,100 tokens per second on HumanEval and averages over 800\ntokens per second across six benchmarks on $8\\times$ H800 GPUs. Compared to\nprior systems, dInfer delivers a $10\\times$ speedup over Fast-dLLM while\nmaintaining similar model performance. Even compared to the AR model (with a\ncomparable number of activation parameters and performance) QWen2.5-3B, which\nis highly optimized with the latest vLLM inference engine, dInfer still\ndelivers a $2$-$3\\times$ speedup. The implementation of dInfer is open-sourced\nat https://github.com/inclusionAI/dInfer."
                },
                "authors": [
                    {
                        "name": "Yuxin Ma"
                    },
                    {
                        "name": "Lun Du"
                    },
                    {
                        "name": "Lanning Wei"
                    },
                    {
                        "name": "Kun Chen"
                    },
                    {
                        "name": "Qian Xu"
                    },
                    {
                        "name": "Kangyu Wang"
                    },
                    {
                        "name": "Guofeng Feng"
                    },
                    {
                        "name": "Guoshan Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Xiaojing Qi"
                    },
                    {
                        "name": "Xinyuan Zhang"
                    },
                    {
                        "name": "Zhen Tao"
                    },
                    {
                        "name": "Haibo Feng"
                    },
                    {
                        "name": "Ziyun Jiang"
                    },
                    {
                        "name": "Ying Xu"
                    },
                    {
                        "name": "Zenan Huang"
                    },
                    {
                        "name": "Yihong Zhuang"
                    },
                    {
                        "name": "Haokai Xu"
                    },
                    {
                        "name": "Jiaqi Hu"
                    },
                    {
                        "name": "Zhenzhong Lan"
                    },
                    {
                        "name": "Junbo Zhao"
                    },
                    {
                        "name": "Jianguo Li"
                    },
                    {
                        "name": "Da Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Da Zheng"
                },
                "author": "Da Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08666v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08666v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19257v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19257v2",
                "updated": "2025-10-13T10:18:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    18,
                    34,
                    0,
                    286,
                    0
                ],
                "published": "2025-08-15T12:03:34Z",
                "published_parsed": [
                    2025,
                    8,
                    15,
                    12,
                    3,
                    34,
                    4,
                    227,
                    0
                ],
                "title": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TTF-VLA: Temporal Token Fusion via Pixel-Attention Integration for\n  Vision-Language-Action Models"
                },
                "summary": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language-Action (VLA) models process visual inputs independently at\neach timestep, discarding valuable temporal information inherent in robotic\nmanipulation tasks. This frame-by-frame processing makes models vulnerable to\nvisual noise while ignoring the substantial coherence between consecutive\nframes in manipulation sequences. We propose Temporal Token Fusion (TTF), a\ntraining-free approach that intelligently integrates historical and current\nvisual representations to enhance VLA inference quality. Our method employs\ndual-dimension detection combining efficient grayscale pixel difference\nanalysis with attention-based semantic relevance assessment, enabling selective\ntemporal token fusion through hard fusion strategies and keyframe anchoring to\nprevent error accumulation. Comprehensive experiments across LIBERO,\nSimplerEnv, and real robot tasks demonstrate consistent improvements: 4.0\npercentage points average on LIBERO (72.4\\% vs 68.4\\% baseline),\ncross-environment validation on SimplerEnv (4.8\\% relative improvement), and\n8.7\\% relative improvement on real robot tasks. Our approach proves\nmodel-agnostic, working across OpenVLA and VLA-Cache architectures. Notably,\nTTF reveals that selective Query matrix reuse in attention mechanisms enhances\nrather than compromises performance, suggesting promising directions for direct\nKQV matrix reuse strategies that achieve computational acceleration while\nimproving task success rates."
                },
                "authors": [
                    {
                        "name": "Chenghao Liu"
                    },
                    {
                        "name": "Jiachen Zhang"
                    },
                    {
                        "name": "Chengxuan Li"
                    },
                    {
                        "name": "Zhimu Zhou"
                    },
                    {
                        "name": "Shixin Wu"
                    },
                    {
                        "name": "Songfang Huang"
                    },
                    {
                        "name": "Huiling Duan"
                    }
                ],
                "author_detail": {
                    "name": "Huiling Duan"
                },
                "author": "Huiling Duan",
                "arxiv_comment": "Manuscript submitted to AAAI 2026, currently under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19257v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19257v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11236v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11236v1",
                "updated": "2025-10-13T10:17:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T10:17:21Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    10,
                    17,
                    21,
                    0,
                    286,
                    0
                ],
                "title": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XQuant: Achieving Ultra-Low Bit KV Cache Quantization with Cross-Layer\n  Compression"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\ndiverse natural language processing tasks. However, their extensive memory\nrequirements, particularly due to KV cache growth during long-text\nunderstanding and generation, present significant challenges for deployment in\nresource-constrained environments. Quantization has emerged as a promising\nsolution to reduce memory consumption while preserving historical information.\nWe propose XQuant, a training-free and plug-and-play framework that achieves\nultra-low equivalent bit-width KV cache quantization. XQuant introduces two key\ninnovations: a computationally negligible data-free calibration method and\ncross-layer KV cache compression, enabling quantization to sub-1.4 bits.\nExtensive experiments on TruthfulQA and LongBench demonstrate that XQuant\noutperforms state-of-the-art methods (e.g., KIVI-2bit and AsymKV-1.5bit) by\nachieving lower bit-width while maintaining superior performance, establishing\na better trade-off between memory efficiency and model accuracy."
                },
                "authors": [
                    {
                        "name": "Haoqi Yang"
                    },
                    {
                        "name": "Yao Yao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Baoyuan Qi"
                    },
                    {
                        "name": "Guoming Liu"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "To be published in The 2025 Conference on Empirical Methods in\n  Natural Language Processing (EMNLP 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11236v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11236v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24695v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24695v2",
                "updated": "2025-10-13T09:12:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    9,
                    12,
                    27,
                    0,
                    286,
                    0
                ],
                "published": "2025-09-29T12:28:09Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    12,
                    28,
                    9,
                    0,
                    272,
                    0
                ],
                "title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SANA-Video: Efficient Video Generation with Block Linear Diffusion\n  Transformer"
                },
                "summary": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce SANA-Video, a small diffusion model that can efficiently\ngenerate videos up to 720x1280 resolution and minute-length duration.\nSANA-Video synthesizes high-resolution, high-quality and long videos with\nstrong text-video alignment at a remarkably fast speed, deployable on RTX 5090\nGPU. Two core designs ensure our efficient, effective and long video\ngeneration: (1) Linear DiT: We leverage linear attention as the core operation,\nwhich is more efficient than vanilla attention given the large number of tokens\nprocessed in video generation. (2) Constant-Memory KV cache for Block Linear\nAttention: we design block-wise autoregressive approach for long video\ngeneration by employing a constant-memory state, derived from the cumulative\nproperties of linear attention. This KV cache provides the Linear DiT with\nglobal context at a fixed memory cost, eliminating the need for a traditional\nKV cache and enabling efficient, minute-long video generation. In addition, we\nexplore effective data filters and model training strategies, narrowing the\ntraining cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of\nMovieGen. Given its low cost, SANA-Video achieves competitive performance\ncompared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B\nand SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover,\nSANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating\nthe inference speed of generating a 5-second 720p video from 71s to 29s (2.4x\nspeedup). In summary, SANA-Video enables low-cost, high-quality video\ngeneration."
                },
                "authors": [
                    {
                        "name": "Junsong Chen"
                    },
                    {
                        "name": "Yuyang Zhao"
                    },
                    {
                        "name": "Jincheng Yu"
                    },
                    {
                        "name": "Ruihang Chu"
                    },
                    {
                        "name": "Junyu Chen"
                    },
                    {
                        "name": "Shuai Yang"
                    },
                    {
                        "name": "Xianbang Wang"
                    },
                    {
                        "name": "Yicheng Pan"
                    },
                    {
                        "name": "Daquan Zhou"
                    },
                    {
                        "name": "Huan Ling"
                    },
                    {
                        "name": "Haozhe Liu"
                    },
                    {
                        "name": "Hongwei Yi"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Muyang Li"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Han Cai"
                    },
                    {
                        "name": "Sanja Fidler"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Song Han"
                    },
                    {
                        "name": "Enze Xie"
                    }
                ],
                "author_detail": {
                    "name": "Enze Xie"
                },
                "author": "Enze Xie",
                "arxiv_comment": "21 pages, 15 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24695v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24695v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08907v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08907v2",
                "updated": "2025-10-13T08:26:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    26,
                    21,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-10T01:42:14Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    1,
                    42,
                    14,
                    4,
                    283,
                    0
                ],
                "title": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoencoding-Free Context Compression for LLMs via Contextual Semantic\n  Anchors"
                },
                "summary": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context compression presents a promising approach for accelerating large\nlanguage model (LLM) inference by compressing long contexts into compact\nrepresentations. Current context compression methods predominantly rely on\nautoencoding tasks to train context-agnostic compression tokens to compress\ncontextual semantics. While autoencoding tasks enable compression tokens to\nacquire compression capabilities, compression via autoencoding tasks creates a\nfundamental mismatch: the models are optimized for reconstruction that diverge\nfrom actual downstream tasks, thereby weakening the features more beneficial\nfor real-world usage. We propose Semantic-Anchor Compression (SAC), a novel\nmethod that shifts from autoencoding task based compression to an architecture\nthat is equipped with this compression capability \\textit{a priori}. Instead of\ntraining models to compress contexts through autoencoding tasks, SAC directly\nselects so-called anchor tokens from the original context and aggregates\ncontextual information into their key-value (KV) representations. By deriving\nrepresentations directly from the contextual tokens, SAC eliminates the need\nfor autoencoding training. To ensure compression performance while directly\nleveraging anchor tokens, SAC incorporates two key designs: (1) anchor\nembeddings that enable the compressor to identify critical tokens, and (2)\nbidirectional attention modification that allows anchor tokens to capture\ninformation from the entire context. Experimental results demonstrate that SAC\nconsistently outperforms existing context compression methods across various\ncompression ratios. On out-of-distribution evaluation using MRQA, SAC achieves\n1 EM improvement at 5x compression over strong baselines, with increasing\nadvantages at higher compression ratios."
                },
                "authors": [
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Runsong Zhao"
                    },
                    {
                        "name": "Pengcheng Huang"
                    },
                    {
                        "name": "Xinyu Liu"
                    },
                    {
                        "name": "Junyi Xiao"
                    },
                    {
                        "name": "Chunyang Xiao"
                    },
                    {
                        "name": "Tong Xiao"
                    },
                    {
                        "name": "Shengxiang Gao"
                    },
                    {
                        "name": "Zhengtao Yu"
                    },
                    {
                        "name": "Jingbo Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Jingbo Zhu"
                },
                "author": "Jingbo Zhu",
                "arxiv_comment": "18 pages,9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08907v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08907v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11121v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11121v1",
                "updated": "2025-10-13T08:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T08:08:58Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    8,
                    8,
                    58,
                    0,
                    286,
                    0
                ],
                "title": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Refining Hybrid Genetic Search for CVRP via Reinforcement\n  Learning-Finetuned LLM"
                },
                "summary": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) are increasingly used as automated\nheuristic designers for vehicle routing problems (VRPs), current\nstate-of-the-art methods predominantly rely on prompting massive,\ngeneral-purpose models like GPT-4. This work challenges that paradigm by\ndemonstrating that a smaller, specialized LLM, when meticulously fine-tuned,\ncan generate components that surpass expert-crafted heuristics within advanced\nsolvers. We propose RFTHGS, a novel Reinforcement learning (RL) framework for\nFine-Tuning a small LLM to generate high-performance crossover operators for\nthe Hybrid Genetic Search (HGS) solver, applied to the Capacitated VRP (CVRP).\nOur method employs a multi-tiered, curriculum-based reward function that\nprogressively guides the LLM to master generating first compilable, then\nexecutable, and finally, superior-performing operators that exceed human expert\ndesigns. This is coupled with an operator caching mechanism that discourages\nplagiarism and promotes diversity during training. Comprehensive experiments\nshow that our fine-tuned LLM produces crossover operators which significantly\noutperform the expert-designed ones in HGS. The performance advantage remains\nconsistent, generalizing from small-scale instances to large-scale problems\nwith up to 1000 nodes. Furthermore, RFTHGS exceeds the performance of leading\nneuro-combinatorial baselines, prompt-based methods, and commercial LLMs such\nas GPT-4o and GPT-4o-mini."
                },
                "authors": [
                    {
                        "name": "Rongjie Zhu"
                    },
                    {
                        "name": "Cong Zhang"
                    },
                    {
                        "name": "Zhiguang Cao"
                    }
                ],
                "author_detail": {
                    "name": "Zhiguang Cao"
                },
                "author": "Zhiguang Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11121v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11121v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11011v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11011v1",
                "updated": "2025-10-13T05:03:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T05:03:23Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    5,
                    3,
                    23,
                    0,
                    286,
                    0
                ],
                "title": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GrASP: A Generalizable Address-based Semantic Prefetcher for Scalable\n  Transactional and Analytical Workloads"
                },
                "summary": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data prefetching--loading data into the cache before it is requested--is\nessential for reducing I/O overhead and improving database performance. While\ntraditional prefetchers focus on sequential patterns, recent learning-based\napproaches, especially those leveraging data semantics, achieve higher accuracy\nfor complex access patterns. However, these methods often struggle with today's\ndynamic, ever-growing datasets and require frequent, timely fine-tuning.\nPrivacy constraints may also restrict access to complete datasets,\nnecessitating prefetchers that can learn effectively from samples. To address\nthese challenges, we present GrASP, a learning-based prefetcher designed for\nboth analytical and transactional workloads. GrASP enhances prefetching\naccuracy and scalability by leveraging logical block address deltas and\ncombining query representations with result encodings. It frames prefetching as\na context-aware multi-label classification task, using multi-layer LSTMs to\npredict delta patterns from embedded context. This delta modeling approach\nenables GrASP to generalize predictions from small samples to larger, dynamic\ndatasets without requiring extensive retraining. Experiments on real-world\ndatasets and industrial benchmarks demonstrate that GrASP generalizes to\ndatasets 250 times larger than the training data, achieving up to 45% higher\nhit ratios, 60% lower I/O time, and 55% lower end-to-end query execution\nlatency than existing baselines. On average, GrASP attains a 91.4% hit ratio, a\n90.8% I/O time reduction, and a 57.1% execution latency reduction."
                },
                "authors": [
                    {
                        "name": "Farzaneh Zirak"
                    },
                    {
                        "name": "Farhana Choudhury"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "arxiv_comment": "This is a preprint version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11011v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11011v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10964v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10964v1",
                "updated": "2025-10-13T03:14:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T03:14:28Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    3,
                    14,
                    28,
                    0,
                    286,
                    0
                ],
                "title": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not All Bits Are Equal: Scale-Dependent Memory Optimization Strategies\n  for Reasoning Models"
                },
                "summary": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While 4-bit quantization has emerged as a memory-optimal choice for\nnon-reasoning models and zero-shot tasks across scales, we show that this\nuniversal prescription fails for reasoning models, where the KV cache rather\nthan model size can dominate memory. Through systematic experiments across\n1,700 inference scenarios on AIME25 and GPQA-Diamond, we find a scale-dependent\ntrade-off: models with an effective size below 8-bit 4B parameters achieve\nbetter accuracy by allocating memory to more weights rather than longer\ngeneration, while larger models achieve better accuracy by allocating memory to\nlonger generations. This scale threshold also determines when parallel scaling\nbecomes memory-efficient and whether KV cache eviction outperforms KV\nquantization. Our findings show that memory optimization for LLMs cannot be\nscale-agnostic, while providing principled guidelines: for small reasoning\nmodels, prioritize model capacity over test-time compute, while for larger\nones, maximize test-time compute. Our results suggest that optimizing reasoning\nmodels for deployment requires fundamentally different strategies from those\nestablished for non-reasoning models."
                },
                "authors": [
                    {
                        "name": "Junhyuck Kim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Taehong Moon"
                    },
                    {
                        "name": "Jongho Park"
                    },
                    {
                        "name": "Dimitris Papailiopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Dimitris Papailiopoulos"
                },
                "author": "Dimitris Papailiopoulos",
                "arxiv_comment": "20 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10964v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10964v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10862v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10862v1",
                "updated": "2025-10-13T00:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "published": "2025-10-13T00:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    0,
                    11,
                    2,
                    0,
                    286,
                    0
                ],
                "title": "A Joint Learning Approach to Hardware Caching and Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Joint Learning Approach to Hardware Caching and Prefetching"
                },
                "summary": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several learned policies have been proposed to replace heuristics for\nscheduling, caching, and other system components in modern systems. By\nleveraging diverse features, learning from historical trends, and predicting\nfuture behaviors, such models promise to keep pace with ever-increasing\nworkload dynamism and continuous hardware evolution. However, policies trained\nin isolation may still achieve suboptimal performance when placed together. In\nthis paper, we inspect one such instance in the domain of hardware caching --\nfor the policies of cache replacement and prefetching. We argue that these two\npolicies are bidirectionally interdependent and make the case for training the\ntwo jointly. We propose a joint learning approach based on developing shared\nrepresentations for the features used by the two policies. We present two\napproaches to develop these shared representations, one based on a joint\nencoder and another based on contrastive learning of the embeddings, and\ndemonstrate promising preliminary results for both of these. Finally, we lay\ndown an agenda for future research in this direction."
                },
                "authors": [
                    {
                        "name": "Samuel Yuan"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Nihal Sharma"
                    },
                    {
                        "name": "Aditya Akella"
                    }
                ],
                "author_detail": {
                    "name": "Aditya Akella"
                },
                "author": "Aditya Akella",
                "arxiv_comment": "Accepted at ML for Systems Workshop at the 39th Conference on Neural\n  Information Processing Systems (NeurIPS 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10862v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10862v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10858v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10858v1",
                "updated": "2025-10-12T23:46:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T23:46:04Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    46,
                    4,
                    6,
                    285,
                    0
                ],
                "title": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DriftBench: Defining and Generating Data and Query Workload Drift for\n  Benchmarking"
                },
                "summary": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data and workload drift are key to evaluating database components such as\ncaching, cardinality estimation, indexing, and query optimization. Yet,\nexisting benchmarks are static, offering little to no support for modeling\ndrift. This limitation stems from the lack of clear definitions and tools for\ngenerating data and workload drift. Motivated by this gap, we propose a unified\ntaxonomy for data and workload drift, grounded in observations from both\nacademia and industry. Building on this foundation, we introduce DriftBench, a\nlightweight and extensible framework for generating data and workload drift in\nbenchmark inputs. Together, the taxonomy and DriftBench provide a standardized\nvocabulary and mechanism for modeling and generating drift in benchmarking. We\ndemonstrate their effectiveness through case studies involving data drift,\nworkload drift, and drift-aware cardinality estimation."
                },
                "authors": [
                    {
                        "name": "Guanli Liu"
                    },
                    {
                        "name": "Renata Borovica-Gajic"
                    }
                ],
                "author_detail": {
                    "name": "Renata Borovica-Gajic"
                },
                "author": "Renata Borovica-Gajic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10858v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10858v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00313v2",
                "updated": "2025-10-12T23:17:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    23,
                    17,
                    39,
                    6,
                    285,
                    0
                ],
                "published": "2024-05-01T04:30:03Z",
                "published_parsed": [
                    2024,
                    5,
                    1,
                    4,
                    30,
                    3,
                    2,
                    122,
                    0
                ],
                "title": "Streamlining Image Editing with Layered Diffusion Brushes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streamlining Image Editing with Layered Diffusion Brushes"
                },
                "summary": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Denoising diffusion models have emerged as powerful tools for image\nmanipulation, yet interactive, localized editing workflows remain\nunderdeveloped. We introduce Layered Diffusion Brushes (LDB), a novel\ntraining-free framework that enables interactive, layer-based editing using\nstandard diffusion models. LDB defines each \"layer\" as a self-contained set of\nparameters guiding the generative process, enabling independent,\nnon-destructive, and fine-grained prompt-guided edits, even in overlapping\nregions. LDB leverages a unique intermediate latent caching approach to reduce\neach edit to only a few denoising steps, achieving 140~ms per edit on consumer\nGPUs. An editor implementing LDB, incorporating familiar layer concepts, was\nevaluated via user study and quantitative metrics. Results demonstrate LDB's\nsuperior speed alongside comparable or improved image quality, background\npreservation, and edit fidelity relative to state-of-the-art methods across\nvarious sequential image manipulation tasks. The findings highlight LDB's\nability to significantly enhance creative workflows by providing an intuitive\nand efficient approach to diffusion-based image editing and its potential for\nexpansion into related subdomains, such as video editing."
                },
                "authors": [
                    {
                        "name": "Peyman Gholami"
                    },
                    {
                        "name": "Robert Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Robert Xiao"
                },
                "author": "Robert Xiao",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2306.00219",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10587v1",
                "updated": "2025-10-12T13:06:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "published": "2025-10-12T13:06:59Z",
                "published_parsed": [
                    2025,
                    10,
                    12,
                    13,
                    6,
                    59,
                    6,
                    285,
                    0
                ],
                "title": "A Simple and Better Baseline for Visual Grounding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Simple and Better Baseline for Visual Grounding"
                },
                "summary": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual grounding aims to predict the locations of target objects specified by\ntextual descriptions. For this task with linguistic and visual modalities,\nthere is a latest research line that focuses on only selecting the\nlinguistic-relevant visual regions for object localization to reduce the\ncomputational overhead. Albeit achieving impressive performance, it is\niteratively performed on different image scales, and at every iteration,\nlinguistic features and visual features need to be stored in a cache, incurring\nextra overhead. To facilitate the implementation, in this paper, we propose a\nfeature selection-based simple yet effective baseline for visual grounding,\ncalled FSVG. Specifically, we directly encapsulate the linguistic and visual\nmodalities into an overall network architecture without complicated iterative\nprocedures, and utilize the language in parallel as guidance to facilitate the\ninteraction between linguistic modal and visual modal for extracting effective\nvisual features. Furthermore, to reduce the computational cost, during the\nvisual feature learning, we introduce a similarity-based feature selection\nmechanism to only exploit language-related visual features for faster\nprediction. Extensive experiments conducted on several benchmark datasets\ncomprehensively substantiate that the proposed FSVG achieves a better balance\nbetween accuracy and efficiency beyond the current state-of-the-art methods.\nCode is available at https://github.com/jcwang0602/FSVG."
                },
                "authors": [
                    {
                        "name": "Jingchao Wang"
                    },
                    {
                        "name": "Wenlong Zhang"
                    },
                    {
                        "name": "Dingjiang Huang"
                    },
                    {
                        "name": "Hong Wang"
                    },
                    {
                        "name": "Yefeng Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Yefeng Zheng"
                },
                "author": "Yefeng Zheng",
                "arxiv_comment": "ICME2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18809v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18809v2",
                "updated": "2025-10-12T10:09:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    10,
                    9,
                    53,
                    6,
                    285,
                    0
                ],
                "published": "2025-05-24T17:46:47Z",
                "published_parsed": [
                    2025,
                    5,
                    24,
                    17,
                    46,
                    47,
                    5,
                    144,
                    0
                ],
                "title": "VORTA: Efficient Video Diffusion via Routing Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VORTA: Efficient Video Diffusion via Routing Sparse Attention"
                },
                "summary": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video diffusion transformers have achieved remarkable progress in\nhigh-quality video generation, but remain computationally expensive due to the\nquadratic complexity of attention over high-dimensional video sequences. Recent\nacceleration methods enhance the efficiency by exploiting the local sparsity of\nattention scores; yet they often struggle with accelerating the long-range\ncomputation. To address this problem, we propose VORTA, an acceleration\nframework with two novel components: 1) a sparse attention mechanism that\nefficiently captures long-range dependencies, and 2) a routing strategy that\nadaptively replaces full 3D attention with specialized sparse attention\nvariants. VORTA achieves an end-to-end speedup $1.76\\times$ without loss of\nquality on VBench. Furthermore, it can seamlessly integrate with various other\nacceleration methods, such as model caching and step distillation, reaching up\nto speedup $14.41\\times$ with negligible performance degradation. VORTA\ndemonstrates its efficiency and enhances the practicality of video diffusion\ntransformers in real-world settings. Codes and weights are available at\nhttps://github.com/wenhao728/VORTA."
                },
                "authors": [
                    {
                        "name": "Wenhao Sun"
                    },
                    {
                        "name": "Rong-Cheng Tu"
                    },
                    {
                        "name": "Yifu Ding"
                    },
                    {
                        "name": "Zhao Jin"
                    },
                    {
                        "name": "Jingyi Liao"
                    },
                    {
                        "name": "Shunyu Liu"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "Accepted by NeurIPS 2025. The code is available at\n  https://github.com/wenhao728/VORTA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18809v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18809v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01092v2",
                "updated": "2025-10-12T04:46:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    46,
                    48,
                    6,
                    285,
                    0
                ],
                "published": "2025-09-01T03:31:44Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    3,
                    31,
                    44,
                    0,
                    244,
                    0
                ],
                "title": "REFRAG: Rethinking RAG based Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REFRAG: Rethinking RAG based Decoding"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable capabilities in\nleveraging extensive external knowledge to enhance responses in multi-turn and\nagentic applications, such as retrieval-augmented generation (RAG). However,\nprocessing long-context inputs introduces significant system latency and\ndemands substantial memory for the key-value cache, resulting in reduced\nthroughput and a fundamental trade-off between knowledge enrichment and system\nefficiency. While minimizing latency for long-context inputs is a primary\nobjective for LLMs, we contend that RAG require specialized consideration. In\nRAG, much of the LLM context consists of concatenated passages from retrieval,\nwith only a small subset directly relevant to the query. These passages often\nexhibit low semantic similarity due to diversity or deduplication during\nre-ranking, leading to block-diagonal attention patterns that differ from those\nin standard LLM generation tasks. Based on this observation, we argue that most\ncomputations over the RAG context during decoding are unnecessary and can be\neliminated with minimal impact on performance. To this end, we propose REFRAG,\nan efficient decoding framework that compresses, senses, and expands to improve\nlatency in RAG applications. By exploiting the sparsity structure, we\ndemonstrate a 30.85 the time-to-first-token acceleration (3.75 improvement to\nprevious work) without loss in perplexity. In addition, our optimization\nframework for large context enables REFRAG to extend the context size of LLMs\nby 16. We provide rigorous validation of REFRAG across diverse long-context\ntasks, including RAG, multi-turn conversations, and long document\nsummarization, spanning a wide range of datasets. Experimental results confirm\nthat REFRAG delivers substantial speedup with no loss in accuracy compared to\nLLaMA models and other state-of-the-art baselines across various context sizes."
                },
                "authors": [
                    {
                        "name": "Xiaoqiang Lin"
                    },
                    {
                        "name": "Aritra Ghosh"
                    },
                    {
                        "name": "Bryan Kian Hsiang Low"
                    },
                    {
                        "name": "Anshumali Shrivastava"
                    },
                    {
                        "name": "Vijai Mohan"
                    }
                ],
                "author_detail": {
                    "name": "Vijai Mohan"
                },
                "author": "Vijai Mohan",
                "arxiv_comment": "fix typo perplexity->log perplexity; added recent papers",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.19274v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.19274v3",
                "updated": "2025-10-12T04:04:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    12,
                    4,
                    4,
                    34,
                    6,
                    285,
                    0
                ],
                "published": "2024-10-25T03:01:19Z",
                "published_parsed": [
                    2024,
                    10,
                    25,
                    3,
                    1,
                    19,
                    4,
                    299,
                    0
                ],
                "title": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuralink: Fast LLM Inference on Smartphones with Neuron Co-Activation\n  Linking"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success across various\ndomains, yet deploying them on mobile devices remains an arduous challenge due\nto their extensive computational and memory demands. While lightweight LLMs\nhave been developed to fit mobile environments, they suffer from degraded model\naccuracy. In contrast, sparsity-based techniques minimize DRAM usage by\nselectively transferring only relevant neurons to DRAM while retaining the full\nmodel in external storage, such as flash. However, such approaches are\ncritically limited by numerous I/O operations, particularly on smartphones with\nsevere IOPS constraints.\n  In this paper, we propose Neuralink, a novel approach that accelerates LLM\ninference on smartphones by optimizing neuron placement in flash memory.\nNeuralink leverages the concept of Neuron Co-Activation, where neurons\nfrequently activated together are linked to facilitate continuous read access\nand optimize I/O efficiency. Our approach incorporates a two-stage solution: an\noffline stage that reorganizes neuron placement based on co-activation\npatterns, and an online stage that employs tailored data access and caching\nstrategies to align well with hardware characteristics. Evaluations conducted\non a variety of smartphones and LLMs demonstrate that Neuralink achieves on\naverage $1.49\\times$ improvements in end-to-end latency compared to the\nstate-of-the-art. As the first solution to optimize storage placement under\nsparsity, Neuralink explores a new optimization space at the intersection of\nsparsity-driven algorithm and storage-level system co-design for LLM inference."
                },
                "authors": [
                    {
                        "name": "Tuowei Wang"
                    },
                    {
                        "name": "Ruwen Fan"
                    },
                    {
                        "name": "Minxing Huang"
                    },
                    {
                        "name": "Zixu Hao"
                    },
                    {
                        "name": "Kun Li"
                    },
                    {
                        "name": "Ting Cao"
                    },
                    {
                        "name": "Youyou Lu"
                    },
                    {
                        "name": "Yaoxue Zhang"
                    },
                    {
                        "name": "Ju Ren"
                    }
                ],
                "author_detail": {
                    "name": "Ju Ren"
                },
                "author": "Ju Ren",
                "arxiv_doi": "10.1145/3676642.3736114",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676642.3736114",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.19274v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.19274v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Vol.\n  3, Rotterdam, Netherlands, 2025, pp. 147-162",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10290v1",
                "updated": "2025-10-11T17:08:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T17:08:45Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    17,
                    8,
                    45,
                    5,
                    284,
                    0
                ],
                "title": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Grounded AI for Code Review: Resource-Efficient Large-Model Serving in\n  Enterprise Pipelines"
                },
                "summary": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated code review adoption lags in compliance-heavy settings, where\nstatic analyzers produce high-volume, low-rationale outputs, and naive LLM use\nrisks hallucination and incurring cost overhead. We present a production system\nfor grounded, PR-native review that pairs static-analysis findings with\nAST-guided context extraction and a single-GPU, on-demand serving stack\n(quantized open-weight model, multi-tier caching) to deliver concise\nexplanations and remediation guidance. Evaluated on safety-oriented C/C++\nstandards, the approach achieves sub-minute median first-feedback (offline p50\nbuild+LLM 59.8s) while maintaining competitive violation reduction and lower\nviolation rates versus larger proprietary models. The architecture is\ndecoupled: teams can adopt the grounding/prompting layer or the serving layer\nindependently. A small internal survey (n=8) provides directional signals of\nreduced triage effort and moderate perceived grounding, with participants\nreporting fewer human review iterations. We outline operational lessons and\nlimitations, emphasizing reproducibility, auditability, and pathways to broader\nstandards and assisted patching."
                },
                "authors": [
                    {
                        "name": "Sayan Mandal"
                    },
                    {
                        "name": "Hua Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Hua Jiang"
                },
                "author": "Hua Jiang",
                "arxiv_comment": "Submitted to MLSys 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10219v1",
                "updated": "2025-10-11T13:52:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T13:52:48Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    13,
                    52,
                    48,
                    5,
                    284,
                    0
                ],
                "title": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Old is Gold: Optimizing Single-threaded Applications with Exgen-Malloc"
                },
                "summary": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory allocators hide beneath nearly every application stack, yet their\nperformance footprint extends far beyond their code size. Even small\ninefficiencies in the allocators ripple through caches and the rest of the\nmemory hierarchy, collectively imposing what operators often call a \"datacenter\ntax\". At hyperscale, even a 1% improvement in allocator efficiency can unlock\nmillions of dollars in savings and measurable reductions in datacenter energy\nconsumption. Modern memory allocators are designed to optimize allocation speed\nand memory fragmentation in multi-threaded environments, relying on complex\nmetadata and control logic to achieve high performance. However, the overhead\nintroduced by this complexity prompts a reevaluation of allocator design.\nNotably, such overhead can be avoided in single-threaded scenarios, which\ncontinue to be widely used across diverse application domains.\n  In this paper, we introduce Exgen-Malloc, a memory allocator purpose-built\nfor single-threaded applications. By specializing for single-threaded\nexecution, Exgen-Malloc eliminates unnecessary metadata, simplifies the control\nflow, thereby reducing overhead and improving allocation efficiency. Its core\ndesign features include a centralized heap, a single free-block list, and a\nbalanced strategy for memory commitment and relocation. Additionally,\nExgen-Malloc incorporates design principles in modern multi-threaded\nallocators, which do not exist in legacy single-threaded allocators such as\ndlmalloc. We evaluate Exgen-Malloc on two Intel Xeon platforms. Across both\nsystems, Exgen-Malloc achieves a speedup of 1.17x, 1.10x, and 1.93x over\ndlmalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench, respectively. In\naddition to performance, Exgen-Malloc achieves 6.2%, 0.1%, and 25.2% memory\nsavings over mimalloc on SPEC CPU2017, redis-benchmark, and mimalloc-bench,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10129v1",
                "updated": "2025-10-11T09:28:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T09:28:26Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    28,
                    26,
                    5,
                    284,
                    0
                ],
                "title": "CacheClip: Accelerating RAG with Effective KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheClip: Accelerating RAG with Effective KV Cache Reuse"
                },
                "summary": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) systems suffer from severe\ntime-to-first-token (TTFT) bottlenecks due to long input sequences. Existing KV\ncache reuse methods face a fundamental trade-off: prefix caching requires\nidentical prefixes that rarely occur in RAG scenarios, while direct\nprecomputation sacrifices quality due to missing inter-chunk attention and\nrepeated attention sinks. Recent methods like APE and CacheBlend partially\naddress these issues but remain inadequate for robust RAG applications. This\npaper presents CacheClip, a novel framework that achieves both fast TTFT and\nhigh generation quality. Our key insight is that small auxiliary LLMs exhibit\nsimilar last-layer attention distributions to primary LLMs (the target model\nfor generation), enabling efficient identification of tokens critical for\nrestoring inter-chunk attention, thereby significantly improving response\nquality on cross-chunk reasoning tasks. CacheClip integrates three techniques:\n(1) auxiliary-model-guided token selection for selective KV cache\nrecomputation, where the auxiliary model is finetuned to improve selection\naccuracy, (2) shared prefixes to eliminate redundant attention sinks, and (3)\ngrouping strategy to maintain local coherence during partial KV cache updates.\nExperiments show CacheClip retains up to 94.8% and 85.0% of full-attention\nperformance on NIAH and LongBench, outperforming APE and CacheBlend by 25.2%\nand 35.1% on NIAH (with reomp% = 20%). Meanwhile, CacheClip accelerates LLM\ninference by up to 1.92x in prefill time, providing a practical solution to the\nefficiency-quality trade-off in RAG systems."
                },
                "authors": [
                    {
                        "name": "Bin Yang"
                    },
                    {
                        "name": "Qiuyu Leng"
                    },
                    {
                        "name": "Jun Zeng"
                    },
                    {
                        "name": "Zhenhua Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zhenhua Wu"
                },
                "author": "Zhenhua Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.17396v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.17396v3",
                "updated": "2025-10-11T09:04:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    9,
                    4,
                    23,
                    5,
                    284,
                    0
                ],
                "published": "2025-09-22T06:56:35Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    6,
                    56,
                    35,
                    0,
                    265,
                    0
                ],
                "title": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EpiCache: Episodic KV Cache Management for Long Conversational Question\n  Answering"
                },
                "summary": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern large language models (LLMs) extend context lengths to millions of\ntokens, enabling coherent, personalized responses grounded in long\nconversational histories. This ability, however, hinges on Key-Value (KV)\ncaching, whose memory grows linearly with dialogue length and quickly becomes\nthe bottleneck in resource-constrained environments. An active line of research\nfor reducing memory bottleneck is KV cache compression, which seeks to limit\ncache size while preserving accuracy. Yet existing methods face two major\nlimitations: (i) evicting the KV cache after full-context prefill causes\nunbounded peak memory, and (ii) query-dependent eviction narrows the cache to a\nsingle query, leading to failure cases in multi-turn conversations. We\nintroduce EpiCache, a training-free KV cache management framework for long\nconversational question answering (LongConvQA) under fixed memory budgets.\nEpiCache bounds cache growth through block-wise prefill and preserves\ntopic-relevant context via episodic KV compression, which clusters conversation\nhistory into coherent episodes and applies episode-specific KV cache eviction.\nWe further design an adaptive layer-wise budget allocation strategy that\nmeasures each layer's sensitivity to eviction and distributes the memory budget\nacross layers accordingly. Across three LongConvQA benchmarks, EpiCache\nimproves accuracy by up to 40%, maintains near-full KV accuracy under 4-6x\ncompression, and reduces latency/memory by up to 2.4x/3.5x, enabling efficient\nmulti-turn interaction under strict resource limits. Our code is available at\nhttps://github.com/apple/ml-epicache."
                },
                "authors": [
                    {
                        "name": "Minsoo Kim"
                    },
                    {
                        "name": "Arnav Kundu"
                    },
                    {
                        "name": "Han-Byul Kim"
                    },
                    {
                        "name": "Richa Dixit"
                    },
                    {
                        "name": "Minsik Cho"
                    }
                ],
                "author_detail": {
                    "name": "Minsik Cho"
                },
                "author": "Minsik Cho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.17396v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.17396v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.10102v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.10102v1",
                "updated": "2025-10-11T08:24:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T08:24:19Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    8,
                    24,
                    19,
                    5,
                    284,
                    0
                ],
                "title": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PANTHER: Generative Pretraining Beyond Language for Sequential User\n  Behavior Modeling"
                },
                "summary": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown that generative pretraining can\ndistill vast world knowledge into compact token representations. While LLMs\nencapsulate extensive world knowledge, they remain limited in modeling the\nbehavioral knowledge contained within user interaction histories. User behavior\nforms a distinct modality, where each action, defined by multi-dimensional\nattributes such as time, context, and transaction type, constitutes a\nbehavioral token. Modeling these high-cardinality sequences is challenging, and\ndiscriminative models often falter under limited supervision. To bridge this\ngap, we extend generative pretraining to user behavior, learning transferable\nrepresentations from unlabeled behavioral data analogous to how LLMs learn from\ntext. We present PANTHER, a hybrid generative-discriminative framework that\nunifies user behavior pretraining and downstream adaptation, enabling\nlarge-scale sequential user representation learning and real-time inference.\nPANTHER introduces: (1) Structured Tokenization to compress multi-dimensional\ntransaction attributes into an interpretable vocabulary; (2) Sequence Pattern\nRecognition Module (SPRM) for modeling periodic transaction motifs; (3) a\nUnified User-Profile Embedding that fuses static demographics with dynamic\ntransaction histories; and (4) Real-time scalability enabled by offline caching\nof pretrained embeddings for millisecond-level inference. Fully deployed and\noperational online at WeChat Pay, PANTHER delivers a 25.6 percent boost in\nnext-transaction prediction HitRate@1 and a 38.6 percent relative improvement\nin fraud detection recall over baselines. Cross-domain evaluations on public\nbenchmarks show strong generalization, achieving up to 21 percent HitRate@1\ngains over transformer baselines, establishing PANTHER as a scalable,\nhigh-performance framework for industrial sequential user behavior modeling."
                },
                "authors": [
                    {
                        "name": "Guilin Li"
                    },
                    {
                        "name": "Yun Zhang"
                    },
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Chengqi Li"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Linghe Kong"
                    },
                    {
                        "name": "Wenjia Wang"
                    },
                    {
                        "name": "Weiran Huang"
                    },
                    {
                        "name": "Matthias Hwai Yong Tan"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Hwai Yong Tan"
                },
                "author": "Matthias Hwai Yong Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.10102v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.10102v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09952v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09952v1",
                "updated": "2025-10-11T01:42:38Z",
                "updated_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "published": "2025-10-11T01:42:38Z",
                "published_parsed": [
                    2025,
                    10,
                    11,
                    1,
                    42,
                    38,
                    5,
                    284,
                    0
                ],
                "title": "HTTP Request Synchronization Defeats Discrepancy Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HTTP Request Synchronization Defeats Discrepancy Attacks"
                },
                "summary": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary web application architectures involve many layers of proxy\nservices that process traffic. Due to the complexity of HTTP and vendor design\ndecisions, these proxies sometimes process a given request in different ways.\nAttackers can exploit these processing discrepancies to launch damaging attacks\nincluding web cache poisoning and request smuggling. Discrepancy attacks are\nsurging, yet, there exists no systemic defense.\n  In this work, we propose the first comprehensive defense to address this\nproblem, called HTTP Request Synchronization. Our scheme uses standard HTTP\nextension mechanisms to augment each request with a complete processing\nhistory. It propagates this context through the traffic path detailing how each\nserver hop has processed said request. Using this history, every proxy server\ncan validate that their processing is consistent with all previous hops,\neliminating discrepancy attacks. We implement our scheme for 5 popular proxy\ntechnologies, Apache, NGINX, HAProxy, Varnish, and Cloudflare, demonstrating\nits practical impact."
                },
                "authors": [
                    {
                        "name": "Cem Topcuoglu"
                    },
                    {
                        "name": "Kaan Onarlioglu"
                    },
                    {
                        "name": "Steven Sprecher"
                    },
                    {
                        "name": "Engin Kirda"
                    }
                ],
                "author_detail": {
                    "name": "Engin Kirda"
                },
                "author": "Engin Kirda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09952v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09952v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09907v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09907v1",
                "updated": "2025-10-10T22:43:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T22:43:54Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    22,
                    43,
                    54,
                    4,
                    283,
                    0
                ],
                "title": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Property-Based Testing: Finding Bugs Across the Python Ecosystem"
                },
                "summary": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Property-based testing (PBT) is a lightweight formal method, typically\nimplemented as a randomized testing framework. Users specify the input domain\nfor their test using combinators supplied by the PBT framework, and the\nexpected properties or invariants as a unit-test function. The framework then\nsearches for a counterexample, e.g. by generating inputs and calling the test\nfunction. In this work, we demonstrate an LLM-based agent which analyzes Python\nmodules, infers function-specific and cross-function properties from code and\ndocumentation, synthesizes and executes PBTs, reflects on outputs of these\ntests to confirm true bugs, and finally outputs actionable bug reports for the\ndeveloper. We perform an extensive evaluation of our agent across 100 popular\nPython packages. Of the bug reports generated by the agent, we found after\nmanual review that 56\\% were valid bugs and 32\\% were valid bugs that we would\nreport to maintainers. We then developed a ranking rubric to surface\nhigh-priority valid bugs to developers, and found that of the 21 top-scoring\nbugs, 86\\% were valid and 81\\% we would report. The bugs span diverse failure\nmodes from serialization failures to numerical precision errors to flawed cache\nimplementations. We reported 5 bugs, 4 with patches, including to NumPy and\ncloud computing SDKs, with 3 patches merged successfully. Our results suggest\nthat LLMs with PBT provides a rigorous and scalable method for autonomously\ntesting software. Our code and artifacts are available at:\nhttps://github.com/mmaaz-git/agentic-pbt."
                },
                "authors": [
                    {
                        "name": "Muhammad Maaz"
                    },
                    {
                        "name": "Liam DeVoe"
                    },
                    {
                        "name": "Zac Hatfield-Dodds"
                    },
                    {
                        "name": "Nicholas Carlini"
                    }
                ],
                "author_detail": {
                    "name": "Nicholas Carlini"
                },
                "author": "Nicholas Carlini",
                "arxiv_comment": "4 pages (main), NeurIPS 2025, The 4th Deep Learning for Code Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09907v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09907v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09883v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09883v1",
                "updated": "2025-10-10T21:37:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T21:37:49Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    21,
                    37,
                    49,
                    4,
                    283,
                    0
                ],
                "title": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DELTA: Dynamic Layer-Aware Token Attention for Efficient Long-Context\n  Reasoning"
                },
                "summary": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large reasoning models (LRMs) achieve state-of-the-art performance on\nchallenging benchmarks by generating long chains of intermediate steps, but\ntheir inference cost is dominated by decoding, where each new token must attend\nto the entire growing sequence. Existing sparse attention methods reduce\ncomputation by pruning the key-value (KV) cache, yet they suffer from severe\naccuracy degradation on reasoning tasks due to cumulative selection errors and\nthe dynamic importance of tokens over long derivations. We present\n\\textbf{DELTA}, a training-free sparse attention mechanism that achieves\ncomputational efficiency without sacrificing model accuracy. DELTA partitions\ntransformer layers into three groups: initial layers that use full attention, a\nsmall set of \\emph{selection layers} that identify salient tokens via\naggregated head-level attention scores, and subsequent \\emph{sparse-attention\nlayers} that attend only to the selected subset. This design preserves the full\nKV cache in GPU memory for accuracy, while avoiding expensive full-attention\ncomputation over many layers. On reasoning benchmarks such as AIME and\nGPQA-Diamond, DELTA matches or surpasses full attention in accuracy, while\nreducing the number of attended tokens by up to $5\\times$ and delivering\n$1.5\\times$ end-to-end speedup. Our results show that selective reuse of\nintermediate attention maps offers a robust path toward efficient long-context\nreasoning."
                },
                "authors": [
                    {
                        "name": "Hossein Entezari Zarch"
                    },
                    {
                        "name": "Lei Gao"
                    },
                    {
                        "name": "Chaoyi Jiang"
                    },
                    {
                        "name": "Murali Annavarm"
                    }
                ],
                "author_detail": {
                    "name": "Murali Annavarm"
                },
                "author": "Murali Annavarm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09883v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09883v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09847v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09847v1",
                "updated": "2025-10-10T20:19:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T20:19:44Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    20,
                    19,
                    44,
                    4,
                    283,
                    0
                ],
                "title": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "THEAS: Efficient Power Management in Multi-Core CPUs via Cache-Aware\n  Resource Scheduling"
                },
                "summary": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The dynamic adaptation of resource levels enables the system to enhance\nenergy efficiency while maintaining the necessary computational resources,\nparticularly in scenarios where workloads fluctuate significantly over time.\nThe proposed approach can play a crucial role in heterogeneous systems where\nworkload characteristics are not uniformly distributed, such as non-pinning\ntasks. The deployed THEAS algorithm in this research work ensures a balance\nbetween performance and power consumption, making it suitable for a wide range\nof real-time applications. A comparative analysis of the proposed THEAS\nalgorithm with well-known scheduling techniques such as Completely Fair\nScheduler (CFS), Energy-Aware Scheduling (EAS), Heterogeneous Scheduling\n(HeteroSched), and Utility-Based Scheduling is presented in Table III. Each\nscheme is compared based on adaptability, core selection criteria, performance\nscaling, cache awareness, overhead, and real-time suitability."
                },
                "authors": [
                    {
                        "name": "Said Muhammad"
                    },
                    {
                        "name": "Lahlou Laaziz"
                    },
                    {
                        "name": "Nadjia Kara"
                    },
                    {
                        "name": "Phat Tan Nguyen"
                    },
                    {
                        "name": "Timothy Murphy"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Murphy"
                },
                "author": "Timothy Murphy",
                "arxiv_comment": "Accepted and presented at the 13th IEEE International Conference on\n  Intelligent Mobile Computing 2025 (IMC), CISOSE 2025 in Tucson, Arizona, USA.\n  This is the author's accepted manuscript (AAM). The final published version\n  will appear in the IEEE conference proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09847v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09847v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09608v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09608v1",
                "updated": "2025-10-10T17:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T17:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    59,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "StreamingVLM: Real-Time Understanding for Infinite Video Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamingVLM: Real-Time Understanding for Infinite Video Streams"
                },
                "summary": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) could power real-time assistants and autonomous\nagents, but they face a critical challenge: understanding near-infinite video\nstreams without escalating latency and memory usage. Processing entire videos\nwith full attention leads to quadratic computational costs and poor performance\non long videos. Meanwhile, simple sliding window methods are also flawed, as\nthey either break coherence or suffer from high latency due to redundant\nrecomputation. In this paper, we introduce StreamingVLM, a model designed for\nreal-time, stable understanding of infinite visual input. Our approach is a\nunified framework that aligns training with streaming inference. During\ninference, we maintain a compact KV cache by reusing states of attention sinks,\na short window of recent vision tokens, and a long window of recent text\ntokens. This streaming ability is instilled via a simple supervised fine-tuning\n(SFT) strategy that applies full attention on short, overlapped video chunks,\nwhich effectively mimics the inference-time attention pattern without training\non prohibitively long contexts. For evaluation, we build Inf-Streams-Eval, a\nnew benchmark with videos averaging over two hours that requires dense,\nper-second alignment between frames and text. On Inf-Streams-Eval, StreamingVLM\nachieves a 66.18% win rate against GPT-4O mini and maintains stable, real-time\nperformance at up to 8 FPS on a single NVIDIA H100. Notably, our SFT strategy\nalso enhances general VQA abilities without any VQA-specific fine-tuning,\nimproving performance on LongVideoBench by +4.30 and OVOBench Realtime by\n+5.96. Code is available at https://github.com/mit-han-lab/streaming-vlm."
                },
                "authors": [
                    {
                        "name": "Ruyi Xu"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Liuning He"
                    },
                    {
                        "name": "Kelly Peng"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first two authors contributed equally to this work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09608v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09608v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.18494v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.18494v2",
                "updated": "2025-10-10T16:56:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    56,
                    23,
                    4,
                    283,
                    0
                ],
                "published": "2025-08-25T21:07:52Z",
                "published_parsed": [
                    2025,
                    8,
                    25,
                    21,
                    7,
                    52,
                    0,
                    237,
                    0
                ],
                "title": "DiskJoin: Large-scale Vector Similarity Join with SSD",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DiskJoin: Large-scale Vector Similarity Join with SSD"
                },
                "summary": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Similarity join--a widely used operation in data science--finds all pairs of\nitems that have distance smaller than a threshold. Prior work has explored\ndistributed computation methods to scale similarity join to large data volumes\nbut these methods require a cluster deployment, and efficiency suffers from\nexpensive inter-machine communication. On the other hand, disk-based solutions\nare more cost-effective by using a single machine and storing the large dataset\non high-performance external storage, such as NVMe SSDs, but in these methods\nthe disk I/O time is a serious bottleneck. In this paper, we propose DiskJoin,\nthe first disk-based similarity join algorithm that can process billion-scale\nvector datasets efficiently on a single machine. DiskJoin improves disk I/O by\ntailoring the data access patterns to avoid repetitive accesses and read\namplification. It also uses main memory as a dynamic cache and carefully\nmanages cache eviction to improve cache hit rate and reduce disk retrieval\ntime. For further acceleration, we adopt a probabilistic pruning technique that\ncan effectively prune a large number of vector pairs from computation. Our\nevaluation on real-world, large-scale datasets shows that DiskJoin\nsignificantly outperforms alternatives, achieving speedups from 50x to 1000x."
                },
                "authors": [
                    {
                        "name": "Yanqi Chen"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Alexandra Meliou"
                    },
                    {
                        "name": "Eric Lo"
                    }
                ],
                "author_detail": {
                    "name": "Eric Lo"
                },
                "author": "Eric Lo",
                "arxiv_doi": "10.1145/3769780",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3769780",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.18494v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.18494v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted at SIGMOD 2026",
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03812v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03812v2",
                "updated": "2025-10-10T16:08:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    8,
                    26,
                    4,
                    283,
                    0
                ],
                "published": "2025-07-04T21:09:51Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    21,
                    9,
                    51,
                    4,
                    185,
                    0
                ],
                "title": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory- and compute-optimized geometric multigrid GMGPolar for\n  curvilinear coordinate representations -- Applications to fusion plasma"
                },
                "summary": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokamak fusion reactors are actively studied as a means of realizing energy\nproduction from plasma fusion. However, due to the substantial cost and time\nrequired to construct fusion reactors and run physical experiments, numerical\nexperiments are indispensable for understanding plasma physics inside tokamaks,\nsupporting the design and engineering phase, and optimizing future reactor\ndesigns. Geometric multigrid methods are optimal solvers for many problems that\narise from the discretization of partial differential equations. It has been\nshown that the multigrid solver GMGPolar solves the 2D gyrokinetic Poisson\nequation in linear complexity and with only small memory requirements compared\nto other state-of-the-art solvers. In this paper, we present a completely\nrefactored and object-oriented version of GMGPolar which offers two different\nmatrix-free implementations. Among other things, we leverage the\nSherman-Morrison formula to solve cyclic tridiagonal systems from circular line\nsolvers without additional fill-in and we apply reordering to optimize cache\naccess of circular and radial smoothing operations. With the Give approach,\nmemory requirements are further reduced and speedups of four to seven are\nobtained for usual test cases. For the Take approach, speedups of 16 to 18 can\nbe attained. In an additionally experimental setup of using GMGPolar as a\npreconditioner for conjugate gradients, this speedup could even be increased to\nfactors between 25 and 37."
                },
                "authors": [
                    {
                        "name": "Julian Litz"
                    },
                    {
                        "name": "Philippe Leleux"
                    },
                    {
                        "name": "Carola Kruse"
                    },
                    {
                        "name": "Joscha Gedicke"
                    },
                    {
                        "name": "Martin J. K√ºhn"
                    }
                ],
                "author_detail": {
                    "name": "Martin J. K√ºhn"
                },
                "author": "Martin J. K√ºhn",
                "arxiv_comment": "29 pages, 11 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03812v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03812v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68Q25, 65Y20, 65Y05, 65N55, 65N06, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09477v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09477v1",
                "updated": "2025-10-10T15:32:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T15:32:58Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    15,
                    32,
                    58,
                    4,
                    283,
                    0
                ],
                "title": "Efficient Autoregressive Inference for Transformer Probabilistic Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Autoregressive Inference for Transformer Probabilistic Models"
                },
                "summary": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models for amortized probabilistic inference, such as\nneural processes, prior-fitted networks, and tabular foundation models, excel\nat single-pass marginal prediction. However, many real-world applications, from\nsignal interpolation to multi-column tabular predictions, require coherent\njoint distributions that capture dependencies between predictions. While purely\nautoregressive architectures efficiently generate such distributions, they\nsacrifice the flexible set-conditioning that makes these models powerful for\nmeta-learning. Conversely, the standard approach to obtain joint distributions\nfrom set-based models requires expensive re-encoding of the entire augmented\nconditioning set at each autoregressive step. We introduce a causal\nautoregressive buffer that preserves the advantages of both paradigms. Our\napproach decouples context encoding from updating the conditioning set. The\nmodel processes the context once and caches it. A dynamic buffer then captures\ntarget dependencies: as targets are incorporated, they enter the buffer and\nattend to both the cached context and previously buffered targets. This enables\nefficient batched autoregressive generation and one-pass joint log-likelihood\nevaluation. A unified training strategy allows seamless integration of\nset-based and autoregressive modes at minimal additional cost. Across synthetic\nfunctions, EEG signals, cognitive models, and tabular data, our method matches\npredictive accuracy of strong baselines while delivering up to 20 times faster\njoint sampling. Our approach combines the efficiency of autoregressive\ngenerative models with the representational power of set-based conditioning,\nmaking joint prediction practical for transformer-based probabilistic models."
                },
                "authors": [
                    {
                        "name": "Conor Hassan"
                    },
                    {
                        "name": "Nasrulloh Loka"
                    },
                    {
                        "name": "Cen-You Li"
                    },
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Paul E. Chang"
                    },
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Francesco Silvestrin"
                    },
                    {
                        "name": "Samuel Kaski"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09477v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09477v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09409v1",
                "updated": "2025-10-10T14:03:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T14:03:42Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    14,
                    3,
                    42,
                    4,
                    283,
                    0
                ],
                "title": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3C Resources Joint Allocation for Time-Deterministic Remote Sensing\n  Image Backhaul in the Space-Ground Integrated Network"
                },
                "summary": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Earth-orbit (LEO) satellites assist observation satellites (OSs) to\ncompress and backhaul more time-determined images (TDI) has become a new\nparadigm, which is used to enhance the timeout caused by the limited computing\nresources of OSs. However, how to capture the time-varying and dynamic\ncharacteristics of multi-dimensional resources is challenging for efficient\ncollaborative scheduling. Motivated by this factor, we design a highly succinct\nmulti-dimensional resource time-expanded graph (MDR-TEG) modell. Specifically,\nby employing a slots division mechanism and introducing an external virtual\nnode, the time-varying communication, caching, and computing (3C) resources are\ndepicted in low complexity by the link weights within, between, and outside the\nslots. Based on the MDR-TEG, the maximizing successful transmission ratio of\nTDI (MSTR-TDI) is modeled as a mixed integer linear programming (MILP) problem.\nWhich further relaxed decomposed into two tractable sub-problems: maximizing\nthe successful transmission rate of images (MSTRI) and ensuring the timeliness\nproblem (ETP). Subsequently, an efficient subgradient of relaxation computing\nconstraint (SRCC) algorithm is proposed. The upper and lower bounds of MSTR-TDI\nare obtained by solving the two subproblems and the dual problem (DP), and the\ndirection of the next iteration is obtained by feedback. Furthermore, arranging\nthe sending sequences of images to improve the quality of the solution. The\napproximate optimal solution of MSTR-TDI is eventually obtained through\nrepeated iterations. The simulation results verify the superiority of the\nproposed MDR-TEG model and the effectiveness of the SRCC."
                },
                "authors": [
                    {
                        "name": "Chongxiao Cai"
                    },
                    {
                        "name": "Yan Zhu"
                    },
                    {
                        "name": "Min Sheng"
                    },
                    {
                        "name": "Jiandong Li"
                    },
                    {
                        "name": "Yan Shi"
                    },
                    {
                        "name": "Di Zhou"
                    },
                    {
                        "name": "Ziwen Xie"
                    },
                    {
                        "name": "Chen Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chen Zhang"
                },
                "author": "Chen Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08272v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08272v2",
                "updated": "2025-10-10T13:15:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    15,
                    40,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-09T14:29:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    14,
                    29,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Assessment of Cache Timing Vulnerabilities on RISC-V\n  Processors"
                },
                "summary": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While interest in the open RISC-V instruction set architecture is growing,\ntools to assess the security of concrete processor implementations are lacking.\nThere are dedicated tools and benchmarks for common microarchitectural\nside-channel vulnerabilities for popular processor families such as Intel\nx86-64 or ARM, but not for RISC-V. In this paper we describe our efforts in\nporting an Intel x86-64 benchmark suite for cache-based timing vulnerabilities\nto RISC-V. We then use this benchmark to evaluate the security of three\ncommercially available RISC-V processors, the T-Head C910 and the SiFive U54\nand U74 cores. We observe that the C910 processor exhibits more distinct timing\ntypes than the other processors, leading to the assumption that code running on\nthe C910 would be exposed to more microarchitectural vulnerability sources. In\naddition, our evaluation reveals that $65.9\\%$ of the vulnerabilities covered\nby the benchmark exist in all processors, while only $6.8\\%$ are absent from\nall cores. Our work, in particular the ported benchmark, aims to support RISC-V\nprocessor designers to identify leakage sources early in their designs and to\nsupport the development of countermeasures."
                },
                "authors": [
                    {
                        "name": "C√©drick Austa"
                    },
                    {
                        "name": "Jan Tobias M√ºhlberg"
                    },
                    {
                        "name": "Jean-Michel Dricot"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Dricot"
                },
                "author": "Jean-Michel Dricot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08272v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08272v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.19433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.19433v2",
                "updated": "2025-10-10T13:08:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    13,
                    8,
                    39,
                    4,
                    283,
                    0
                ],
                "published": "2025-06-24T09:00:43Z",
                "published_parsed": [
                    2025,
                    6,
                    24,
                    9,
                    0,
                    43,
                    1,
                    175,
                    0
                ],
                "title": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mem4Nav: Boosting Vision-and-Language Navigation in Urban Environments\n  with a Hierarchical Spatial-Cognition Long-Short Memory System"
                },
                "summary": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-and-Language Navigation (VLN) in large-scale urban environments\nrequires embodied agents to ground linguistic instructions in complex scenes\nand recall relevant experiences over extended time horizons. Prior modular\npipelines offer interpretability but lack unified memory, while end-to-end\n(M)LLM agents excel at fusing vision and language yet remain constrained by\nfixed context windows and implicit spatial reasoning. We introduce\n\\textbf{Mem4Nav}, a hierarchical spatial-cognition long-short memory system\nthat can augment any VLN backbone. Mem4Nav fuses a sparse octree for\nfine-grained voxel indexing with a semantic topology graph for high-level\nlandmark connectivity, storing both in trainable memory tokens embedded via a\nreversible Transformer. Long-term memory (LTM) compresses and retains\nhistorical observations at both octree and graph nodes, while short-term memory\n(STM) caches recent multimodal entries in relative coordinates for real-time\nobstacle avoidance and local planning. At each step, STM retrieval sharply\nprunes dynamic context, and, when deeper history is needed, LTM tokens are\ndecoded losslessly to reconstruct past embeddings. Evaluated on Touchdown and\nMap2Seq across three backbones (modular, state-of-the-art VLN with prompt-based\nLLM, and state-of-the-art VLN with strided-attention MLLM), Mem4Nav yields 7-13\npp gains in Task Completion, sufficient SPD reduction, and >10 pp nDTW\nimprovement. Ablations confirm the indispensability of both the hierarchical\nmap and dual memory modules. Our codes are open-sourced via\nhttps://github.com/tsinghua-fib-lab/Mem4Nav."
                },
                "authors": [
                    {
                        "name": "Lixuan He"
                    },
                    {
                        "name": "Haoyu Dong"
                    },
                    {
                        "name": "Zhenxing Chen"
                    },
                    {
                        "name": "Yangcheng Yu"
                    },
                    {
                        "name": "Jie Feng"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_comment": "The paper is currently under investigation regarding concerns of\n  potential academic misconduct. While the investigation is ongoing, the\n  authors have voluntarily requested to withdraw the manuscript",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.19433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.19433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09309v1",
                "updated": "2025-10-10T12:01:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T12:01:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    12,
                    1,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mask Tokens as Prophet: Fine-Grained Cache Eviction for Efficient dLLM\n  Inference"
                },
                "summary": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion large language models (dLLMs) present a promising alternative to\ndominant autoregressive models (ARMs) by the ability of parallel decoding at\nthe expense of substantial computation and memory costs. Specifically, the\ncache mechanism for bidirectional attention in dLLMs demands large memory\nfootprint, restricting their ability to handle long contexts under\nresource-limited settings. Existing cache eviction strategies are designed for\nARMs and ignore the unique characteristics of dLLMs, thus leading to\nunsatisfactory performance. To address these challenges, we introduce MaskKV, a\ntraining-free cache eviction framework tailored to dLLMs, focusing on the\neffect of mask tokens in dLLMs. MaskKV is built on two key innovations: (1) a\nmask-query guided scoring mechanism that leverages attention weights to\nidentify and evict less critical prompt tokens for each head; (2) an adaptive\ncache budgeting strategy that improves efficiency by reducing allocation in\nintermediate layers and concentrating resources on prompt-preferring heads. On\nLLaDA with MaskKV, compressing the KV cache to only 256 pairs (less than 5% of\ntokens) retains 94% of the full-cache performance on LongBench and achieves up\nto 31x acceleration at 32k prompt length. The code is publicly available at:\nhttps://github.com/jianuo-huang/MaskKV"
                },
                "authors": [
                    {
                        "name": "Jianuo Huang"
                    },
                    {
                        "name": "Yaojie Zhang"
                    },
                    {
                        "name": "Yicun Yang"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Biqing Qi"
                    },
                    {
                        "name": "Dongrui Liu"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "17 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09182v1",
                "updated": "2025-10-10T09:24:53Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T09:24:53Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    9,
                    24,
                    53,
                    4,
                    283,
                    0
                ],
                "title": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Video Depth Anything: Temporally-Consistent Depth Prediction with\n  Low Memory Consumption"
                },
                "summary": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Depth estimation from monocular video has become a key component of many\nreal-world computer vision systems. Recently, Video Depth Anything (VDA) has\ndemonstrated strong performance on long video sequences. However, it relies on\nbatch-processing which prohibits its use in an online setting. In this work, we\novercome this limitation and introduce online VDA (oVDA). The key innovation is\nto employ techniques from Large Language Models (LLMs), namely, caching latent\nfeatures during inference and masking frames at training. Our oVDA method\noutperforms all competing online video depth estimation methods in both\naccuracy and VRAM usage. Low VRAM usage is particularly important for\ndeployment on edge devices. We demonstrate that oVDA runs at 42 FPS on an\nNVIDIA A100 and at 20 FPS on an NVIDIA Jetson edge device. We will release\nboth, code and compilation scripts, making oVDA easy to deploy on low-power\nhardware."
                },
                "authors": [
                    {
                        "name": "Johann-Friedrich Feiden"
                    },
                    {
                        "name": "Tim K√ºchler"
                    },
                    {
                        "name": "Denis Zavadski"
                    },
                    {
                        "name": "Bogdan Savchynskyy"
                    },
                    {
                        "name": "Carsten Rother"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Rother"
                },
                "author": "Carsten Rother",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09154v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09154v1",
                "updated": "2025-10-10T08:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "published": "2025-10-10T08:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    8,
                    57,
                    16,
                    4,
                    283,
                    0
                ],
                "title": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown and RF Performance in Field-Plated AlGaN/GaN HEMT for\n  High-Power Applications"
                },
                "summary": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Electron Mobility Transistors (HEMTs) are most suitable for harsh\nenvironments as they operate reliably under extreme conditions such as high\nvoltages, high temperatures, radiation exposure and corrosive atmospheres. In\nthis article, gate field-plated engineering Al0.295GaN/GaN HEMT is proposed for\nachieving high breakdown voltage to reliably operate in harsh environments. The\nAl0.295GaN/GaN heterointerface results in a 2DEG (two-dimensional electron gas)\ndensity of the order of 1013 cm-2 obtained from the self-consistent solution of\nSchr\\\"odinger and Poisson equations. The device has undergone DC and breakdown\nsimulations which result in threshold voltage of -5.5 V, drain saturation\ncurrent of 3000 mA, and breakdown voltage of 1 kV. The HEMT also shows\nexcellent RF characteristics which include cut-off frequency (ft) of 28 GHz and\nmaximum frequency of oscillation (fmax) of 38 GHz. The proposed gate\nfield-plated HEMT is stable up to 40 GHz and suitable for high-voltage and\nhigh-power RF operation during harsh environment applications."
                },
                "authors": [
                    {
                        "name": "Tanjim Rahman"
                    },
                    {
                        "name": "Trupti Ranjan Lenka"
                    }
                ],
                "author_detail": {
                    "name": "Trupti Ranjan Lenka"
                },
                "author": "Trupti Ranjan Lenka",
                "arxiv_comment": "13 pages, 13 figures including DC, RF, and breakdown analysis of\n  field-plated AlGaN/GaN HEMT using TCAD simulations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09154v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09154v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v3",
                "updated": "2025-10-09T20:37:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    37,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into subquadratic architectures.\nTransformers faces severe computational and memory bottlenecks with long\nsequences due to the quadratic complexity of softmax attention and the growing\nKey-Value (KV) cache that makes inference memory-bound by context length.\nLizard addresses these limitations by introducing a subquadratic attention\nmechanism that closely approximates softmax attention while preserving model\nquality. Unlike prior linearization methods constrained by fixed, non-adaptive\nstructures, Lizard augments the architecture with compact, learnable modules\nthat enable adaptive memory control and robust length generalization. Moreover,\nwe introduce a hardwareaware algorithm that solves numerical instability in\ngated attention to accelerate training. Extensive experiments show that Lizard\nachieves near-lossless recovery of its teacher model's performance,\nsignificantly outperforming previous methods by up to 9.4 - 24.5 points on the\n5-shot MMLU benchmark and demonstrating superior associative recall."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08803v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08803v1",
                "updated": "2025-10-09T20:35:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T20:35:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    20,
                    35,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Man-Made Heuristics Are Dead. Long Live Code Generators!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Man-Made Heuristics Are Dead. Long Live Code Generators!"
                },
                "summary": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Policy design for various systems controllers has conventionally been a\nmanual process, with domain experts carefully tailoring heuristics for the\nspecific instance in which the policy will be deployed. In this paper, we\nre-imagine policy design via a novel automated search technique fueled by\nrecent advances in generative models, specifically Large Language Model\n(LLM)-driven code generation. We outline the design and implementation of\nPolicySmith, a framework that applies LLMs to synthesize instance-optimal\nheuristics. We apply PolicySmith to two long-standing systems policies - web\ncaching and congestion control, highlighting the opportunities unraveled by\nthis LLM-driven heuristic search. For caching, PolicySmith discovers heuristics\nthat outperform established baselines on standard open-source traces. For\ncongestion control, we show that PolicySmith can generate safe policies that\nintegrate directly into the Linux kernel."
                },
                "authors": [
                    {
                        "name": "Rohit Dwivedula"
                    },
                    {
                        "name": "Divyanshu Saxena"
                    },
                    {
                        "name": "Aditya Akella"
                    },
                    {
                        "name": "Swarat Chaudhuri"
                    },
                    {
                        "name": "Daehyeok Kim"
                    }
                ],
                "author_detail": {
                    "name": "Daehyeok Kim"
                },
                "author": "Daehyeok Kim",
                "arxiv_comment": "10 pages, 2 figures, 2 tables. To be presented at HotNets 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08803v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08803v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08774v1",
                "updated": "2025-10-09T19:45:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T19:45:54Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    19,
                    45,
                    54,
                    3,
                    282,
                    0
                ],
                "title": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Struc-EMB: The Potential of Structure-Aware Encoding in Language\n  Embeddings"
                },
                "summary": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text embeddings from Large Language Models (LLMs) have become foundational\nfor numerous applications. However, these models typically operate on raw text,\noverlooking the rich structural information, such as hyperlinks or citations,\nthat provides crucial context in many real-world datasets. This paper\nintroduces and systematically evaluates a new paradigm for generating\nstructure-aware text embeddings by integrating these structural relations\ndirectly into the LLM's internal encoding process, rather than relying on\ntraditional post-hoc aggregation. We investigate two primary in-process\nmethods: sequential concatenation and parallel caching. Through extensive\nzero-shot experiments across retrieval, clustering, classification, and\nrecommendation tasks, we demonstrate that our structure-aware approaches\nconsistently outperform both text-only and post-hoc baselines. Our analysis\nreveals critical trade-offs: sequential concatenation excels with noisy,\nmoderate-length contexts, while parallel caching scales more effectively to\nlong, high-signal contexts but is more susceptible to distractors. To address\nthe challenge of noisy structural data, we also introduce and validate two\neffective techniques: Context Distillation and Semantic Balancing. This work\nprovides the first comprehensive analysis of in-process structure-aware\nencoding, offering a blueprint for building more powerful and contextually\naware embedding models."
                },
                "authors": [
                    {
                        "name": "Shikun Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    },
                    {
                        "name": "Mufei Li"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08525v1",
                "updated": "2025-10-09T17:50:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:50:00Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    50,
                    0,
                    3,
                    282,
                    0
                ],
                "title": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Which Heads Matter for Reasoning? RL-Guided KV Cache Compression"
                },
                "summary": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning large language models exhibit complex reasoning behaviors through\nthe extended chain-of-thought generation, creating unprecedented Key-Value (KV)\ncache overhead during the decoding phase. Existing KV cache compression methods\nunderperform on reasoning models: token-dropping methods break reasoning\nintegrity by discarding critical information, while head-reallocating methods\nmistakenly compress reasoning-critical heads since they are designed for\nretrieval tasks, resulting in significant performance degradation as\ncompression rates increase. We hypothesize that KV heads exhibit functional\nheterogeneity in reasoning models-some heads are critical for chain-of-thought\nconsistency while others are compressible. To validate and exploit this\ninsight, we propose RLKV, a novel reasoning-critical head identification\nframework, which uses reinforcement learning to directly optimize the\nrelationship between each head's cache usage and reasoning quality. As RLKV\nproduces rewards from actual generated samples during training, it naturally\nidentifies heads relevant to reasoning behaviors. We then allocate full KV\ncache to these heads while applying compressed constant KV cache to others for\nefficient inference. Our experiments reveal that only a small fraction of\nattention heads is essential for reasoning, enabling our KV compression\napproach to outperform baseline methods while achieving 20-50% cache reduction\nwith near lossless performance compared to uncompressed results."
                },
                "authors": [
                    {
                        "name": "Wenjie Du"
                    },
                    {
                        "name": "Li Jiang"
                    },
                    {
                        "name": "Keda Tao"
                    },
                    {
                        "name": "Xue Liu"
                    },
                    {
                        "name": "Huan Wang"
                    }
                ],
                "author_detail": {
                    "name": "Huan Wang"
                },
                "author": "Huan Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.06493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.06493v2",
                "updated": "2025-10-09T17:45:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    45,
                    50,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-08T09:54:18Z",
                "published_parsed": [
                    2025,
                    9,
                    8,
                    9,
                    54,
                    18,
                    0,
                    251,
                    0
                ],
                "title": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling up Multi-Turn Off-Policy RL and Multi-Agent Tree Search for LLM\n  Step-Provers"
                },
                "summary": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into automated theorem\nproving has shown immense promise, yet is fundamentally constrained by\nchallenges in scaling up both training-time reinforcement learning (RL) and\ninference-time compute. This paper introduces \\texttt{BFS-Prover-V2}, a system\ndesigned to address this dual scaling problem. We present two primary\ninnovations. The first is a novel multi-turn off-policy RL framework for\ncontinually improving the performance of LLM step-prover at training time. This\nframework, inspired by the principles of AlphaZero, utilizes a multi-stage\nexpert iteration pipeline featuring adaptive tactic-level data filtering and\nperiodic retraining to surmount the performance plateaus that typically curtail\nlong-term RL in LLM-based agents. The second innovation is a planner-enhanced\nmulti-agent search architecture that scales reasoning capabilities at inference\ntime. This architecture employs a general reasoning model as a high-level\nplanner to iteratively decompose complex theorems into a sequence of simpler\nsubgoals. This hierarchical approach substantially reduces the search space,\nenabling a team of parallel prover agents to collaborate efficiently by\nleveraging a shared proof cache. We demonstrate that this dual approach to\nscaling yields state-of-the-art results on established formal mathematics\nbenchmarks. \\texttt{BFS-Prover-V2} achieves 95.08\\% and 41.4\\% on the MiniF2F\nand ProofNet test sets respectively. While demonstrated in the domain of formal\nmathematics, the RL and inference techniques presented in this work are of\nbroader interest and may be applied to other domains requiring long-horizon\nmulti-turn reasoning and complex search."
                },
                "authors": [
                    {
                        "name": "Ran Xin"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yanchen Nie"
                    },
                    {
                        "name": "Kun Yuan"
                    },
                    {
                        "name": "Xia Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xia Xiao"
                },
                "author": "Xia Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.06493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.06493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18085v2",
                "updated": "2025-10-09T17:38:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    38,
                    52,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-22T17:58:21Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    17,
                    58,
                    21,
                    0,
                    265,
                    0
                ],
                "title": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiffy: Multiplying Diffusion LLM Acceleration via Lossless Speculative\n  Decoding"
                },
                "summary": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion LLMs (dLLMs) have recently emerged as a powerful alternative to\nautoregressive LLMs (AR-LLMs) with the potential to operate at significantly\nhigher token generation rates. However, currently available open-source dLLMs\noften generate at much lower rates, typically decoding only a single token at\nevery denoising timestep in order to maximize output quality. We present\nSpiffy, a speculative decoding algorithm that accelerates dLLM inference by\n$\\mathbf{2.8{-}3.1\\times}$ while provably preserving the model's output\ndistribution. This work addresses the unique challenges involved in applying\nideas from speculative decoding of AR-LLMs to the dLLM setting. Spiffy proposes\ndraft states by leveraging the dLLM's distribution itself in an\nauto-speculative manner. This approach is efficient and effective, and\neliminates the overheads of training and running an independent draft model. To\nstructure the candidate draft states, we propose a novel directed draft graph\nwhich is uniquely designed to take advantage of the bidirectional, block-wise\nnature of dLLM generation and can be verified in parallel by the dLLM. To\nfurther optimize the structure of these draft graphs, we introduce an\nefficient, offline calibration algorithm that procedurally determines\nhigh-quality graph configurations. These optimized draft graphs, enabling\nincreased acceptance rates, lead to a significant boost in the overall speedup\nachieved by the system. Crucially, Spiffy is also complementary to other recent\ninnovations in improving dLLM generation speeds such as KV-caching and\nmulti-token unmasking. We demonstrate that when combined with such parallel\ndecoding algorithms, Spiffy is able to effectively multiply the benefits of\nthese methods leading to total speedups of up to $\\mathbf{7.9\\times}$."
                },
                "authors": [
                    {
                        "name": "Sudhanshu Agrawal"
                    },
                    {
                        "name": "Risheek Garrepalli"
                    },
                    {
                        "name": "Raghavv Goel"
                    },
                    {
                        "name": "Mingu Lee"
                    },
                    {
                        "name": "Christopher Lott"
                    },
                    {
                        "name": "Fatih Porikli"
                    }
                ],
                "author_detail": {
                    "name": "Fatih Porikli"
                },
                "author": "Fatih Porikli",
                "arxiv_comment": "Original version uploaded on Sep 22, 2025. (v2): Extended Table 2\n  with additional analysis and referenced it in Sec 5.2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08669v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08669v1",
                "updated": "2025-10-09T17:22:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T17:22:23Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    17,
                    22,
                    23,
                    3,
                    282,
                    0
                ],
                "title": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqCa: Accelerating Diffusion Models via Frequency-Aware Caching"
                },
                "summary": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of diffusion transformers is suffering from their significant\ninference costs. Recently, feature caching has been proposed to solve this\nproblem by reusing features from previous timesteps, thereby skipping\ncomputation in future timesteps. However, previous feature caching assumes that\nfeatures in adjacent timesteps are similar or continuous, which does not always\nhold in all settings. To investigate this, this paper begins with an analysis\nfrom the frequency domain, which reveal that different frequency bands in the\nfeatures of diffusion models exhibit different dynamics across timesteps.\nConcretely, low-frequency components, which decide the structure of images,\nexhibit higher similarity but poor continuity. In contrast, the high-frequency\nbands, which decode the details of images, show significant continuity but poor\nsimilarity. These interesting observations motivate us to propose\nFrequency-aware Caching (FreqCa)\n  which directly reuses features of low-frequency components based on their\nsimilarity, while using a second-order Hermite interpolator to predict the\nvolatile high-frequency ones based on its continuity.\n  Besides, we further propose to cache Cumulative Residual Feature (CRF)\ninstead of the features in all the layers, which reduces the memory footprint\nof feature caching by 99%.\n  Extensive experiments on FLUX.1-dev, FLUX.1-Kontext-dev, Qwen-Image, and\nQwen-Image-Edit demonstrate its effectiveness in both generation and editing.\nCodes are available in the supplementary materials and will be released on\nGitHub."
                },
                "authors": [
                    {
                        "name": "Jiacheng Liu"
                    },
                    {
                        "name": "Peiliang Cai"
                    },
                    {
                        "name": "Qinming Zhou"
                    },
                    {
                        "name": "Yuqi Lin"
                    },
                    {
                        "name": "Deyang Kong"
                    },
                    {
                        "name": "Benhao Huang"
                    },
                    {
                        "name": "Yupei Pan"
                    },
                    {
                        "name": "Haowen Xu"
                    },
                    {
                        "name": "Chang Zou"
                    },
                    {
                        "name": "Junshu Tang"
                    },
                    {
                        "name": "Shikang Zheng"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "15 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08669v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08669v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08351v1",
                "updated": "2025-10-09T15:38:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T15:38:13Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    15,
                    38,
                    13,
                    3,
                    282,
                    0
                ],
                "title": "FMCache: File-System Metadata Caching in Programmable Switches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMCache: File-System Metadata Caching in Programmable Switches"
                },
                "summary": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fast and scalable metadata management across multiple metadata servers is\ncrucial for distributed file systems to handle numerous files and directories.\nClient-side caching of frequently accessed metadata can mitigate server loads,\nbut incurs significant overhead and complexity in maintaining cache consistency\nwhen the number of clients increases. We propose FMCache, an in-switch\nfile-system metadata caching framework that leverages programmable switches to\nserve file-system metadata requests from multiple clients directly in the\nswitch data plane. Unlike prior in-switch key-value caching approaches, FMCache\naddresses file-system-specific path dependencies under stringent switch\nresource constraints. We implement FMCache atop Hadoop HDFS and evaluate it on\na Tofino-switch testbed using real-world file-system metadata workloads.\nFMCache achieves up to 181.6% higher throughput than vanilla HDFS and\ncomplements client-side caching with additional throughput gains of up to\n139.6%. It also incurs low latencies and limited switch resource usage."
                },
                "authors": [
                    {
                        "name": "Qingxiu Liu"
                    },
                    {
                        "name": "Jiazhen Cai"
                    },
                    {
                        "name": "Siyuan Sheng"
                    },
                    {
                        "name": "Yuhui Chen"
                    },
                    {
                        "name": "Lu Tang"
                    },
                    {
                        "name": "Zhirong Shen"
                    },
                    {
                        "name": "Patrick P. C. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Patrick P. C. Lee"
                },
                "arxiv_affiliation": "The Chinese University of Hong Kong",
                "author": "Patrick P. C. Lee",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08180v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08180v1",
                "updated": "2025-10-09T13:06:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T13:06:16Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    6,
                    16,
                    3,
                    282,
                    0
                ],
                "title": "Towards Energy-Efficient Serverless Computing with Hardware Isolation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Energy-Efficient Serverless Computing with Hardware Isolation"
                },
                "summary": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless computing provides just-in-time infrastructure provisioning with\nrapid elasticity and a finely-grained pricing model. As full control of\nresource allocation is in the hands of the cloud provider and applications only\nconsume resources when they actually perform work, we believe that serverless\ncomputing is uniquely positioned to maximize energy efficiency.\n  However, the focus of current serverless platforms is to run hundreds or\nthousands of serverless functions from different tenants on traditional server\nhardware, requiring expensive software isolation mechanisms and a high degree\nof overprovisioning, i.e., idle servers, to anticipate load spikes. With shared\ncaches, high clock frequencies, and many-core architectures, servers today are\noptimized for large, singular workloads but not to run thousands of isolated\nfunctions.\n  We propose rethinking the serverless hardware architecture to align it with\nthe requirements of serverless software. Specifically, we propose using\nhardware isolation with individual processors per function instead of software\nisolation resulting in a serverless hardware stack that consumes energy only\nwhen an application actually performs work. In preliminary evaluation with real\nhardware and a typical serverless workload we find that this could reduce\nenergy consumption overheads by 90.63% or an average 70.8MW."
                },
                "authors": [
                    {
                        "name": "Natalie Carl"
                    },
                    {
                        "name": "Tobias Pfandzelter"
                    },
                    {
                        "name": "David Bermbach"
                    }
                ],
                "author_detail": {
                    "name": "David Bermbach"
                },
                "author": "David Bermbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08180v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08180v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.26541v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.26541v2",
                "updated": "2025-10-09T13:03:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    13,
                    3,
                    29,
                    3,
                    282,
                    0
                ],
                "published": "2025-09-30T17:15:27Z",
                "published_parsed": [
                    2025,
                    9,
                    30,
                    17,
                    15,
                    27,
                    1,
                    273,
                    0
                ],
                "title": "TASP: Topology-aware Sequence Parallelism",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TASP: Topology-aware Sequence Parallelism"
                },
                "summary": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) face constraints due to the\nquadratic complexity of the self-attention mechanism. The mainstream sequence\nparallelism (SP) method, Ring Attention, attempts to solve this by distributing\nthe query into multiple query chunks across accelerators and enable each Q\ntensor to access all KV tensors from other accelerators via the Ring AllGather\ncommunication primitive. However, it exhibits low communication efficiency,\nrestricting its practical applicability. This inefficiency stems from the\nmismatch between the Ring AllGather communication primitive it adopts and the\nAlltoAll topology of modern accelerators. A Ring AllGather primitive is\ncomposed of iterations of ring-styled data transfer, which can only utilize a\nvery limited fraction of an AlltoAll topology.\n  Inspired by the Hamiltonian decomposition of complete directed graphs, we\nidentify that modern accelerator topology can be decomposed into multiple\northogonal ring datapaths which can concurrently transfer data without\ninterference. Based on this, we further observe that the Ring AllGather\nprimitive can also be decomposed into the same number of concurrent ring-styled\ndata transfer at every iteration. Based on these insights, we propose TASP, a\ntopology-aware SP method for long-context LLMs that fully utilizes the\ncommunication capacity of modern accelerators via topology decomposition and\nprimitive decomposition. Experimental results on both single-node and\nmulti-node NVIDIA H100 systems and a single-node AMD MI300X system demonstrate\nthat TASP achieves higher communication efficiency than Ring Attention on these\nmodern accelerator topologies and achieves up to 3.58 speedup than Ring\nAttention and its variant Zigzag-Ring Attention. The code is available at\nhttps://github.com/infinigence/HamiltonAttention."
                },
                "authors": [
                    {
                        "name": "Yida Wang"
                    },
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Yuanchao Xu"
                    },
                    {
                        "name": "Wenxun Wang"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "arxiv_affiliation": "Tsinghua University",
                "author": "Yu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.26541v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.26541v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02886v4",
                "updated": "2025-10-09T12:05:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    5,
                    4,
                    3,
                    282,
                    0
                ],
                "published": "2024-11-05T07:56:24Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    7,
                    56,
                    24,
                    1,
                    310,
                    0
                ],
                "title": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection"
                },
                "summary": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid advances in Large Language Models (LLMs) have spurred demand for\nprocessing extended context sequences in contemporary applications. However,\nthis progress faces two challenges: performance degradation due to sequence\nlengths out-of-distribution, and excessively long inference times caused by the\nquadratic computational complexity of attention. These issues limit LLMs in\nlong-context scenarios. In this paper, we propose Dynamic Token-Level KV Cache\nSelection (TokenSelect), a training-free method for efficient and accurate\nlong-context inference. TokenSelect builds upon the observation of\nnon-contiguous attention sparsity, using QK dot products to measure per-head KV\nCache criticality at token-level. By per-head soft voting mechanism,\nTokenSelect selectively involves a few critical KV cache tokens in attention\ncalculation without sacrificing accuracy. To further accelerate TokenSelect, we\ndesign the Selection Cache based on observations of consecutive Query\nsimilarity and implemented the efficient Paged Dot Product Kernel,\nsignificantly reducing the selection overhead. A comprehensive evaluation of\nTokenSelect demonstrates up to $23.84\\times$ speedup in attention computation\nand up to $2.28\\times$ acceleration in end-to-end latency, while providing\nsuperior performance compared to state-of-the-art long-context inference\nmethods."
                },
                "authors": [
                    {
                        "name": "Wei Wu"
                    },
                    {
                        "name": "Zhuoshi Pan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Liyi Chen"
                    },
                    {
                        "name": "Yunchu Bai"
                    },
                    {
                        "name": "Tianfu Wang"
                    },
                    {
                        "name": "Kun Fu"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "arxiv_comment": "Accepted by EMNLP2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.00566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.00566v2",
                "updated": "2025-10-09T12:01:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    1,
                    20,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-01T06:38:45Z",
                "published_parsed": [
                    2025,
                    10,
                    1,
                    6,
                    38,
                    45,
                    2,
                    274,
                    0
                ],
                "title": "Panorama: Fast-Track Nearest Neighbors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Panorama: Fast-Track Nearest Neighbors"
                },
                "summary": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate Nearest-Neighbor Search (ANNS) efficiently finds data items whose\nembeddings are close to that of a given query in a high-dimensional space,\naiming to balance accuracy with speed. Used in recommendation systems, image\nand video retrieval, natural language processing, and retrieval-augmented\ngeneration (RAG), ANNS algorithms such as IVFPQ, HNSW graphs, Annoy, and MRPT\nutilize graph, tree, clustering, and quantization techniques to navigate large\nvector spaces. Despite this progress, ANNS systems spend up to 99\\% of query\ntime to compute distances in their final refinement phase. In this paper, we\npresent PANORAMA, a machine learning-driven approach that tackles the ANNS\nverification bottleneck through data-adaptive learned orthogonal transforms\nthat facilitate the accretive refinement of distance bounds. Such transforms\ncompact over 90\\% of signal energy into the first half of dimensions, enabling\nearly candidate pruning with partial distance computations. We integrate\nPANORAMA into state-of-the-art ANNS methods, namely IVFPQ/Flat, HNSW, MRPT, and\nAnnoy, without index modification, using level-major memory layouts,\nSIMD-vectorized partial distance computations, and cache-aware access patterns.\nExperiments across diverse datasets -- from image-based CIFAR-10 and GIST to\nmodern embedding spaces including OpenAI's Ada 2 and Large 3 -- demonstrate\nthat PANORAMA affords a 2--30$\\times$ end-to-end speedup with no recall loss."
                },
                "authors": [
                    {
                        "name": "Vansh Ramani"
                    },
                    {
                        "name": "Alexis Schlomer"
                    },
                    {
                        "name": "Akash Nayar"
                    },
                    {
                        "name": "Panagiotis Karras"
                    },
                    {
                        "name": "Sayan Ranu"
                    },
                    {
                        "name": "Jignesh M. Patel"
                    }
                ],
                "author_detail": {
                    "name": "Jignesh M. Patel"
                },
                "author": "Jignesh M. Patel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.00566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.00566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19740v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19740v4",
                "updated": "2025-10-09T09:33:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    33,
                    47,
                    3,
                    282,
                    0
                ],
                "published": "2025-08-27T10:11:27Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    10,
                    11,
                    27,
                    2,
                    239,
                    0
                ],
                "title": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spotlight Attention: Towards Efficient LLM Generation via Non-linear\n  Hashing-based KV Cache Retrieval"
                },
                "summary": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reducing the key-value (KV) cache burden in Large Language Models (LLMs)\nsignificantly accelerates inference. Dynamically selecting critical KV caches\nduring decoding helps maintain performance. Existing methods use random linear\nhashing to identify important tokens, but this approach is inefficient due to\nthe orthogonal distribution of queries and keys within two narrow cones in\nLLMs. We introduce Spotlight Attention, a novel method that employs non-linear\nhashing functions to optimize the embedding distribution of queries and keys,\nenhancing coding efficiency and robustness. We also developed a lightweight,\nstable training framework using a Bradley-Terry ranking-based loss, enabling\noptimization of the non-linear hashing module on GPUs with 16GB memory in 8\nhours. Experimental results show that Spotlight Attention drastically improves\nretrieval precision while shortening the length of the hash code at least\n5$\\times$ compared to traditional linear hashing. Finally, we exploit the\ncomputational advantages of bitwise operations by implementing specialized CUDA\nkernels, achieving hashing retrieval for 512K tokens in under 100$\\mu$s on a\nsingle A100 GPU, with end-to-end throughput up to 3$\\times$ higher than vanilla\ndecoding."
                },
                "authors": [
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Yuxin Zhang"
                    },
                    {
                        "name": "Gen Luo"
                    },
                    {
                        "name": "Haiyuan Wan"
                    },
                    {
                        "name": "Ziyang Gong"
                    },
                    {
                        "name": "Fei Chao"
                    },
                    {
                        "name": "Rongrong Ji"
                    }
                ],
                "author_detail": {
                    "name": "Rongrong Ji"
                },
                "author": "Rongrong Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19740v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19740v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.23488v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.23488v2",
                "updated": "2025-10-09T09:14:43Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    9,
                    14,
                    43,
                    3,
                    282,
                    0
                ],
                "published": "2025-06-30T03:22:32Z",
                "published_parsed": [
                    2025,
                    6,
                    30,
                    3,
                    22,
                    32,
                    0,
                    181,
                    0
                ],
                "title": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI-enhanced Low-Altitude UAV-Mounted Stacked Intelligent\n  Metasurfaces"
                },
                "summary": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Wireless communication systems face challenges in meeting the demand for\nhigher data rates and reliable connectivity in complex environments. Stacked\nintelligent metasurfaces (SIMs) have emerged as a promising technology for\nadvanced wave-domain signal processing, where mobile SIMs can outperform fixed\ncounterparts. In this paper, we propose a novel unmanned aerial vehicle\n(UAV)-mounted SIM (UAV-SIM) assisted communication system within low-altitude\neconomy (LAE) networks, where UAVs act as both cache-enabled base stations and\nmobile SIM carriers to enhance uplink transmissions. To maximize network\ncapacity, we formulate a UAV-SIM-based joint optimization problem (USBJOP) that\nintegrates user association, UAV-SIM three-dimensional positioning, and\nmulti-layer SIM phase shift design. Due to the non-convexity and NP-hardness of\nUSBJOP, we decompose it into three subproblems, which are the association\nbetween UAV-SIMs and users optimization problem (AUUOP), the UAV location\noptimization problem (ULOP), and the UAV-SIM phase shifts optimization problem\n(USPSOP). Then, we solve them through an alternating optimization strategy.\nSpecifically, AUUOP and ULOP are transformed into convex forms solvable via the\nCVX tool, while USPSOP is addressed by a generative artificial intelligence\n(GAI)-based hybrid optimization algorithm. Simulation results show that the\nproposed approach achieves approximately 1.5 times higher network capacity\ncompared with suboptimal schemes, effectively mitigates multi-user interference\nwith increasing SIM layers and meta-atoms, and reduces runtime by 10\\% while\nmaintaining solution quality, thereby demonstrating its practicality for\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Lei Zhang"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiahui Li"
                    },
                    {
                        "name": "Chuang Zhang"
                    },
                    {
                        "name": "Linyao Li"
                    },
                    {
                        "name": "Changyuan Zhao"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This paper has been already submitted to TCCN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.23488v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.23488v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.07467v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.07467v4",
                "updated": "2025-10-09T02:37:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    2,
                    37,
                    26,
                    3,
                    282,
                    0
                ],
                "published": "2024-06-11T17:13:18Z",
                "published_parsed": [
                    2024,
                    6,
                    11,
                    17,
                    13,
                    18,
                    1,
                    163,
                    0
                ],
                "title": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM meets ML: Data-efficient Anomaly Detection on Unstable Logs"
                },
                "summary": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Most log-based anomaly detectors assume logs are stable, though logs are\noften unstable due to software or environmental changes. Anomaly detection on\nunstable logs (ULAD) is therefore a more realistic, yet under-investigated\nchallenge. Current approaches predominantly employ machine learning (ML)\nmodels, which often require extensive labeled data for training. To mitigate\ndata insufficiency, we propose FlexLog, a novel hybrid approach for ULAD that\ncombines ML models -- decision tree, k-nearest neighbors, and a feedforward\nneural network -- with a Large Language Model (Mistral) through ensemble\nlearning. FlexLog also incorporates a cache and retrieval-augmented generation\n(RAG) to further enhance efficiency and effectiveness. To evaluate FlexLog, we\nconfigured four datasets for \\task, namely ADFA-U, LOGEVOL-U, SynHDFS-U, and\nSYNEVOL-U. FlexLog outperforms all baselines by at least 1.2 percentage points\n(pp) in F1 score while using much less labeled data (62.87 pp reduction). When\ntrained on the same amount of data as the baselines, FlexLog achieves up to a\n13 pp increase in F1 score on ADFA-U across varying training dataset sizes.\nAdditionally, FlexLog maintains inference time under one second per log\nsequence, making it suitable for most applications, except latency-sensitive\nsystems. Further analysis reveals the positive impact of FlexLog's key\ncomponents: cache, RAG and ensemble learning."
                },
                "authors": [
                    {
                        "name": "Fatemeh Hadadi"
                    },
                    {
                        "name": "Qinghua Xu"
                    },
                    {
                        "name": "Domenico Bianculli"
                    },
                    {
                        "name": "Lionel Briand"
                    }
                ],
                "author_detail": {
                    "name": "Lionel Briand"
                },
                "author": "Lionel Briand",
                "arxiv_doi": "10.1145/3771283",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771283",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2406.07467v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.07467v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by ACM Transactions on Software Engineering and Methodology\n  (TOSEM) 2025",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.21467v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.21467v2",
                "updated": "2025-10-09T01:43:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    43,
                    2,
                    3,
                    282,
                    0
                ],
                "published": "2025-05-27T17:39:39Z",
                "published_parsed": [
                    2025,
                    5,
                    27,
                    17,
                    39,
                    39,
                    1,
                    147,
                    0
                ],
                "title": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashDLM: Accelerating Diffusion Language Model Inference via Efficient\n  KV Caching and Guided Diffusion"
                },
                "summary": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion language models offer parallel token generation and inherent\nbidirectionality, promising more efficient and powerful sequence modeling\ncompared to autoregressive approaches. However, state-of-the-art diffusion\nmodels (e.g., Dream 7B, LLaDA 8B) suffer from slow inference. While they match\nthe quality of similarly sized autoregressive (AR) models (e.g., Qwen2.5 7B,\nLlama3 8B), their iterative denoising requires multiple full-sequence forward\npasses, resulting in high computational costs and latency, particularly for\nlong input prompts and long-context scenarios. Furthermore, parallel token\ngeneration introduces token incoherence problems, and current sampling\nheuristics suffer from significant quality drops with decreasing denoising\nsteps. We address these limitations with two training-free techniques. First,\nwe propose FreeCache, a Key-Value (KV) approximation caching technique that\nreuses stable KV projections across denoising steps, effectively reducing the\ncomputational cost of DLM inference. Second, we introduce Guided Diffusion, a\ntraining-free method that uses a lightweight pretrained autoregressive model to\nsupervise token unmasking, dramatically reducing the total number of denoising\niterations without sacrificing quality. We conduct extensive evaluations on\nopen-source reasoning benchmarks, and our combined methods deliver an average\nof 12.14x end-to-end speedup across various tasks with negligible accuracy\ndegradation. For the first time, diffusion language models achieve a comparable\nand even faster latency as the widely adopted autoregressive models. Our work\nsuccessfully paved the way for scaling up the diffusion language model to a\nbroader scope of applications across different domains."
                },
                "authors": [
                    {
                        "name": "Zhanqiu Hu"
                    },
                    {
                        "name": "Jian Meng"
                    },
                    {
                        "name": "Yash Akhauri"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Jae-sun Seo"
                    },
                    {
                        "name": "Zhiru Zhang"
                    },
                    {
                        "name": "Udit Gupta"
                    }
                ],
                "author_detail": {
                    "name": "Udit Gupta"
                },
                "author": "Udit Gupta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.21467v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.21467v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07667v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07667v1",
                "updated": "2025-10-09T01:40:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T01:40:39Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    1,
                    40,
                    39,
                    3,
                    282,
                    0
                ],
                "title": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Energy-Efficient Edge Coprocessor for Neural Rendering with Explicit\n  Data Reuse Strategies"
                },
                "summary": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural radiance fields (NeRF) have transformed 3D reconstruction and\nrendering, facilitating photorealistic image synthesis from sparse viewpoints.\nThis work introduces an explicit data reuse neural rendering (EDR-NR)\narchitecture, which reduces frequent external memory accesses (EMAs) and cache\nmisses by exploiting the spatial locality from three phases, including rays,\nray packets (RPs), and samples. The EDR-NR architecture features a four-stage\nscheduler that clusters rays on the basis of Z-order, prioritize lagging rays\nwhen ray divergence happens, reorders RPs based on spatial proximity, and\nissues samples out-of-orderly (OoO) according to the availability of on-chip\nfeature data. In addition, a four-tier hierarchical RP marching (HRM) technique\nis integrated with an axis-aligned bounding box (AABB) to facilitate spatial\nskipping (SS), reducing redundant computations and improving throughput.\nMoreover, a balanced allocation strategy for feature storage is proposed to\nmitigate SRAM bank conflicts. Fabricated using a 40 nm process with a die area\nof 10.5 mmX, the EDR-NR chip demonstrates a 2.41X enhancement in normalized\nenergy efficiency, a 1.21X improvement in normalized area efficiency, a 1.20X\nincrease in normalized throughput, and a 53.42% reduction in on-chip SRAM\nconsumption compared to state-of-the-art accelerators."
                },
                "authors": [
                    {
                        "name": "Binzhe Yuan"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Zeyu Zheng"
                    },
                    {
                        "name": "Yuefeng Zhang"
                    },
                    {
                        "name": "Haochuan Wan"
                    },
                    {
                        "name": "Zhechen Yuan"
                    },
                    {
                        "name": "Junsheng Chen"
                    },
                    {
                        "name": "Yunxiang He"
                    },
                    {
                        "name": "Junran Ding"
                    },
                    {
                        "name": "Xiaoming Zhang"
                    },
                    {
                        "name": "Chaolin Rao"
                    },
                    {
                        "name": "Wenyan Su"
                    },
                    {
                        "name": "Pingqiang Zhou"
                    },
                    {
                        "name": "Jingyi Yu"
                    },
                    {
                        "name": "Xin Lou"
                    }
                ],
                "author_detail": {
                    "name": "Xin Lou"
                },
                "author": "Xin Lou",
                "arxiv_comment": "11 pages, 17 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07667v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07667v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07651v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07651v1",
                "updated": "2025-10-09T00:58:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "published": "2025-10-09T00:58:28Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    0,
                    58,
                    28,
                    3,
                    282,
                    0
                ],
                "title": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OBCache: Optimal Brain KV Cache Pruning for Efficient Long-Context LLM\n  Inference"
                },
                "summary": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with extended context windows enable powerful\ndownstream applications but impose significant memory overhead, as caching all\nkey-value (KV) states scales linearly with sequence length and batch size.\nExisting cache eviction methods address this by exploiting attention sparsity,\nyet they typically rank tokens heuristically using accumulated attention\nweights without considering their true impact on attention outputs. We propose\nOptimal Brain Cache (OBCache), a principled framework that formulates cache\neviction as a layer-wise structured pruning problem. Building upon the Optimal\nBrain Damage (OBD) theory, OBCache quantifies token saliency by measuring the\nperturbation in attention outputs induced by pruning tokens, with closed-form\nscores derived for isolated keys, isolated values, and joint key-value pairs.\nOur scores account not only for attention weights but also for information from\nvalue states and attention outputs, thereby enhancing existing eviction\nstrategies with output-aware signals. Experiments on LLaMA and Qwen models\ndemonstrate that replacing the heuristic scores in existing works, which\nestimate token saliency across different query positions, with OBCache's\noutput-aware scores consistently improves long-context accuracy."
                },
                "authors": [
                    {
                        "name": "Yuzhe Gu"
                    },
                    {
                        "name": "Xiyu Liang"
                    },
                    {
                        "name": "Jiaojiao Zhao"
                    },
                    {
                        "name": "Enmao Diao"
                    }
                ],
                "author_detail": {
                    "name": "Enmao Diao"
                },
                "author": "Enmao Diao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07651v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07651v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07499v1",
                "updated": "2025-10-08T19:52:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:52:35Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    52,
                    35,
                    2,
                    281,
                    0
                ],
                "title": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When Thoughts Meet Facts: Reusable Reasoning for Long-Context LMs"
                },
                "summary": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent Long-Context Language Models (LCLMs) can process hundreds of thousands\nof tokens in a single prompt, enabling new opportunities for\nknowledge-intensive multi-hop reasoning by integrating large sets of retrieved\ndocuments or, in some cases, directly all necessary information. However,\nsimply feeding more documents into the context window fails to capture how\nevidence should be connected. We address this gap with thought templates, which\nrecast reasoning as reusable thought caches, derived from prior problem solving\ntraces, structuring how evidence is combined and guiding multi-hop inference\nwith factual documents. To keep these templates effective, we propose an update\nstrategy that iteratively refines templates derived from training data through\nnatural-language feedback. Across diverse benchmarks and LCLM families, our\napproach delivers consistent gains over strong baselines in both\nretrieval-based and retrieval-free settings. Furthermore, we show that\noptimized templates can be distilled into smaller open-source models,\ndemonstrating its broad applicability and transparent reasoning reuse. We refer\nto our framework as Thought Template Augmented LCLMs (ToTAL)."
                },
                "authors": [
                    {
                        "name": "Soyeong Jeong"
                    },
                    {
                        "name": "Taehee Jung"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    },
                    {
                        "name": "Joo-Kyung Kim"
                    },
                    {
                        "name": "Dongyeop Kang"
                    }
                ],
                "author_detail": {
                    "name": "Dongyeop Kang"
                },
                "author": "Dongyeop Kang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07486v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07486v1",
                "updated": "2025-10-08T19:36:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T19:36:11Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    19,
                    36,
                    11,
                    2,
                    281,
                    0
                ],
                "title": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AsyncSpade: Efficient Test-Time Scaling with Asynchronous Sparse\n  Decoding"
                },
                "summary": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time scaling (TTS) boosts LLM reasoning via long chain-of-thought (CoT),\nbut the linear KV-cache growth amplifies the memory-bound bottleneck of LLM\ndecoding. Query-aware page-level sparse decoding can achieve state-of-the-art\nperformance under constrained FLOPs budgets, but is limited by both\nsequential-dependent page filtering and coarse-grained token selection,\nhampering serving efficiency and model performance on TTS tasks under high\nconcurrency and long CoT scenarios (consuming even higher runtime than the\nforward pipeline itself). In this paper, we first find that the current-step\nquery state can be accurately approximated in a unified manner from a short\nwindow of recent queries, enabling training-free query-aware sparsity without\nwaiting in the decoding loop. We propose AsyncSpade, an asynchronous framework\nfor efficient TTS built on two core components: (1) a novel light-weight\ntemporal-regressive module that predicts the next-token query state; (2) an\nasynchronous and disaggregated framework that decouples the KV cache filtering\nfrom the auto-regressive decoding loop, overlapping the token-level KV\nselection with the forward inference computation through asynchronism. To our\nknowledge, AsyncSpade is the first to eliminate the sequential dependence\nwithout sacrificing model performance. We validate the effectiveness of\nAsyncSpade on common LLM serving setups with an A100 node, where AsyncSpade\nfully overlaps KV-cache operations with the inference pipeline, achieving\ntheoretical optimal time-per-output-token (TPOT). Specifically, AsyncSpade\ndelivers over 20% reduction on TPOT compared to SoTA baseline (i.e. Quest) and\nat least 50% TPOT reduction compared to full attention on Qwen3-8B and\nQwen3-32B models, while matching or surpassing their accuracy on various TTS\nbenchmarks (AIME-24/25, GPQA-Diamond, MATH-500)."
                },
                "authors": [
                    {
                        "name": "Shuqing Luo"
                    },
                    {
                        "name": "Yilin Guan"
                    },
                    {
                        "name": "Pingzhi Li"
                    },
                    {
                        "name": "Hanrui Wang"
                    },
                    {
                        "name": "Tianlong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Tianlong Chen"
                },
                "author": "Tianlong Chen",
                "arxiv_comment": "14 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07486v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07486v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.18344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.18344v2",
                "updated": "2025-10-08T18:16:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    18,
                    16,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-09-22T19:08:57Z",
                "published_parsed": [
                    2025,
                    9,
                    22,
                    19,
                    8,
                    57,
                    0,
                    265,
                    0
                ],
                "title": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculate Deep and Accurate: Lossless and Training-Free Acceleration for\n  Offloaded LLMs via Substitute Speculative Decoding"
                },
                "summary": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The immense model sizes of large language models (LLMs) challenge deployment\non memory-limited consumer GPUs. Although model compression and parameter\noffloading are common strategies to address memory limitations, compression can\ndegrade quality, and offloading maintains quality but suffers from slow\ninference. Speculative decoding presents a promising avenue to accelerate\nparameter offloading, utilizing a fast draft model to propose multiple draft\ntokens, which are then verified by the target LLM in parallel with a single\nforward pass. This method reduces the time-consuming data transfers in forward\npasses that involve offloaded weight transfers. Existing methods often rely on\npretrained weights of the same family, but require additional training to align\nwith custom-trained models. Moreover, approaches that involve draft model\ntraining usually yield only modest speedups. This limitation arises from\ninsufficient alignment with the target model, preventing higher token\nacceptance lengths. To address these challenges and achieve greater speedups,\nwe propose SubSpec, a plug-and-play method to accelerate parameter offloading\nthat is lossless and training-free. SubSpec constructs a highly aligned draft\nmodel by generating low-bit quantized substitute layers from offloaded target\nLLM portions. Additionally, our method shares the remaining GPU-resident layers\nand the KV-Cache, further reducing memory overhead and enhance alignment.\nSubSpec achieves a high average acceptance length, delivering 9.1x speedup for\nQwen2.5 7B on MT-Bench (8GB VRAM limit) and an average of 12.5x speedup for\nQwen2.5 32B on popular generation benchmarks (24GB VRAM limit)."
                },
                "authors": [
                    {
                        "name": "Pei-Shuo Wang"
                    },
                    {
                        "name": "Jian-Jia Chen"
                    },
                    {
                        "name": "Chun-Che Yang"
                    },
                    {
                        "name": "Chi-Chih Chang"
                    },
                    {
                        "name": "Ning-Chi Huang"
                    },
                    {
                        "name": "Mohamed S. Abdelfattah"
                    },
                    {
                        "name": "Kai-Chiang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Kai-Chiang Wu"
                },
                "author": "Kai-Chiang Wu",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.18344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.18344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07318v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07318v1",
                "updated": "2025-10-08T17:59:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:59:55Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    59,
                    55,
                    2,
                    281,
                    0
                ],
                "title": "Artificial Hippocampus Networks for Efficient Long-Context Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Hippocampus Networks for Efficient Long-Context Modeling"
                },
                "summary": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-sequence modeling faces a fundamental trade-off between the efficiency\nof compressive fixed-size memory in RNN-like models and the fidelity of\nlossless growing memory in attention-based Transformers. Inspired by the\nMulti-Store Model in cognitive science, we introduce a memory framework of\nartificial neural networks. Our method maintains a sliding window of the\nTransformer's KV cache as lossless short-term memory, while a learnable module\ntermed Artificial Hippocampus Network (AHN) recurrently compresses\nout-of-window information into a fixed-size compact long-term memory. To\nvalidate this framework, we instantiate AHNs using modern RNN-like\narchitectures, including Mamba2, DeltaNet, and Gated DeltaNet. Extensive\nexperiments on long-context benchmarks LV-Eval and InfiniteBench demonstrate\nthat AHN-augmented models consistently outperform sliding window baselines and\nachieve performance comparable or even superior to full-attention models, while\nsubstantially reducing computational and memory requirements. For instance,\naugmenting the Qwen2.5-3B-Instruct with AHNs reduces inference FLOPs by 40.5%\nand memory cache by 74.0%, while improving its average score on LV-Eval (128k\nsequence length) from 4.41 to 5.88. Code is available at:\nhttps://github.com/ByteDance-Seed/AHN."
                },
                "authors": [
                    {
                        "name": "Yunhao Fang"
                    },
                    {
                        "name": "Weihao Yu"
                    },
                    {
                        "name": "Shu Zhong"
                    },
                    {
                        "name": "Qinghao Ye"
                    },
                    {
                        "name": "Xuehan Xiong"
                    },
                    {
                        "name": "Lai Wei"
                    }
                ],
                "author_detail": {
                    "name": "Lai Wei"
                },
                "author": "Lai Wei",
                "arxiv_comment": "Code: https://github.com/ByteDance-Seed/AHN",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07318v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07318v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07297v1",
                "updated": "2025-10-08T17:51:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:51:34Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    51,
                    34,
                    2,
                    281,
                    0
                ],
                "title": "Agentic generative AI for media content discovery at the national\n  football league",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic generative AI for media content discovery at the national\n  football league"
                },
                "summary": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI has unlocked new possibilities in content discovery and\nmanagement. Through collaboration with the National Football League (NFL), we\ndemonstrate how a generative-AI based workflow enables media researchers and\nanalysts to query relevant historical plays using natural language rather than\ntraditional filter-and-click interfaces. The agentic workflow takes a user\nquery as input, breaks it into elements, and translates them into the\nunderlying database query language. Accuracy and latency are further improved\nthrough carefully designed semantic caching. The solution achieves over 95\npercent accuracy and reduces the average time to find relevant videos from 10\nminutes to 30 seconds, significantly increasing the NFL's operational\nefficiency and allowing users to focus on producing creative content and\nengaging storylines."
                },
                "authors": [
                    {
                        "name": "Henry Wang"
                    },
                    {
                        "name": "Md Sirajus Salekin"
                    },
                    {
                        "name": "Jake Lee"
                    },
                    {
                        "name": "Ross Claytor"
                    },
                    {
                        "name": "Shinan Zhang"
                    },
                    {
                        "name": "Michael Chi"
                    }
                ],
                "author_detail": {
                    "name": "Michael Chi"
                },
                "author": "Michael Chi",
                "arxiv_comment": "13 pages, 7 figures, International Sports Analytics Conference and\n  Exhibition",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07293v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07293v1",
                "updated": "2025-10-08T17:50:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T17:50:16Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    50,
                    16,
                    2,
                    281,
                    0
                ],
                "title": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioMarathon: A Comprehensive Benchmark for Long-Context Audio\n  Understanding and Efficiency in Audio LLMs"
                },
                "summary": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-form audio is a major challenge for Large Audio Language\nmodels (LALMs). These models struggle with the quadratic cost of attention\n($O(N^2)$) and with modeling long-range temporal dependencies. Existing audio\nbenchmarks are built mostly from short clips and do not evaluate models in\nrealistic long context settings. To address this gap, we introduce\nAudioMarathon, a benchmark designed to evaluate both understanding and\ninference efficiency on long-form audio. AudioMarathon provides a diverse set\nof tasks built upon three pillars: long-context audio inputs with durations\nranging from 90.0 to 300.0 seconds, which correspond to encoded sequences of\n2,250 to 7,500 audio tokens, respectively, full domain coverage across speech,\nsound, and music, and complex reasoning that requires multi-hop inference. We\nevaluate state-of-the-art LALMs and observe clear performance drops as audio\nlength grows. We also study acceleration techniques and analyze the trade-offs\nof token pruning and KV cache eviction. The results show large gaps across\ncurrent LALMs and highlight the need for better temporal reasoning and\nmemory-efficient architectures. We believe AudioMarathon will drive the audio\nand multimodal research community to develop more advanced audio understanding\nmodels capable of solving complex audio tasks."
                },
                "authors": [
                    {
                        "name": "Peize He"
                    },
                    {
                        "name": "Zichen Wen"
                    },
                    {
                        "name": "Yubo Wang"
                    },
                    {
                        "name": "Yuxuan Wang"
                    },
                    {
                        "name": "Xiaoqian Liu"
                    },
                    {
                        "name": "Jiajie Huang"
                    },
                    {
                        "name": "Zehui Lei"
                    },
                    {
                        "name": "Zhuangcheng Gu"
                    },
                    {
                        "name": "Xiangqi Jin"
                    },
                    {
                        "name": "Jiabing Yang"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Zhifei Liu"
                    },
                    {
                        "name": "Weijia Li"
                    },
                    {
                        "name": "Cunxiang Wang"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Linfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Linfeng Zhang"
                },
                "author": "Linfeng Zhang",
                "arxiv_comment": "26 pages, 23 figures, the code is available at\n  \\url{https://github.com/DabDans/AudioMarathon}",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07293v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07293v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09665v1",
                "updated": "2025-10-08T00:15:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "published": "2025-10-08T00:15:04Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    15,
                    4,
                    2,
                    281,
                    0
                ],
                "title": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LMCache: An Efficient KV Cache Layer for Enterprise-Scale LLM Inference"
                },
                "summary": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Today's LLM inference systems treat individual engines and queries\nindependently for simplicity, but this causes significant resource\ninefficiencies. While there are proposals to avoid redundant computation by\nreusing KV caches across queries and to increase GPU utilization by\ndisaggregating a single query to different engines, their promises cannot be\nrealized without efficiently offloading and communicating KV cache across LLM\ninference engines and queries.\n  We present LMCache, the first and so far the most efficient open-source KV\ncaching solution, which extracts and stores KV caches generated by modern LLM\nengines (vLLM and SGLang) and shares the KV caches across engines and queries.\nLMCache exposes KV caches in the LLM engine interface, effectively transforming\nLLM engines from individual token processors to a collection of engines with KV\ncache as the storage and communication medium. In particular, it supports both\ncache offloading (prefix reuse across queries) and prefill-decode\ndisaggregation (cross-engine cache transfer). LMCache's high performance and\nwide adoption stem from the following contributions: highly optimized KV cache\ndata movement with performance optimizations including batched data movement\noperations, compute and I/O pipelining; a modular KV cache connector component,\ndecoupling LMCache from the rapid evolution of inference engines; a first-class\ncontrol API, such as pinning, lookup, cleanup, movement, and compression, for\nflexible cache orchestration across GPU, CPU, storage, and network layers.\nEvaluation shows that combining LMCache with vLLM achieves up to 15x\nimprovement in throughput across diverse workloads. With a growing community,\nLMCache has seen dramatic growth in adoption by enterprise inference systems,\nwhich provides valuable lessons for future KV caching solutions. The source\ncode of LMCache is at: https://github.com/LMCache/LMCache."
                },
                "authors": [
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Yuhan Liu"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Yuwei An"
                    },
                    {
                        "name": "Xiaokun Chen"
                    },
                    {
                        "name": "Shaoting Feng"
                    },
                    {
                        "name": "Yuyang Huang"
                    },
                    {
                        "name": "Samuel Shen"
                    },
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.15347v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.15347v2",
                "updated": "2025-10-08T00:06:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    8,
                    0,
                    6,
                    52,
                    2,
                    281,
                    0
                ],
                "published": "2025-05-21T10:20:46Z",
                "published_parsed": [
                    2025,
                    5,
                    21,
                    10,
                    20,
                    46,
                    2,
                    141,
                    0
                ],
                "title": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowKV: Enhancing Multi-Turn Conversational Coherence in LLMs via\n  Isolated Key-Value Cache Management"
                },
                "summary": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly deployed in multi-turn\nconversational applications, where the management of the Key-Value (KV) Cache\npresents a significant bottleneck. The linear growth of the KV Cache with\ndialogue history imposes substantial computational costs, and existing eviction\nstrategies often degrade performance by repeatedly compressing early\nconversational context, leading to information loss and context forgetting.\nThis paper introduces FlowKV, a novel \\textbf{multi-turn isolation mechanism}\nfor KV Cache management, which can be applied to any KV Cache compression\nmethod without training. FlowKV's core innovation is a multi-turn isolation\nmechanism that preserves the accumulated compressed KV cache from past turns.\nCompression is then strategically applied only to the newly generated KV pairs\nof the latest completed turn, effectively preventing the re-compression of\nolder context and thereby mitigating catastrophic forgetting. Our results\ndemonstrate that FlowKV consistently and significantly outperforms baseline\nstrategies in maintaining instruction-following accuracy and user preference\nretention from 10.90\\% to 75.40\\%, particularly in later conversational turns."
                },
                "authors": [
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Hong Chen"
                    },
                    {
                        "name": "Xuming Hu"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "NeurIPS 2025 Workshop on Multi-Turn Interactions in Large Language\n  Models",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.15347v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.15347v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04975v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04975v3",
                "updated": "2025-10-07T22:07:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    22,
                    7,
                    44,
                    1,
                    280,
                    0
                ],
                "published": "2024-11-07T18:49:33Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    18,
                    49,
                    33,
                    3,
                    312,
                    0
                ],
                "title": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SuffixDecoding: Extreme Speculative Decoding for Emerging AI\n  Applications"
                },
                "summary": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding is widely adopted to reduce latency in large language\nmodel (LLM) inference by leveraging smaller draft models capable of handling\ndiverse user tasks. However, emerging AI applications, such as LLM-based\nagents, present unique workload characteristics: instead of diverse independent\nrequests, agentic frameworks typically submit repetitive inference requests,\nsuch as multi-agent pipelines performing similar subtasks or self-refinement\nloops iteratively enhancing outputs. These workloads result in long and highly\npredictable sequences, which current speculative decoding methods do not\neffectively exploit. To address this gap, we introduce \\emph{SuffixDecoding}, a\nnovel method that utilizes efficient suffix trees to cache long token sequences\nfrom prompts and previous outputs. By adaptively speculating more tokens when\nacceptance likelihood is high and fewer when it is low, SuffixDecoding\neffectively exploits opportunities for longer speculations while conserving\ncomputation when those opportunities are limited. Evaluations on agentic\nbenchmarks, including SWE-Bench and Text-to-SQL, demonstrate that\nSuffixDecoding achieves speedups of up to 5.3$\\times$, outperforming\nstate-of-the-art methods -- 2.8$\\times$ faster than model-based approaches like\nEAGLE-2/3 and 1.9$\\times$ faster than model-free approaches such as Token\nRecycling. SuffixDecoding is open-sourced at\nhttps://github.com/snowflakedb/ArcticInference"
                },
                "authors": [
                    {
                        "name": "Gabriele Oliaro"
                    },
                    {
                        "name": "Zhihao Jia"
                    },
                    {
                        "name": "Daniel Campos"
                    },
                    {
                        "name": "Aurick Qiao"
                    }
                ],
                "author_detail": {
                    "name": "Aurick Qiao"
                },
                "author": "Aurick Qiao",
                "arxiv_comment": "NeurIPS 2025 (Spotlight)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04975v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04975v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06415v1",
                "updated": "2025-10-07T19:50:52Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T19:50:52Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    19,
                    50,
                    52,
                    1,
                    280,
                    0
                ],
                "title": "Enhanced Breakdown Voltage in $Œ≤$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhanced Breakdown Voltage in $Œ≤$-Ga$_2$O$_3$ Schottky Barrier\n  Diodes via Fast Neutron Irradiation and Electrothermal Annealing"
                },
                "summary": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the impact of fast neutron irradiation and\npost-radiation electro-thermal annealing on the electrical performance of\n$\\beta$-Ga$_2$O$_3$ Schottky barrier diodes. Devices irradiated with 1 MeV\nneutrons at a high fluence of 1E15 n/cm^2 exhibited substantial degradation,\nincluding a drastic reduction in on-current and an increase in on-resistance.\nElectrothermal testing, conducted through simultaneous current-voltage (J-V)\nmeasurements and thermal annealing, resulted in significant recovery. After\nfour cycles of electro-thermal testing, the devices demonstrated significant\nimprovements in performance, with a substantial recovery of on-current and a\nreduction in on-resistance compared to the post-radiation condition,\napproaching pre-radiation levels. Most recovery occurred during the first two\ncycles, with diminishing improvements in later cycles, indicating that most\nthermally recoverable traps were mitigated early. Capacitance-voltage (C-V)\nmeasurements revealed a substantial reduction in carrier concentration,\ndecreasing from 3.2E16 cm^-3 pre-radiation to 5.5E15 cm^-3 after the first\nelectro-thermal testing cycle, indicating an over 82% reduction. Following the\nthird cycle, the carrier concentration partially recovered to 9.9E15 cm^-3,\nreflecting a carrier removal rate of ~22 cm^-1. The breakdown voltage exhibited\na remarkable enhancement, increasing from approximately 300 V to 1.28 kV (a\n~325% improvement) after the first electro-thermal testing, attributed to the\nreduction in carrier concentration by compensating radiation-induced traps.\nSubsequent testing reduced breakdown voltage slightly to 940 V due to partial\nrecovery of carrier concentration, but it remained significantly higher than\npre-radiation levels, highlighting the promise of $\\beta$-Ga$_2$O$_3$ power\ndevices for high-power applications in radiation-intense environments."
                },
                "authors": [
                    {
                        "name": "Saleh Ahmed Khan"
                    },
                    {
                        "name": "Sudipto Saha"
                    },
                    {
                        "name": "Ahmed Ibreljic"
                    },
                    {
                        "name": "Stephen Margiotta"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Walid Amir"
                    },
                    {
                        "name": "Surajit Chakraborty"
                    },
                    {
                        "name": "Uttam Singisetti"
                    },
                    {
                        "name": "A F M Anhar Uddin Bhuiyan"
                    }
                ],
                "author_detail": {
                    "name": "A F M Anhar Uddin Bhuiyan"
                },
                "author": "A F M Anhar Uddin Bhuiyan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.app-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06175v1",
                "updated": "2025-10-07T17:35:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T17:35:28Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    17,
                    35,
                    28,
                    1,
                    280,
                    0
                ],
                "title": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VecInfer: Efficient LLM Inference with Low-Bit KV Cache via\n  Outlier-Suppressed Vector Quantization"
                },
                "summary": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Key-Value (KV) cache introduces substantial memory overhead during large\nlanguage model (LLM) inference. Although existing vector quantization (VQ)\nmethods reduce KV cache usage and provide flexible representational capacity\nacross bit-widths, they suffer severe performance degradation at ultra-low\nbit-widths due to key cache outliers that hinder effective codebook\nutilization. To address this challenge, we propose VecInfer, a novel VQ method\nfor aggressive KV cache compression while enabling efficient inference. By\napplying smooth and Hadamard transformations, VecInfer suppresses outliers in\nthe key cache, enabling the codebook to comprehensively cover the original data\ndistribution and thereby reducing quantization difficulty. To facilitate\nefficient deployment, we design an optimized CUDA kernel that fuses computation\nwith dequantization to minimize memory access overhead. Extensive evaluations\ndemonstrate that VecInfer consistently outperforms existing quantization\nbaselines across both long-context understanding and mathematical reasoning\ntasks. With only 2-bit quantization, VecInfer achieves performance comparable\nto full precision, while delivering up to $\\mathbf{2.7\\times}$ speedup in\nlarge-batch self-attention computation and $\\mathbf{8.3\\times}$ reduction in\nsingle-batch end-to-end latency on Llama-3.1-8B with a 196k sequence length."
                },
                "authors": [
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Zhengyang Tong"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Jian Luan"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05686v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05686v1",
                "updated": "2025-10-07T08:43:07Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T08:43:07Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    8,
                    43,
                    7,
                    1,
                    280,
                    0
                ],
                "title": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On Enhancing Delay SLAs in TCP Networks through Joint Routing and\n  Transport Assistant Deployment"
                },
                "summary": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transport Control Protocol has long been the primary transport protocol\nfor applications requiring performance and reliability over the Internet.\nUnfortunately, due its retransmission mechanism, TCP incurs high packet\ndelivery delays when segments are lost. To address this issue, previous\nresearch proposed to use a novel network function, namely Transport Assistant,\ndeployed within the network to cache and retransmit lost packets, thus reducing\nretransmission delays. In this paper, we propose to jointly route the flows and\ndeploy TAs in order to minimize packet delivery delays in best-effort networks\n(scenario 1) or to satisfy delay-based Service Level Agreements in QoS-based\nnetworks (scenario 2). We hence formulate the joint routing and TA deployment\nproblem as Integer Linear Program for the two scenarios and propose a heuristic\nsolution for large-scale instances of the problem. Through extensive\nsimulations, we demonstrate the benefits of performing joint routing flows and\nTA deployment in reducing packet delivery delays (up to 16.4%) while minimizing\ndeployment costs (up to 60.98%)."
                },
                "authors": [
                    {
                        "name": "Jos√© G√≥mez-delaHiz"
                    },
                    {
                        "name": "Mohamed Faten Zhani"
                    },
                    {
                        "name": "Jaime Gal√°n-Jim√©nez"
                    },
                    {
                        "name": "John Kaippallimalil"
                    }
                ],
                "author_detail": {
                    "name": "John Kaippallimalil"
                },
                "author": "John Kaippallimalil",
                "arxiv_comment": "10 pages, 17 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05686v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05686v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05529v1",
                "updated": "2025-10-07T02:39:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T02:39:35Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    2,
                    39,
                    35,
                    1,
                    280,
                    0
                ],
                "title": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "H1B-KV: Hybrid One-Bit Caches for Memory-Efficient Large Language Model\n  Inference"
                },
                "summary": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Autoregressive decoding in large language models (LLMs) requires caching a\ngrowing list of past key-value (KV) pairs, making long-context inference a\nmemory-bound problem. While recent methods have explored quantizing the cache,\nevicting tokens, or using binary sketches for keys (e.g., Loki), these\napproaches often provide an incomplete solution by leaving one component (like\nvalues) uncompressed or by discarding context information. This paper\nintroduces the Hybrid One-Bit KV Cache (H1B-KV), a comprehensive compression\nscheme that radically reduces memory usage without sacrificing context. H1B-KV\nrepresents each key vector using a 1-bit binary sketch, enabling\nhardware-friendly bitwise attention, and further compresses value vectors using\n4-bit quantization. This holistic, hybrid approach allows a 7-billion parameter\nLLM to handle an 8k-token context with under 60 MB of cache memory - a 70x\nreduction. We demonstrate that after a lightweight finetuning, H1B-KV matches\nfull-precision performance not only on perplexity benchmarks but also on\ncomplex downstream tasks like mathematical reasoning (GSM8K), multi-task\nunderstanding (MMLU), and code generation (HumanEval). Our results show H1B-KV\nsignificantly outperforms leading quantization (KIVI), token eviction\n(SparseLLM), and key-only sketching (Loki) methods in quality-per-byte,\nestablishing it as a robust solution for deploying LLMs in memory-constrained\nenvironments."
                },
                "authors": [
                    {
                        "name": "Harshil Vejendla"
                    }
                ],
                "author_detail": {
                    "name": "Harshil Vejendla"
                },
                "author": "Harshil Vejendla",
                "arxiv_comment": "MIT URTC 2025 Technical Paper (Oral), 5 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05476v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05476v1",
                "updated": "2025-10-07T00:32:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "published": "2025-10-07T00:32:45Z",
                "published_parsed": [
                    2025,
                    10,
                    7,
                    0,
                    32,
                    45,
                    1,
                    280,
                    0
                ],
                "title": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "cMPI: Using CXL Memory Sharing for MPI One-Sided and Two-Sided\n  Inter-Node Communications"
                },
                "summary": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Message Passing Interface (MPI) is a foundational programming model for\nhigh-performance computing. MPI libraries traditionally employ network\ninterconnects (e.g., Ethernet and InfiniBand) and network protocols (e.g., TCP\nand RoCE) with complex software stacks for cross-node communication. We present\ncMPI, the first work to optimize MPI point-to-point communication (both\none-sided and two-sided) using CXL memory sharing on a real CXL platform,\ntransforming cross-node communication into memory transactions and data copies\nwithin CXL memory, bypassing traditional network protocols. We analyze\nperformance across various interconnects and find that CXL memory sharing\nachieves 7.2x-8.1x lower latency than TCP-based interconnects deployed in\nsmall- and medium-scale clusters. We address challenges of CXL memory sharing\nfor MPI communication, including data object management over the dax\nrepresentation [50], cache coherence, and atomic operations. Overall, cMPI\noutperforms TCP over standard Ethernet NIC and high-end SmartNIC by up to 49x\nand 72x in latency and bandwidth, respectively, for small messages."
                },
                "authors": [
                    {
                        "name": "Xi Wang"
                    },
                    {
                        "name": "Bin Ma"
                    },
                    {
                        "name": "Jongryool Kim"
                    },
                    {
                        "name": "Byungil Koh"
                    },
                    {
                        "name": "Hoshik Kim"
                    },
                    {
                        "name": "Dong Li"
                    }
                ],
                "author_detail": {
                    "name": "Dong Li"
                },
                "author": "Dong Li",
                "arxiv_doi": "10.1145/3712285.3759816",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3712285.3759816",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.05476v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05476v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05373v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05373v1",
                "updated": "2025-10-06T21:08:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T21:08:11Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    21,
                    8,
                    11,
                    0,
                    279,
                    0
                ],
                "title": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLinC : KV Cache Quantization with Hadamard Rotation and Linear\n  Correction"
                },
                "summary": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantizing the key-value (KV) cache is a promising strategy for improving the\ninference efficiency of large language models (LLMs). However, aggressive\nquantization to very low precision (e.g., 2 bits) introduces significant errors\nin the stored key and value tensors, which propagate through the dot-product\nattention mechanism and ultimately degrade generation quality. To address this,\nwe propose KVLinC, a framework to mitigate attention errors introduced by KV\ncache quantization in the extreme low-precision regime. KVLinC combines a\nHadamard rotation, which reduces quantization error in values, with lightweight\nlinear correction adapters that explicitly compensate for errors introduced by\nquantized keys. Across extensive evaluations on the LLaMA, Qwen2.5, and Qwen3\nmodel families, KVLinC consistently matches or surpasses strong baselines while\nachieving higher KV-cache compression. Furthermore, we implement a custom\nattention kernel that results in upto 2.55x faster inference compared to Flash\nAttention baseline, enabling efficient long-context LLM inference."
                },
                "authors": [
                    {
                        "name": "Utkarsh Saxena"
                    },
                    {
                        "name": "Kaushik Roy"
                    }
                ],
                "author_detail": {
                    "name": "Kaushik Roy"
                },
                "author": "Kaushik Roy",
                "arxiv_comment": "14 pages, 7 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05373v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05373v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.05367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.05367v1",
                "updated": "2025-10-06T20:54:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T20:54:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    20,
                    54,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LightCache: Memory-Efficient, Training-Free Acceleration for Video\n  Generation"
                },
                "summary": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training-free acceleration has emerged as an advanced research area in video\ngeneration based on diffusion models. The redundancy of latents in diffusion\nmodel inference provides a natural entry point for acceleration. In this paper,\nwe decompose the inference process into the encoding, denoising, and decoding\nstages, and observe that cache-based acceleration methods often lead to\nsubstantial memory surges in the latter two stages. To address this problem, we\nanalyze the characteristics of inference across different stages and propose\nstage-specific strategies for reducing memory consumption: 1) Asynchronous\nCache Swapping. 2) Feature chunk. 3) Slicing latents to decode. At the same\ntime, we ensure that the time overhead introduced by these three strategies\nremains lower than the acceleration gains themselves. Compared with the\nbaseline, our approach achieves faster inference speed and lower memory usage,\nwhile maintaining quality degradation within an acceptable range. The Code is\navailable at https://github.com/NKUShaw/LightCache ."
                },
                "authors": [
                    {
                        "name": "Yang Xiao"
                    },
                    {
                        "name": "Gen Li"
                    },
                    {
                        "name": "Kaiyuan Deng"
                    },
                    {
                        "name": "Yushu Wu"
                    },
                    {
                        "name": "Zheng Zhan"
                    },
                    {
                        "name": "Yanzhi Wang"
                    },
                    {
                        "name": "Xiaolong Ma"
                    },
                    {
                        "name": "Bo Hui"
                    }
                ],
                "author_detail": {
                    "name": "Bo Hui"
                },
                "author": "Bo Hui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.05367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.05367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.06416v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.06416v2",
                "updated": "2025-10-06T17:09:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    17,
                    9,
                    39,
                    0,
                    279,
                    0
                ],
                "published": "2025-04-08T20:32:10Z",
                "published_parsed": [
                    2025,
                    4,
                    8,
                    20,
                    32,
                    10,
                    1,
                    98,
                    0
                ],
                "title": "Unifying Autoregressive and Diffusion-Based Sequence Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Autoregressive and Diffusion-Based Sequence Generation"
                },
                "summary": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present significant extensions to diffusion-based sequence generation\nmodels, blurring the line with autoregressive language models. We introduce\nhyperschedules, which assign distinct noise schedules to individual token\npositions, generalizing both autoregressive models (e.g., GPT) and conventional\ndiffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two\nhybrid token-wise noising processes that interpolate between absorbing and\nuniform processes, enabling the model to fix past mistakes, and we introduce a\nnovel inference algorithm that leverages this new feature in a simplified\ncontext inspired from MDLM. To support efficient training and inference, we\ndesign attention masks compatible with KV-caching. Our methods achieve\nstate-of-the-art perplexity and generate diverse, high-quality sequences across\nstandard benchmarks, suggesting a promising path for autoregressive\ndiffusion-based sequence generation. See code and resources at\nhttps://hdlm-colm.github.io/"
                },
                "authors": [
                    {
                        "name": "Nima Fathi"
                    },
                    {
                        "name": "Torsten Scholak"
                    },
                    {
                        "name": "Pierre-Andr√© No√´l"
                    }
                ],
                "author_detail": {
                    "name": "Pierre-Andr√© No√´l"
                },
                "author": "Pierre-Andr√© No√´l",
                "arxiv_comment": "Published as a conference paper at COLM 2025 Website:\n  https://hdlm-colm.github.io/",
                "arxiv_journal_ref": "Second Conference on Language Modeling,\n  https://openreview.net/forum?id=rgq9BFXSFl (2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.06416v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.06416v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.19341v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.19341v2",
                "updated": "2025-10-06T13:23:04Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    13,
                    23,
                    4,
                    0,
                    279,
                    0
                ],
                "published": "2025-09-16T09:14:15Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    9,
                    14,
                    15,
                    1,
                    259,
                    0
                ],
                "title": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-Grained AI Model Caching and Downloading With Coordinated\n  Multipoint Broadcasting in Multi-Cell Edge Networks"
                },
                "summary": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "6G networks are envisioned to support on-demand AI model downloading to\naccommodate diverse inference requirements of end users. By proactively caching\nmodels at edge nodes, users can retrieve the requested models with low latency\nfor on-device AI inference. However, the substantial size of contemporary AI\nmodels poses significant challenges for edge caching under limited storage\ncapacity, as well as for the concurrent delivery of heterogeneous models over\nwireless channels. To address these challenges, we propose a fine-grained AI\nmodel caching and downloading system that exploits parameter reusability,\nstemming from the common practice of fine-tuning task-specific models from a\nshared pre-trained model with frozen parameters. This system selectively caches\nmodel parameter blocks (PBs) at edge nodes, eliminating redundant storage of\nreusable parameters across different cached models. Additionally, it\nincorporates coordinated multipoint (CoMP) broadcasting to simultaneously\ndeliver reusable PBs to multiple users, thereby enhancing downlink spectrum\nutilization. Under this arrangement, we formulate a model downloading delay\nminimization problem to jointly optimize PB caching, migration (among edge\nnodes), and broadcasting beamforming. To tackle this intractable problem, we\ndevelop a distributed multi-agent learning framework that enables edge nodes to\nexplicitly learn mutual influence among their actions, thereby facilitating\ncooperation. Furthermore, a data augmentation approach is proposed to\nadaptively generate synthetic training samples through a predictive model,\nboosting sample efficiency and accelerating policy learning. Both theoretical\nanalysis and simulation experiments validate the superior convergence\nperformance of the proposed learning framework."
                },
                "authors": [
                    {
                        "name": "Yang Fu"
                    },
                    {
                        "name": "Peng Qin"
                    },
                    {
                        "name": "Yueyue Zhang"
                    },
                    {
                        "name": "Pao Cheng"
                    },
                    {
                        "name": "Jun Lu"
                    },
                    {
                        "name": "Yifei Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Wang"
                },
                "author": "Yifei Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.19341v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.19341v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04646v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04646v1",
                "updated": "2025-10-06T09:49:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T09:49:14Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    9,
                    49,
                    14,
                    0,
                    279,
                    0
                ],
                "title": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Predictive Feature Caching for Training-free Acceleration of Molecular\n  Geometry Generation"
                },
                "summary": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flow matching models generate high-fidelity molecular geometries but incur\nsignificant computational costs during inference, requiring hundreds of network\nevaluations. This inference overhead becomes the primary bottleneck when such\nmodels are employed in practice to sample large numbers of molecular\ncandidates. This work discusses a training-free caching strategy that\naccelerates molecular geometry generation by predicting intermediate hidden\nstates across solver steps. The proposed method operates directly on the\nSE(3)-equivariant backbone, is compatible with pretrained models, and is\northogonal to existing training-based accelerations and system-level\noptimizations. Experiments on the GEOM-Drugs dataset demonstrate that caching\nachieves a twofold reduction in wall-clock inference time at matched sample\nquality and a speedup of up to 3x compared to the base model with minimal\nsample quality degradation. Because these gains compound with other\noptimizations, applying caching alongside other general, lossless optimizations\nyield as much as a 7x speedup."
                },
                "authors": [
                    {
                        "name": "Johanna Sommer"
                    },
                    {
                        "name": "John Rachwan"
                    },
                    {
                        "name": "Nils Fleischmann"
                    },
                    {
                        "name": "Stephan G√ºnnemann"
                    },
                    {
                        "name": "Bertrand Charpentier"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Charpentier"
                },
                "author": "Bertrand Charpentier",
                "arxiv_comment": "Accepted at the AI for Science Workshop @ NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04646v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04646v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04525v1",
                "updated": "2025-10-06T06:30:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T06:30:22Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    6,
                    30,
                    22,
                    0,
                    279,
                    0
                ],
                "title": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demystifying MaskGIT Sampler and Beyond: Adaptive Order Selection in\n  Masked Diffusion"
                },
                "summary": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Masked diffusion models have shown promising performance in generating\nhigh-quality samples in a wide range of domains, but accelerating their\nsampling process remains relatively underexplored. To investigate efficient\nsamplers for masked diffusion, this paper theoretically analyzes the MaskGIT\nsampler for image modeling, revealing its implicit temperature sampling\nmechanism. Through this analysis, we introduce the \"moment sampler,\" an\nasymptotically equivalent but more tractable and interpretable alternative to\nMaskGIT, which employs a \"choose-then-sample\" approach by selecting unmasking\npositions before sampling tokens. In addition, we improve the efficiency of\nchoose-then-sample algorithms through two key innovations: a partial caching\ntechnique for transformers that approximates longer sampling trajectories\nwithout proportional computational cost, and a hybrid approach formalizing the\nexploration-exploitation trade-off in adaptive unmasking. Experiments in image\nand text domains demonstrate our theory as well as the efficiency of our\nproposed methods, advancing both theoretical understanding and practical\nimplementation of masked diffusion samplers."
                },
                "authors": [
                    {
                        "name": "Satoshi Hayakawa"
                    },
                    {
                        "name": "Yuhta Takida"
                    },
                    {
                        "name": "Masaaki Imaizumi"
                    },
                    {
                        "name": "Hiromi Wakaki"
                    },
                    {
                        "name": "Yuki Mitsufuji"
                    }
                ],
                "author_detail": {
                    "name": "Yuki Mitsufuji"
                },
                "author": "Yuki Mitsufuji",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.04492v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.04492v1",
                "updated": "2025-10-06T05:04:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "published": "2025-10-06T05:04:57Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    5,
                    4,
                    57,
                    0,
                    279,
                    0
                ],
                "title": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Probing and Scheduling for Cache-Aided Hybrid\n  Satellite-Terrestrial Networks"
                },
                "summary": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is crucial in hybrid satellite-terrestrial networks to reduce\nlatency, optimize throughput, and improve data availability by storing\nfrequently accessed content closer to users, especially in bandwidth-limited\nsatellite systems, requiring strategic Medium Access Control (MAC) layer. This\npaper addresses throughput optimization in satellite-terrestrial integrated\nnetworks through opportunistic cooperative caching. We propose a joint probing\nand scheduling strategy to enhance content retrieval efficiency. The strategy\nleverages the LEO satellite to probe satellite-to-ground links and cache states\nof multiple cooperative terrestrial stations, enabling dynamic user scheduling\nfor content delivery. Using an optimal stopping theoretic approach with two\nlevels of incomplete information, we make real-time decisions on\nsatellite-terrestrial hybrid links and caching probing. Our threshold-based\nstrategy optimizes probing and scheduling, significantly improving average\nsystem throughput by exploiting cooperative caching, satellite-terrestrial link\ntransmission, and time diversity from dynamic user requests. Simulation results\nvalidate the effectiveness and practicality of the proposed strategies."
                },
                "authors": [
                    {
                        "name": "Zhou Zhang"
                    },
                    {
                        "name": "Yizhu Wang"
                    },
                    {
                        "name": "Saman Atapattu"
                    },
                    {
                        "name": "Sumei Sun"
                    }
                ],
                "author_detail": {
                    "name": "Sumei Sun"
                },
                "author": "Sumei Sun",
                "arxiv_comment": "6 pages, IEEE Global Communications Conference (GLOBECOM), December\n  2025, Taipei, Taiwan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.04492v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.04492v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2510.13805v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13805v1",
                "updated": "2025-10-15T17:59:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    25,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:59:25Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    25,
                    2,
                    288,
                    0
                ],
                "title": "$\\texttt{SBi3PCF:}$ Simulation-based inference with the integrated 3PCF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$\\texttt{SBi3PCF:}$ Simulation-based inference with the integrated 3PCF"
                },
                "summary": "We present $\\texttt{SBi3PCF}$, a simulation-based inference (SBI) framework\nfor analysing a higher-order weak lensing statistic, the integrated 3-point\ncorrelation function (i3PCF). Our approach forward-models the cosmic shear\nfield using the $\\texttt{CosmoGridV1}$ suite of N-body simulations, including a\ncomprehensive set of systematic effects such as intrinsic alignment, baryonic\nfeedback, photometric redshift uncertainty, shear calibration bias, and shape\nnoise. Using this, we have produced a set of DES Y3-like synthetic measurements\nfor 2-point shear correlation functions $\\xi_{\\pm}$ (2PCFs) and i3PCFs\n$\\zeta_{\\pm}$ across 6 cosmological and 11 systematic parameters. Having\nvalidated these measurements against theoretical predictions and thoroughly\nexamined for potential systematic biases, we have found that the impact of\nsource galaxy clustering and reduced shear on the i3PCF is negligible for\nStage-III surveys. Furthermore, we have tested the Gaussianity assumption for\nthe likelihood of our data vector and found that while the sampling\ndistribution of the 2PCF can be well approximated by a Gaussian function, the\nlikelihood of the combined 2PCF + i3PCF data vector including filter sizes of\n$90'$ and larger can deviate from this assumption. Our SBI pipeline employs\nmasked autoregressive flows to perform neural likelihood estimation and is\nvalidated to give statistically accurate posterior estimates. On mock data, we\nfind that including the i3PCF yields a substantial $63.8\\%$ median improvement\nin the figure of merit for $\\Omega_m - \\sigma_8 - w_0$. These findings are\nconsistent with previous works on the i3PCF and demonstrate that our SBI\nframework can achieve the accuracy and realism needed to analyse the i3PCF in\nwide-area weak lensing surveys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present $\\texttt{SBi3PCF}$, a simulation-based inference (SBI) framework\nfor analysing a higher-order weak lensing statistic, the integrated 3-point\ncorrelation function (i3PCF). Our approach forward-models the cosmic shear\nfield using the $\\texttt{CosmoGridV1}$ suite of N-body simulations, including a\ncomprehensive set of systematic effects such as intrinsic alignment, baryonic\nfeedback, photometric redshift uncertainty, shear calibration bias, and shape\nnoise. Using this, we have produced a set of DES Y3-like synthetic measurements\nfor 2-point shear correlation functions $\\xi_{\\pm}$ (2PCFs) and i3PCFs\n$\\zeta_{\\pm}$ across 6 cosmological and 11 systematic parameters. Having\nvalidated these measurements against theoretical predictions and thoroughly\nexamined for potential systematic biases, we have found that the impact of\nsource galaxy clustering and reduced shear on the i3PCF is negligible for\nStage-III surveys. Furthermore, we have tested the Gaussianity assumption for\nthe likelihood of our data vector and found that while the sampling\ndistribution of the 2PCF can be well approximated by a Gaussian function, the\nlikelihood of the combined 2PCF + i3PCF data vector including filter sizes of\n$90'$ and larger can deviate from this assumption. Our SBI pipeline employs\nmasked autoregressive flows to perform neural likelihood estimation and is\nvalidated to give statistically accurate posterior estimates. On mock data, we\nfind that including the i3PCF yields a substantial $63.8\\%$ median improvement\nin the figure of merit for $\\Omega_m - \\sigma_8 - w_0$. These findings are\nconsistent with previous works on the i3PCF and demonstrate that our SBI\nframework can achieve the accuracy and realism needed to analyse the i3PCF in\nwide-area weak lensing surveys."
                },
                "authors": [
                    {
                        "name": "David Gebauer"
                    },
                    {
                        "name": "Anik Halder"
                    },
                    {
                        "name": "Stella Seitz"
                    },
                    {
                        "name": "Dhayaa Anbajagane"
                    }
                ],
                "author_detail": {
                    "name": "Dhayaa Anbajagane"
                },
                "author": "Dhayaa Anbajagane",
                "arxiv_comment": "28 pages + appendix. 21 figures. Comments are welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13805v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13805v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14896v2",
                "updated": "2025-10-15T17:59:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    6,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-20T17:59:51Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    59,
                    51,
                    2,
                    232,
                    0
                ],
                "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs"
                },
                "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. Our code is\npublicly available at https://github.com/FelixMessi/QDLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. Our code is\npublicly available at https://github.com/FelixMessi/QDLM."
                },
                "authors": [
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Zhenan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Sun"
                },
                "author": "Zhenan Sun",
                "arxiv_comment": "Technical Report, Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13800v1",
                "updated": "2025-10-15T17:58:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    58,
                    8,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:58:08Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    58,
                    8,
                    2,
                    288,
                    0
                ],
                "title": "Reasoning in Space via Grounding in the World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning in Space via Grounding in the World"
                },
                "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Zekun Qi"
                    },
                    {
                        "name": "Wenyao Zhang"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Peidong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peidong Liu"
                },
                "author": "Peidong Liu",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07312v2",
                "updated": "2025-10-15T17:57:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    26,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-08T17:58:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    58,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning"
                },
                "summary": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x. It\nalso transfers significantly to diverse out-of-distribution ReasoningGym\ndomains and long-context benchmarks, indicating broader generalization.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x. It\nalso transfers significantly to diverse out-of-distribution ReasoningGym\ndomains and long-context benchmarks, indicating broader generalization.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Alesia Ivanova"
                    },
                    {
                        "name": "Ziyang Cai"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Riashat Islam"
                    },
                    {
                        "name": "Shital Shah"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Charles London"
                    }
                ],
                "author_detail": {
                    "name": "Charles London"
                },
                "author": "Charles London",
                "arxiv_comment": "Preprint, 31 pages, 8 figures, long-horizon reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14304v2",
                "updated": "2025-10-15T17:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    43,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-18T18:21:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    18,
                    21,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Aligning Large Language Models to Low-Resource Languages through\n  LLM-Based Selective Translation: A Systematic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models to Low-Resource Languages through\n  LLM-Based Selective Translation: A Systematic Study"
                },
                "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs."
                },
                "authors": [
                    {
                        "name": "Rakesh Paul"
                    },
                    {
                        "name": "Anusha Kamath"
                    },
                    {
                        "name": "Kanishk Singla"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Utkarsh Vaidya"
                    },
                    {
                        "name": "Sanjay Singh Chauhan"
                    },
                    {
                        "name": "Niranjan Wartikar"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Wartikar"
                },
                "author": "Niranjan Wartikar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13786v1",
                "updated": "2025-10-15T17:43:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    43,
                    3,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:43:03Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    43,
                    3,
                    2,
                    288,
                    0
                ],
                "title": "The Art of Scaling Reinforcement Learning Compute for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Art of Scaling Reinforcement Learning Compute for LLMs"
                },
                "summary": "Reinforcement learning (RL) has become central to training large language\nmodels (LLMs), yet the field lacks predictive scaling methodologies comparable\nto those established for pre-training. Despite rapidly rising compute budgets,\nthere is no principled understanding of how to evaluate algorithmic\nimprovements for scaling RL compute. We present the first large-scale\nsystematic study, amounting to more than 400,000 GPU-hours, that defines a\nprincipled framework for analyzing and predicting RL scaling in LLMs. We fit\nsigmoidal compute-performance curves for RL training and ablate a wide range of\ncommon design choices to analyze their effects on asymptotic performance and\ncompute efficiency. We observe: (1) Not all recipes yield similar asymptotic\nperformance, (2) Details such as loss aggregation, normalization, curriculum,\nand off-policy algorithm primarily modulate compute efficiency without\nmaterially shifting the asymptote, and (3) Stable, scalable recipes follow\npredictable scaling trajectories, enabling extrapolation from smaller-scale\nruns. Combining these insights, we propose a best-practice recipe, ScaleRL, and\ndemonstrate its effectiveness by successfully scaling and predicting validation\nperformance on a single RL run scaled up to 100,000 GPU-hours. Our work\nprovides both a scientific framework for analyzing scaling in RL and a\npractical recipe that brings RL training closer to the predictability long\nachieved in pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become central to training large language\nmodels (LLMs), yet the field lacks predictive scaling methodologies comparable\nto those established for pre-training. Despite rapidly rising compute budgets,\nthere is no principled understanding of how to evaluate algorithmic\nimprovements for scaling RL compute. We present the first large-scale\nsystematic study, amounting to more than 400,000 GPU-hours, that defines a\nprincipled framework for analyzing and predicting RL scaling in LLMs. We fit\nsigmoidal compute-performance curves for RL training and ablate a wide range of\ncommon design choices to analyze their effects on asymptotic performance and\ncompute efficiency. We observe: (1) Not all recipes yield similar asymptotic\nperformance, (2) Details such as loss aggregation, normalization, curriculum,\nand off-policy algorithm primarily modulate compute efficiency without\nmaterially shifting the asymptote, and (3) Stable, scalable recipes follow\npredictable scaling trajectories, enabling extrapolation from smaller-scale\nruns. Combining these insights, we propose a best-practice recipe, ScaleRL, and\ndemonstrate its effectiveness by successfully scaling and predicting validation\nperformance on a single RL run scaled up to 100,000 GPU-hours. Our work\nprovides both a scientific framework for analyzing scaling in RL and a\npractical recipe that brings RL training closer to the predictability long\nachieved in pre-training."
                },
                "authors": [
                    {
                        "name": "Devvrit Khatri"
                    },
                    {
                        "name": "Lovish Madaan"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Rachit Bansal"
                    },
                    {
                        "name": "Sai Surya Duvvuri"
                    },
                    {
                        "name": "Manzil Zaheer"
                    },
                    {
                        "name": "Inderjit S. Dhillon"
                    },
                    {
                        "name": "David Brandfonbrener"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "arxiv_comment": "28 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12470v3",
                "updated": "2025-10-15T17:36:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    36,
                    19,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-18T02:58:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    2,
                    58,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive reasoning abilities, yet\ntheir reliance on structured step-by-step processing reveals a critical\nlimitation. In contrast, human cognition fluidly adapts between intuitive,\nheuristic (System 1) and analytical, deliberative (System 2) reasoning\ndepending on the context. This difference between human cognitive flexibility\nand LLMs' reliance on a single reasoning style raises a critical question:\nwhile human fast heuristic reasoning evolved for its efficiency and\nadaptability, is a uniform reasoning approach truly optimal for LLMs, or does\nits inflexibility make them brittle and unreliable when faced with tasks\ndemanding more agile, intuitive responses? To answer these questions, we\nexplicitly align LLMs to these reasoning styles by curating a dataset with\nvalid System 1 and System 2 answers, and evaluate their performance across\nreasoning benchmarks. Our results reveal an accuracy-efficiency trade-off:\nSystem 2-aligned models excel in arithmetic and symbolic reasoning, while\nSystem 1-aligned models perform better in commonsense reasoning tasks. To\nanalyze the reasoning spectrum, we interpolated between the two extremes by\nvarying the proportion of alignment data, which resulted in a monotonic change\nin accuracy. A mechanistic analysis of model responses shows that System 1\nmodels employ more definitive outputs, whereas System 2 models demonstrate\ngreater uncertainty. Building on these findings, we further combine System 1-\nand System 2-aligned models based on the entropy of their generations, without\nadditional training, and obtain a dynamic model that outperforms across nearly\nall benchmarks. This work challenges the assumption that step-by-step reasoning\nis always optimal and highlights the need for adapting reasoning strategies\nbased on task demands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive reasoning abilities, yet\ntheir reliance on structured step-by-step processing reveals a critical\nlimitation. In contrast, human cognition fluidly adapts between intuitive,\nheuristic (System 1) and analytical, deliberative (System 2) reasoning\ndepending on the context. This difference between human cognitive flexibility\nand LLMs' reliance on a single reasoning style raises a critical question:\nwhile human fast heuristic reasoning evolved for its efficiency and\nadaptability, is a uniform reasoning approach truly optimal for LLMs, or does\nits inflexibility make them brittle and unreliable when faced with tasks\ndemanding more agile, intuitive responses? To answer these questions, we\nexplicitly align LLMs to these reasoning styles by curating a dataset with\nvalid System 1 and System 2 answers, and evaluate their performance across\nreasoning benchmarks. Our results reveal an accuracy-efficiency trade-off:\nSystem 2-aligned models excel in arithmetic and symbolic reasoning, while\nSystem 1-aligned models perform better in commonsense reasoning tasks. To\nanalyze the reasoning spectrum, we interpolated between the two extremes by\nvarying the proportion of alignment data, which resulted in a monotonic change\nin accuracy. A mechanistic analysis of model responses shows that System 1\nmodels employ more definitive outputs, whereas System 2 models demonstrate\ngreater uncertainty. Building on these findings, we further combine System 1-\nand System 2-aligned models based on the entropy of their generations, without\nadditional training, and obtain a dynamic model that outperforms across nearly\nall benchmarks. This work challenges the assumption that step-by-step reasoning\nis always optimal and highlights the need for adapting reasoning strategies\nbased on task demands."
                },
                "authors": [
                    {
                        "name": "Alireza S. Ziabari"
                    },
                    {
                        "name": "Nona Ghazizadeh"
                    },
                    {
                        "name": "Zhivar Sourati"
                    },
                    {
                        "name": "Farzan Karimi-Malekabadi"
                    },
                    {
                        "name": "Payam Piray"
                    },
                    {
                        "name": "Morteza Dehghani"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Dehghani"
                },
                "author": "Morteza Dehghani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13774v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13774v1",
                "updated": "2025-10-15T17:26:24Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    26,
                    24,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:26:24Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    26,
                    24,
                    2,
                    288,
                    0
                ],
                "title": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of\n  Robust Spatial Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UrbanFusion: Stochastic Multimodal Fusion for Contrastive Learning of\n  Robust Spatial Representations"
                },
                "summary": "Forecasting urban phenomena such as housing prices and public health\nindicators requires the effective integration of various geospatial data.\nCurrent methods primarily utilize task-specific models, while recent foundation\nmodels for spatial representations often support only limited modalities and\nlack multimodal fusion capabilities. To overcome these challenges, we present\nUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal\nFusion (SMF). The framework employs modality-specific encoders to process\ndifferent types of inputs, including street view imagery, remote sensing data,\ncartographic maps, and points of interest (POIs) data. These multimodal inputs\nare integrated via a Transformer-based fusion module that learns unified\nrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwide\ndemonstrates UrbanFusion's strong generalization and predictive performance\ncompared to state-of-the-art GeoAI models. Specifically, it 1) outperforms\nprior foundation models on location-encoding, 2) allows multimodal input during\ninference, and 3) generalizes well to regions unseen during training.\nUrbanFusion can flexibly utilize any subset of available modalities for a given\nlocation during both pretraining and inference, enabling broad applicability\nacross diverse data availability scenarios. All source code is available at\nhttps://github.com/DominikM198/UrbanFusion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting urban phenomena such as housing prices and public health\nindicators requires the effective integration of various geospatial data.\nCurrent methods primarily utilize task-specific models, while recent foundation\nmodels for spatial representations often support only limited modalities and\nlack multimodal fusion capabilities. To overcome these challenges, we present\nUrbanFusion, a Geo-Foundation Model (GeoFM) that features Stochastic Multimodal\nFusion (SMF). The framework employs modality-specific encoders to process\ndifferent types of inputs, including street view imagery, remote sensing data,\ncartographic maps, and points of interest (POIs) data. These multimodal inputs\nare integrated via a Transformer-based fusion module that learns unified\nrepresentations. An extensive evaluation across 41 tasks in 56 cities worldwide\ndemonstrates UrbanFusion's strong generalization and predictive performance\ncompared to state-of-the-art GeoAI models. Specifically, it 1) outperforms\nprior foundation models on location-encoding, 2) allows multimodal input during\ninference, and 3) generalizes well to regions unseen during training.\nUrbanFusion can flexibly utilize any subset of available modalities for a given\nlocation during both pretraining and inference, enabling broad applicability\nacross diverse data availability scenarios. All source code is available at\nhttps://github.com/DominikM198/UrbanFusion."
                },
                "authors": [
                    {
                        "name": "Dominik J. M√ºhlematter"
                    },
                    {
                        "name": "Lin Che"
                    },
                    {
                        "name": "Ye Hong"
                    },
                    {
                        "name": "Martin Raubal"
                    },
                    {
                        "name": "Nina Wiedemann"
                    }
                ],
                "author_detail": {
                    "name": "Nina Wiedemann"
                },
                "author": "Nina Wiedemann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13774v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13774v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19831v2",
                "updated": "2025-10-15T17:12:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    12,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-27T12:35:31Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    35,
                    31,
                    2,
                    239,
                    0
                ],
                "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative\n  Analysis"
                },
                "summary": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages."
                },
                "authors": [
                    {
                        "name": "Anusha Kamath"
                    },
                    {
                        "name": "Kanishk Singla"
                    },
                    {
                        "name": "Rakesh Paul"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Utkarsh Vaidya"
                    },
                    {
                        "name": "Sanjay Singh Chauhan"
                    },
                    {
                        "name": "Niranjan Wartikar"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Wartikar"
                },
                "author": "Niranjan Wartikar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13763v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13763v1",
                "updated": "2025-10-15T17:11:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    11,
                    19,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:11:19Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    11,
                    19,
                    2,
                    288,
                    0
                ],
                "title": "PriorGuide: Test-Time Prior Adaptation for Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PriorGuide: Test-Time Prior Adaptation for Simulation-Based Inference"
                },
                "summary": "Amortized simulator-based inference offers a powerful framework for tackling\nBayesian inference in computational fields such as engineering or neuroscience,\nincreasingly leveraging modern generative methods like diffusion models to map\nobserved data to model parameters or future predictions. These approaches yield\nposterior or posterior-predictive samples for new datasets without requiring\nfurther simulator calls after training on simulated parameter-data pairs.\nHowever, their applicability is often limited by the prior distribution(s) used\nto generate model parameters during this training phase. To overcome this\nconstraint, we introduce PriorGuide, a technique specifically designed for\ndiffusion-based amortized inference methods. PriorGuide leverages a novel\nguidance approximation that enables flexible adaptation of the trained\ndiffusion model to new priors at test time, crucially without costly\nretraining. This allows users to readily incorporate updated information or\nexpert knowledge post-training, enhancing the versatility of pre-trained\ninference models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Amortized simulator-based inference offers a powerful framework for tackling\nBayesian inference in computational fields such as engineering or neuroscience,\nincreasingly leveraging modern generative methods like diffusion models to map\nobserved data to model parameters or future predictions. These approaches yield\nposterior or posterior-predictive samples for new datasets without requiring\nfurther simulator calls after training on simulated parameter-data pairs.\nHowever, their applicability is often limited by the prior distribution(s) used\nto generate model parameters during this training phase. To overcome this\nconstraint, we introduce PriorGuide, a technique specifically designed for\ndiffusion-based amortized inference methods. PriorGuide leverages a novel\nguidance approximation that enables flexible adaptation of the trained\ndiffusion model to new priors at test time, crucially without costly\nretraining. This allows users to readily incorporate updated information or\nexpert knowledge post-training, enhancing the versatility of pre-trained\ninference models."
                },
                "authors": [
                    {
                        "name": "Yang Yang"
                    },
                    {
                        "name": "Severi Rissanen"
                    },
                    {
                        "name": "Paul E. Chang"
                    },
                    {
                        "name": "Nasrulloh Loka"
                    },
                    {
                        "name": "Daolang Huang"
                    },
                    {
                        "name": "Arno Solin"
                    },
                    {
                        "name": "Markus Heinonen"
                    },
                    {
                        "name": "Luigi Acerbi"
                    }
                ],
                "author_detail": {
                    "name": "Luigi Acerbi"
                },
                "author": "Luigi Acerbi",
                "arxiv_comment": "35 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13763v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13763v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13760v1",
                "updated": "2025-10-15T17:10:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    10,
                    39,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:10:39Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    10,
                    39,
                    2,
                    288,
                    0
                ],
                "title": "Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for\n  Medical AI Assistants on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for\n  Medical AI Assistants on the Edge"
                },
                "summary": "Vision Transformers (ViTs) have demonstrated strong capabilities in\ninterpreting complex medical imaging data. However, their significant\ncomputational and memory demands pose challenges for deployment in real-time,\nresource-constrained mobile and wearable devices used in clinical environments.\nWe introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI\nassistants that perform structured analysis of medical images directly on the\nedge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical\nimaging and com- bines a training procedure with multi-query attention,\npreserving stability under ternary weights with low-precision activations.\nFurthermore, BiTMedViT employs task-aware distillation from a high-capacity\nteacher to recover accuracy lost due to extreme quantization. Lastly, we also\npresent a pipeline that maps the ternarized ViTs to a custom CUDA kernel for\nefficient memory bandwidth utilization and latency reduction on the Jetson Orin\nNano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on\nMedMNIST across 12 datasets, while reducing model size by 43x, memory traffic\nby 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that\nof SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a\npractical and scientifically grounded route for extreme-precision medical\nimaging ViTs deployable on the edge, narrowing the gap between algorithmic\nadvances and deployable clinical tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have demonstrated strong capabilities in\ninterpreting complex medical imaging data. However, their significant\ncomputational and memory demands pose challenges for deployment in real-time,\nresource-constrained mobile and wearable devices used in clinical environments.\nWe introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI\nassistants that perform structured analysis of medical images directly on the\nedge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical\nimaging and com- bines a training procedure with multi-query attention,\npreserving stability under ternary weights with low-precision activations.\nFurthermore, BiTMedViT employs task-aware distillation from a high-capacity\nteacher to recover accuracy lost due to extreme quantization. Lastly, we also\npresent a pipeline that maps the ternarized ViTs to a custom CUDA kernel for\nefficient memory bandwidth utilization and latency reduction on the Jetson Orin\nNano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on\nMedMNIST across 12 datasets, while reducing model size by 43x, memory traffic\nby 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that\nof SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a\npractical and scientifically grounded route for extreme-precision medical\nimaging ViTs deployable on the edge, narrowing the gap between algorithmic\nadvances and deployable clinical tools."
                },
                "authors": [
                    {
                        "name": "Mikolaj Walczak"
                    },
                    {
                        "name": "Uttej Kallakuri"
                    },
                    {
                        "name": "Edward Humes"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "arxiv_comment": "Accepted at 2025 IEEE/ACM International Conf. on Computer-Aided\n  Design (ICCAD) Oct. 26-30 2025, Munich, DE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12400v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12400v3",
                "updated": "2025-10-15T17:09:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    9,
                    41,
                    2,
                    288,
                    0
                ],
                "published": "2024-10-16T09:28:58Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    28,
                    58,
                    2,
                    290,
                    0
                ],
                "title": "QUIDS: Query Intent Description for Exploratory Search via Dual Space\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUIDS: Query Intent Description for Exploratory Search via Dual Space\n  Modeling"
                },
                "summary": "In exploratory search, users often submit vague queries to investigate\nunfamiliar topics, but receive limited feedback about how the search engine\nunderstood their input. This leads to a self-reinforcing cycle of mismatched\nresults and trial-and-error reformulation. To address this, we study the task\nof generating user-facing natural language query intent descriptions that\nsurface what the system likely inferred the query to mean, based on\npost-retrieval evidence. We propose QUIDS, a method that leverages dual-space\ncontrastive learning to isolate intent-relevant information while suppressing\nirrelevant content. QUIDS combines a dual-encoder representation space with a\ndisentangling decoder that works together to produce concise and accurate\nintent descriptions. Enhanced by intent-driven hard negative sampling, the\nmodel significantly outperforms state-of-the-art baselines across ROUGE,\nBERTScore, and human/LLM evaluations. Our qualitative analysis confirms QUIDS'\neffectiveness in generating accurate intent descriptions for exploratory\nsearch. Our work contributes to improving the interaction between users and\nsearch engines by providing feedback to the user in exploratory search\nsettings. Our code is available at https://github.com/menauwy/QUIDS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In exploratory search, users often submit vague queries to investigate\nunfamiliar topics, but receive limited feedback about how the search engine\nunderstood their input. This leads to a self-reinforcing cycle of mismatched\nresults and trial-and-error reformulation. To address this, we study the task\nof generating user-facing natural language query intent descriptions that\nsurface what the system likely inferred the query to mean, based on\npost-retrieval evidence. We propose QUIDS, a method that leverages dual-space\ncontrastive learning to isolate intent-relevant information while suppressing\nirrelevant content. QUIDS combines a dual-encoder representation space with a\ndisentangling decoder that works together to produce concise and accurate\nintent descriptions. Enhanced by intent-driven hard negative sampling, the\nmodel significantly outperforms state-of-the-art baselines across ROUGE,\nBERTScore, and human/LLM evaluations. Our qualitative analysis confirms QUIDS'\neffectiveness in generating accurate intent descriptions for exploratory\nsearch. Our work contributes to improving the interaction between users and\nsearch engines by providing feedback to the user in exploratory search\nsettings. Our code is available at https://github.com/menauwy/QUIDS"
                },
                "authors": [
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12400v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12400v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13756v1",
                "updated": "2025-10-15T17:05:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    5,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:05:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    5,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "RECODE: Reasoning Through Code Generation for Visual Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RECODE: Reasoning Through Code Generation for Visual Question Answering"
                },
                "summary": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for\nstructured visuals like charts and diagrams, as pixel-based perception lacks a\nmechanism for verification. To address this, we propose to leverage derendering\n-- the process of reverse-engineering visuals into executable code -- as a new\nmodality for verifiable visual reasoning. Specifically, we propose RECODE, an\nagentic framework that first generates multiple candidate programs to reproduce\nthe input image. It then uses a critic to select the most faithful\nreconstruction and iteratively refines the code. This process not only\ntransforms an ambiguous perceptual task into a verifiable, symbolic problem,\nbut also enables precise calculations and logical inferences later on. On\nvarious visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,\nRECODE significantly outperforms methods that do not leverage code or only use\ncode for drawing auxiliary lines or cropping. Our work demonstrates that\ngrounding visual perception in executable code provides a new path toward more\naccurate and verifiable multimodal reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Large Language Models (MLLMs) struggle with precise reasoning for\nstructured visuals like charts and diagrams, as pixel-based perception lacks a\nmechanism for verification. To address this, we propose to leverage derendering\n-- the process of reverse-engineering visuals into executable code -- as a new\nmodality for verifiable visual reasoning. Specifically, we propose RECODE, an\nagentic framework that first generates multiple candidate programs to reproduce\nthe input image. It then uses a critic to select the most faithful\nreconstruction and iteratively refines the code. This process not only\ntransforms an ambiguous perceptual task into a verifiable, symbolic problem,\nbut also enables precise calculations and logical inferences later on. On\nvarious visual reasoning benchmarks such as CharXiv, ChartQA, and Geometry3K,\nRECODE significantly outperforms methods that do not leverage code or only use\ncode for drawing auxiliary lines or cropping. Our work demonstrates that\ngrounding visual perception in executable code provides a new path toward more\naccurate and verifiable multimodal reasoning."
                },
                "authors": [
                    {
                        "name": "Junhong Shen"
                    },
                    {
                        "name": "Mu Cai"
                    },
                    {
                        "name": "Bo Hu"
                    },
                    {
                        "name": "Ameet Talwalkar"
                    },
                    {
                        "name": "David A Ross"
                    },
                    {
                        "name": "Cordelia Schmid"
                    },
                    {
                        "name": "Alireza Fathi"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Fathi"
                },
                "author": "Alireza Fathi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09653v2",
                "updated": "2025-10-15T16:57:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    57,
                    20,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-06T23:28:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    23,
                    28,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and\n  YOLOv5 Object Detectors for Computer Vision and Pattern Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and\n  YOLOv5 Object Detectors for Computer Vision and Pattern Recognition"
                },
                "summary": "This paper presents a comprehensive overview of the Ultralytics YOLO(You Only\nLook Once) family of object detectors, focusing the architectural evolution,\nbenchmarking, deployment perspectives, and future challenges. The review begins\nwith the most recent release, YOLO26 (or YOLOv26), which introduces key\ninnovations including Distribution Focal Loss (DFL) removal, native NMS-free\ninference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label\nAssignment (STAL), and the MuSGD optimizer for stable training. The progression\nis then traced through YOLO11, with its hybrid task assignment and\nefficiency-focused modules; YOLOv8, which advanced with a decoupled detection\nhead and anchor-free predictions; and YOLOv5, which established the modular\nPyTorch foundation that enabled modern YOLO development. Benchmarking on the MS\nCOCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,\nYOLO11, and YOLO26 (YOLOv26), alongside cross-comparisons with YOLOv12,\nYOLOv13, RT-DETR, and DEIM(DETR with Improved Matching). Metrics including\nprecision, recall, F1 score, mean Average Precision, and inference speed are\nanalyzed to highlight trade-offs between accuracy and efficiency. Deployment\nand application perspectives are further discussed, covering export formats,\nquantization strategies, and real-world use in robotics, agriculture,\nsurveillance, and manufacturing. Finally, the paper identifies challenges and\nfuture directions, including dense-scene limitations, hybrid CNN-Transformer\nintegration, open-vocabulary detection, and edge-aware training approaches.\n(Object Detection, YOLOv26, YOLO)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive overview of the Ultralytics YOLO(You Only\nLook Once) family of object detectors, focusing the architectural evolution,\nbenchmarking, deployment perspectives, and future challenges. The review begins\nwith the most recent release, YOLO26 (or YOLOv26), which introduces key\ninnovations including Distribution Focal Loss (DFL) removal, native NMS-free\ninference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label\nAssignment (STAL), and the MuSGD optimizer for stable training. The progression\nis then traced through YOLO11, with its hybrid task assignment and\nefficiency-focused modules; YOLOv8, which advanced with a decoupled detection\nhead and anchor-free predictions; and YOLOv5, which established the modular\nPyTorch foundation that enabled modern YOLO development. Benchmarking on the MS\nCOCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,\nYOLO11, and YOLO26 (YOLOv26), alongside cross-comparisons with YOLOv12,\nYOLOv13, RT-DETR, and DEIM(DETR with Improved Matching). Metrics including\nprecision, recall, F1 score, mean Average Precision, and inference speed are\nanalyzed to highlight trade-offs between accuracy and efficiency. Deployment\nand application perspectives are further discussed, covering export formats,\nquantization strategies, and real-world use in robotics, agriculture,\nsurveillance, and manufacturing. Finally, the paper identifies challenges and\nfuture directions, including dense-scene limitations, hybrid CNN-Transformer\nintegration, open-vocabulary detection, and edge-aware training approaches.\n(Object Detection, YOLOv26, YOLO)"
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13750v1",
                "updated": "2025-10-15T16:55:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    55,
                    56,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:55:56Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    55,
                    56,
                    2,
                    288,
                    0
                ],
                "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation"
                },
                "summary": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment."
                },
                "authors": [
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Vivek Datla"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Alfy Samuel"
                    },
                    {
                        "name": "Daben Liu"
                    },
                    {
                        "name": "Anoop Kumar"
                    },
                    {
                        "name": "Ritesh Soni"
                    }
                ],
                "author_detail": {
                    "name": "Ritesh Soni"
                },
                "author": "Ritesh Soni",
                "arxiv_comment": "UncertaiNLP at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13744v1",
                "updated": "2025-10-15T16:50:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    50,
                    54,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:50:54Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    50,
                    54,
                    2,
                    288,
                    0
                ],
                "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math"
                },
                "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics."
                },
                "authors": [
                    {
                        "name": "Shrey Pandit"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "21 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13738v1",
                "updated": "2025-10-15T16:45:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    45,
                    59,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:45:59Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    45,
                    59,
                    2,
                    288,
                    0
                ],
                "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation"
                },
                "summary": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jingyi Zhou"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Kai Zuo"
                    },
                    {
                        "name": "Manjie Xu"
                    },
                    {
                        "name": "Zhendong Fu"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13734v1",
                "updated": "2025-10-15T16:40:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    40,
                    28,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:40:28Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    40,
                    28,
                    2,
                    288,
                    0
                ],
                "title": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI\n  Clinicians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI\n  Clinicians"
                },
                "summary": "Current benchmarks for AI clinician systems, often based on multiple-choice\nexams or manual rubrics, fail to capture the depth, robustness, and safety\nrequired for real-world clinical practice. To address this, we introduce the\nGAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding\n(cognitive depth), \\textbf{A}dequacy (answer completeness),\n\\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we\ndeveloped a fully automated, guideline-anchored pipeline to construct a\nGAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity\nlimitations of prior work. Our pipeline assembles an evidence neighborhood,\ncreates dual graph and tree representations, and automatically generates\nquestions across G-levels. Rubrics are synthesized by a DeepResearch agent that\nmimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring\nis performed by an ensemble of large language model (LLM) judges. Validation\nconfirmed our automated questions are high-quality and align with clinician\njudgment. Evaluating state-of-the-art models on the benchmark revealed key\nfailure modes: performance degrades sharply with increased reasoning depth\n(G-axis), models struggle with answer completeness (A-axis), and they are\nhighly vulnerable to adversarial perturbations (P-axis) as well as certain\nsafety issues (S-axis). This automated, clinically-grounded approach provides a\nreproducible and scalable method for rigorously evaluating AI clinician systems\nand guiding their development toward safer, more reliable clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current benchmarks for AI clinician systems, often based on multiple-choice\nexams or manual rubrics, fail to capture the depth, robustness, and safety\nrequired for real-world clinical practice. To address this, we introduce the\nGAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding\n(cognitive depth), \\textbf{A}dequacy (answer completeness),\n\\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we\ndeveloped a fully automated, guideline-anchored pipeline to construct a\nGAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity\nlimitations of prior work. Our pipeline assembles an evidence neighborhood,\ncreates dual graph and tree representations, and automatically generates\nquestions across G-levels. Rubrics are synthesized by a DeepResearch agent that\nmimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring\nis performed by an ensemble of large language model (LLM) judges. Validation\nconfirmed our automated questions are high-quality and align with clinician\njudgment. Evaluating state-of-the-art models on the benchmark revealed key\nfailure modes: performance degrades sharply with increased reasoning depth\n(G-axis), models struggle with answer completeness (A-axis), and they are\nhighly vulnerable to adversarial perturbations (P-axis) as well as certain\nsafety issues (S-axis). This automated, clinically-grounded approach provides a\nreproducible and scalable method for rigorously evaluating AI clinician systems\nand guiding their development toward safer, more reliable clinical practice."
                },
                "authors": [
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Dexin Su"
                    },
                    {
                        "name": "Ailing Yu"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Gangzeng Jin"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Jingnan Liu"
                    },
                    {
                        "name": "Hansong Xiao"
                    },
                    {
                        "name": "Hualei Zhou"
                    },
                    {
                        "name": "Dongjie Tao"
                    },
                    {
                        "name": "Chunxiao Guo"
                    },
                    {
                        "name": "Minghui Yang"
                    },
                    {
                        "name": "Yuan Xia"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Qianrui Fan"
                    },
                    {
                        "name": "Yanyun Wang"
                    },
                    {
                        "name": "Shuai Zhen"
                    },
                    {
                        "name": "Kezhong Chen"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zewen Sun"
                    },
                    {
                        "name": "Heng Zhao"
                    },
                    {
                        "name": "Tian Guan"
                    },
                    {
                        "name": "Shaodong Wang"
                    },
                    {
                        "name": "Geyun Chang"
                    },
                    {
                        "name": "Jiaming Deng"
                    },
                    {
                        "name": "Hongchengcheng Chen"
                    },
                    {
                        "name": "Kexin Feng"
                    },
                    {
                        "name": "Ruzhen Li"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Changtai Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Guihu Lin"
                    },
                    {
                        "name": "Peihao Li"
                    },
                    {
                        "name": "Liqi Liu"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Fan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Yang"
                },
                "author": "Fan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13727v1",
                "updated": "2025-10-15T16:30:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    30,
                    57,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:30:57Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    30,
                    57,
                    2,
                    288,
                    0
                ],
                "title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI\n  Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI\n  Guardrails"
                },
                "summary": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails."
                },
                "authors": [
                    {
                        "name": "Ravi Pandya"
                    },
                    {
                        "name": "Madison Bland"
                    },
                    {
                        "name": "Duy P. Nguyen"
                    },
                    {
                        "name": "Changliu Liu"
                    },
                    {
                        "name": "Jaime Fern√°ndez Fisac"
                    },
                    {
                        "name": "Andrea Bajcsy"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bajcsy"
                },
                "author": "Andrea Bajcsy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13724v1",
                "updated": "2025-10-15T16:28:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    28,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:28:34Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    28,
                    34,
                    2,
                    288,
                    0
                ],
                "title": "FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI\n  Model Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI\n  Model Access"
                },
                "summary": "We present the Federated Inference Resource Scheduling Toolkit (FIRST), a\nframework enabling Inference-as-a-Service across distributed High-Performance\nComputing (HPC) clusters. FIRST provides cloud-like access to diverse AI\nmodels, like Large Language Models (LLMs), on existing HPC infrastructure.\nLeveraging Globus Auth and Globus Compute, the system allows researchers to run\nparallel inference workloads via an OpenAI-compliant API on private, secure\nenvironments. This cluster-agnostic API allows requests to be distributed\nacross federated clusters, targeting numerous hosted models. FIRST supports\nmultiple inference backends (e.g., vLLM), auto-scales resources, maintains\n\"hot\" nodes for low-latency execution, and offers both high-throughput batch\nand interactive modes. The framework addresses the growing demand for private,\nsecure, and scalable AI inference in scientific workflows, allowing researchers\nto generate billions of tokens daily on-premises without relying on commercial\ncloud infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Federated Inference Resource Scheduling Toolkit (FIRST), a\nframework enabling Inference-as-a-Service across distributed High-Performance\nComputing (HPC) clusters. FIRST provides cloud-like access to diverse AI\nmodels, like Large Language Models (LLMs), on existing HPC infrastructure.\nLeveraging Globus Auth and Globus Compute, the system allows researchers to run\nparallel inference workloads via an OpenAI-compliant API on private, secure\nenvironments. This cluster-agnostic API allows requests to be distributed\nacross federated clusters, targeting numerous hosted models. FIRST supports\nmultiple inference backends (e.g., vLLM), auto-scales resources, maintains\n\"hot\" nodes for low-latency execution, and offers both high-throughput batch\nand interactive modes. The framework addresses the growing demand for private,\nsecure, and scalable AI inference in scientific workflows, allowing researchers\nto generate billions of tokens daily on-premises without relying on commercial\ncloud infrastructure."
                },
                "authors": [
                    {
                        "name": "Aditya Tanikanti"
                    },
                    {
                        "name": "Benoit C√¥t√©"
                    },
                    {
                        "name": "Yanfei Guo"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Nickolaus Saint"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Ken Raffenetti"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Thomas Uram"
                    },
                    {
                        "name": "Ian Foster"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13713v1",
                "updated": "2025-10-15T16:13:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe"
                },
                "summary": "Pruning is a common technique to reduce the compute and storage requirements\nof Neural Networks. While conventional approaches typically retrain the model\nto recover pruning-induced performance degradation, state-of-the-art Large\nLanguage Model (LLM) pruning methods operate layer-wise, minimizing the\nper-layer pruning error on a small calibration dataset to avoid full\nretraining, which is considered computationally prohibitive for LLMs. However,\nfinding the optimal pruning mask is a hard combinatorial problem and solving it\nto optimality is intractable. Existing methods hence rely on greedy heuristics\nthat ignore the weight interactions in the pruning objective. In this work, we\ninstead consider the convex relaxation of these combinatorial constraints and\nsolve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method\ndrastically reduces the per-layer pruning error, outperforms strong baselines\non state-of-the-art GPT architectures, and remains memory-efficient. We provide\ntheoretical justification by showing that, combined with the convergence\nguarantees of the FW algorithm, we obtain an approximate solution to the\noriginal combinatorial problem upon rounding the relaxed solution to\nintegrality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning is a common technique to reduce the compute and storage requirements\nof Neural Networks. While conventional approaches typically retrain the model\nto recover pruning-induced performance degradation, state-of-the-art Large\nLanguage Model (LLM) pruning methods operate layer-wise, minimizing the\nper-layer pruning error on a small calibration dataset to avoid full\nretraining, which is considered computationally prohibitive for LLMs. However,\nfinding the optimal pruning mask is a hard combinatorial problem and solving it\nto optimality is intractable. Existing methods hence rely on greedy heuristics\nthat ignore the weight interactions in the pruning objective. In this work, we\ninstead consider the convex relaxation of these combinatorial constraints and\nsolve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method\ndrastically reduces the per-layer pruning error, outperforms strong baselines\non state-of-the-art GPT architectures, and remains memory-efficient. We provide\ntheoretical justification by showing that, combined with the convergence\nguarantees of the FW algorithm, we obtain an approximate solution to the\noriginal combinatorial problem upon rounding the relaxed solution to\nintegrality."
                },
                "authors": [
                    {
                        "name": "Christophe Roux"
                    },
                    {
                        "name": "Max Zimmer"
                    },
                    {
                        "name": "Alexandre d'Aspremont"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13714v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13714v1",
                "updated": "2025-10-15T16:13:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "Dedelayed: Deleting remote inference delay via on-device correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dedelayed: Deleting remote inference delay via on-device correction"
                },
                "summary": "Remote inference allows lightweight devices to leverage powerful cloud\nmodels. However, communication network latency makes predictions stale and\nunsuitable for real-time tasks. To address this, we introduce Dedelayed, a\ndelay-corrective method that mitigates arbitrary remote inference delays,\nallowing the local device to produce low-latency outputs in real time. Our\nmethod employs a lightweight local model that processes the current frame and\nfuses in features that a heavyweight remote model computes from past frames. On\nvideo from the BDD100K driving dataset, Dedelayed improves semantic\nsegmentation accuracy over the stronger of the local-only and remote-only\nbaselines across all realistic communication network delays beyond 33 ms.\nWithout incurring additional delay, it improves accuracy by 6.4 mIoU compared\nto fully local inference and 9.8 mIoU compared to remote inference, for a\nround-trip delay of 100 ms. The advantage grows under longer delays and\nhigher-motion scenes, as delay-mitigated split inference sustains accuracy more\neffectively, providing clear advantages for real-time tasks that must remain\naligned with the current world state.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Remote inference allows lightweight devices to leverage powerful cloud\nmodels. However, communication network latency makes predictions stale and\nunsuitable for real-time tasks. To address this, we introduce Dedelayed, a\ndelay-corrective method that mitigates arbitrary remote inference delays,\nallowing the local device to produce low-latency outputs in real time. Our\nmethod employs a lightweight local model that processes the current frame and\nfuses in features that a heavyweight remote model computes from past frames. On\nvideo from the BDD100K driving dataset, Dedelayed improves semantic\nsegmentation accuracy over the stronger of the local-only and remote-only\nbaselines across all realistic communication network delays beyond 33 ms.\nWithout incurring additional delay, it improves accuracy by 6.4 mIoU compared\nto fully local inference and 9.8 mIoU compared to remote inference, for a\nround-trip delay of 100 ms. The advantage grows under longer delays and\nhigher-motion scenes, as delay-mitigated split inference sustains accuracy more\neffectively, providing clear advantages for real-time tasks that must remain\naligned with the current world state."
                },
                "authors": [
                    {
                        "name": "Dan Jacobellis"
                    },
                    {
                        "name": "Mateen Ulhaq"
                    },
                    {
                        "name": "Fabien Racap√©"
                    },
                    {
                        "name": "Hyomin Choi"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13714v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13714v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13709v2",
                "updated": "2025-10-16T03:39:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    3,
                    39,
                    31,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T16:09:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    9,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "Training LLM Agents to Empower Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLM Agents to Empower Humans"
                },
                "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards."
                },
                "authors": [
                    {
                        "name": "Evan Ellis"
                    },
                    {
                        "name": "Vivek Myers"
                    },
                    {
                        "name": "Jens Tuyls"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Anca Dragan"
                    },
                    {
                        "name": "Benjamin Eysenbach"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Eysenbach"
                },
                "author": "Benjamin Eysenbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.07450v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.07450v2",
                "updated": "2025-10-15T16:08:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    8,
                    58,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-10T18:25:01Z",
                "published_parsed": [
                    2025,
                    8,
                    10,
                    18,
                    25,
                    1,
                    6,
                    222,
                    0
                ],
                "title": "Health Care Waste Classification Using Deep Learning Aligned with\n  Nepal's Bin Color Guidelines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Health Care Waste Classification Using Deep Learning Aligned with\n  Nepal's Bin Color Guidelines"
                },
                "summary": "The increasing number of Health Care facilities in Nepal has added up the\nchallenges on managing health care waste (HCW). Improper segregation and\ndisposal of HCW leads to contamination, spreading of infectious diseases and\nrisk for waste handlers. This study benchmarks the state of the art waste\nclassification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S, YOLOv8-n and\nYOLOv5-s using stratified 5-fold cross-validation technique on combined HCW\ndata. YOLOv5-s achieved the highest accuracy (95.06%) but fell short with the\nYOLOv8-n model in inference speed with few milliseconds. The EfficientNet-B0\nshowed promising results of 93.22% accuracy but took the highest inference\ntime. Following a repetitive ANOVA test to confirm the statistical\nsignificance, the best performing model (YOLOv5-s) was deployed to the web with\nbin color mapped using Nepal's HCW management standards. Further work is\nsuggested to address data limitation and ensure localized context.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing number of Health Care facilities in Nepal has added up the\nchallenges on managing health care waste (HCW). Improper segregation and\ndisposal of HCW leads to contamination, spreading of infectious diseases and\nrisk for waste handlers. This study benchmarks the state of the art waste\nclassification models: ResNeXt-50, EfficientNet-B0, MobileNetV3-S, YOLOv8-n and\nYOLOv5-s using stratified 5-fold cross-validation technique on combined HCW\ndata. YOLOv5-s achieved the highest accuracy (95.06%) but fell short with the\nYOLOv8-n model in inference speed with few milliseconds. The EfficientNet-B0\nshowed promising results of 93.22% accuracy but took the highest inference\ntime. Following a repetitive ANOVA test to confirm the statistical\nsignificance, the best performing model (YOLOv5-s) was deployed to the web with\nbin color mapped using Nepal's HCW management standards. Further work is\nsuggested to address data limitation and ensure localized context."
                },
                "authors": [
                    {
                        "name": "Suman Kunwar"
                    },
                    {
                        "name": "Prabesh Rai"
                    }
                ],
                "author_detail": {
                    "name": "Prabesh Rai"
                },
                "author": "Prabesh Rai",
                "arxiv_comment": "7 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.07450v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.07450v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08615v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08615v3",
                "updated": "2025-10-16T03:40:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    3,
                    40,
                    22,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-08T01:26:48Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    1,
                    26,
                    48,
                    2,
                    281,
                    0
                ],
                "title": "Iterative LLM-Based Generation and Refinement of Distracting Conditions\n  in Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative LLM-Based Generation and Refinement of Distracting Conditions\n  in Math Word Problems"
                },
                "summary": "Mathematical reasoning serves as a crucial testbed for the intelligence of\nlarge language models (LLMs), and math word problems (MWPs) are a popular type\nof math problems. Most MWP datasets consist of problems containing only the\nnecessary information, while problems with distracting and excessive conditions\nare often overlooked. Prior works have tested popular LLMs and found a dramatic\nperformance drop in the presence of distracting conditions. However, datasets\nof MWPs with distracting conditions are limited, and most suffer from lower\nlevels of difficulty and out-of-context expressions. This makes distracting\nconditions easy to identify and exclude, thus reducing the credibility of\nbenchmarking on them. Moreover, when adding distracting conditions, the\nreasoning and answers may also change, requiring intensive labor to check and\nwrite the solutions. To address these issues, we design an iterative framework\nto generate distracting conditions using LLMs. We develop a set of prompts to\nrevise MWPs from different perspectives and cognitive levels, encouraging the\ngeneration of distracting conditions as well as suggestions for further\nrevision. Another advantage is the shared solutions between original and\nrevised problems: we explicitly guide the LLMs to generate distracting\nconditions that do not alter the original solutions, thus avoiding the need to\ngenerate new solutions. This framework is efficient and easy to deploy,\nreducing the overhead of generating MWPs with distracting conditions while\nmaintaining data quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning serves as a crucial testbed for the intelligence of\nlarge language models (LLMs), and math word problems (MWPs) are a popular type\nof math problems. Most MWP datasets consist of problems containing only the\nnecessary information, while problems with distracting and excessive conditions\nare often overlooked. Prior works have tested popular LLMs and found a dramatic\nperformance drop in the presence of distracting conditions. However, datasets\nof MWPs with distracting conditions are limited, and most suffer from lower\nlevels of difficulty and out-of-context expressions. This makes distracting\nconditions easy to identify and exclude, thus reducing the credibility of\nbenchmarking on them. Moreover, when adding distracting conditions, the\nreasoning and answers may also change, requiring intensive labor to check and\nwrite the solutions. To address these issues, we design an iterative framework\nto generate distracting conditions using LLMs. We develop a set of prompts to\nrevise MWPs from different perspectives and cognitive levels, encouraging the\ngeneration of distracting conditions as well as suggestions for further\nrevision. Another advantage is the shared solutions between original and\nrevised problems: we explicitly guide the LLMs to generate distracting\nconditions that do not alter the original solutions, thus avoiding the need to\ngenerate new solutions. This framework is efficient and easy to deploy,\nreducing the overhead of generating MWPs with distracting conditions while\nmaintaining data quality."
                },
                "authors": [
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Zitao Liu"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08615v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08615v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13702v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13702v1",
                "updated": "2025-10-15T16:00:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    0,
                    26,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:00:26Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    0,
                    26,
                    2,
                    288,
                    0
                ],
                "title": "MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering\n  and Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MVCustom: Multi-View Customized Diffusion via Geometric Latent Rendering\n  and Completion"
                },
                "summary": "Multi-view generation with camera pose control and prompt-based customization\nare both essential elements for achieving controllable generative models.\nHowever, existing multi-view generation models do not support customization\nwith geometric consistency, whereas customization models lack explicit\nviewpoint control, making them challenging to unify. Motivated by these gaps,\nwe introduce a novel task, multi-view customization, which aims to jointly\nachieve multi-view camera pose control and customization. Due to the scarcity\nof training data in customization, existing multi-view generation models, which\ninherently rely on large-scale datasets, struggle to generalize to diverse\nprompts. To address this, we propose MVCustom, a novel diffusion-based\nframework explicitly designed to achieve both multi-view consistency and\ncustomization fidelity. In the training stage, MVCustom learns the subject's\nidentity and geometry using a feature-field representation, incorporating the\ntext-to-video diffusion backbone enhanced with dense spatio-temporal attention,\nwhich leverages temporal coherence for multi-view consistency. In the inference\nstage, we introduce two novel techniques: depth-aware feature rendering\nexplicitly enforces geometric consistency, and consistent-aware latent\ncompletion ensures accurate perspective alignment of the customized subject and\nsurrounding backgrounds. Extensive experiments demonstrate that MVCustom is the\nonly framework that simultaneously achieves faithful multi-view generation and\ncustomization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-view generation with camera pose control and prompt-based customization\nare both essential elements for achieving controllable generative models.\nHowever, existing multi-view generation models do not support customization\nwith geometric consistency, whereas customization models lack explicit\nviewpoint control, making them challenging to unify. Motivated by these gaps,\nwe introduce a novel task, multi-view customization, which aims to jointly\nachieve multi-view camera pose control and customization. Due to the scarcity\nof training data in customization, existing multi-view generation models, which\ninherently rely on large-scale datasets, struggle to generalize to diverse\nprompts. To address this, we propose MVCustom, a novel diffusion-based\nframework explicitly designed to achieve both multi-view consistency and\ncustomization fidelity. In the training stage, MVCustom learns the subject's\nidentity and geometry using a feature-field representation, incorporating the\ntext-to-video diffusion backbone enhanced with dense spatio-temporal attention,\nwhich leverages temporal coherence for multi-view consistency. In the inference\nstage, we introduce two novel techniques: depth-aware feature rendering\nexplicitly enforces geometric consistency, and consistent-aware latent\ncompletion ensures accurate perspective alignment of the customized subject and\nsurrounding backgrounds. Extensive experiments demonstrate that MVCustom is the\nonly framework that simultaneously achieves faithful multi-view generation and\ncustomization."
                },
                "authors": [
                    {
                        "name": "Minjung Shin"
                    },
                    {
                        "name": "Hyunin Cho"
                    },
                    {
                        "name": "Sooyeon Go"
                    },
                    {
                        "name": "Jin-Hwa Kim"
                    },
                    {
                        "name": "Youngjung Uh"
                    }
                ],
                "author_detail": {
                    "name": "Youngjung Uh"
                },
                "author": "Youngjung Uh",
                "arxiv_comment": "Project page: https://minjung-s.github.io/mvcustom",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13702v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13702v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14556v2",
                "updated": "2025-10-15T15:59:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    59,
                    15,
                    2,
                    288,
                    0
                ],
                "published": "2025-04-20T10:05:07Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    10,
                    5,
                    7,
                    6,
                    110,
                    0
                ],
                "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in\n  UAV-assisted Sensor Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enabled In-Context Learning for Data Collection Scheduling in\n  UAV-assisted Sensor Networks"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly being utilized in various\nprivate and commercial applications, e.g., traffic control, parcel delivery,\nand Search and Rescue (SAR) missions. Machine Learning (ML) methods used in\nUAV-Assisted Sensor Networks (UASNETs) and, especially, in Deep Reinforcement\nLearning (DRL) face challenges such as complex and lengthy model training, gaps\nbetween simulation and reality, and low sampling efficiency, which conflict\nwith the urgency of emergencies, such as SAR missions. In this paper, an\nIn-Context Learning (ICL)-Data Collection Scheduling (ICLDC) system is proposed\nas an alternative to DRL in emergencies. The UAV collects sensory data and\ntransmits it to a Large Language Model (LLM), which creates a task description\nin natural language. From this description, the UAV receives a data collection\nschedule that must be executed. A verifier ensures safe UAV operations by\nevaluating the schedules generated by the LLM and overriding unsafe schedules\nbased on predefined rules. The system continuously adapts by incorporating\nfeedback into the task descriptions and using this for future decisions. This\nmethod is tested against jailbreaking attacks, where the task description is\nmanipulated to undermine network performance, highlighting the vulnerability of\nLLMs to such attacks. The proposed ICLDC significantly reduces cumulative\npacket loss compared to both the DQN and Maximum Channel Gain baselines. ICLDC\npresents a promising direction for intelligent scheduling and control in\nUASNETs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) are increasingly being utilized in various\nprivate and commercial applications, e.g., traffic control, parcel delivery,\nand Search and Rescue (SAR) missions. Machine Learning (ML) methods used in\nUAV-Assisted Sensor Networks (UASNETs) and, especially, in Deep Reinforcement\nLearning (DRL) face challenges such as complex and lengthy model training, gaps\nbetween simulation and reality, and low sampling efficiency, which conflict\nwith the urgency of emergencies, such as SAR missions. In this paper, an\nIn-Context Learning (ICL)-Data Collection Scheduling (ICLDC) system is proposed\nas an alternative to DRL in emergencies. The UAV collects sensory data and\ntransmits it to a Large Language Model (LLM), which creates a task description\nin natural language. From this description, the UAV receives a data collection\nschedule that must be executed. A verifier ensures safe UAV operations by\nevaluating the schedules generated by the LLM and overriding unsafe schedules\nbased on predefined rules. The system continuously adapts by incorporating\nfeedback into the task descriptions and using this for future decisions. This\nmethod is tested against jailbreaking attacks, where the task description is\nmanipulated to undermine network performance, highlighting the vulnerability of\nLLMs to such attacks. The proposed ICLDC significantly reduces cumulative\npacket loss compared to both the DQN and Maximum Channel Gain baselines. ICLDC\npresents a promising direction for intelligent scheduling and control in\nUASNETs."
                },
                "authors": [
                    {
                        "name": "Yousef Emami"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "SeyedSina Nabavirazani"
                    },
                    {
                        "name": "Luis Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Luis Almeida"
                },
                "author": "Luis Almeida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13698v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13698v1",
                "updated": "2025-10-15T15:57:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    57,
                    17,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:57:17Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    57,
                    17,
                    2,
                    288,
                    0
                ],
                "title": "Risk-adaptive Activation Steering for Safe Multimodal Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Risk-adaptive Activation Steering for Safe Multimodal Large Language\n  Models"
                },
                "summary": "One of the key challenges of modern AI models is ensuring that they provide\nhelpful responses to benign queries while refusing malicious ones. But often,\nthe models are vulnerable to multimodal queries with harmful intent embedded in\nimages. One approach for safety alignment is training with extensive safety\ndatasets at the significant costs in both dataset curation and training.\nInference-time alignment mitigates these costs, but introduces two drawbacks:\nexcessive refusals from misclassified benign queries and slower inference speed\ndue to iterative output adjustments. To overcome these limitations, we propose\nto reformulate queries to strengthen cross-modal attention to safety-critical\nimage regions, enabling accurate risk assessment at the query level. Using the\nassessed risk, it adaptively steers activations to generate responses that are\nsafe and helpful without overhead from iterative output adjustments. We call\nthis Risk-adaptive Activation Steering (RAS). Extensive experiments across\nmultiple benchmarks on multimodal safety and utility demonstrate that the RAS\nsignificantly reduces attack success rates, preserves general task performance,\nand improves inference speed over prior inference-time defenses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "One of the key challenges of modern AI models is ensuring that they provide\nhelpful responses to benign queries while refusing malicious ones. But often,\nthe models are vulnerable to multimodal queries with harmful intent embedded in\nimages. One approach for safety alignment is training with extensive safety\ndatasets at the significant costs in both dataset curation and training.\nInference-time alignment mitigates these costs, but introduces two drawbacks:\nexcessive refusals from misclassified benign queries and slower inference speed\ndue to iterative output adjustments. To overcome these limitations, we propose\nto reformulate queries to strengthen cross-modal attention to safety-critical\nimage regions, enabling accurate risk assessment at the query level. Using the\nassessed risk, it adaptively steers activations to generate responses that are\nsafe and helpful without overhead from iterative output adjustments. We call\nthis Risk-adaptive Activation Steering (RAS). Extensive experiments across\nmultiple benchmarks on multimodal safety and utility demonstrate that the RAS\nsignificantly reduces attack success rates, preserves general task performance,\nand improves inference speed over prior inference-time defenses."
                },
                "authors": [
                    {
                        "name": "Jonghyun Park"
                    },
                    {
                        "name": "Minhyuk Seo"
                    },
                    {
                        "name": "Jonghyun Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jonghyun Choi"
                },
                "author": "Jonghyun Choi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13698v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13698v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13694v1",
                "updated": "2025-10-15T15:51:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    51,
                    59,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:51:59Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    51,
                    59,
                    2,
                    288,
                    0
                ],
                "title": "Information-Theoretic Reward Modeling for Stable RLHF: Detecting and\n  Mitigating Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Theoretic Reward Modeling for Stable RLHF: Detecting and\n  Mitigating Reward Hacking"
                },
                "summary": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in\naligning language models with human values, reward hacking-or reward\nover-optimization-remains a major challenge. We identify two key obstacles to\nits mitigation: (1) reward misgeneralization in reward modeling, where reward\nmodels overfit to spurious, preference-irrelevant features; and (2) the lack of\nsuitable regularization during RL optimization, as existing token-level\nconstraints often over-restrict the policy space. To address these issues, we\npropose InfoRM, an information-theoretic reward modeling framework based on the\nInformation Bottleneck (IB) principle, which filters out preference-irrelevant\ninformation to alleviate reward misgeneralization. We further observe that\nreward-hacked responses manifest as pronounced outliers in InfoRM's IB latent\nspace, measured by Mahalanobis distance from the SFT-induced distribution.\nMotivated by this, we introduce IBL, a distribution-level regularization that\npenalizes such deviations, effectively expanding the optimization landscape\nwhile maintaining alignment. We prove that IBL is theoretically equivalent to\nthe pessimistic RL objective within the IB latent space. Finally, we present\nMahalanobis Outlier Probability (MOP), a statistical metric for quantifying\nreward hacking severity, enabling principled hyperparameter tuning and online\nmitigation such as early stopping. Extensive experiments across diverse LLMs\nand datasets confirm the generality of our findings, the effectiveness of\nInfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively\nadvancing the state of RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in\naligning language models with human values, reward hacking-or reward\nover-optimization-remains a major challenge. We identify two key obstacles to\nits mitigation: (1) reward misgeneralization in reward modeling, where reward\nmodels overfit to spurious, preference-irrelevant features; and (2) the lack of\nsuitable regularization during RL optimization, as existing token-level\nconstraints often over-restrict the policy space. To address these issues, we\npropose InfoRM, an information-theoretic reward modeling framework based on the\nInformation Bottleneck (IB) principle, which filters out preference-irrelevant\ninformation to alleviate reward misgeneralization. We further observe that\nreward-hacked responses manifest as pronounced outliers in InfoRM's IB latent\nspace, measured by Mahalanobis distance from the SFT-induced distribution.\nMotivated by this, we introduce IBL, a distribution-level regularization that\npenalizes such deviations, effectively expanding the optimization landscape\nwhile maintaining alignment. We prove that IBL is theoretically equivalent to\nthe pessimistic RL objective within the IB latent space. Finally, we present\nMahalanobis Outlier Probability (MOP), a statistical metric for quantifying\nreward hacking severity, enabling principled hyperparameter tuning and online\nmitigation such as early stopping. Extensive experiments across diverse LLMs\nand datasets confirm the generality of our findings, the effectiveness of\nInfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively\nadvancing the state of RLHF."
                },
                "authors": [
                    {
                        "name": "Yuchun Miao"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Rong Bao"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "46 pages, 36 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13313v2",
                "updated": "2025-10-15T15:51:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    51,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-16T17:57:22Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    22,
                    1,
                    259,
                    0
                ],
                "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization"
                },
                "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching solutions. To overcome this challenge,\nwe introduce ReSum, a novel paradigm that enables indefinite exploration\nthrough periodic context summarization. ReSum converts growing interaction\nhistories into compact reasoning states, maintaining awareness of prior\ndiscoveries while bypassing context constraints. For paradigm adaptation, we\npropose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents across three benchmarks\ndemonstrate that ReSum delivers an average absolute improvement of 4.5% over\nReAct, with further gains of 8.2% following ReSum-GRPO training. Notably, with\nonly 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of\nWebSailor-30B) achieves 33.3% Pass@1 on BrowseComp-zh and 18.3% on\nBrowseComp-en, surpassing most open-source web agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching solutions. To overcome this challenge,\nwe introduce ReSum, a novel paradigm that enables indefinite exploration\nthrough periodic context summarization. ReSum converts growing interaction\nhistories into compact reasoning states, maintaining awareness of prior\ndiscoveries while bypassing context constraints. For paradigm adaptation, we\npropose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents across three benchmarks\ndemonstrate that ReSum delivers an average absolute improvement of 4.5% over\nReAct, with further gains of 8.2% following ReSum-GRPO training. Notably, with\nonly 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of\nWebSailor-30B) achieves 33.3% Pass@1 on BrowseComp-zh and 18.3% on\nBrowseComp-en, surpassing most open-source web agents."
                },
                "authors": [
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Xinmiao Yu"
                    },
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.16971v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.16971v2",
                "updated": "2025-10-15T15:51:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    51,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-21T08:08:08Z",
                "published_parsed": [
                    2025,
                    9,
                    21,
                    8,
                    8,
                    8,
                    6,
                    264,
                    0
                ],
                "title": "AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for\n  Coarse-to-Fine Audio Deep Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AudioGenie-Reasoner: A Training-Free Multi-Agent Framework for\n  Coarse-to-Fine Audio Deep Reasoning"
                },
                "summary": "Audio deep reasoning is a challenging task that requires expert-level\nperception, multi-step logical inference, and the integration of contextual\nknowledge. However, existing models suffer from a gap between audio perception\nand reasoning abilities due to the lack of training data with explicit\nreasoning chains and the absence of mechanisms for active exploration and\niterative refinement. To address these challenges, we propose\nAudioGenie-Reasoner (AGR), the first unified training-free multi-agent system\nthat coordinates perception and reasoning over an evolving chain of textual\nevidence. Our key idea is a paradigm shift that transforms audio deep reasoning\ninto complex text understanding task from a new perspective, thereby unlocking\nthe full potential of large language models. Specifically, the design of AGR\nmimics the human coarse-to-fine cognitive process. It first transforms the\ninput audio into a coarse text-based document. Then, we design a novel\nproactive iterative document refinement loop, featuring tool-augmented routes\nand specialized agents, to continuously search for missing information and\naugment the evidence chain in a coarse-to-fine manner until sufficient\nquestion-related information is gathered for making final predictions.\nExperimental results show that AGR achieves state-of-the-art (SOTA) performance\nover existing open-source audio deep reasoning models across various\nbenchmarks. The code will be available at\nhttps://github.com/ryysayhi/AudioGenie-Reasoner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Audio deep reasoning is a challenging task that requires expert-level\nperception, multi-step logical inference, and the integration of contextual\nknowledge. However, existing models suffer from a gap between audio perception\nand reasoning abilities due to the lack of training data with explicit\nreasoning chains and the absence of mechanisms for active exploration and\niterative refinement. To address these challenges, we propose\nAudioGenie-Reasoner (AGR), the first unified training-free multi-agent system\nthat coordinates perception and reasoning over an evolving chain of textual\nevidence. Our key idea is a paradigm shift that transforms audio deep reasoning\ninto complex text understanding task from a new perspective, thereby unlocking\nthe full potential of large language models. Specifically, the design of AGR\nmimics the human coarse-to-fine cognitive process. It first transforms the\ninput audio into a coarse text-based document. Then, we design a novel\nproactive iterative document refinement loop, featuring tool-augmented routes\nand specialized agents, to continuously search for missing information and\naugment the evidence chain in a coarse-to-fine manner until sufficient\nquestion-related information is gathered for making final predictions.\nExperimental results show that AGR achieves state-of-the-art (SOTA) performance\nover existing open-source audio deep reasoning models across various\nbenchmarks. The code will be available at\nhttps://github.com/ryysayhi/AudioGenie-Reasoner."
                },
                "authors": [
                    {
                        "name": "Yan Rong"
                    },
                    {
                        "name": "Chenxing Li"
                    },
                    {
                        "name": "Dong Yu"
                    },
                    {
                        "name": "Li Liu"
                    }
                ],
                "author_detail": {
                    "name": "Li Liu"
                },
                "author": "Li Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.16971v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.16971v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13553v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13553v2",
                "updated": "2025-10-15T15:47:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    47,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-16T14:40:28Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    14,
                    40,
                    28,
                    0,
                    167,
                    0
                ],
                "title": "RelTopo: Multi-Level Relational Modeling for Driving Scene Topology\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RelTopo: Multi-Level Relational Modeling for Driving Scene Topology\n  Reasoning"
                },
                "summary": "Accurate road topology reasoning is critical for autonomous driving, enabling\neffective navigation and adherence to traffic regulations. Central to this task\nare lane perception and topology reasoning. However, existing methods typically\nfocus on either lane detection or Lane-to-Lane (L2L) topology reasoning, often\n\\textit{neglecting} Lane-to-Traffic-element (L2T) relationships or\n\\textit{failing} to optimize these tasks jointly. Furthermore, most approaches\neither overlook relational modeling or apply it in a limited scope, despite the\ninherent spatial relationships among road elements. We argue that relational\nmodeling is beneficial for both perception and reasoning, as humans naturally\nleverage contextual relationships for road element recognition and their\nconnectivity inference. To this end, we introduce relational modeling into both\nperception and reasoning, \\textit{jointly} enhancing structural understanding.\nSpecifically, we propose: 1) a relation-aware lane detector, where our\ngeometry-biased self-attention and \\curve\\ cross-attention refine lane\nrepresentations by capturing relational dependencies; 2) relation-enhanced\ntopology heads, including a geometry-enhanced L2L head and a cross-view L2T\nhead, boosting reasoning with relational cues; and 3) a contrastive learning\nstrategy with InfoNCE loss to regularize relationship embeddings. Extensive\nexperiments on OpenLane-V2 demonstrate that our approach significantly improves\nboth detection and topology reasoning metrics, achieving +3.1 in DET$_l$, +5.3\nin TOP$_{ll}$, +4.9 in TOP$_{lt}$, and an overall +4.4 in OLS, setting a new\nstate-of-the-art. Code will be released.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate road topology reasoning is critical for autonomous driving, enabling\neffective navigation and adherence to traffic regulations. Central to this task\nare lane perception and topology reasoning. However, existing methods typically\nfocus on either lane detection or Lane-to-Lane (L2L) topology reasoning, often\n\\textit{neglecting} Lane-to-Traffic-element (L2T) relationships or\n\\textit{failing} to optimize these tasks jointly. Furthermore, most approaches\neither overlook relational modeling or apply it in a limited scope, despite the\ninherent spatial relationships among road elements. We argue that relational\nmodeling is beneficial for both perception and reasoning, as humans naturally\nleverage contextual relationships for road element recognition and their\nconnectivity inference. To this end, we introduce relational modeling into both\nperception and reasoning, \\textit{jointly} enhancing structural understanding.\nSpecifically, we propose: 1) a relation-aware lane detector, where our\ngeometry-biased self-attention and \\curve\\ cross-attention refine lane\nrepresentations by capturing relational dependencies; 2) relation-enhanced\ntopology heads, including a geometry-enhanced L2L head and a cross-view L2T\nhead, boosting reasoning with relational cues; and 3) a contrastive learning\nstrategy with InfoNCE loss to regularize relationship embeddings. Extensive\nexperiments on OpenLane-V2 demonstrate that our approach significantly improves\nboth detection and topology reasoning metrics, achieving +3.1 in DET$_l$, +5.3\nin TOP$_{ll}$, +4.9 in TOP$_{lt}$, and an overall +4.4 in OLS, setting a new\nstate-of-the-art. Code will be released."
                },
                "authors": [
                    {
                        "name": "Yueru Luo"
                    },
                    {
                        "name": "Changqing Zhou"
                    },
                    {
                        "name": "Yiming Yang"
                    },
                    {
                        "name": "Erlong Li"
                    },
                    {
                        "name": "Chao Zheng"
                    },
                    {
                        "name": "Shuqi Mei"
                    },
                    {
                        "name": "Shuguang Cui"
                    },
                    {
                        "name": "Zhen Li"
                    }
                ],
                "author_detail": {
                    "name": "Zhen Li"
                },
                "author": "Zhen Li",
                "arxiv_comment": "Preprint. Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13553v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13553v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13681v1",
                "updated": "2025-10-15T15:36:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    36,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:36:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    36,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "How Sampling Affects the Detectability of Machine-written texts: A\n  Comprehensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sampling Affects the Detectability of Machine-written texts: A\n  Comprehensive Study"
                },
                "summary": "As texts generated by Large Language Models (LLMs) are ever more common and\noften indistinguishable from human-written content, research on automatic text\ndetection has attracted growing attention. Many recent detectors report\nnear-perfect accuracy, often boasting AUROC scores above 99\\%. However, these\nclaims typically assume fixed generation settings, leaving open the question of\nhow robust such systems are to changes in decoding strategies. In this work, we\nsystematically examine how sampling-based decoding impacts detectability, with\na focus on how subtle variations in a model's (sub)word-level distribution\naffect detection performance. We find that even minor adjustments to decoding\nparameters - such as temperature, top-p, or nucleus sampling - can severely\nimpair detector accuracy, with AUROC dropping from near-perfect levels to 1\\%\nin some settings. Our findings expose critical blind spots in current detection\nmethods and emphasize the need for more comprehensive evaluation protocols. To\nfacilitate future research, we release a large-scale dataset encompassing 37\ndecoding configurations, along with our code and evaluation framework\nhttps://github.com/BaggerOfWords/Sampling-and-Detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As texts generated by Large Language Models (LLMs) are ever more common and\noften indistinguishable from human-written content, research on automatic text\ndetection has attracted growing attention. Many recent detectors report\nnear-perfect accuracy, often boasting AUROC scores above 99\\%. However, these\nclaims typically assume fixed generation settings, leaving open the question of\nhow robust such systems are to changes in decoding strategies. In this work, we\nsystematically examine how sampling-based decoding impacts detectability, with\na focus on how subtle variations in a model's (sub)word-level distribution\naffect detection performance. We find that even minor adjustments to decoding\nparameters - such as temperature, top-p, or nucleus sampling - can severely\nimpair detector accuracy, with AUROC dropping from near-perfect levels to 1\\%\nin some settings. Our findings expose critical blind spots in current detection\nmethods and emphasize the need for more comprehensive evaluation protocols. To\nfacilitate future research, we release a large-scale dataset encompassing 37\ndecoding configurations, along with our code and evaluation framework\nhttps://github.com/BaggerOfWords/Sampling-and-Detection"
                },
                "authors": [
                    {
                        "name": "Matthieu Dubois"
                    },
                    {
                        "name": "Fran√ßois Yvon"
                    },
                    {
                        "name": "Pablo Piantanida"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Piantanida"
                },
                "author": "Pablo Piantanida",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13678v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13678v1",
                "updated": "2025-10-15T15:35:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    35,
                    48,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:35:48Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    35,
                    48,
                    2,
                    288,
                    0
                ],
                "title": "FlashWorld: High-quality 3D Scene Generation within Seconds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashWorld: High-quality 3D Scene Generation within Seconds"
                },
                "summary": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100$\\times$ faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose FlashWorld, a generative model that produces 3D scenes from a\nsingle image or text prompt in seconds, 10~100$\\times$ faster than previous\nworks while possessing superior rendering quality. Our approach shifts from the\nconventional multi-view-oriented (MV-oriented) paradigm, which generates\nmulti-view images for subsequent 3D reconstruction, to a 3D-oriented approach\nwhere the model directly produces 3D Gaussian representations during multi-view\ngeneration. While ensuring 3D consistency, 3D-oriented method typically suffers\npoor visual quality. FlashWorld includes a dual-mode pre-training phase\nfollowed by a cross-mode post-training phase, effectively integrating the\nstrengths of both paradigms. Specifically, leveraging the prior from a video\ndiffusion model, we first pre-train a dual-mode multi-view diffusion model,\nwhich jointly supports MV-oriented and 3D-oriented generation modes. To bridge\nthe quality gap in 3D-oriented generation, we further propose a cross-mode\npost-training distillation by matching distribution from consistent 3D-oriented\nmode to high-quality MV-oriented mode. This not only enhances visual quality\nwhile maintaining 3D consistency, but also reduces the required denoising steps\nfor inference. Also, we propose a strategy to leverage massive single-view\nimages and text prompts during this process to enhance the model's\ngeneralization to out-of-distribution inputs. Extensive experiments demonstrate\nthe superiority and efficiency of our method."
                },
                "authors": [
                    {
                        "name": "Xinyang Li"
                    },
                    {
                        "name": "Tengfei Wang"
                    },
                    {
                        "name": "Zixiao Gu"
                    },
                    {
                        "name": "Shengchuan Zhang"
                    },
                    {
                        "name": "Chunchao Guo"
                    },
                    {
                        "name": "Liujuan Cao"
                    }
                ],
                "author_detail": {
                    "name": "Liujuan Cao"
                },
                "author": "Liujuan Cao",
                "arxiv_comment": "Project Page: https://imlixinyang.github.io/FlashWorld-Project-Page/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13678v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13678v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09558v2",
                "updated": "2025-10-15T15:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    32,
                    50,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-10T17:08:36Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    8,
                    36,
                    4,
                    283,
                    0
                ],
                "title": "AutoPR: Let's Automate Your Academic Promotion!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPR: Let's Automate Your Academic Promotion!"
                },
                "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Zheng Yan"
                    },
                    {
                        "name": "Mingda Yang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Yixin Yuan"
                    },
                    {
                        "name": "Hanjing Li"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Yiyan Ji"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Yantao Du"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Preprint. Code: https://github.com/LightChen233/AutoPR . Benchmark:\n  https://huggingface.co/datasets/yzweak/PRBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.06762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.06762v2",
                "updated": "2025-10-15T15:30:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    30,
                    47,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-08T08:41:14Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    8,
                    41,
                    14,
                    2,
                    281,
                    0
                ],
                "title": "Function regression using the forward forward training and inferring\n  paradigm",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function regression using the forward forward training and inferring\n  paradigm"
                },
                "summary": "Function regression/approximation is a fundamental application of machine\nlearning. Neural networks (NNs) can be easily trained for function regression\nusing a sufficient number of neurons and epochs. The forward-forward learning\nalgorithm is a novel approach for training neural networks without\nbackpropagation, and is well suited for implementation in neuromorphic\ncomputing and physical analogs for neural networks. To the best of the authors'\nknowledge, the Forward Forward paradigm of training and inferencing NNs is\ncurrently only restricted to classification tasks. This paper introduces a new\nmethodology for approximating functions (function regression) using the\nForward-Forward algorithm. Furthermore, the paper evaluates the developed\nmethodology on univariate and multivariate functions, and provides preliminary\nstudies of extending the proposed Forward-Forward regression to Kolmogorov\nArnold Networks, and Deep Physical Neural Networks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Function regression/approximation is a fundamental application of machine\nlearning. Neural networks (NNs) can be easily trained for function regression\nusing a sufficient number of neurons and epochs. The forward-forward learning\nalgorithm is a novel approach for training neural networks without\nbackpropagation, and is well suited for implementation in neuromorphic\ncomputing and physical analogs for neural networks. To the best of the authors'\nknowledge, the Forward Forward paradigm of training and inferencing NNs is\ncurrently only restricted to classification tasks. This paper introduces a new\nmethodology for approximating functions (function regression) using the\nForward-Forward algorithm. Furthermore, the paper evaluates the developed\nmethodology on univariate and multivariate functions, and provides preliminary\nstudies of extending the proposed Forward-Forward regression to Kolmogorov\nArnold Networks, and Deep Physical Neural Networks."
                },
                "authors": [
                    {
                        "name": "Shivam Padmani"
                    },
                    {
                        "name": "Akshay Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Akshay Joshi"
                },
                "author": "Akshay Joshi",
                "arxiv_comment": "Keywords: Neural Networks, Forward Forward training, Function\n  Regression, Physical Neural Networks, Analog Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.06762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.06762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13672v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13672v1",
                "updated": "2025-10-15T15:30:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    30,
                    40,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:30:40Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    30,
                    40,
                    2,
                    288,
                    0
                ],
                "title": "Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024):\n  The Role of Spatial Granularity and Data Quality for Epidemiological Risk\n  Mapping",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Bayesian Modeling of Dengue in Recife, Brazil (2015-2024):\n  The Role of Spatial Granularity and Data Quality for Epidemiological Risk\n  Mapping"
                },
                "summary": "Dengue remains one of Brazil's major epidemiological challenges, marked by\nstrong intra-urban inequalities and the influence of climatic and\nsocio-environmental factors. This study analyzed confirmed dengue cases in\nRecife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model\nimplemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal\ncomponent. Covariates included population density, household size, income,\ndrainage channels, lagged precipitation, and mean temperature. Population\ndensity and household size had positive effects on dengue risk, while income\nand channel presence were protective. Lagged precipitation increased risk, and\nhigher temperatures showed an inverse association, suggesting thermal\nthresholds for vector activity. The model achieved good fit (DIC=65817;\nWAIC=64506) and stable convergence, with moderate residual spatial\nautocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019.\nSpatio-temporal estimates revealed persistent high-risk clusters in northern\nand western Recife, overlapping with areas of higher density and social\nvulnerability. Beyond reproducing historical patterns, the Bayesian model\nsupports probabilistic forecasting and early warning systems. Compared with\nclassical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty\nand spatial-temporal dependence, offering credible interval inference for\ndecision-making in urban health management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dengue remains one of Brazil's major epidemiological challenges, marked by\nstrong intra-urban inequalities and the influence of climatic and\nsocio-environmental factors. This study analyzed confirmed dengue cases in\nRecife from 2015 to 2024 using a Bayesian hierarchical spatio-temporal model\nimplemented in R-INLA, combining a BYM2 spatial structure with an RW1 temporal\ncomponent. Covariates included population density, household size, income,\ndrainage channels, lagged precipitation, and mean temperature. Population\ndensity and household size had positive effects on dengue risk, while income\nand channel presence were protective. Lagged precipitation increased risk, and\nhigher temperatures showed an inverse association, suggesting thermal\nthresholds for vector activity. The model achieved good fit (DIC=65817;\nWAIC=64506) and stable convergence, with moderate residual spatial\nautocorrelation (phi=0.06) and a smooth temporal trend between 2016 and 2019.\nSpatio-temporal estimates revealed persistent high-risk clusters in northern\nand western Recife, overlapping with areas of higher density and social\nvulnerability. Beyond reproducing historical patterns, the Bayesian model\nsupports probabilistic forecasting and early warning systems. Compared with\nclassical models (GLM, SAR, GWR, GTWR), INLA explicitly integrates uncertainty\nand spatial-temporal dependence, offering credible interval inference for\ndecision-making in urban health management."
                },
                "authors": [
                    {
                        "name": "Marc√≠lio Ferreira dos Santos"
                    },
                    {
                        "name": "Andreza dos Santos Rodrigues de Melo"
                    }
                ],
                "author_detail": {
                    "name": "Andreza dos Santos Rodrigues de Melo"
                },
                "author": "Andreza dos Santos Rodrigues de Melo",
                "arxiv_comment": "12 pages, 12 figures, 8 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13672v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13672v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13668v1",
                "updated": "2025-10-15T15:29:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    29,
                    8,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:29:08Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    29,
                    8,
                    2,
                    288,
                    0
                ],
                "title": "Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference"
                },
                "summary": "Large Language Model (LLM) inference has emerged as a fundamental paradigm.\nIn real-world scenarios, variations in output length cause severe workload\nimbalance in the decode phase, particularly for long-output reasoning tasks.\nExisting systems, such as PD disaggregation architectures, rely on static\nprefill-to-decode scheduling, which often results in SLO violations and OOM\nfailures under evolving decode workloads.\n  In this paper, we propose ARES, an adaptive decoding rescheduling system\npowered by length prediction to anticipate future workloads. Our core\ncontributions include: (1) A lightweight and continuous LLM-native prediction\nmethod that leverages LLM hidden state to model remaining generation length\nwith high precision (reducing MAE by 49.42%) and low overhead (cutting\npredictor parameters by 93.28%); (2) A rescheduling solution in decode phase\nwith : A dynamic balancing mechanism that integrates current and predicted\nworkloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference has emerged as a fundamental paradigm.\nIn real-world scenarios, variations in output length cause severe workload\nimbalance in the decode phase, particularly for long-output reasoning tasks.\nExisting systems, such as PD disaggregation architectures, rely on static\nprefill-to-decode scheduling, which often results in SLO violations and OOM\nfailures under evolving decode workloads.\n  In this paper, we propose ARES, an adaptive decoding rescheduling system\npowered by length prediction to anticipate future workloads. Our core\ncontributions include: (1) A lightweight and continuous LLM-native prediction\nmethod that leverages LLM hidden state to model remaining generation length\nwith high precision (reducing MAE by 49.42%) and low overhead (cutting\npredictor parameters by 93.28%); (2) A rescheduling solution in decode phase\nwith : A dynamic balancing mechanism that integrates current and predicted\nworkloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher\ngoodput."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Zetao Hong"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Zibo Wang"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Qingkai Meng"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13665v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13665v1",
                "updated": "2025-10-15T15:25:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    25,
                    20,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:25:20Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    25,
                    20,
                    2,
                    288,
                    0
                ],
                "title": "Axial Neural Networks for Dimension-Free Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Axial Neural Networks for Dimension-Free Foundation Models"
                },
                "summary": "The advent of foundation models in AI has significantly advanced\ngeneral-purpose learning, enabling remarkable capabilities in zero-shot\ninference and in-context learning. However, training such models on physics\ndata, including solutions to partial differential equations (PDEs), poses a\nunique challenge due to varying dimensionalities across different systems.\nTraditional approaches either fix a maximum dimension or employ separate\nencoders for different dimensionalities, resulting in inefficiencies. To\naddress this, we propose a dimension-agnostic neural network architecture, the\nAxial Neural Network (XNN), inspired by parameter-sharing structures such as\nDeep Sets and Graph Neural Networks. XNN generalizes across varying tensor\ndimensions while maintaining computational efficiency. We convert existing PDE\nfoundation models into axial neural networks and evaluate their performance\nacross three training scenarios: training from scratch, pretraining on multiple\nPDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform\ncompetitively with original models and exhibit superior generalization to\nunseen dimensions, highlighting the importance of multidimensional pretraining\nfor foundation models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advent of foundation models in AI has significantly advanced\ngeneral-purpose learning, enabling remarkable capabilities in zero-shot\ninference and in-context learning. However, training such models on physics\ndata, including solutions to partial differential equations (PDEs), poses a\nunique challenge due to varying dimensionalities across different systems.\nTraditional approaches either fix a maximum dimension or employ separate\nencoders for different dimensionalities, resulting in inefficiencies. To\naddress this, we propose a dimension-agnostic neural network architecture, the\nAxial Neural Network (XNN), inspired by parameter-sharing structures such as\nDeep Sets and Graph Neural Networks. XNN generalizes across varying tensor\ndimensions while maintaining computational efficiency. We convert existing PDE\nfoundation models into axial neural networks and evaluate their performance\nacross three training scenarios: training from scratch, pretraining on multiple\nPDEs, and fine-tuning on a single PDE. Our experiments show that XNNs perform\ncompetitively with original models and exhibit superior generalization to\nunseen dimensions, highlighting the importance of multidimensional pretraining\nfor foundation models."
                },
                "authors": [
                    {
                        "name": "Hyunsu Kim"
                    },
                    {
                        "name": "Jonggeon Park"
                    },
                    {
                        "name": "Joan Bruna"
                    },
                    {
                        "name": "Hongseok Yang"
                    },
                    {
                        "name": "Juho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Juho Lee"
                },
                "author": "Juho Lee",
                "arxiv_journal_ref": "NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13665v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13665v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19234v2",
                "updated": "2025-10-15T15:21:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    21,
                    41,
                    2,
                    288,
                    0
                ],
                "published": "2025-05-25T17:15:55Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    17,
                    15,
                    55,
                    6,
                    145,
                    0
                ],
                "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal\n  Graph Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal\n  Graph Modeling"
                },
                "summary": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration faces critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization. The code is\navailable at https://github.com/JialongZhou666/GUARDIAN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration faces critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization. The code is\navailable at https://github.com/JialongZhou666/GUARDIAN"
                },
                "authors": [
                    {
                        "name": "Jialong Zhou"
                    },
                    {
                        "name": "Lichao Wang"
                    },
                    {
                        "name": "Xiao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Yang"
                },
                "author": "Xiao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13662v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13662v1",
                "updated": "2025-10-15T15:20:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    20,
                    20,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:20:20Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    20,
                    20,
                    2,
                    288,
                    0
                ],
                "title": "The Past Still Matters: A Temporally-Valid Data Discovery System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Past Still Matters: A Temporally-Valid Data Discovery System"
                },
                "summary": "Over the past decade, the proliferation of public and enterprise data lakes\nhas fueled intensive research into data discovery, aiming to identify the most\nrelevant data from vast and complex corpora to support diverse user tasks.\nSignificant progress has been made through the development of innovative index\nstructures, similarity measures, and querying infrastructures. Despite these\nadvances, a critical aspect remains overlooked: relevance is time-varying.\nExisting discovery methods largely ignore this temporal dimension, especially\nwhen explicit date/time metadata is missing. To fill this gap, we outline a\nvision for a data discovery system that incorporates the temporal dimension of\ndata. Specifically, we define the problem of temporally-valid data discovery\nand argue that addressing it requires techniques for version discovery,\ntemporal lineage inference, change log synthesis, and time-aware data\ndiscovery. We then present a system architecture to deliver these techniques,\nbefore we summarize research challenges and opportunities. As such, we lay the\nfoundation for a new class of data discovery systems, transforming how we\ninteract with evolving data lakes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Over the past decade, the proliferation of public and enterprise data lakes\nhas fueled intensive research into data discovery, aiming to identify the most\nrelevant data from vast and complex corpora to support diverse user tasks.\nSignificant progress has been made through the development of innovative index\nstructures, similarity measures, and querying infrastructures. Despite these\nadvances, a critical aspect remains overlooked: relevance is time-varying.\nExisting discovery methods largely ignore this temporal dimension, especially\nwhen explicit date/time metadata is missing. To fill this gap, we outline a\nvision for a data discovery system that incorporates the temporal dimension of\ndata. Specifically, we define the problem of temporally-valid data discovery\nand argue that addressing it requires techniques for version discovery,\ntemporal lineage inference, change log synthesis, and time-aware data\ndiscovery. We then present a system architecture to deliver these techniques,\nbefore we summarize research challenges and opportunities. As such, we lay the\nfoundation for a new class of data discovery systems, transforming how we\ninteract with evolving data lakes."
                },
                "authors": [
                    {
                        "name": "Mahdi Esmailoghli"
                    },
                    {
                        "name": "Matthias Weidlich"
                    }
                ],
                "author_detail": {
                    "name": "Matthias Weidlich"
                },
                "author": "Matthias Weidlich",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13662v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13662v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13654v1",
                "updated": "2025-10-15T15:15:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    15,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:15:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    15,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Time Series Foundation Models: Benchmarking Challenges and Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Foundation Models: Benchmarking Challenges and Requirements"
                },
                "summary": "Time Series Foundation Models (TSFMs) represent a new paradigm for time\nseries forecasting, offering zero-shot forecasting capabilities without the\nneed for domain-specific pre-training or fine-tuning. However, as with Large\nLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive\ntraining sets, it becomes more and more challenging to ensure the integrity of\nbenchmarking data. Our investigation of existing TSFM evaluation highlights\nmultiple challenges, ranging from the representativeness of the benchmark\ndatasets, over the lack of spatiotemporal evaluation, to risks of information\nleakage due to overlapping and obscure datasets, and the memorization of global\npatterns caused by external shocks like economic crises or pandemics. Our\nfindings reveal widespread confusion regarding data partitions, risking\ninflated performance estimates and incorrect transfer of global knowledge to\nlocal time series. We argue for the development of robust evaluation\nmethodologies to prevent pitfalls already observed in LLM and classical time\nseries benchmarking, and call upon the research community to design new,\nprincipled approaches, such as evaluations on truly out-of-sample future data,\nto safeguard the integrity of TSFM assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Foundation Models (TSFMs) represent a new paradigm for time\nseries forecasting, offering zero-shot forecasting capabilities without the\nneed for domain-specific pre-training or fine-tuning. However, as with Large\nLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive\ntraining sets, it becomes more and more challenging to ensure the integrity of\nbenchmarking data. Our investigation of existing TSFM evaluation highlights\nmultiple challenges, ranging from the representativeness of the benchmark\ndatasets, over the lack of spatiotemporal evaluation, to risks of information\nleakage due to overlapping and obscure datasets, and the memorization of global\npatterns caused by external shocks like economic crises or pandemics. Our\nfindings reveal widespread confusion regarding data partitions, risking\ninflated performance estimates and incorrect transfer of global knowledge to\nlocal time series. We argue for the development of robust evaluation\nmethodologies to prevent pitfalls already observed in LLM and classical time\nseries benchmarking, and call upon the research community to design new,\nprincipled approaches, such as evaluations on truly out-of-sample future data,\nto safeguard the integrity of TSFM assessment."
                },
                "authors": [
                    {
                        "name": "Marcel Meyer"
                    },
                    {
                        "name": "Sascha Kaltenpoth"
                    },
                    {
                        "name": "Kevin Zalipski"
                    },
                    {
                        "name": "Oliver M√ºller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver M√ºller"
                },
                "author": "Oliver M√ºller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13653v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13653v1",
                "updated": "2025-10-15T15:13:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    13,
                    49,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:13:49Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    13,
                    49,
                    2,
                    288,
                    0
                ],
                "title": "International AI Safety Report 2025: First Key Update: Capabilities and\n  Risk Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "International AI Safety Report 2025: First Key Update: Capabilities and\n  Risk Implications"
                },
                "summary": "Since the publication of the first International AI Safety Report, AI\ncapabilities have continued to improve across key domains. New training\ntechniques that teach AI systems to reason step-by-step and inference-time\nenhancements have primarily driven these advances, rather than simply training\nlarger models. As a result, general-purpose AI systems can solve more complex\nproblems in a range of domains, from scientific research to software\ndevelopment. Their performance on benchmarks that measure performance in\ncoding, mathematics, and answering expert-level science questions has continued\nto improve, though reliability challenges persist, with systems excelling on\nsome tasks while failing completely on others. These capability improvements\nalso have implications for multiple risks, including risks from biological\nweapons and cyber attacks. Finally, they pose new challenges for monitoring and\ncontrollability. This update examines how AI capabilities have improved since\nthe first Report, then focuses on key risk areas where substantial new evidence\nwarrants updated assessments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since the publication of the first International AI Safety Report, AI\ncapabilities have continued to improve across key domains. New training\ntechniques that teach AI systems to reason step-by-step and inference-time\nenhancements have primarily driven these advances, rather than simply training\nlarger models. As a result, general-purpose AI systems can solve more complex\nproblems in a range of domains, from scientific research to software\ndevelopment. Their performance on benchmarks that measure performance in\ncoding, mathematics, and answering expert-level science questions has continued\nto improve, though reliability challenges persist, with systems excelling on\nsome tasks while failing completely on others. These capability improvements\nalso have implications for multiple risks, including risks from biological\nweapons and cyber attacks. Finally, they pose new challenges for monitoring and\ncontrollability. This update examines how AI capabilities have improved since\nthe first Report, then focuses on key risk areas where substantial new evidence\nwarrants updated assessments."
                },
                "authors": [
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Stephen Clare"
                    },
                    {
                        "name": "Carina Prunkl"
                    },
                    {
                        "name": "Shalaleh Rismani"
                    },
                    {
                        "name": "Maksym Andriushchenko"
                    },
                    {
                        "name": "Ben Bucknall"
                    },
                    {
                        "name": "Philip Fox"
                    },
                    {
                        "name": "Tiancheng Hu"
                    },
                    {
                        "name": "Cameron Jones"
                    },
                    {
                        "name": "Sam Manning"
                    },
                    {
                        "name": "Nestor Maslej"
                    },
                    {
                        "name": "Vasilios Mavroudis"
                    },
                    {
                        "name": "Conor McGlynn"
                    },
                    {
                        "name": "Malcolm Murray"
                    },
                    {
                        "name": "Charlotte Stix"
                    },
                    {
                        "name": "Lucia Velasco"
                    },
                    {
                        "name": "Nicole Wheeler"
                    },
                    {
                        "name": "Daniel Privitera"
                    },
                    {
                        "name": "S√∂ren Mindermann"
                    },
                    {
                        "name": "Daron Acemoglu"
                    },
                    {
                        "name": "Thomas G. Dietterich"
                    },
                    {
                        "name": "Fredrik Heintz"
                    },
                    {
                        "name": "Geoffrey Hinton"
                    },
                    {
                        "name": "Nick Jennings"
                    },
                    {
                        "name": "Susan Leavy"
                    },
                    {
                        "name": "Teresa Ludermir"
                    },
                    {
                        "name": "Vidushi Marda"
                    },
                    {
                        "name": "Helen Margetts"
                    },
                    {
                        "name": "John McDermid"
                    },
                    {
                        "name": "Jane Munga"
                    },
                    {
                        "name": "Arvind Narayanan"
                    },
                    {
                        "name": "Alondra Nelson"
                    },
                    {
                        "name": "Clara Neppel"
                    },
                    {
                        "name": "Gopal Ramchurn"
                    },
                    {
                        "name": "Stuart Russell"
                    },
                    {
                        "name": "Marietje Schaake"
                    },
                    {
                        "name": "Bernhard Sch√∂lkopf"
                    },
                    {
                        "name": "Alavaro Soto"
                    },
                    {
                        "name": "Lee Tiedrich"
                    },
                    {
                        "name": "Ga√´l Varoquaux"
                    },
                    {
                        "name": "Andrew Yao"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Leandro Aguirre"
                    },
                    {
                        "name": "Olubunmi Ajala"
                    },
                    {
                        "name": "Fahad Albalawi Noora AlMalek"
                    },
                    {
                        "name": "Christian Busch"
                    },
                    {
                        "name": "Andr√© Carvalho"
                    },
                    {
                        "name": "Jonathan Collas"
                    },
                    {
                        "name": "Amandeep Gill"
                    },
                    {
                        "name": "Ahmet Hatip"
                    },
                    {
                        "name": "Juha Heikkil√§"
                    },
                    {
                        "name": "Chris Johnson"
                    },
                    {
                        "name": "Gill Jolly"
                    },
                    {
                        "name": "Ziv Katzir"
                    },
                    {
                        "name": "Mary Kerema"
                    },
                    {
                        "name": "Hiroaki Kitano"
                    },
                    {
                        "name": "Antonio Kr√ºger"
                    },
                    {
                        "name": "Aoife McLysaght"
                    },
                    {
                        "name": "Oleksii Molchanovskyi"
                    },
                    {
                        "name": "Andrea Monti"
                    },
                    {
                        "name": "Kyoung Mu Lee"
                    },
                    {
                        "name": "Mona Nemer"
                    },
                    {
                        "name": "Nuria Oliver"
                    },
                    {
                        "name": "Raquel Pezoa"
                    },
                    {
                        "name": "Audrey Plonk"
                    },
                    {
                        "name": "Jos√© Portillo"
                    },
                    {
                        "name": "Balaraman Ravindran"
                    },
                    {
                        "name": "Hammam Riza"
                    },
                    {
                        "name": "Crystal Rugege"
                    },
                    {
                        "name": "Haroon Sheikh"
                    },
                    {
                        "name": "Denise Wong"
                    },
                    {
                        "name": "Yi Zeng"
                    },
                    {
                        "name": "Liming Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Liming Zhu"
                },
                "author": "Liming Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13653v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13653v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14755v2",
                "updated": "2025-10-15T15:13:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    13,
                    1,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-20T14:58:05Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    58,
                    5,
                    2,
                    232,
                    0
                ],
                "title": "Reliable generation of isomorphic physics problems using Generative AI\n  with prompt-chaining and tool use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable generation of isomorphic physics problems using Generative AI\n  with prompt-chaining and tool use"
                },
                "summary": "We present a method for generating large numbers of isomorphic physics\nproblems using generative AI services such as ChatGPT, through prompt chaining\nand tool use. This approach enables precise control over structural\nvariations-such as numeric values and spatial relations-while supporting\ndiverse contextual variations in the problem body. By utilizing the Python code\ninterpreter, the method supports automatic solution validation and simple\ndiagram generation, addressing key limitations in existing LLM-based methods.\nWe generated two example isomorphic problem banks and compared the outcome\nagainst two simpler prompt-based approaches. Results show that prompt-chaining\nproduces significantly higher quality and more consistent outputs than simpler,\nnon-chaining prompts. We also show that GenAI services can be used to validate\nthe quality of the generated isomorphic problems. This work demonstrates a\npromising method for efficient and scalable problem creation accessible to the\naverage instructor, which opens new possibilities for personalized adaptive\ntesting and automated content development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a method for generating large numbers of isomorphic physics\nproblems using generative AI services such as ChatGPT, through prompt chaining\nand tool use. This approach enables precise control over structural\nvariations-such as numeric values and spatial relations-while supporting\ndiverse contextual variations in the problem body. By utilizing the Python code\ninterpreter, the method supports automatic solution validation and simple\ndiagram generation, addressing key limitations in existing LLM-based methods.\nWe generated two example isomorphic problem banks and compared the outcome\nagainst two simpler prompt-based approaches. Results show that prompt-chaining\nproduces significantly higher quality and more consistent outputs than simpler,\nnon-chaining prompts. We also show that GenAI services can be used to validate\nthe quality of the generated isomorphic problems. This work demonstrates a\npromising method for efficient and scalable problem creation accessible to the\naverage instructor, which opens new possibilities for personalized adaptive\ntesting and automated content development."
                },
                "authors": [
                    {
                        "name": "Zhongzhou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhongzhou Chen"
                },
                "author": "Zhongzhou Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13632v1",
                "updated": "2025-10-15T14:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    57,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    57,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Gap Between Text and Speech Understanding in LLMs"
                },
                "summary": "Large Language Models (LLMs) can be adapted to extend their text capabilities\nto speech inputs. However, these speech-adapted LLMs consistently underperform\ntheir text-based counterparts--and even cascaded pipelines--on language\nunderstanding tasks. We term this shortfall the text-speech understanding gap:\nthe performance drop observed when a speech-adapted LLM processes spoken inputs\nrelative to when the original text-based LLM processes the equivalent text.\nRecent approaches to narrowing this gap either rely on large-scale speech\nsynthesis of text corpora, which is costly and heavily dependent on synthetic\ndata, or on large-scale proprietary speech datasets, which are not\nreproducible. As a result, there remains a need for more data-efficient\nalternatives for closing the text-speech understanding gap. In this work, we\nanalyze the gap as driven by two factors: (i) forgetting of text capabilities\nduring adaptation, and (ii) cross-modal misalignment between speech and text.\nBased on this analysis, we introduce SALAD--Sample-efficient Alignment with\nLearning through Active selection and cross-modal Distillation--which combines\ncross-modal distillation with targeted synthetic data to improve alignment\nwhile mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves\ncompetitive performance with a strong open-weight model across broad-domain\nbenchmarks in knowledge, language understanding, and reasoning, while training\non over an order of magnitude less speech data from public corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can be adapted to extend their text capabilities\nto speech inputs. However, these speech-adapted LLMs consistently underperform\ntheir text-based counterparts--and even cascaded pipelines--on language\nunderstanding tasks. We term this shortfall the text-speech understanding gap:\nthe performance drop observed when a speech-adapted LLM processes spoken inputs\nrelative to when the original text-based LLM processes the equivalent text.\nRecent approaches to narrowing this gap either rely on large-scale speech\nsynthesis of text corpora, which is costly and heavily dependent on synthetic\ndata, or on large-scale proprietary speech datasets, which are not\nreproducible. As a result, there remains a need for more data-efficient\nalternatives for closing the text-speech understanding gap. In this work, we\nanalyze the gap as driven by two factors: (i) forgetting of text capabilities\nduring adaptation, and (ii) cross-modal misalignment between speech and text.\nBased on this analysis, we introduce SALAD--Sample-efficient Alignment with\nLearning through Active selection and cross-modal Distillation--which combines\ncross-modal distillation with targeted synthetic data to improve alignment\nwhile mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves\ncompetitive performance with a strong open-weight model across broad-domain\nbenchmarks in knowledge, language understanding, and reasoning, while training\non over an order of magnitude less speech data from public corpora."
                },
                "authors": [
                    {
                        "name": "Santiago Cuervo"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maureen de Seyssel"
                    },
                    {
                        "name": "Richard He Bai"
                    },
                    {
                        "name": "Zijin Gu"
                    },
                    {
                        "name": "Tatiana Likhomanenko"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Zakaria Aldeneh"
                    }
                ],
                "author_detail": {
                    "name": "Zakaria Aldeneh"
                },
                "author": "Zakaria Aldeneh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09534v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09534v2",
                "updated": "2025-10-15T14:51:36Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    51,
                    36,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-10T16:48:28Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    16,
                    48,
                    28,
                    4,
                    283,
                    0
                ],
                "title": "Conditional Flow Matching for Bayesian Posterior Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conditional Flow Matching for Bayesian Posterior Inference"
                },
                "summary": "We propose a generative multivariate posterior sampler via flow matching. It\noffers a simple training objective, and does not require access to likelihood\nevaluation. The method learns a dynamic, block-triangular velocity field in the\njoint space of data and parameters, which results in a deterministic transport\nmap from a source distribution to the desired posterior. The inverse map, named\nvector rank, is accessible by reversibly integrating the velocity over time. It\nis advantageous to leverage the dynamic design: proper constraints on the\nvelocity yield a monotone map, which leads to a conditional Brenier map,\nenabling a fast and simultaneous generation of Bayesian credible sets whose\ncontours correspond to level sets of Monge-Kantorovich data depth. Our approach\nis computationally lighter compared to GAN-based and diffusion-based\ncounterparts, and is capable of capturing complex posterior structures.\nFinally, frequentist theoretical guarantee on the consistency of the recovered\nposterior distribution, and of the corresponding Bayesian credible sets, is\nprovided.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a generative multivariate posterior sampler via flow matching. It\noffers a simple training objective, and does not require access to likelihood\nevaluation. The method learns a dynamic, block-triangular velocity field in the\njoint space of data and parameters, which results in a deterministic transport\nmap from a source distribution to the desired posterior. The inverse map, named\nvector rank, is accessible by reversibly integrating the velocity over time. It\nis advantageous to leverage the dynamic design: proper constraints on the\nvelocity yield a monotone map, which leads to a conditional Brenier map,\nenabling a fast and simultaneous generation of Bayesian credible sets whose\ncontours correspond to level sets of Monge-Kantorovich data depth. Our approach\nis computationally lighter compared to GAN-based and diffusion-based\ncounterparts, and is capable of capturing complex posterior structures.\nFinally, frequentist theoretical guarantee on the consistency of the recovered\nposterior distribution, and of the corresponding Bayesian credible sets, is\nprovided."
                },
                "authors": [
                    {
                        "name": "So Won Jeong"
                    },
                    {
                        "name": "Percy S. Zhai"
                    },
                    {
                        "name": "Veronika Roƒçkov√°"
                    }
                ],
                "author_detail": {
                    "name": "Veronika Roƒçkov√°"
                },
                "author": "Veronika Roƒçkov√°",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09534v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09534v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13624v1",
                "updated": "2025-10-15T14:51:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    51,
                    28,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:51:28Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    51,
                    28,
                    2,
                    288,
                    0
                ],
                "title": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of\n  German Tumor Diagnoses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of\n  German Tumor Diagnoses"
                },
                "summary": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential\nfor structured cancer documentation in Germany. Smaller open-weight LLMs are\nappealing for privacy-preserving automation but often struggle with coding\naccuracy in German-language contexts. This study investigates whether\ninstruction-based fine-tuning on public datasets improves the coding accuracy\nof open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded\ndiagnoses from the local tumor documentation system as test data. In a\nsystematic data quality assessment, the upper limit for ICD-10 coding\nperformance was estimated at 60-79% for exact and 81-94% for partial\n(three-character codes only) derivation. As training data, over 500,000\nquestion-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS\ncatalogues. Eight open-weight models from the Qwen, Llama, and Mistral families\n(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to\n41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3\ntopography coding also improved but started and remained considerably lower\nwith an exact accuracy of 22-40% and a partial accuracy of 56-67% after\nfine-tuning. Malformed code outputs dropped to 0% for all models.\nTumor-diagnosis recognition reached 99%. Accuracy correlated positively with\nmodel size, but gaps between small and large models narrowed after fine-tuning.\nThe reasoning mode in Qwen3 generally yielded a lower performance than\nfine-tuning and was over 100 times slower. Our findings highlight the potential\nof leveraging public catalogues to build instruction datasets that improve LLMs\nin medical documentation tasks. The complete training dataset and the\nbest-performing checkpoints of the fine-tuned models are available from\nhttps://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential\nfor structured cancer documentation in Germany. Smaller open-weight LLMs are\nappealing for privacy-preserving automation but often struggle with coding\naccuracy in German-language contexts. This study investigates whether\ninstruction-based fine-tuning on public datasets improves the coding accuracy\nof open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded\ndiagnoses from the local tumor documentation system as test data. In a\nsystematic data quality assessment, the upper limit for ICD-10 coding\nperformance was estimated at 60-79% for exact and 81-94% for partial\n(three-character codes only) derivation. As training data, over 500,000\nquestion-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS\ncatalogues. Eight open-weight models from the Qwen, Llama, and Mistral families\n(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to\n41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3\ntopography coding also improved but started and remained considerably lower\nwith an exact accuracy of 22-40% and a partial accuracy of 56-67% after\nfine-tuning. Malformed code outputs dropped to 0% for all models.\nTumor-diagnosis recognition reached 99%. Accuracy correlated positively with\nmodel size, but gaps between small and large models narrowed after fine-tuning.\nThe reasoning mode in Qwen3 generally yielded a lower performance than\nfine-tuning and was over 100 times slower. Our findings highlight the potential\nof leveraging public catalogues to build instruction datasets that improve LLMs\nin medical documentation tasks. The complete training dataset and the\nbest-performing checkpoints of the fine-tuned models are available from\nhttps://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Lakisha Ortiz Rosario"
                    },
                    {
                        "name": "Georg Vollmar"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Fatma Alickovic"
                    },
                    {
                        "name": "Thomas Kindler"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08728v2",
                "updated": "2025-10-15T14:49:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    49,
                    35,
                    2,
                    288,
                    0
                ],
                "published": "2023-08-17T01:58:04Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    1,
                    58,
                    4,
                    3,
                    229,
                    0
                ],
                "title": "Translating Regulatory Clauses into Executable Codes for Building Design\n  Checking via Large Language Model Driven Function Matching and Composing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Regulatory Clauses into Executable Codes for Building Design\n  Checking via Large Language Model Driven Function Matching and Composing"
                },
                "summary": "Translating clauses into executable code is a vital stage of automated rule\nchecking (ARC) and is essential for effective building design compliance\nchecking, particularly for rules with implicit properties or complex logic\nrequiring domain knowledge. Thus, by systematically analyzing building clauses,\n66 atomic functions are defined first to encapsulate common computational\nlogics. Then, LLM-FuncMapper is proposed, a large language model (LLM)-based\napproach with rule-based adaptive prompts that match clauses to atomic\nfunctions. Finally, executable code is generated by composing functions through\nthe LLMs. Experiments show LLM-FuncMapper outperforms fine-tuning methods by\n19% in function matching while significantly reducing manual annotation\nefforts. Case study demonstrates that LLM-FuncMapper can automatically compose\nmultiple atomic functions to generate executable code, boosting rule-checking\nefficiency. To our knowledge, this research represents the first application of\nLLMs for interpreting complex design clauses into executable code, which may\nshed light on further adoption of LLMs in the construction domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating clauses into executable code is a vital stage of automated rule\nchecking (ARC) and is essential for effective building design compliance\nchecking, particularly for rules with implicit properties or complex logic\nrequiring domain knowledge. Thus, by systematically analyzing building clauses,\n66 atomic functions are defined first to encapsulate common computational\nlogics. Then, LLM-FuncMapper is proposed, a large language model (LLM)-based\napproach with rule-based adaptive prompts that match clauses to atomic\nfunctions. Finally, executable code is generated by composing functions through\nthe LLMs. Experiments show LLM-FuncMapper outperforms fine-tuning methods by\n19% in function matching while significantly reducing manual annotation\nefforts. Case study demonstrates that LLM-FuncMapper can automatically compose\nmultiple atomic functions to generate executable code, boosting rule-checking\nefficiency. To our knowledge, this research represents the first application of\nLLMs for interpreting complex design clauses into executable code, which may\nshed light on further adoption of LLMs in the construction domain."
                },
                "authors": [
                    {
                        "name": "Zhe Zheng"
                    },
                    {
                        "name": "Jin Han"
                    },
                    {
                        "name": "Ke-Yin Chen"
                    },
                    {
                        "name": "Xin-Yu Cao"
                    },
                    {
                        "name": "Xin-Zheng Lu"
                    },
                    {
                        "name": "Jia-Rui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jia-Rui Lin"
                },
                "author": "Jia-Rui Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.08728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13614v1",
                "updated": "2025-10-15T14:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    43,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:43:31Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    43,
                    31,
                    2,
                    288,
                    0
                ],
                "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large\n  Language Model Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities,\nbut struggle with temporal understanding, especially when questions involve\nmultiple entities, compound operators, and evolving event sequences. Temporal\nKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in a\nstructured format, offer a reliable source for temporal reasoning. However,\nexisting TKG-based LLM reasoning methods still struggle with four major\nchallenges: maintaining temporal faithfulness in multi-hop reasoning, achieving\nmulti-entity temporal synchronization, adapting retrieval to diverse temporal\noperators, and reusing prior reasoning experience for stability and efficiency.\nTo address these issues, we propose MemoTime, a memory-augmented temporal\nknowledge graph framework that enhances LLM reasoning through structured\ngrounding, recursive reasoning, and continual experience learning. MemoTime\ndecomposes complex temporal questions into a hierarchical Tree of Time,\nenabling operator-aware reasoning that enforces monotonic timestamps and\nco-constrains multiple entities under unified temporal bounds. A dynamic\nevidence retrieval layer adaptively selects operator-specific retrieval\nstrategies, while a self-evolving experience memory stores verified reasoning\ntraces, toolkit decisions, and sub-question embeddings for cross-type reuse.\nComprehensive experiments on multiple temporal QA benchmarks show that MemoTime\nachieves overall state-of-the-art results, outperforming the strong baseline by\nup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to\nachieve reasoning performance comparable to that of GPT-4-Turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive reasoning abilities,\nbut struggle with temporal understanding, especially when questions involve\nmultiple entities, compound operators, and evolving event sequences. Temporal\nKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in a\nstructured format, offer a reliable source for temporal reasoning. However,\nexisting TKG-based LLM reasoning methods still struggle with four major\nchallenges: maintaining temporal faithfulness in multi-hop reasoning, achieving\nmulti-entity temporal synchronization, adapting retrieval to diverse temporal\noperators, and reusing prior reasoning experience for stability and efficiency.\nTo address these issues, we propose MemoTime, a memory-augmented temporal\nknowledge graph framework that enhances LLM reasoning through structured\ngrounding, recursive reasoning, and continual experience learning. MemoTime\ndecomposes complex temporal questions into a hierarchical Tree of Time,\nenabling operator-aware reasoning that enforces monotonic timestamps and\nco-constrains multiple entities under unified temporal bounds. A dynamic\nevidence retrieval layer adaptively selects operator-specific retrieval\nstrategies, while a self-evolving experience memory stores verified reasoning\ntraces, toolkit decisions, and sub-question embeddings for cross-type reuse.\nComprehensive experiments on multiple temporal QA benchmarks show that MemoTime\nachieves overall state-of-the-art results, outperforming the strong baseline by\nup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to\nachieve reasoning performance comparable to that of GPT-4-Turbo."
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04329v3",
                "updated": "2025-10-15T14:42:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    42,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-06T10:32:32Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    10,
                    32,
                    32,
                    6,
                    187,
                    0
                ],
                "title": "No Language Data Left Behind: A Comparative Study of CJK Language\n  Datasets in the Hugging Face Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Language Data Left Behind: A Comparative Study of CJK Language\n  Datasets in the Hugging Face Ecosystem"
                },
                "summary": "Recent advances in Natural Language Processing (NLP) have underscored the\ncrucial role of high-quality datasets in building large language models (LLMs).\nHowever, while extensive resources and analyses exist for English, the\nlandscape for East Asian languages - particularly Chinese, Japanese, and Korean\n(CJK) - remains fragmented and underexplored, despite these languages together\nserving over 1.6 billion speakers. To address this gap, we investigate the\nHuggingFace ecosystem from a cross-linguistic perspective, focusing on how\ncultural norms, research environments, and institutional practices shape\ndataset availability and quality. Drawing on more than 3,300 datasets, we\nemploy quantitative and qualitative methods to examine how these factors drive\ndistinct creation and curation patterns across Chinese, Japanese, and Korean\nNLP communities. Our findings highlight the large-scale and often\ninstitution-driven nature of Chinese datasets, grassroots community-led\ndevelopment in Korean NLP, and an entertainment- and subculture-focused\nemphasis on Japanese collections. By uncovering these patterns, we reveal\npractical strategies for enhancing dataset documentation, licensing clarity,\nand cross-lingual resource sharing - ultimately guiding more effective and\nculturally attuned LLM development in East Asia. We conclude by discussing best\npractices for future dataset curation and collaboration, aiming to strengthen\nresource development across all three languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Natural Language Processing (NLP) have underscored the\ncrucial role of high-quality datasets in building large language models (LLMs).\nHowever, while extensive resources and analyses exist for English, the\nlandscape for East Asian languages - particularly Chinese, Japanese, and Korean\n(CJK) - remains fragmented and underexplored, despite these languages together\nserving over 1.6 billion speakers. To address this gap, we investigate the\nHuggingFace ecosystem from a cross-linguistic perspective, focusing on how\ncultural norms, research environments, and institutional practices shape\ndataset availability and quality. Drawing on more than 3,300 datasets, we\nemploy quantitative and qualitative methods to examine how these factors drive\ndistinct creation and curation patterns across Chinese, Japanese, and Korean\nNLP communities. Our findings highlight the large-scale and often\ninstitution-driven nature of Chinese datasets, grassroots community-led\ndevelopment in Korean NLP, and an entertainment- and subculture-focused\nemphasis on Japanese collections. By uncovering these patterns, we reveal\npractical strategies for enhancing dataset documentation, licensing clarity,\nand cross-lingual resource sharing - ultimately guiding more effective and\nculturally attuned LLM development in East Asia. We conclude by discussing best\npractices for future dataset curation and collaboration, aiming to strengthen\nresource development across all three languages."
                },
                "authors": [
                    {
                        "name": "Dasol Choi"
                    },
                    {
                        "name": "Woomyoung Park"
                    },
                    {
                        "name": "Youngsook Song"
                    }
                ],
                "author_detail": {
                    "name": "Youngsook Song"
                },
                "author": "Youngsook Song",
                "arxiv_comment": "Accepted to EMNLP 2025 MRL Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13598v1",
                "updated": "2025-10-15T14:31:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    31,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:31:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    31,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation"
                },
                "summary": "Table-to-text generation (insight generation from tables) is a challenging\ntask that requires precision in analyzing the data. In addition, the evaluation\nof existing benchmarks is affected by contamination of Large Language Model\n(LLM) training data as well as domain imbalance. We introduce FreshTab, an\non-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM\ndata contamination problem and enable domain-sensitive evaluation. While\nnon-English table-to-text datasets are limited, FreshTab collects datasets in\ndifferent languages on demand (we experiment with German, Russian and French in\naddition to English). We find that insights generated by LLMs from recent\ntables collected by our method appear clearly worse by automatic metrics, but\nthis does not translate into LLM and human evaluations. Domain effects are\nvisible in all evaluations, showing that a~domain-balanced benchmark is more\nchallenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-to-text generation (insight generation from tables) is a challenging\ntask that requires precision in analyzing the data. In addition, the evaluation\nof existing benchmarks is affected by contamination of Large Language Model\n(LLM) training data as well as domain imbalance. We introduce FreshTab, an\non-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM\ndata contamination problem and enable domain-sensitive evaluation. While\nnon-English table-to-text datasets are limited, FreshTab collects datasets in\ndifferent languages on demand (we experiment with German, Russian and French in\naddition to English). We find that insights generated by LLMs from recent\ntables collected by our method appear clearly worse by automatic metrics, but\nthis does not translate into LLM and human evaluations. Domain effects are\nvisible in all evaluations, showing that a~domain-balanced benchmark is more\nchallenging."
                },
                "authors": [
                    {
                        "name": "Krist√Ωna Onderkov√°"
                    },
                    {
                        "name": "Ond≈ôej Pl√°tek"
                    },
                    {
                        "name": "Zdenƒõk Kasner"
                    },
                    {
                        "name": "Ond≈ôej Du≈°ek"
                    }
                ],
                "author_detail": {
                    "name": "Ond≈ôej Du≈°ek"
                },
                "author": "Ond≈ôej Du≈°ek",
                "arxiv_comment": "To be published in INLG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20957v3",
                "updated": "2025-10-15T14:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    31,
                    15,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-28T16:09:38Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    9,
                    38,
                    0,
                    209,
                    0
                ],
                "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis"
                },
                "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\narising from discrepancies between their pre-trained parametric knowledge and\nreal-time market data. These conflicts are especially problematic in real-world\ninvestment services, where a model's inherent biases can misalign with\ninstitutional objectives, leading to unreliable recommendations. Despite this\nrisk, the intrinsic investment biases of LLMs remain underexplored. We propose\nan experimental framework to investigate emergent behaviors in such conflict\nscenarios, offering a quantitative analysis of bias in LLM-based investment\nanalysis. Using hypothetical scenarios with balanced and imbalanced arguments,\nwe extract the latent biases of models and measure their persistence. Our\nanalysis, centered on sector, size, and momentum, reveals distinct,\nmodel-specific biases. Across most models, a tendency to prefer technology\nstocks, large-cap stocks, and contrarian strategies is observed. These\nfoundational biases often escalate into confirmation bias, causing models to\ncling to initial judgments even when faced with increasing counter-evidence. A\npublic leaderboard benchmarking bias across a broader set of models is\navailable at https://linqalpha.com/leaderboard",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\narising from discrepancies between their pre-trained parametric knowledge and\nreal-time market data. These conflicts are especially problematic in real-world\ninvestment services, where a model's inherent biases can misalign with\ninstitutional objectives, leading to unreliable recommendations. Despite this\nrisk, the intrinsic investment biases of LLMs remain underexplored. We propose\nan experimental framework to investigate emergent behaviors in such conflict\nscenarios, offering a quantitative analysis of bias in LLM-based investment\nanalysis. Using hypothetical scenarios with balanced and imbalanced arguments,\nwe extract the latent biases of models and measure their persistence. Our\nanalysis, centered on sector, size, and momentum, reveals distinct,\nmodel-specific biases. Across most models, a tendency to prefer technology\nstocks, large-cap stocks, and contrarian strategies is observed. These\nfoundational biases often escalate into confirmation bias, causing models to\ncling to initial judgments even when faced with increasing counter-evidence. A\npublic leaderboard benchmarking bias across a broader set of models is\navailable at https://linqalpha.com/leaderboard"
                },
                "authors": [
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Junhyuk Seo"
                    },
                    {
                        "name": "Suhwan Park"
                    },
                    {
                        "name": "Junhyeong Lee"
                    },
                    {
                        "name": "Wonbin Ahn"
                    },
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "arxiv_comment": "Accepted at ACM International Conference on AI in Finance (ICAIF)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.08922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.08922v2",
                "updated": "2025-10-15T14:24:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    24,
                    9,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-11T17:16:13Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    17,
                    16,
                    13,
                    4,
                    192,
                    0
                ],
                "title": "The Bayesian Approach to Continual Learning: An Overview",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Bayesian Approach to Continual Learning: An Overview"
                },
                "summary": "Continual learning is an online paradigm where a learner continually\naccumulates knowledge from different tasks encountered over sequential time\nsteps. Importantly, the learner is required to extend and update its knowledge\nwithout forgetting about the learning experience acquired from the past, and\nwhile avoiding the need to retrain from scratch. Given its sequential nature\nand its resemblance to the way humans think, continual learning offers an\nopportunity to address several challenges which currently stand in the way of\nwidening the range of applicability of deep models to further real-world\nproblems. The continual need to update the learner with data arriving\nsequentially strikes inherent congruence between continual learning and\nBayesian inference which provides a principal platform to keep updating the\nprior beliefs of a model given new data, without completely forgetting the\nknowledge acquired from the old data. This survey inspects different settings\nof Bayesian continual learning, namely task-incremental learning and\nclass-incremental learning. We begin by discussing definitions of continual\nlearning along with its Bayesian setting, as well as the links with related\nfields, such as domain adaptation, transfer learning and meta-learning.\nAfterwards, we introduce a taxonomy offering a comprehensive categorization of\nalgorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we\nanalyze the state-of-the-art while zooming in on some of the most prominent\nBayesian continual learning algorithms to date. Furthermore, we shed some light\non links between continual learning and developmental psychology, and\ncorrespondingly introduce analogies between both fields. We follow that with a\ndiscussion of current challenges, and finally conclude with potential areas for\nfuture research on Bayesian continual learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual learning is an online paradigm where a learner continually\naccumulates knowledge from different tasks encountered over sequential time\nsteps. Importantly, the learner is required to extend and update its knowledge\nwithout forgetting about the learning experience acquired from the past, and\nwhile avoiding the need to retrain from scratch. Given its sequential nature\nand its resemblance to the way humans think, continual learning offers an\nopportunity to address several challenges which currently stand in the way of\nwidening the range of applicability of deep models to further real-world\nproblems. The continual need to update the learner with data arriving\nsequentially strikes inherent congruence between continual learning and\nBayesian inference which provides a principal platform to keep updating the\nprior beliefs of a model given new data, without completely forgetting the\nknowledge acquired from the old data. This survey inspects different settings\nof Bayesian continual learning, namely task-incremental learning and\nclass-incremental learning. We begin by discussing definitions of continual\nlearning along with its Bayesian setting, as well as the links with related\nfields, such as domain adaptation, transfer learning and meta-learning.\nAfterwards, we introduce a taxonomy offering a comprehensive categorization of\nalgorithms belonging to the Bayesian continual learning paradigm. Meanwhile, we\nanalyze the state-of-the-art while zooming in on some of the most prominent\nBayesian continual learning algorithms to date. Furthermore, we shed some light\non links between continual learning and developmental psychology, and\ncorrespondingly introduce analogies between both fields. We follow that with a\ndiscussion of current challenges, and finally conclude with potential areas for\nfuture research on Bayesian continual learning."
                },
                "authors": [
                    {
                        "name": "Tameem Adel"
                    }
                ],
                "author_detail": {
                    "name": "Tameem Adel"
                },
                "author": "Tameem Adel",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.08922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.08922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13590v1",
                "updated": "2025-10-15T14:21:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    21,
                    8,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:21:08Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    21,
                    8,
                    2,
                    288,
                    0
                ],
                "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for\n  Evolving Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for\n  Evolving Knowledge"
                },
                "summary": "Knowledge is inherently time-sensitive and continuously evolves over time.\nAlthough current Retrieval-Augmented Generation (RAG) systems enrich LLMs with\nexternal knowledge, they largely ignore this temporal nature. This raises two\nchallenges for RAG. First, current RAG methods lack effective time-aware\nrepresentations. Same facts of different time are difficult to distinguish with\nvector embeddings or conventional knowledge graphs. Second, most RAG\nevaluations assume a static corpus, leaving a blind spot regarding update costs\nand retrieval stability as knowledge evolves. To make RAG time-aware, we\npropose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level\ntemporal graph consisting of a temporal knowledge graph with timestamped\nrelations and a hierarchical time graph. Multi-granularity temporal summaries\nare generated for each time node to capture both key events and broader trends\nat that time. The design supports incremental updates by extracting new\ntemporal facts from the incoming corpus and merging them into the existing\ngraph. The temporal graph explicitly represents identical facts at different\ntimes as distinct edges to avoid ambiguity, and the time hierarchy graph allows\nonly generating reports for new leaf time nodes and their ancestors, ensuring\neffective and efficient updates. During inference, TG-RAG dynamically retrieves\na subgraph within the temporal and semantic scope of the query, enabling\nprecise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive\nquestion-answering dataset featuring both specific and abstract queries, along\nwith a comprehensive evaluation protocol designed to assess incremental update\ncapabilities of RAG systems. Extensive experiments show that TG-RAG\nsignificantly outperforms existing baselines, demonstrating the effectiveness\nof our method in handling temporal knowledge and incremental updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge is inherently time-sensitive and continuously evolves over time.\nAlthough current Retrieval-Augmented Generation (RAG) systems enrich LLMs with\nexternal knowledge, they largely ignore this temporal nature. This raises two\nchallenges for RAG. First, current RAG methods lack effective time-aware\nrepresentations. Same facts of different time are difficult to distinguish with\nvector embeddings or conventional knowledge graphs. Second, most RAG\nevaluations assume a static corpus, leaving a blind spot regarding update costs\nand retrieval stability as knowledge evolves. To make RAG time-aware, we\npropose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level\ntemporal graph consisting of a temporal knowledge graph with timestamped\nrelations and a hierarchical time graph. Multi-granularity temporal summaries\nare generated for each time node to capture both key events and broader trends\nat that time. The design supports incremental updates by extracting new\ntemporal facts from the incoming corpus and merging them into the existing\ngraph. The temporal graph explicitly represents identical facts at different\ntimes as distinct edges to avoid ambiguity, and the time hierarchy graph allows\nonly generating reports for new leaf time nodes and their ancestors, ensuring\neffective and efficient updates. During inference, TG-RAG dynamically retrieves\na subgraph within the temporal and semantic scope of the query, enabling\nprecise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive\nquestion-answering dataset featuring both specific and abstract queries, along\nwith a comprehensive evaluation protocol designed to assess incremental update\ncapabilities of RAG systems. Extensive experiments show that TG-RAG\nsignificantly outperforms existing baselines, demonstrating the effectiveness\nof our method in handling temporal knowledge and incremental updates."
                },
                "authors": [
                    {
                        "name": "Jiale Han"
                    },
                    {
                        "name": "Austin Cheung"
                    },
                    {
                        "name": "Yubai Wei"
                    },
                    {
                        "name": "Zheng Yu"
                    },
                    {
                        "name": "Xusheng Wang"
                    },
                    {
                        "name": "Bing Zhu"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13586v1",
                "updated": "2025-10-15T14:17:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    17,
                    23,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:17:23Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    17,
                    23,
                    2,
                    288,
                    0
                ],
                "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs"
                },
                "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track)."
                },
                "authors": [
                    {
                        "name": "Pasin Buakhaw"
                    },
                    {
                        "name": "Kun Kerdthaisong"
                    },
                    {
                        "name": "Phuree Phenhiran"
                    },
                    {
                        "name": "Pitikorn Khlaisamniang"
                    },
                    {
                        "name": "Supasate Vorathammathorn"
                    },
                    {
                        "name": "Piyalitt Ittichaiwong"
                    },
                    {
                        "name": "Nutchanon Yongsatianchot"
                    }
                ],
                "author_detail": {
                    "name": "Nutchanon Yongsatianchot"
                },
                "author": "Nutchanon Yongsatianchot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13580v1",
                "updated": "2025-10-15T14:14:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    14,
                    49,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:14:49Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    14,
                    49,
                    2,
                    288,
                    0
                ],
                "title": "Sparse Subnetwork Enhancement for Underrepresented Languages in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Subnetwork Enhancement for Underrepresented Languages in Large\n  Language Models"
                },
                "summary": "Large language models exhibit uneven performance across languages, with\nsubstantial gaps between high- and low-resource languages. We present a\nframework for enhancing monolingual capabilities of LLMs in underrepresented\nlanguages while preserving their general-purpose performance through targeted\nfine-tuning of language-specific subnetworks. Our approach identifies\nlanguage-specific neurons using Language Activation Probability Entropy and\nfine-tunes only the weights associated with these neurons, a dedicated\nsubnetwork, on target-language data. Experiments on Llama-3.1-8B and\nMistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our\nmethod consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA\nadaptation, and random subset fine-tuning baselines while efficiently updating\nonly up to 1% of model parameters. Beyond performance improvements, we observe\nenhanced favorable training dynamics, cross-lingual representational alignment,\nand systematic weight update changes. To facilitate future research, we release\nlanguage-specific neuron identifications for over 100 languages as well as our\nadaptation pipeline, offering a cost-effective pathway for adapting\nstate-of-the-art models to underrepresented languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models exhibit uneven performance across languages, with\nsubstantial gaps between high- and low-resource languages. We present a\nframework for enhancing monolingual capabilities of LLMs in underrepresented\nlanguages while preserving their general-purpose performance through targeted\nfine-tuning of language-specific subnetworks. Our approach identifies\nlanguage-specific neurons using Language Activation Probability Entropy and\nfine-tunes only the weights associated with these neurons, a dedicated\nsubnetwork, on target-language data. Experiments on Llama-3.1-8B and\nMistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our\nmethod consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA\nadaptation, and random subset fine-tuning baselines while efficiently updating\nonly up to 1% of model parameters. Beyond performance improvements, we observe\nenhanced favorable training dynamics, cross-lingual representational alignment,\nand systematic weight update changes. To facilitate future research, we release\nlanguage-specific neuron identifications for over 100 languages as well as our\nadaptation pipeline, offering a cost-effective pathway for adapting\nstate-of-the-art models to underrepresented languages."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13575v1",
                "updated": "2025-10-15T14:13:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    13,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:13:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    13,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "Auto-repair without test cases: How LLMs fix compilation errors in large\n  industrial embedded code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-repair without test cases: How LLMs fix compilation errors in large\n  industrial embedded code"
                },
                "summary": "The co-development of hardware and software in industrial embedded systems\nfrequently leads to compilation errors during continuous integration (CI).\nAutomated repair of such failures is promising, but existing techniques rely on\ntest cases, which are not available for non-compilable code.\n  We employ an automated repair approach for compilation errors driven by large\nlanguage models (LLMs). Our study encompasses the collection of more than 40000\ncommits from the product's source code. We assess the performance of an\nindustrial CI system enhanced by four state-of-the-art LLMs, comparing their\noutcomes with manual corrections provided by human programmers. LLM-equipped CI\nsystems can resolve up to 63 % of the compilation errors in our baseline\ndataset. Among the fixes associated with successful CI builds, 83 % are deemed\nreasonable. Moreover, LLMs significantly reduce debugging time, with the\nmajority of successful cases completed within 8 minutes, compared to hours\ntypically required for manual debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-development of hardware and software in industrial embedded systems\nfrequently leads to compilation errors during continuous integration (CI).\nAutomated repair of such failures is promising, but existing techniques rely on\ntest cases, which are not available for non-compilable code.\n  We employ an automated repair approach for compilation errors driven by large\nlanguage models (LLMs). Our study encompasses the collection of more than 40000\ncommits from the product's source code. We assess the performance of an\nindustrial CI system enhanced by four state-of-the-art LLMs, comparing their\noutcomes with manual corrections provided by human programmers. LLM-equipped CI\nsystems can resolve up to 63 % of the compilation errors in our baseline\ndataset. Among the fixes associated with successful CI builds, 83 % are deemed\nreasonable. Moreover, LLMs significantly reduce debugging time, with the\nmajority of successful cases completed within 8 minutes, compared to hours\ntypically required for manual debugging."
                },
                "authors": [
                    {
                        "name": "Han Fu"
                    },
                    {
                        "name": "Sigrid Eldh"
                    },
                    {
                        "name": "Kristian Wiklund"
                    },
                    {
                        "name": "Andreas Ermedahl"
                    },
                    {
                        "name": "Philipp Haller"
                    },
                    {
                        "name": "Cyrille Artho"
                    }
                ],
                "author_detail": {
                    "name": "Cyrille Artho"
                },
                "author": "Cyrille Artho",
                "arxiv_comment": "9 pages, 4 figures, conference: 2025 28th Euromicro Conference on\n  Digital System Design (DSD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12119v2",
                "updated": "2025-10-15T14:10:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    10,
                    40,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-17T18:43:41Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    18,
                    43,
                    41,
                    0,
                    48,
                    0
                ],
                "title": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRISM: Self-Pruning Intrinsic Selection Method for Training-Free\n  Multimodal Data Selection"
                },
                "summary": "Visual instruction tuning adapts pre-trained Multimodal Large Language Models\n(MLLMs) to follow human instructions for real-world applications. However, the\nrapid growth of these datasets introduces significant redundancy, leading to\nincreased computational costs. Existing methods for selecting instruction data\naim to prune this redundancy, but predominantly rely on computationally\ndemanding techniques such as proxy-based inference or training-based metrics.\nConsequently, the substantial computational costs incurred by these selection\nprocesses often exacerbate the very efficiency bottlenecks they are intended to\nresolve, posing a significant challenge to the scalable and effective tuning of\nMLLMs. To address this challenge, we first identify a critical, yet previously\noverlooked, factor: the anisotropy inherent in visual feature distributions. We\nfind that this anisotropy induces a \\textit{Global Semantic Drift}, and\noverlooking this phenomenon is a key factor limiting the efficiency of current\ndata selection methods. Motivated by this insight, we devise \\textbf{PRISM},\nthe first training-free framework for efficient visual instruction selection.\nPRISM surgically removes the corrupting influence of global background features\nby modeling the intrinsic visual semantics via implicit re-centering.\nEmpirically, PRISM reduces the end-to-end time for data selection and model\ntuning to just 30\\% of conventional pipelines. More remarkably, it achieves\nthis efficiency while simultaneously enhancing performance, surpassing models\nfine-tuned on the full dataset across eight multimodal and three language\nunderstanding benchmarks, culminating in a 101.7\\% relative improvement over\nthe baseline. The code is available for access via\n\\href{https://github.com/bibisbar/PRISM}{this repository}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual instruction tuning adapts pre-trained Multimodal Large Language Models\n(MLLMs) to follow human instructions for real-world applications. However, the\nrapid growth of these datasets introduces significant redundancy, leading to\nincreased computational costs. Existing methods for selecting instruction data\naim to prune this redundancy, but predominantly rely on computationally\ndemanding techniques such as proxy-based inference or training-based metrics.\nConsequently, the substantial computational costs incurred by these selection\nprocesses often exacerbate the very efficiency bottlenecks they are intended to\nresolve, posing a significant challenge to the scalable and effective tuning of\nMLLMs. To address this challenge, we first identify a critical, yet previously\noverlooked, factor: the anisotropy inherent in visual feature distributions. We\nfind that this anisotropy induces a \\textit{Global Semantic Drift}, and\noverlooking this phenomenon is a key factor limiting the efficiency of current\ndata selection methods. Motivated by this insight, we devise \\textbf{PRISM},\nthe first training-free framework for efficient visual instruction selection.\nPRISM surgically removes the corrupting influence of global background features\nby modeling the intrinsic visual semantics via implicit re-centering.\nEmpirically, PRISM reduces the end-to-end time for data selection and model\ntuning to just 30\\% of conventional pipelines. More remarkably, it achieves\nthis efficiency while simultaneously enhancing performance, surpassing models\nfine-tuned on the full dataset across eight multimodal and three language\nunderstanding benchmarks, culminating in a 101.7\\% relative improvement over\nthe baseline. The code is available for access via\n\\href{https://github.com/bibisbar/PRISM}{this repository}."
                },
                "authors": [
                    {
                        "name": "Jinhe Bi"
                    },
                    {
                        "name": "Yifan Wang"
                    },
                    {
                        "name": "Danqi Yan"
                    },
                    {
                        "name": "Aniri"
                    },
                    {
                        "name": "Wenke Huang"
                    },
                    {
                        "name": "Zengjie Jin"
                    },
                    {
                        "name": "Xiaowen Ma"
                    },
                    {
                        "name": "Artur Hecker"
                    },
                    {
                        "name": "Mang Ye"
                    },
                    {
                        "name": "Xun Xiao"
                    },
                    {
                        "name": "Hinrich Schuetze"
                    },
                    {
                        "name": "Volker Tresp"
                    },
                    {
                        "name": "Yunpu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yunpu Ma"
                },
                "author": "Yunpu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13570v1",
                "updated": "2025-10-15T14:08:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    8,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:08:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    8,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "Selective Adversarial Attacks on LLM Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Adversarial Attacks on LLM Benchmarks"
                },
                "summary": "Benchmarking outcomes increasingly govern trust, selection, and deployment of\nLLMs, yet these evaluations remain vulnerable to semantically equivalent\nadversarial perturbations. Prior work on adversarial robustness in NLP has\nemphasized text attacks that affect many models equally, leaving open the\nquestion of whether it is possible to selectively degrade or enhance\nperformance while minimally affecting other models. We formalize this problem\nand study selective adversarial attacks on MMLU - a widely used benchmark\ndesigned to measure a language model's broad general knowledge and reasoning\nability across different subjects. Using canonical attacks integrated into\nTextAttack framework, we introduce a protocol for selectivity assessment,\ndevelop a custom constraint to increase selectivity of attacks and propose a\nsurrogate-LLM pipeline that generates selective perturbations. Empirically, we\nfind that selective adversarial attacks exist and can materially alter relative\nrankings, challenging the fairness, reproducibility, and transparency of\nleaderboard-driven evaluation. Our results motivate perturbation-aware\nreporting and robustness diagnostics for LLM evaluation and demonstrate that\neven subtle edits can shift comparative judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking outcomes increasingly govern trust, selection, and deployment of\nLLMs, yet these evaluations remain vulnerable to semantically equivalent\nadversarial perturbations. Prior work on adversarial robustness in NLP has\nemphasized text attacks that affect many models equally, leaving open the\nquestion of whether it is possible to selectively degrade or enhance\nperformance while minimally affecting other models. We formalize this problem\nand study selective adversarial attacks on MMLU - a widely used benchmark\ndesigned to measure a language model's broad general knowledge and reasoning\nability across different subjects. Using canonical attacks integrated into\nTextAttack framework, we introduce a protocol for selectivity assessment,\ndevelop a custom constraint to increase selectivity of attacks and propose a\nsurrogate-LLM pipeline that generates selective perturbations. Empirically, we\nfind that selective adversarial attacks exist and can materially alter relative\nrankings, challenging the fairness, reproducibility, and transparency of\nleaderboard-driven evaluation. Our results motivate perturbation-aware\nreporting and robustness diagnostics for LLM evaluation and demonstrate that\neven subtle edits can shift comparative judgments."
                },
                "authors": [
                    {
                        "name": "Ivan Dubrovsky"
                    },
                    {
                        "name": "Anastasia Orlova"
                    },
                    {
                        "name": "Illarion Iov"
                    },
                    {
                        "name": "Nina Gubina"
                    },
                    {
                        "name": "Irena Gureeva"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.22637v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.22637v2",
                "updated": "2025-10-15T14:08:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    8,
                    12,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-26T17:58:10Z",
                "published_parsed": [
                    2025,
                    9,
                    26,
                    17,
                    58,
                    10,
                    4,
                    269,
                    0
                ],
                "title": "Variational Reasoning for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Reasoning for Language Models"
                },
                "summary": "We introduce a variational reasoning framework for language models that\ntreats thinking traces as latent variables and optimizes them through\nvariational inference. Starting from the evidence lower bound (ELBO), we extend\nit to a multi-trace objective for tighter bounds and propose a forward-KL\nformulation that stabilizes the training of the variational posterior. We\nfurther show that rejection sampling finetuning and binary-reward RL, including\nGRPO, can be interpreted as local forward-KL objectives, where an implicit\nweighting by model accuracy naturally arises from the derivation and reveals a\npreviously unnoticed bias toward easier questions. We empirically validate our\nmethod on the Qwen 2.5 and Qwen 3 model families across a wide range of\nreasoning tasks. Overall, our work provides a principled probabilistic\nperspective that unifies variational inference with RL-style methods and yields\nstable objectives for improving the reasoning ability of language models. Our\ncode is available at https://github.com/sail-sg/variational-reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a variational reasoning framework for language models that\ntreats thinking traces as latent variables and optimizes them through\nvariational inference. Starting from the evidence lower bound (ELBO), we extend\nit to a multi-trace objective for tighter bounds and propose a forward-KL\nformulation that stabilizes the training of the variational posterior. We\nfurther show that rejection sampling finetuning and binary-reward RL, including\nGRPO, can be interpreted as local forward-KL objectives, where an implicit\nweighting by model accuracy naturally arises from the derivation and reveals a\npreviously unnoticed bias toward easier questions. We empirically validate our\nmethod on the Qwen 2.5 and Qwen 3 model families across a wide range of\nreasoning tasks. Overall, our work provides a principled probabilistic\nperspective that unifies variational inference with RL-style methods and yields\nstable objectives for improving the reasoning ability of language models. Our\ncode is available at https://github.com/sail-sg/variational-reasoning."
                },
                "authors": [
                    {
                        "name": "Xiangxin Zhou"
                    },
                    {
                        "name": "Zichen Liu"
                    },
                    {
                        "name": "Haonan Wang"
                    },
                    {
                        "name": "Chao Du"
                    },
                    {
                        "name": "Min Lin"
                    },
                    {
                        "name": "Chongxuan Li"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Tianyu Pang"
                    }
                ],
                "author_detail": {
                    "name": "Tianyu Pang"
                },
                "author": "Tianyu Pang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.22637v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.22637v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00893v2",
                "updated": "2025-10-15T13:56:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    56,
                    3,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-31T15:12:51Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    12,
                    51,
                    6,
                    243,
                    0
                ],
                "title": "SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset"
                },
                "summary": "Satire, irony, and sarcasm are techniques typically used to express humor and\ncritique, rather than deceive; however, they can occasionally be mistaken for\nfactual reporting, akin to fake news. These techniques can be applied at a more\ngranular level, allowing satirical information to be incorporated into news\narticles. In this paper, we introduce the first sentence-level dataset for\nRomanian satire detection for news articles, called SeLeRoSa. The dataset\ncomprises 13,873 manually annotated sentences spanning various domains,\nincluding social issues, IT, science, and movies. With the rise and recent\nprogress of large language models (LLMs) in the natural language processing\nliterature, LLMs have demonstrated enhanced capabilities to tackle various\ntasks in zero-shot settings. We evaluate multiple baseline models based on LLMs\nin both zero-shot and fine-tuning settings, as well as baseline\ntransformer-based models. Our findings reveal the current limitations of these\nmodels in the sentence-level satire detection task, paving the way for new\nresearch directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satire, irony, and sarcasm are techniques typically used to express humor and\ncritique, rather than deceive; however, they can occasionally be mistaken for\nfactual reporting, akin to fake news. These techniques can be applied at a more\ngranular level, allowing satirical information to be incorporated into news\narticles. In this paper, we introduce the first sentence-level dataset for\nRomanian satire detection for news articles, called SeLeRoSa. The dataset\ncomprises 13,873 manually annotated sentences spanning various domains,\nincluding social issues, IT, science, and movies. With the rise and recent\nprogress of large language models (LLMs) in the natural language processing\nliterature, LLMs have demonstrated enhanced capabilities to tackle various\ntasks in zero-shot settings. We evaluate multiple baseline models based on LLMs\nin both zero-shot and fine-tuning settings, as well as baseline\ntransformer-based models. Our findings reveal the current limitations of these\nmodels in the sentence-level satire detection task, paving the way for new\nresearch directions."
                },
                "authors": [
                    {
                        "name": "RƒÉzvan-Alexandru SmƒÉdu"
                    },
                    {
                        "name": "Andreea Iuga"
                    },
                    {
                        "name": "Dumitru-Clementin Cercel"
                    },
                    {
                        "name": "Florin Pop"
                    }
                ],
                "author_detail": {
                    "name": "Florin Pop"
                },
                "author": "Florin Pop",
                "arxiv_doi": "10.1145/3746252.3761632",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761632",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.00893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 2 Figures",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13558v1",
                "updated": "2025-10-15T13:54:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    54,
                    42,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:54:42Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    54,
                    42,
                    2,
                    288,
                    0
                ],
                "title": "Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts\n  Steering Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts\n  Steering Module"
                },
                "summary": "Aligning pretrained audio encoders and Large Language Models (LLMs) offers a\npromising, parameter-efficient path to building powerful multimodal agents.\nHowever, existing methods often require costly full-model finetuning or rely on\nstatic adapters that may lack expressive power. Drawing inspiration from the\nPlatonic Representation Hypothesis, we introduce SteerMoE, a novel and modular\nframework for audio-language alignment. SteerMoE freezes both the audio encoder\nand the LLM decoder, training only a lightweight steering module integrated\nwithin the encoder's layers. This module uses a Mixture-of-Experts (MoE) router\nto dynamically select and apply learned steering vectors, progressively\ntransforming continuous audio representations into a space comprehensible to\nthe LLM. By operating entirely in the continuous embedding space, our approach\nrequires no modifications to the LLM's vocabulary and preserves its advanced\nreasoning and agentic capabilities. We demonstrate through experiments on ASR,\naudio understanding, and a qualitative function-calling task that SteerMoE\nachieves strong performance while remaining highly modular and computationally\nefficient, offering a robust new paradigm for developing sophisticated\naudio-language systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning pretrained audio encoders and Large Language Models (LLMs) offers a\npromising, parameter-efficient path to building powerful multimodal agents.\nHowever, existing methods often require costly full-model finetuning or rely on\nstatic adapters that may lack expressive power. Drawing inspiration from the\nPlatonic Representation Hypothesis, we introduce SteerMoE, a novel and modular\nframework for audio-language alignment. SteerMoE freezes both the audio encoder\nand the LLM decoder, training only a lightweight steering module integrated\nwithin the encoder's layers. This module uses a Mixture-of-Experts (MoE) router\nto dynamically select and apply learned steering vectors, progressively\ntransforming continuous audio representations into a space comprehensible to\nthe LLM. By operating entirely in the continuous embedding space, our approach\nrequires no modifications to the LLM's vocabulary and preserves its advanced\nreasoning and agentic capabilities. We demonstrate through experiments on ASR,\naudio understanding, and a qualitative function-calling task that SteerMoE\nachieves strong performance while remaining highly modular and computationally\nefficient, offering a robust new paradigm for developing sophisticated\naudio-language systems."
                },
                "authors": [
                    {
                        "name": "Ruitao Feng"
                    },
                    {
                        "name": "Bixi Zhang"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Zheng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yuan"
                },
                "author": "Zheng Yuan",
                "arxiv_comment": "5 pages, 1 figures. Code is available at:\n  https://github.com/forfrt/SteerMoE. Submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08141v3",
                "updated": "2025-10-15T13:54:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    54,
                    25,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-09T12:24:08Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    24,
                    8,
                    3,
                    282,
                    0
                ],
                "title": "Arbitrary Entropy Policy Optimization: Entropy Is Controllable in\n  Reinforcement Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arbitrary Entropy Policy Optimization: Entropy Is Controllable in\n  Reinforcement Fine-tuning"
                },
                "summary": "Reinforcement fine-tuning (RFT) is essential for enhancing the reasoning\ncapabilities of large language models (LLM), yet the widely adopted Group\nRelative Policy Optimization (GRPO) suffers from entropy collapse, where\nentropy monotonically decreases, exploration vanishes, and policies converge\nprematurely. Existing entropy-regularized methods only partially alleviate this\nissue while introducing bias and instability, leaving entropy control\nunresolved and the connection between entropy, exploration, and performance\nunclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which\neliminates entropy collapse by replacing entropy bonuses with REINFORCE policy\ngradient on temperature-adjusted distributions and stabilizing entropy through\ntemperature regulation. AEPO integrates three key designs: policy gradient as\nregularization, distribution as regularization, and REINFORCE as\nregularization, enabling precise entropy control without distorting\noptimization. Experiments demonstrate three major contributions: AEPO (1)\nstabilizes entropy at arbitrary target levels, effectively removing collapse in\nGRPO; (2) reveals a non-monotonic relation where performance first improves\nthen declines with increasing entropy, clarifying the link between entropy,\nexploration, and reasoning; and (3) generalizes beyond entropy, providing a\nbroader RFT paradigm where superior target distributions can serve as REINFORCE\nregularizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement fine-tuning (RFT) is essential for enhancing the reasoning\ncapabilities of large language models (LLM), yet the widely adopted Group\nRelative Policy Optimization (GRPO) suffers from entropy collapse, where\nentropy monotonically decreases, exploration vanishes, and policies converge\nprematurely. Existing entropy-regularized methods only partially alleviate this\nissue while introducing bias and instability, leaving entropy control\nunresolved and the connection between entropy, exploration, and performance\nunclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which\neliminates entropy collapse by replacing entropy bonuses with REINFORCE policy\ngradient on temperature-adjusted distributions and stabilizing entropy through\ntemperature regulation. AEPO integrates three key designs: policy gradient as\nregularization, distribution as regularization, and REINFORCE as\nregularization, enabling precise entropy control without distorting\noptimization. Experiments demonstrate three major contributions: AEPO (1)\nstabilizes entropy at arbitrary target levels, effectively removing collapse in\nGRPO; (2) reveals a non-monotonic relation where performance first improves\nthen declines with increasing entropy, clarifying the link between entropy,\nexploration, and reasoning; and (3) generalizes beyond entropy, providing a\nbroader RFT paradigm where superior target distributions can serve as REINFORCE\nregularizers."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhaochun Li"
                    },
                    {
                        "name": "Jionghao Bai"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "Shisheng Cui"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13554v1",
                "updated": "2025-10-15T13:49:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    49,
                    51,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:49:51Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    49,
                    51,
                    2,
                    288,
                    0
                ],
                "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization"
                },
                "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Zhichen Dong"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Jiashun Liu"
                    },
                    {
                        "name": "Han Lu"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "23 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04886v4",
                "updated": "2025-10-15T13:46:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    46,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-07T11:17:32Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    17,
                    32,
                    0,
                    188,
                    0
                ],
                "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations"
                },
                "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch."
                },
                "authors": [
                    {
                        "name": "A. Bochkov"
                    }
                ],
                "author_detail": {
                    "name": "A. Bochkov"
                },
                "author": "A. Bochkov",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (10/2025).\n  OpenReview: https://openreview.net/forum?id=Odh8IynO1o",
                "arxiv_journal_ref": "Transactions on Machine Learning Research (TMLR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02477v3",
                "updated": "2025-10-15T13:42:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    42,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-04-03T10:53:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    53,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision"
                },
                "summary": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We adopt a task-oriented\nperspective to systematically review the applications and advancements of\nmultimodal fusion methods and VLMs in the field of robot vision. For semantic\nscene understanding tasks, we categorize fusion approaches into encoder-decoder\nframeworks, attention-based architectures, and graph neural networks.\nMeanwhile, we also analyze the architectural characteristics and practical\nimplementations of these fusion strategies in key tasks such as simultaneous\nlocalization and mapping (SLAM), 3D object detection, navigation, and\nmanipulation. We compare the evolutionary paths and applicability of VLMs based\non large language models (LLMs) with traditional multimodal fusion\nmethods.Additionally, we conduct an in-depth analysis of commonly used\ndatasets, evaluating their applicability and challenges in real-world robotic\nscenarios. Building on this analysis, we identify key challenges in current\nresearch, including cross-modal alignment, efficient fusion, real-time\ndeployment, and domain adaptation. We propose future directions such as\nself-supervised learning for robust multimodal representations, structured\nspatial memory and environment modeling to enhance spatial intelligence, and\nthe integration of adversarial robustness and human feedback mechanisms to\nenable ethically aligned system deployment. Through a comprehensive review,\ncomparative analysis, and forward-looking discussion, we provide a valuable\nreference for advancing multimodal perception and interaction in robotic\nvision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We adopt a task-oriented\nperspective to systematically review the applications and advancements of\nmultimodal fusion methods and VLMs in the field of robot vision. For semantic\nscene understanding tasks, we categorize fusion approaches into encoder-decoder\nframeworks, attention-based architectures, and graph neural networks.\nMeanwhile, we also analyze the architectural characteristics and practical\nimplementations of these fusion strategies in key tasks such as simultaneous\nlocalization and mapping (SLAM), 3D object detection, navigation, and\nmanipulation. We compare the evolutionary paths and applicability of VLMs based\non large language models (LLMs) with traditional multimodal fusion\nmethods.Additionally, we conduct an in-depth analysis of commonly used\ndatasets, evaluating their applicability and challenges in real-world robotic\nscenarios. Building on this analysis, we identify key challenges in current\nresearch, including cross-modal alignment, efficient fusion, real-time\ndeployment, and domain adaptation. We propose future directions such as\nself-supervised learning for robust multimodal representations, structured\nspatial memory and environment modeling to enhance spatial intelligence, and\nthe integration of adversarial robustness and human feedback mechanisms to\nenable ethically aligned system deployment. Through a comprehensive review,\ncomparative analysis, and forward-looking discussion, we provide a valuable\nreference for advancing multimodal perception and interaction in robotic\nvision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Han"
                    },
                    {
                        "name": "Shunpeng Chen"
                    },
                    {
                        "name": "Zenghuang Fu"
                    },
                    {
                        "name": "Zhe Feng"
                    },
                    {
                        "name": "Lue Fan"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Li Guo"
                    },
                    {
                        "name": "Weiliang Meng"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Shibiao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shibiao Xu"
                },
                "author": "Shibiao Xu",
                "arxiv_doi": "10.1016/j.inffus.2025.103652",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2025.103652",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.02477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 11 figures. Accepted to Information Fusion. Final journal\n  version: volume 126 (Part B), February 2026",
                "arxiv_journal_ref": "Information Fusion, 126 (Part B), February 2026, 103652",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20712v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20712v4",
                "updated": "2025-10-15T13:41:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    41,
                    22,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-25T03:22:04Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    22,
                    4,
                    3,
                    268,
                    0
                ],
                "title": "CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}oordinating \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}oordinating \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales."
                },
                "authors": [
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Minxuan Lv"
                    },
                    {
                        "name": "Yuntao Li"
                    },
                    {
                        "name": "Wenping Hu"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20712v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20712v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13543v1",
                "updated": "2025-10-15T13:39:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    39,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:39:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    39,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in\n  Agentic AI Browsers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in\n  Agentic AI Browsers"
                },
                "summary": "Large Language Model (LLM) based agents integrated into web browsers (often\ncalled agentic AI browsers) offer powerful automation of web tasks. However,\nthey are vulnerable to indirect prompt injection attacks, where malicious\ninstructions hidden in a webpage deceive the agent into unwanted actions. These\nattacks can bypass traditional web security boundaries, as the AI agent\noperates with the user privileges across sites. In this paper, we present a\nnovel fuzzing framework that runs entirely in the browser and is guided by an\nLLM to automatically discover such prompt injection vulnerabilities in real\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based agents integrated into web browsers (often\ncalled agentic AI browsers) offer powerful automation of web tasks. However,\nthey are vulnerable to indirect prompt injection attacks, where malicious\ninstructions hidden in a webpage deceive the agent into unwanted actions. These\nattacks can bypass traditional web security boundaries, as the AI agent\noperates with the user privileges across sites. In this paper, we present a\nnovel fuzzing framework that runs entirely in the browser and is guided by an\nLLM to automatically discover such prompt injection vulnerabilities in real\ntime."
                },
                "authors": [
                    {
                        "name": "Avihay Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Avihay Cohen"
                },
                "author": "Avihay Cohen",
                "arxiv_comment": "37 pages , 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13537v1",
                "updated": "2025-10-15T13:32:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    32,
                    25,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:32:25Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    32,
                    25,
                    2,
                    288,
                    0
                ],
                "title": "K-Merge: Online Continual Merging of Adapters for On-device Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K-Merge: Online Continual Merging of Adapters for On-device Large\n  Language Models"
                },
                "summary": "On-device deployment of Large Language Models (LLMs) frequently leverages\nLow-Rank Adapters (LoRAs) to support diverse downstream tasks under tight\nresource constraints. To address the limited storage capacity of mobile\ndevices, recent works have explored model merging techniques to fuse multiple\nLoRAs into a single one. In practice, however, LoRAs are often delivered\nincrementally, as users request support for new tasks (e.g., novel problem\ntypes or languages). This scenario introduces a new challenge: on-device online\ncontinual merging, where the objective is to incorporate new LoRAs while\npreserving the performance on previously supported tasks. In this paper, we\npropose a data-free and computationally efficient strategy for selecting and\nmerging LoRAs when a new one becomes available, assuming the device can store\nonly a limited number of adapters. Extensive experiments across real-world\ntasks demonstrate the superiority of our approach compared to alternative\nstrategies while adhering to the storage budget and compute limitations of\non-device settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device deployment of Large Language Models (LLMs) frequently leverages\nLow-Rank Adapters (LoRAs) to support diverse downstream tasks under tight\nresource constraints. To address the limited storage capacity of mobile\ndevices, recent works have explored model merging techniques to fuse multiple\nLoRAs into a single one. In practice, however, LoRAs are often delivered\nincrementally, as users request support for new tasks (e.g., novel problem\ntypes or languages). This scenario introduces a new challenge: on-device online\ncontinual merging, where the objective is to incorporate new LoRAs while\npreserving the performance on previously supported tasks. In this paper, we\npropose a data-free and computationally efficient strategy for selecting and\nmerging LoRAs when a new one becomes available, assuming the device can store\nonly a limited number of adapters. Extensive experiments across real-world\ntasks demonstrate the superiority of our approach compared to alternative\nstrategies while adhering to the storage budget and compute limitations of\non-device settings."
                },
                "authors": [
                    {
                        "name": "Donald Shenaj"
                    },
                    {
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "name": "Taha Ceritli"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Pietro Zanuttigh"
                    },
                    {
                        "name": "Umberto Michieli"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michieli"
                },
                "author": "Umberto Michieli",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13524v1",
                "updated": "2025-10-15T13:17:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    17,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:17:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    17,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within\n  the Financial Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within\n  the Financial Domain"
                },
                "summary": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics"
                },
                "authors": [
                    {
                        "name": "William Flanagan"
                    },
                    {
                        "name": "Mukunda Das"
                    },
                    {
                        "name": "Rajitha Ramanyake"
                    },
                    {
                        "name": "Swaunja Maslekar"
                    },
                    {
                        "name": "Meghana Manipuri"
                    },
                    {
                        "name": "Joong Ho Choi"
                    },
                    {
                        "name": "Shruti Nair"
                    },
                    {
                        "name": "Shambhavi Bhusan"
                    },
                    {
                        "name": "Sanjana Dulam"
                    },
                    {
                        "name": "Mouni Pendharkar"
                    },
                    {
                        "name": "Nidhi Singh"
                    },
                    {
                        "name": "Vashisth Doshi"
                    },
                    {
                        "name": "Sachi Shah Paresh"
                    }
                ],
                "author_detail": {
                    "name": "Sachi Shah Paresh"
                },
                "author": "Sachi Shah Paresh",
                "arxiv_comment": "NeurIPS 2025 GenAI in Finance Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08885v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08885v2",
                "updated": "2025-10-15T13:11:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    11,
                    40,
                    2,
                    288,
                    0
                ],
                "published": "2025-01-15T15:56:06Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    15,
                    56,
                    6,
                    2,
                    15,
                    0
                ],
                "title": "Perspective-Aware Teaching: Adapting Knowledge for Heterogeneous\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perspective-Aware Teaching: Adapting Knowledge for Heterogeneous\n  Distillation"
                },
                "summary": "Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a perspective-aware\nteaching (PAT) KD framework to enable feature distillation across diverse\narchitectures. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architectures. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method. Our code is available at\nhttps://github.com/jimmylin0979/PAT.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation (KD) involves transferring knowledge from a\npre-trained heavy teacher model to a lighter student model, thereby reducing\nthe inference cost while maintaining comparable effectiveness. Prior KD\ntechniques typically assume homogeneity between the teacher and student models.\nHowever, as technology advances, a wide variety of architectures have emerged,\nranging from initial Convolutional Neural Networks (CNNs) to Vision\nTransformers (ViTs), and Multi-Level Perceptrons (MLPs). Consequently,\ndeveloping a universal KD framework compatible with any architecture has become\nan important research topic. In this paper, we introduce a perspective-aware\nteaching (PAT) KD framework to enable feature distillation across diverse\narchitectures. Our framework comprises two key components. First, we design\nprompt tuning blocks that incorporate student feedback, allowing teacher\nfeatures to adapt to the student model's learning process. Second, we propose\nregion-aware attention to mitigate the view mismatch problem between\nheterogeneous architectures. By leveraging these two modules, effective\ndistillation of intermediate features can be achieved across heterogeneous\narchitectures. Extensive experiments on CIFAR, ImageNet, and COCO demonstrate\nthe superiority of the proposed method. Our code is available at\nhttps://github.com/jimmylin0979/PAT.git."
                },
                "authors": [
                    {
                        "name": "Jhe-Hao Lin"
                    },
                    {
                        "name": "Yi Yao"
                    },
                    {
                        "name": "Chan-Feng Hsu"
                    },
                    {
                        "name": "Hongxia Xie"
                    },
                    {
                        "name": "Hong-Han Shuai"
                    },
                    {
                        "name": "Wen-Huang Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Wen-Huang Cheng"
                },
                "author": "Wen-Huang Cheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08885v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08885v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10379v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10379v2",
                "updated": "2025-10-15T13:11:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    11,
                    29,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-12T06:09:41Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    6,
                    9,
                    41,
                    3,
                    163,
                    0
                ],
                "title": "Hamiltonian Learning via Inverse Physics-Informed Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian Learning via Inverse Physics-Informed Neural Networks"
                },
                "summary": "Hamiltonian learning (HL), enabling precise estimation of system parameters\nand underlying dynamics, plays a critical role in characterizing quantum\nsystems. However, conventional HL methods face challenges in noise robustness\nand resource efficiency, especially under limited measurements. In this work,\nwe present \\textit{Inverse Physics-Informed Neural Networks for Hamiltonian\nLearning (iPINN-HL)}, an approach that incorporates the Schr\\\"{o}dinger\nequation as a soft constraint via a loss function penalty into the ML\nprocedure. This formulation allows the model to integrate both observational\ndata and known physical laws to infer Hamiltonian parameters with greater\naccuracy and resource efficiency. We benchmark iPINN-HL against a\ndeep-neural-network-based quantum state tomography method (denoted as DNN-HL)\nand demonstrate its effectiveness across several different scenarios, including\none-dimensional spin chains, cross-resonance gate calibration, crosstalk\nidentification, and real-time compensation to parameter drift. Our results show\nthat iPINN-HL can approach the Heisenberg limit and exhibits robustness to\nnoises, while outperforming DNN-HL in accuracy and resource efficiency.\nTherefore, iPINN-HL is a powerful and flexible framework for quantum system\ncharacterization for practical tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hamiltonian learning (HL), enabling precise estimation of system parameters\nand underlying dynamics, plays a critical role in characterizing quantum\nsystems. However, conventional HL methods face challenges in noise robustness\nand resource efficiency, especially under limited measurements. In this work,\nwe present \\textit{Inverse Physics-Informed Neural Networks for Hamiltonian\nLearning (iPINN-HL)}, an approach that incorporates the Schr\\\"{o}dinger\nequation as a soft constraint via a loss function penalty into the ML\nprocedure. This formulation allows the model to integrate both observational\ndata and known physical laws to infer Hamiltonian parameters with greater\naccuracy and resource efficiency. We benchmark iPINN-HL against a\ndeep-neural-network-based quantum state tomography method (denoted as DNN-HL)\nand demonstrate its effectiveness across several different scenarios, including\none-dimensional spin chains, cross-resonance gate calibration, crosstalk\nidentification, and real-time compensation to parameter drift. Our results show\nthat iPINN-HL can approach the Heisenberg limit and exhibits robustness to\nnoises, while outperforming DNN-HL in accuracy and resource efficiency.\nTherefore, iPINN-HL is a powerful and flexible framework for quantum system\ncharacterization for practical tasks."
                },
                "authors": [
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "18 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10379v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10379v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.dis-nn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13519v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13519v1",
                "updated": "2025-10-15T13:10:32Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    10,
                    32,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:10:32Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    10,
                    32,
                    2,
                    288,
                    0
                ],
                "title": "Data-Driven Reduced Modeling of Recurrent Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Reduced Modeling of Recurrent Neural Networks"
                },
                "summary": "Artificial Recurrent Neural Networks (RNNs) are widely used in neuroscience\nto model the collective activity of neurons during behavioral tasks. The high\ndimensionality of their parameter and activity spaces, however, often make it\nchallenging to infer and interpret the fundamental features of their dynamics.\n  In this study, we employ recent nonlinear dynamical system techniques to\nuncover the core dynamics of several RNNs used in contemporary neuroscience.\nSpecifically, using a data-driven approach, we identify Spectral Submanifolds\n(SSMs), i.e., low-dimensional attracting invariant manifolds tangent to the\neigenspaces of fixed points. The internal dynamics of SSMs serve as nonlinear\nmodels that reduce the dimensionality of the full RNNs by orders of magnitude.\n  Through low-dimensional, SSM-reduced models, we give mathematically precise\ndefinitions of line and ring attractors, which are intuitive concepts commonly\nused to explain decision-making and working memory. The new level of\nunderstanding of RNNs obtained from SSM reduction enables the interpretation of\nmathematically well-defined and robust structures in neuronal dynamics, leading\nto novel predictions about the neural computations underlying behavior.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Artificial Recurrent Neural Networks (RNNs) are widely used in neuroscience\nto model the collective activity of neurons during behavioral tasks. The high\ndimensionality of their parameter and activity spaces, however, often make it\nchallenging to infer and interpret the fundamental features of their dynamics.\n  In this study, we employ recent nonlinear dynamical system techniques to\nuncover the core dynamics of several RNNs used in contemporary neuroscience.\nSpecifically, using a data-driven approach, we identify Spectral Submanifolds\n(SSMs), i.e., low-dimensional attracting invariant manifolds tangent to the\neigenspaces of fixed points. The internal dynamics of SSMs serve as nonlinear\nmodels that reduce the dimensionality of the full RNNs by orders of magnitude.\n  Through low-dimensional, SSM-reduced models, we give mathematically precise\ndefinitions of line and ring attractors, which are intuitive concepts commonly\nused to explain decision-making and working memory. The new level of\nunderstanding of RNNs obtained from SSM reduction enables the interpretation of\nmathematically well-defined and robust structures in neuronal dynamics, leading\nto novel predictions about the neural computations underlying behavior."
                },
                "authors": [
                    {
                        "name": "Alice Marraffa"
                    },
                    {
                        "name": "Renate Krause"
                    },
                    {
                        "name": "Valerio Mante"
                    },
                    {
                        "name": "George Haller"
                    }
                ],
                "author_detail": {
                    "name": "George Haller"
                },
                "author": "George Haller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13519v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13519v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13513v1",
                "updated": "2025-10-15T13:05:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    5,
                    3,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:05:03Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    5,
                    3,
                    2,
                    288,
                    0
                ],
                "title": "Magnetomechanical Coupling in Ferronematic Phases: Influence of\n  Spindle-Shaped Nanodopants on Liquid Crystalline Order",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Magnetomechanical Coupling in Ferronematic Phases: Influence of\n  Spindle-Shaped Nanodopants on Liquid Crystalline Order"
                },
                "summary": "Ferronematic phases, composed of liquid crystals doped with magnetic\nnanoparticles, exhibit unique magnetomechanical coupling effects that are of\ninterest for responsive materials. In this study, we investigate the influence\nof spindle-shaped {\\alpha}-Fe_2O_3 nanoparticles functionalized with a\nmesogen-decorated polymer brush on the phase behavior and field-induced\ntransitions of a nematic host (5CB). Differential scanning calorimetry (DSC),\nrefractometry, and dielectric spectroscopy reveal a non-monotonic dependence of\nthe nematic-isotropic transition temperature on particle concentration,\nindicating a competition between stabilizing and destabilizing effects. The\norder parameter increases with increasing nanoparticle content, in contrast to\nnon-magnetic reference systems, suggesting an alignment effect induced by the\nmagnetic rather than the geometric anisotropy axis of the dopants. Capacitance\nmeasurements of the Freedericksz transition show a pronounced shift in\nthreshold fields, with a critical concentration marking a transition to\nenhanced magnetic responsiveness. Additional information on nanospindle\ndiffusion correlated to the direction-dependent flow of the nematic host was\ninferred from comparison of Moessbauer spectroscopy and rheology data. Our\nfindings provide insights into the interplay of magnetic and geometric\nanisotropy in ferronematic systems and highlight their potential for\napplications in tunable soft matter devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ferronematic phases, composed of liquid crystals doped with magnetic\nnanoparticles, exhibit unique magnetomechanical coupling effects that are of\ninterest for responsive materials. In this study, we investigate the influence\nof spindle-shaped {\\alpha}-Fe_2O_3 nanoparticles functionalized with a\nmesogen-decorated polymer brush on the phase behavior and field-induced\ntransitions of a nematic host (5CB). Differential scanning calorimetry (DSC),\nrefractometry, and dielectric spectroscopy reveal a non-monotonic dependence of\nthe nematic-isotropic transition temperature on particle concentration,\nindicating a competition between stabilizing and destabilizing effects. The\norder parameter increases with increasing nanoparticle content, in contrast to\nnon-magnetic reference systems, suggesting an alignment effect induced by the\nmagnetic rather than the geometric anisotropy axis of the dopants. Capacitance\nmeasurements of the Freedericksz transition show a pronounced shift in\nthreshold fields, with a critical concentration marking a transition to\nenhanced magnetic responsiveness. Additional information on nanospindle\ndiffusion correlated to the direction-dependent flow of the nematic host was\ninferred from comparison of Moessbauer spectroscopy and rheology data. Our\nfindings provide insights into the interplay of magnetic and geometric\nanisotropy in ferronematic systems and highlight their potential for\napplications in tunable soft matter devices."
                },
                "authors": [
                    {
                        "name": "Karin Koch"
                    },
                    {
                        "name": "Joachim Landers"
                    },
                    {
                        "name": "Damian G√ºnzing"
                    },
                    {
                        "name": "Hajnalka N√°dasi"
                    },
                    {
                        "name": "Heiko Wende"
                    },
                    {
                        "name": "Alexey Eremin"
                    },
                    {
                        "name": "Annette M. Schmidt"
                    }
                ],
                "author_detail": {
                    "name": "Annette M. Schmidt"
                },
                "author": "Annette M. Schmidt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08891v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08891v4",
                "updated": "2025-10-15T13:04:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    4,
                    9,
                    2,
                    288,
                    0
                ],
                "published": "2024-10-28T06:41:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    41,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Reliable Decision Making via Calibration Oriented Retrieval Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Decision Making via Calibration Oriented Retrieval Augmented\n  Generation"
                },
                "summary": "Recently, Large Language Models (LLMs) have been increasingly used to support\nvarious decision-making tasks, assisting humans in making informed decisions.\nHowever, when LLMs confidently provide incorrect information, it can lead\nhumans to make suboptimal decisions. To prevent LLMs from generating incorrect\ninformation on topics they are unsure of and to improve the accuracy of\ngenerated content, prior works have proposed Retrieval Augmented Generation\n(RAG), where external documents are referenced to generate responses. However,\nprevious RAG methods focus only on retrieving documents most relevant to the\ninput query, without specifically aiming to ensure that the human user's\ndecisions are well-calibrated. To address this limitation, we propose a novel\nretrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG),\nwhich ensures that decisions informed by RAG are well-calibrated. Then we\nempirically validate that CalibRAG improves calibration performance as well as\naccuracy, compared to other baselines across various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have been increasingly used to support\nvarious decision-making tasks, assisting humans in making informed decisions.\nHowever, when LLMs confidently provide incorrect information, it can lead\nhumans to make suboptimal decisions. To prevent LLMs from generating incorrect\ninformation on topics they are unsure of and to improve the accuracy of\ngenerated content, prior works have proposed Retrieval Augmented Generation\n(RAG), where external documents are referenced to generate responses. However,\nprevious RAG methods focus only on retrieving documents most relevant to the\ninput query, without specifically aiming to ensure that the human user's\ndecisions are well-calibrated. To address this limitation, we propose a novel\nretrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG),\nwhich ensures that decisions informed by RAG are well-calibrated. Then we\nempirically validate that CalibRAG improves calibration performance as well as\naccuracy, compared to other baselines across various datasets."
                },
                "authors": [
                    {
                        "name": "Chaeyun Jang"
                    },
                    {
                        "name": "Deukhwan Cho"
                    },
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Hyungi Lee"
                    },
                    {
                        "name": "Juho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Juho Lee"
                },
                "author": "Juho Lee",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08891v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08891v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04364v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04364v4",
                "updated": "2025-10-15T13:02:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    2,
                    18,
                    2,
                    288,
                    0
                ],
                "published": "2025-05-07T12:32:01Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    32,
                    1,
                    2,
                    127,
                    0
                ],
                "title": "Benchmarking LLMs' Swarm intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs' Swarm intelligence"
                },
                "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict swarm-like constraints-limited local perception and\ncommunication-remains largely unexplored. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination when agents\noperate with incomplete spatio-temporal information. To bridge this gap, we\nintroduce SwarmBench, a novel benchmark designed to systematically evaluate the\nswarm intelligence capabilities of LLMs acting as decentralized agents.\nSwarmBench features five foundational MAS coordination tasks (Pursuit,\nSynchronization, Foraging, Flocking, Transport) within a configurable 2D grid\nenvironment, forcing agents to rely solely on local sensory input ($k\\times k$\nview) and local communication. We propose metrics for coordination\neffectiveness and analyze emergent group dynamics. Zero-shot evaluations of\nleading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent\nperformance variations. While some rudimentary coordination is observed, our\nresults indicate that current LLMs significantly struggle with robust\nlong-range planning and adaptive strategy formation under the uncertainty\ninherent in these decentralized scenarios. Assessing LLMs under such swarm-like\nconstraints is crucial for understanding their utility in future decentralized\nintelligent systems. We release SwarmBench as an open, extensible toolkit-built\non a customizable physical system-providing environments, prompts, evaluation\nscripts, and comprehensive datasets. This aims to foster reproducible research\ninto LLM-based MAS coordination and the theoretical underpinnings of emergent\ncollective behavior under severe informational decentralization. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict swarm-like constraints-limited local perception and\ncommunication-remains largely unexplored. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination when agents\noperate with incomplete spatio-temporal information. To bridge this gap, we\nintroduce SwarmBench, a novel benchmark designed to systematically evaluate the\nswarm intelligence capabilities of LLMs acting as decentralized agents.\nSwarmBench features five foundational MAS coordination tasks (Pursuit,\nSynchronization, Foraging, Flocking, Transport) within a configurable 2D grid\nenvironment, forcing agents to rely solely on local sensory input ($k\\times k$\nview) and local communication. We propose metrics for coordination\neffectiveness and analyze emergent group dynamics. Zero-shot evaluations of\nleading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent\nperformance variations. While some rudimentary coordination is observed, our\nresults indicate that current LLMs significantly struggle with robust\nlong-range planning and adaptive strategy formation under the uncertainty\ninherent in these decentralized scenarios. Assessing LLMs under such swarm-like\nconstraints is crucial for understanding their utility in future decentralized\nintelligent systems. We release SwarmBench as an open, extensible toolkit-built\non a customizable physical system-providing environments, prompts, evaluation\nscripts, and comprehensive datasets. This aims to foster reproducible research\ninto LLM-based MAS coordination and the theoretical underpinnings of emergent\ncollective behavior under severe informational decentralization. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Mowen Huang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04364v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04364v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14138v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14138v5",
                "updated": "2025-10-15T12:56:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    56,
                    42,
                    2,
                    288,
                    0
                ],
                "published": "2024-10-18T03:22:06Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    22,
                    6,
                    4,
                    292,
                    0
                ],
                "title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom"
                },
                "summary": "Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., limited multi-modal reasoning capacities, and insufficient and\nirrelevant visual descriptions). We then decompose visual reasoning process\ninto two stages: proactive visual perception (i.e., eyesight) and textual\nreasoning (i.e., wisdom), and introduce a novel visual reasoning framework\nnamed ProReason. This framework features decoupled vision-reasoning\ncapabilities and multi-run proactive perception. Briefly, given a multi-modal\nquestion, ProReason iterates proactive information collection and reasoning\nuntil the answer can be concluded with necessary and sufficient visual\ndescriptions. Notably, the disassociation of capabilities allows seamless\nintegration of existing large language models (LLMs) to compensate for the\nreasoning deficits of LVLMs. Our extensive experiments demonstrate that\nProReason outperforms existing multi-step reasoning frameworks on various\nbenchmarks for both open-source and closed-source models, with the average\nperformance gain reaching 13.2%. Besides, the integration of LLMs allows\nProReason to produce high-quality visual reasoning data, which empowers\nProReason-distilled models (i.e., ProReason-VL and ProReason-Q3) to achieve\nsuperior performance in downstream tasks. Our insights into existing solutions\nand the decoupled perspective for feasible integration of LLMs illuminate\nfuture research on visual reasoning techniques, especially LLM-assisted ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., limited multi-modal reasoning capacities, and insufficient and\nirrelevant visual descriptions). We then decompose visual reasoning process\ninto two stages: proactive visual perception (i.e., eyesight) and textual\nreasoning (i.e., wisdom), and introduce a novel visual reasoning framework\nnamed ProReason. This framework features decoupled vision-reasoning\ncapabilities and multi-run proactive perception. Briefly, given a multi-modal\nquestion, ProReason iterates proactive information collection and reasoning\nuntil the answer can be concluded with necessary and sufficient visual\ndescriptions. Notably, the disassociation of capabilities allows seamless\nintegration of existing large language models (LLMs) to compensate for the\nreasoning deficits of LVLMs. Our extensive experiments demonstrate that\nProReason outperforms existing multi-step reasoning frameworks on various\nbenchmarks for both open-source and closed-source models, with the average\nperformance gain reaching 13.2%. Besides, the integration of LLMs allows\nProReason to produce high-quality visual reasoning data, which empowers\nProReason-distilled models (i.e., ProReason-VL and ProReason-Q3) to achieve\nsuperior performance in downstream tasks. Our insights into existing solutions\nand the decoupled perspective for feasible integration of LLMs illuminate\nfuture research on visual reasoning techniques, especially LLM-assisted ones."
                },
                "authors": [
                    {
                        "name": "Jingqi Zhou"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Jingwei Dong"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14138v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14138v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14483v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14483v2",
                "updated": "2025-10-15T12:55:48Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    55,
                    48,
                    2,
                    288,
                    0
                ],
                "published": "2024-10-18T14:06:49Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    6,
                    49,
                    4,
                    292,
                    0
                ],
                "title": "Interventional Processes for Causal Uncertainty Quantification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interventional Processes for Causal Uncertainty Quantification"
                },
                "summary": "Reliable uncertainty quantification for causal effects is crucial in various\napplications, but remains difficult in nonparametric models, particularly for\ncontinuous treatments. We introduce IMPspec, a Gaussian process (GP) framework\nfor modeling uncertainty over interventional causal functions under continuous\ntreatments, which can be represented using reproducing Kernel Hilbert Spaces\n(RKHSs). By using principled function class expansions and a spectral\nrepresentation of RKHS features, IMPspec yields tractable training and\ninference, a spectral algorithm to calibrate posterior credible intervals, and\navoids the underfitting and variance collapse pathologies of earlier GP-on-RKHS\nmethods. Across synthetic benchmarks and an application in healthcare, IMPspec\ndelivers state-of-the-art performance in causal uncertainty quantification and\ndownstream causal Bayesian optimization tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable uncertainty quantification for causal effects is crucial in various\napplications, but remains difficult in nonparametric models, particularly for\ncontinuous treatments. We introduce IMPspec, a Gaussian process (GP) framework\nfor modeling uncertainty over interventional causal functions under continuous\ntreatments, which can be represented using reproducing Kernel Hilbert Spaces\n(RKHSs). By using principled function class expansions and a spectral\nrepresentation of RKHS features, IMPspec yields tractable training and\ninference, a spectral algorithm to calibrate posterior credible intervals, and\navoids the underfitting and variance collapse pathologies of earlier GP-on-RKHS\nmethods. Across synthetic benchmarks and an application in healthcare, IMPspec\ndelivers state-of-the-art performance in causal uncertainty quantification and\ndownstream causal Bayesian optimization tasks."
                },
                "authors": [
                    {
                        "name": "Hugh Dance"
                    },
                    {
                        "name": "Peter Orbanz"
                    },
                    {
                        "name": "Arthur Gretton"
                    }
                ],
                "author_detail": {
                    "name": "Arthur Gretton"
                },
                "author": "Arthur Gretton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14483v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14483v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13501v1",
                "updated": "2025-10-15T12:51:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    51,
                    47,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:51:47Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    51,
                    47,
                    2,
                    288,
                    0
                ],
                "title": "Confidence as a Reward: Transforming LLMs into Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence as a Reward: Transforming LLMs into Reward Models"
                },
                "summary": "Reward models can significantly enhance the reasoning capabilities of large\nlanguage models (LLMs), but they typically require extensive curated data and\ncostly training. To mitigate these challenges, training-free approaches such as\nLLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate\nresponses, achieving promising results. Recent works have also indicated that\nmodel confidence can serve effectively as a reward metric, distinguishing\nbetween chain-of-thought (CoT) and non-CoT paths. However, the concept of using\nconfidence as a reward has not been comprehensively studied. In this work, we\nsystematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful\ntraining-free method that utilizes token-level confidence in the model's final\nanswers as a proxy for reward, especially suitable for close-ended tasks.\nThrough extensive experiments on mathematical reasoning tasks, we demonstrate\nthat CRew outperforms existing training-free reward approaches on the MATH500\nand RewardMATH benchmarks, and even surpasses most trained reward models. We\nfurther identify a strong correlation between CRew scores and the actual\nreasoning performance of the model. Additionally, we find that CRew can\neffectively filter high-quality training data. Building upon these insights, we\npropose CRew-DPO, a training strategy that constructs preference data from\nconfidence scores combined with correctness signals. Finetuning with CRew-DPO\nfurther enhances the model's judging capabilities and consistently outperforms\nexisting self-training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models can significantly enhance the reasoning capabilities of large\nlanguage models (LLMs), but they typically require extensive curated data and\ncostly training. To mitigate these challenges, training-free approaches such as\nLLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate\nresponses, achieving promising results. Recent works have also indicated that\nmodel confidence can serve effectively as a reward metric, distinguishing\nbetween chain-of-thought (CoT) and non-CoT paths. However, the concept of using\nconfidence as a reward has not been comprehensively studied. In this work, we\nsystematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful\ntraining-free method that utilizes token-level confidence in the model's final\nanswers as a proxy for reward, especially suitable for close-ended tasks.\nThrough extensive experiments on mathematical reasoning tasks, we demonstrate\nthat CRew outperforms existing training-free reward approaches on the MATH500\nand RewardMATH benchmarks, and even surpasses most trained reward models. We\nfurther identify a strong correlation between CRew scores and the actual\nreasoning performance of the model. Additionally, we find that CRew can\neffectively filter high-quality training data. Building upon these insights, we\npropose CRew-DPO, a training strategy that constructs preference data from\nconfidence scores combined with correctness signals. Finetuning with CRew-DPO\nfurther enhances the model's judging capabilities and consistently outperforms\nexisting self-training methods."
                },
                "authors": [
                    {
                        "name": "He Du"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13500v1",
                "updated": "2025-10-15T12:50:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    50,
                    33,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:50:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    50,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts"
                },
                "summary": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, \\hk{an enhanced\nbenchmark with broader coverage of medical subjects, designed to evaluate both\nsingle and batch edits under strict locality constraints}. We then propose\nMedREK, a retrieval-based editing framework that integrates a shared query-key\nmodule for precise matching with an attention-based prompt encoder for\ninformative guidance. Experimental results on various medical benchmarks\ndemonstrate that our MedREK achieves superior performance across different core\nmetrics and provides the first validated solution for batch-editing in medical\nLLMs. Our code and dataset are available at\nhttps://github.com/mylittleriver/MedREK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, \\hk{an enhanced\nbenchmark with broader coverage of medical subjects, designed to evaluate both\nsingle and batch edits under strict locality constraints}. We then propose\nMedREK, a retrieval-based editing framework that integrates a shared query-key\nmodule for precise matching with an attention-based prompt encoder for\ninformative guidance. Experimental results on various medical benchmarks\ndemonstrate that our MedREK achieves superior performance across different core\nmetrics and provides the first validated solution for batch-editing in medical\nLLMs. Our code and dataset are available at\nhttps://github.com/mylittleriver/MedREK."
                },
                "authors": [
                    {
                        "name": "Shujun Xia"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Yinan Zhou"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Zhenan Sun"
                    },
                    {
                        "name": "Quanzheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Quanzheng Li"
                },
                "author": "Quanzheng Li",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13499v1",
                "updated": "2025-10-15T12:49:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    49,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:49:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    49,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent\n  Understanding"
                },
                "summary": "Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline."
                },
                "authors": [
                    {
                        "name": "Xiaozhe Li"
                    },
                    {
                        "name": "TianYi Lyu"
                    },
                    {
                        "name": "Siyi Yang"
                    },
                    {
                        "name": "Yuxi Gong"
                    },
                    {
                        "name": "Yizhao Yang"
                    },
                    {
                        "name": "Jinxuan Huang"
                    },
                    {
                        "name": "Ligao Zhang"
                    },
                    {
                        "name": "Zhuoyi Huang"
                    },
                    {
                        "name": "Qingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qingwen Liu"
                },
                "author": "Qingwen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16843v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16843v5",
                "updated": "2025-10-15T12:44:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    44,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2024-03-25T15:04:11Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    15,
                    4,
                    11,
                    0,
                    85,
                    0
                ],
                "title": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games"
                },
                "summary": "Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases."
                },
                "authors": [
                    {
                        "name": "Chanwoo Park"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Asuman Ozdaglar"
                    },
                    {
                        "name": "Kaiqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqing Zhang"
                },
                "author": "Kaiqing Zhang",
                "arxiv_comment": "Camera ready version of ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16843v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16843v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13494v1",
                "updated": "2025-10-15T12:43:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    43,
                    59,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:43:59Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    43,
                    59,
                    2,
                    288,
                    0
                ],
                "title": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA"
                },
                "summary": "Question Answering (QA) on narrative text poses a unique challenge to current\nsystems, requiring a deep understanding of long, complex documents. However,\nthe reliability of NarrativeQA, the most widely used benchmark in this domain,\nis hindered by noisy documents and flawed QA pairs. In this work, we introduce\nLiteraryQA, a high-quality subset of NarrativeQA focused on literary works.\nUsing a human- and LLM-validated pipeline, we identify and correct low-quality\nQA samples while removing extraneous text from source documents. We then carry\nout a meta-evaluation of automatic metrics to clarify how systems should be\nevaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics\nhave a low system-level correlation to human judgment, while LLM-as-a-Judge\nevaluations, even with small open-weight models, can strongly agree with the\nranking identified by humans. Finally, we benchmark a set of long-context LLMs\non LiteraryQA. We release our code and data at\nhttps://github.com/SapienzaNLP/LiteraryQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) on narrative text poses a unique challenge to current\nsystems, requiring a deep understanding of long, complex documents. However,\nthe reliability of NarrativeQA, the most widely used benchmark in this domain,\nis hindered by noisy documents and flawed QA pairs. In this work, we introduce\nLiteraryQA, a high-quality subset of NarrativeQA focused on literary works.\nUsing a human- and LLM-validated pipeline, we identify and correct low-quality\nQA samples while removing extraneous text from source documents. We then carry\nout a meta-evaluation of automatic metrics to clarify how systems should be\nevaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics\nhave a low system-level correlation to human judgment, while LLM-as-a-Judge\nevaluations, even with small open-weight models, can strongly agree with the\nranking identified by humans. Finally, we benchmark a set of long-context LLMs\non LiteraryQA. We release our code and data at\nhttps://github.com/SapienzaNLP/LiteraryQA."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonomo"
                    },
                    {
                        "name": "Luca Gioffr√©"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference. 22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13481v1",
                "updated": "2025-10-15T12:27:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    27,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:27:34Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    27,
                    34,
                    2,
                    288,
                    0
                ],
                "title": "Tahakom LLM guidelines and receipts: from pre-training data to an Arabic\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tahakom LLM guidelines and receipts: from pre-training data to an Arabic\n  LLM"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the field of natural\nlanguage processing, enhancing capabilities in both language understanding and\ngeneration across diverse domains. However, developing LLMs for Arabic presents\nunique challenges. This paper explores these challenges by focusing on critical\naspects such as data curation, tokenizer design, and evaluation. We detail our\napproach to the collection and filtration of Arabic pre-training datasets,\nassess the impact of various tokenizer designs on model performance, and\nexamine the limitations of existing Arabic evaluation frameworks, for which we\npropose a systematic corrective methodology. To promote transparency and\nfacilitate collaborative development, we share our data and methodologies,\ncontributing to the advancement of language modeling, particularly for the\nArabic language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the field of natural\nlanguage processing, enhancing capabilities in both language understanding and\ngeneration across diverse domains. However, developing LLMs for Arabic presents\nunique challenges. This paper explores these challenges by focusing on critical\naspects such as data curation, tokenizer design, and evaluation. We detail our\napproach to the collection and filtration of Arabic pre-training datasets,\nassess the impact of various tokenizer designs on model performance, and\nexamine the limitations of existing Arabic evaluation frameworks, for which we\npropose a systematic corrective methodology. To promote transparency and\nfacilitate collaborative development, we share our data and methodologies,\ncontributing to the advancement of language modeling, particularly for the\nArabic language."
                },
                "authors": [
                    {
                        "name": "Areej AlOtaibi"
                    },
                    {
                        "name": "Lina Alyahya"
                    },
                    {
                        "name": "Raghad Alshabanah"
                    },
                    {
                        "name": "Shahad Alfawzan"
                    },
                    {
                        "name": "Shuruq Alarefei"
                    },
                    {
                        "name": "Reem Alsabti"
                    },
                    {
                        "name": "Nouf Alsubaie"
                    },
                    {
                        "name": "Abdulaziz Alhuzaymi"
                    },
                    {
                        "name": "Lujain Alkhelb"
                    },
                    {
                        "name": "Majd Alsayari"
                    },
                    {
                        "name": "Waad Alahmed"
                    },
                    {
                        "name": "Omar Talabay"
                    },
                    {
                        "name": "Jalal Alowibdi"
                    },
                    {
                        "name": "Salem Alelyani"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13677v2",
                "updated": "2025-10-15T12:26:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    26,
                    54,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-16T16:31:45Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    16,
                    31,
                    45,
                    0,
                    167,
                    0
                ],
                "title": "Bayesian Quantification of Observability and Equation of State of Twin\n  Stars",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian Quantification of Observability and Equation of State of Twin\n  Stars"
                },
                "summary": "The possibility of discovering twin stars, two neutron stars (NSs) with the\nsame mass but different radii, is usually studied in forward modelings by using\na restricted number of NS matter equation of state (EOS) encapsulating a\nfirst-order phase transition from hadronic to quark matter (QM). Informing our\nlikelihood function with the NS radius data from GW170817 and using a\nmeta-model with 9-parameters capable of mimicking most NS EOSs available in the\nliterature, we conduct a Bayesian quantification of the observability and\nunderlying EOSs of twin stars. Of the accepted EOSs, between 12-18\\% yield twin\nstars, depending on the restrictions we place on the second branch. The\npossibility of twin stars remains robust even under recent observational\nconstraints. We show that many of these twin star scenarios are observable with\ncurrently available levels of accuracy in measuring NS radii. We also present\nthe marginalized posterior probability density functions (PDFs) of every EOS\nparameter for each of four mass-radius correlation topologies. We find that the\ninferred EOS depends sensitively on not only whether twin stars are present,\nbut also the category of twin stars, indicating that the observation of twin\nstars would provide a strong constraint on the underlying EOS. In particular,\nfor two coexisting hybrid stars having QM cores at different densities, the PDF\nfor QM speed of sound squared $c_{\\rm qm}^2$ has two peaks, one below and\nanother above the conformal limit $c_{\\rm qm}^2=1/3$ predicted by perturbative\nQCD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The possibility of discovering twin stars, two neutron stars (NSs) with the\nsame mass but different radii, is usually studied in forward modelings by using\na restricted number of NS matter equation of state (EOS) encapsulating a\nfirst-order phase transition from hadronic to quark matter (QM). Informing our\nlikelihood function with the NS radius data from GW170817 and using a\nmeta-model with 9-parameters capable of mimicking most NS EOSs available in the\nliterature, we conduct a Bayesian quantification of the observability and\nunderlying EOSs of twin stars. Of the accepted EOSs, between 12-18\\% yield twin\nstars, depending on the restrictions we place on the second branch. The\npossibility of twin stars remains robust even under recent observational\nconstraints. We show that many of these twin star scenarios are observable with\ncurrently available levels of accuracy in measuring NS radii. We also present\nthe marginalized posterior probability density functions (PDFs) of every EOS\nparameter for each of four mass-radius correlation topologies. We find that the\ninferred EOS depends sensitively on not only whether twin stars are present,\nbut also the category of twin stars, indicating that the observation of twin\nstars would provide a strong constraint on the underlying EOS. In particular,\nfor two coexisting hybrid stars having QM cores at different densities, the PDF\nfor QM speed of sound squared $c_{\\rm qm}^2$ has two peaks, one below and\nanother above the conformal limit $c_{\\rm qm}^2=1/3$ predicted by perturbative\nQCD."
                },
                "authors": [
                    {
                        "name": "Xavier Grundler"
                    },
                    {
                        "name": "Bao-An Li"
                    }
                ],
                "author_detail": {
                    "name": "Bao-An Li"
                },
                "author": "Bao-An Li",
                "arxiv_comment": "21 pages with 10 figures. Phys. Rev. D in press",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nucl-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "85xx",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13467v1",
                "updated": "2025-10-15T12:13:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "NetMCP: Network-Aware Model Context Protocol Platform for LLM Capability\n  Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetMCP: Network-Aware Model Context Protocol Platform for LLM Capability\n  Extension"
                },
                "summary": "Large Language Models (LLMs) remain static in functionality after training,\nand extending their capabilities requires integration with external data,\ncomputation, and services. The Model Context Protocol (MCP) has emerged as a\nstandard interface for such extensions, but current implementations rely solely\non semantic matching between users' requests and server function descriptions,\nwhich makes current deployments and simulation testbeds fragile under latency\nfluctuations or server failures. We address this gap by enhancing MCP tool\nrouting algorithms with real-time awareness of network and server status. To\nprovide a controlled test environment for development and evaluation, we\nconstruct a heterogeneous experimental platform, namely Network-aware MCP\n(NetMCP), which offers five representative network states and build a benchmark\nfor latency sequence generation and MCP server datasets. On top of NetMCP\nplatform, we analyze latency sequences and propose a Semantic-Oriented and\nNetwork-Aware Routing (SONAR) algorithm, which jointly optimizes semantic\nsimilarity and network Quality of Service (QoS) metrics for adaptive tool\nrouting. Results show that SONAR consistently improves task success rate and\nreduces completion time and failure number compared with semantic-only,\nLLM-based baselines, demonstrating the value of network-aware design for\nproduction-scale LLM systems. The code for NetMCP is available at\nhttps://github.com/NICE-HKU/NetMCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain static in functionality after training,\nand extending their capabilities requires integration with external data,\ncomputation, and services. The Model Context Protocol (MCP) has emerged as a\nstandard interface for such extensions, but current implementations rely solely\non semantic matching between users' requests and server function descriptions,\nwhich makes current deployments and simulation testbeds fragile under latency\nfluctuations or server failures. We address this gap by enhancing MCP tool\nrouting algorithms with real-time awareness of network and server status. To\nprovide a controlled test environment for development and evaluation, we\nconstruct a heterogeneous experimental platform, namely Network-aware MCP\n(NetMCP), which offers five representative network states and build a benchmark\nfor latency sequence generation and MCP server datasets. On top of NetMCP\nplatform, we analyze latency sequences and propose a Semantic-Oriented and\nNetwork-Aware Routing (SONAR) algorithm, which jointly optimizes semantic\nsimilarity and network Quality of Service (QoS) metrics for adaptive tool\nrouting. Results show that SONAR consistently improves task success rate and\nreduces completion time and failure number compared with semantic-only,\nLLM-based baselines, demonstrating the value of network-aware design for\nproduction-scale LLM systems. The code for NetMCP is available at\nhttps://github.com/NICE-HKU/NetMCP."
                },
                "authors": [
                    {
                        "name": "Enhan Li"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13462v1",
                "updated": "2025-10-15T12:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    11,
                    2,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    11,
                    2,
                    2,
                    288,
                    0
                ],
                "title": "Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored\n  Mixture-of-Experts Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored\n  Mixture-of-Experts Transformers"
                },
                "summary": "Large language models (LLMs) with Mixture-of-Experts (MoE) architectures\nachieve impressive performance and efficiency by dynamically routing inputs to\nspecialized subnetworks, known as experts. However, this sparse routing\nmechanism inherently exhibits task preferences due to expert specialization,\nintroducing a new and underexplored vulnerability to backdoor attacks. In this\nwork, we investigate the feasibility and effectiveness of injecting backdoors\ninto MoE-based LLMs by exploiting their inherent expert routing preferences. We\nthus propose BadSwitch, a novel backdoor framework that integrates task-coupled\ndynamic trigger optimization with a sensitivity-guided Top-S expert tracing\nmechanism. Our approach jointly optimizes trigger embeddings during pretraining\nwhile identifying S most sensitive experts, subsequently constraining the Top-K\ngating mechanism to these targeted experts. Unlike traditional backdoor attacks\nthat rely on superficial data poisoning or model editing, BadSwitch primarily\nembeds malicious triggers into expert routing paths with strong task affinity,\nenabling precise and stealthy model manipulation. Through comprehensive\nevaluations across three prominent MoE architectures (Switch Transformer,\nQwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack\npre-trained models with up to 100% success rate (ASR) while maintaining the\nhighest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch\nexhibits strong resilience against both text-level and model-level defense\nmechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our\nanalysis of expert activation patterns reveals fundamental insights into MoE\nvulnerabilities. We anticipate this work will expose security risks in MoE\nsystems and contribute to advancing AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with Mixture-of-Experts (MoE) architectures\nachieve impressive performance and efficiency by dynamically routing inputs to\nspecialized subnetworks, known as experts. However, this sparse routing\nmechanism inherently exhibits task preferences due to expert specialization,\nintroducing a new and underexplored vulnerability to backdoor attacks. In this\nwork, we investigate the feasibility and effectiveness of injecting backdoors\ninto MoE-based LLMs by exploiting their inherent expert routing preferences. We\nthus propose BadSwitch, a novel backdoor framework that integrates task-coupled\ndynamic trigger optimization with a sensitivity-guided Top-S expert tracing\nmechanism. Our approach jointly optimizes trigger embeddings during pretraining\nwhile identifying S most sensitive experts, subsequently constraining the Top-K\ngating mechanism to these targeted experts. Unlike traditional backdoor attacks\nthat rely on superficial data poisoning or model editing, BadSwitch primarily\nembeds malicious triggers into expert routing paths with strong task affinity,\nenabling precise and stealthy model manipulation. Through comprehensive\nevaluations across three prominent MoE architectures (Switch Transformer,\nQwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack\npre-trained models with up to 100% success rate (ASR) while maintaining the\nhighest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch\nexhibits strong resilience against both text-level and model-level defense\nmechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our\nanalysis of expert activation patterns reveals fundamental insights into MoE\nvulnerabilities. We anticipate this work will expose security risks in MoE\nsystems and contribute to advancing AI safety."
                },
                "authors": [
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Bingshan Liu"
                    },
                    {
                        "name": "Haoyu Gao"
                    },
                    {
                        "name": "Zhendong Zhao"
                    },
                    {
                        "name": "Yilong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yilong Chen"
                },
                "author": "Yilong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.13331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.13331v2",
                "updated": "2025-10-15T12:04:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    4,
                    23,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-16T10:21:54Z",
                "published_parsed": [
                    2025,
                    6,
                    16,
                    10,
                    21,
                    54,
                    0,
                    167,
                    0
                ],
                "title": "Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like\n  Specialization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Cognitive Reasoners: Modular Reasoning with Brain-Like\n  Specialization"
                },
                "summary": "Human cognitive behavior arises from the interaction of specialized brain\nnetworks dedicated to distinct functions, such as language, logic, and social\nreasoning. Inspired by this organization, we propose Mixture of Cognitive\nReasoners (MiCRo): a modular, transformer-based architecture post-trained with\na curriculum that induces functional specialization across experts. Concretely,\nwe partition the layers of a pretrained language model into four expert modules\naligned with well-studied cognitive networks in the human brain. MiCRo offers\nthree key advantages over standard language models. (1) The specialized experts\nare interpretable and causally meaningful -- ablating a module causes\nsubstantial drops on benchmarks requiring its specialized domain. (2) MiCRo's\nbehavior can be dynamically steered at inference time by routing tokens to\nparticular experts (e.g., favoring social over logical reasoning), enabling\nfine-grained control over outputs. (3) MiCRo outperforms or matches comparable\nbaselines on both machine-learning reasoning benchmarks (e.g., GSM8K, BBH) and\nalignment to human behavior (CogBench), while maintaining interpretability.\nTaken together, cognitively grounded functional specialization yields models\nthat are both more human-like and more human-interpretable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Human cognitive behavior arises from the interaction of specialized brain\nnetworks dedicated to distinct functions, such as language, logic, and social\nreasoning. Inspired by this organization, we propose Mixture of Cognitive\nReasoners (MiCRo): a modular, transformer-based architecture post-trained with\na curriculum that induces functional specialization across experts. Concretely,\nwe partition the layers of a pretrained language model into four expert modules\naligned with well-studied cognitive networks in the human brain. MiCRo offers\nthree key advantages over standard language models. (1) The specialized experts\nare interpretable and causally meaningful -- ablating a module causes\nsubstantial drops on benchmarks requiring its specialized domain. (2) MiCRo's\nbehavior can be dynamically steered at inference time by routing tokens to\nparticular experts (e.g., favoring social over logical reasoning), enabling\nfine-grained control over outputs. (3) MiCRo outperforms or matches comparable\nbaselines on both machine-learning reasoning benchmarks (e.g., GSM8K, BBH) and\nalignment to human behavior (CogBench), while maintaining interpretability.\nTaken together, cognitively grounded functional specialization yields models\nthat are both more human-like and more human-interpretable."
                },
                "authors": [
                    {
                        "name": "Badr AlKhamissi"
                    },
                    {
                        "name": "C. Nicol√≤ De Sabbata"
                    },
                    {
                        "name": "Greta Tuckute"
                    },
                    {
                        "name": "Zeming Chen"
                    },
                    {
                        "name": "Martin Schrimpf"
                    },
                    {
                        "name": "Antoine Bosselut"
                    }
                ],
                "author_detail": {
                    "name": "Antoine Bosselut"
                },
                "author": "Antoine Bosselut",
                "arxiv_comment": "Preprint. Project Page at https://cognitive-reasoners.epfl.ch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.13331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.13331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13451v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13451v1",
                "updated": "2025-10-15T11:50:40Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    50,
                    40,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:50:40Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    50,
                    40,
                    2,
                    288,
                    0
                ],
                "title": "Toward Efficient Inference Attacks: Shadow Model Sharing via\n  Mixture-of-Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward Efficient Inference Attacks: Shadow Model Sharing via\n  Mixture-of-Experts"
                },
                "summary": "Machine learning models are often vulnerable to inference attacks that expose\nsensitive information from their training data. Shadow model technique is\ncommonly employed in such attacks, such as membership inference. However, the\nneed for a large number of shadow models leads to high computational costs,\nlimiting their practical applicability. Such inefficiency mainly stems from the\nindependent training and use of these shadow models. To address this issue, we\npresent a novel shadow pool training framework SHAPOOL, which constructs\nmultiple shared models and trains them jointly within a single process. In\nparticular, we leverage the Mixture-of-Experts mechanism as the shadow pool to\ninterconnect individual models, enabling them to share some sub-networks and\nthereby improving efficiency. To ensure the shared models closely resemble\nindependent models and serve as effective substitutes, we introduce three novel\nmodules: path-choice routing, pathway regularization, and pathway alignment.\nThese modules guarantee random data allocation for pathway learning, promote\ndiversity among shared models, and maintain consistency with target models. We\nevaluate SHAPOOL in the context of various membership inference attacks and\nshow that it significantly reduces the computational cost of shadow model\nconstruction while maintaining comparable attack performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning models are often vulnerable to inference attacks that expose\nsensitive information from their training data. Shadow model technique is\ncommonly employed in such attacks, such as membership inference. However, the\nneed for a large number of shadow models leads to high computational costs,\nlimiting their practical applicability. Such inefficiency mainly stems from the\nindependent training and use of these shadow models. To address this issue, we\npresent a novel shadow pool training framework SHAPOOL, which constructs\nmultiple shared models and trains them jointly within a single process. In\nparticular, we leverage the Mixture-of-Experts mechanism as the shadow pool to\ninterconnect individual models, enabling them to share some sub-networks and\nthereby improving efficiency. To ensure the shared models closely resemble\nindependent models and serve as effective substitutes, we introduce three novel\nmodules: path-choice routing, pathway regularization, and pathway alignment.\nThese modules guarantee random data allocation for pathway learning, promote\ndiversity among shared models, and maintain consistency with target models. We\nevaluate SHAPOOL in the context of various membership inference attacks and\nshow that it significantly reduces the computational cost of shadow model\nconstruction while maintaining comparable attack performance."
                },
                "authors": [
                    {
                        "name": "Li Bai"
                    },
                    {
                        "name": "Qingqing Ye"
                    },
                    {
                        "name": "Xinwei Zhang"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Zi Liang"
                    },
                    {
                        "name": "Jianliang Xu"
                    },
                    {
                        "name": "Haibo Hu"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Hu"
                },
                "author": "Haibo Hu",
                "arxiv_comment": "To appear in NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13451v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13451v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00597v3",
                "updated": "2025-10-15T11:50:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    50,
                    5,
                    2,
                    288,
                    0
                ],
                "published": "2025-04-01T09:55:23Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    55,
                    23,
                    1,
                    91,
                    0
                ],
                "title": "On the Consistency of Multilingual Context Utilization in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Consistency of Multilingual Context Utilization in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from passages in a different language than the query, but a much\nweaker ability to formulate a full answer in the correct language. Our\nanalysis, based on both accuracy and feature attribution techniques, further\nshows that distracting passages negatively impact answer quality regardless of\ntheir language. However, distractors in the query language exert a slightly\nstronger influence. Taken together, our findings deepen the understanding of\nhow LLMs utilize context in mRAG systems, providing directions for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from passages in a different language than the query, but a much\nweaker ability to formulate a full answer in the correct language. Our\nanalysis, based on both accuracy and feature attribution techniques, further\nshows that distracting passages negatively impact answer quality regardless of\ntheir language. However, distractors in the query language exert a slightly\nstronger influence. Taken together, our findings deepen the understanding of\nhow LLMs utilize context in mRAG systems, providing directions for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Raquel Fern√°ndez"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "arxiv_comment": "MRL Workshop 2025, co-located with EMNLP 2025. All codes and data are\n  released at https://github.com/Betswish/mRAG-Context-Consistency",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.12495v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.12495v2",
                "updated": "2025-10-15T11:49:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    49,
                    1,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-14T13:28:57Z",
                "published_parsed": [
                    2025,
                    10,
                    14,
                    13,
                    28,
                    57,
                    1,
                    287,
                    0
                ],
                "title": "Limits of Standard Tidal Models at Quaoar: Matching Weywot's Orbit,\n  Missing the Spin",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of Standard Tidal Models at Quaoar: Matching Weywot's Orbit,\n  Missing the Spin"
                },
                "summary": "Weywot, Quaoar's small satellite, follows a nearly circular orbit at a\ndistance of 12.9 times Quaoar's diameter and coexists with a compact ring\nsystem. Nevertheless, Quaoar's flattening of 0.16, slow 17.7hr rotation and\nWeywot's low mass are difficult to reconcile with conventional tidal-evolution\ntheory. We assess whether standard tides can reproduce the present-day\narchitecture of the Quaoar-Weywot system and identify the initial conditions\nrequired. Orbit-averaged integrations spanning 4.5Gyr were carried out with two\nformalisms: (i) a constant phase-lag (CPL) and (ii) an Andrade creep-tide (ACT)\nframework. With the nominal Weywot mass, both tidal prescriptions converge on\nWeywot's observed orbital distance for a wide range of initial orbital\ndistances and eccentricities; eccentricity is damped and present-day tidal\ntorques are negligible, rendering the orbit quasi-stationary. Quaoar's spin,\nhowever, remains essentially unchanged from its inferred primordial period\nbased on its present-day flattening, and does not reproduce the observed value.\nA match is possible only if Weywot is 5-10x more massive than current estimates\nand if its initial eccentricity is finely tuned; such scenarios are\ninconsistent with occultation-derived masses and imply an implausibly dense\nsatellite. Based on the best fitting viscoelastic parameters, the most\nplausible composition for Quaoar is found to be a partially differentiated\ndwarf planet containing roughly equal masses of silicate rock and H2O-dominated\nwarm (150-180K) ices. Standard tidal models reproduce Weywot's semimajor axis\nbut cannot account for Quaoar's slow 17.7hr rotation without invoking an\nunrealistically massive satellite or external torques, suggesting that\nnon-tidal processes - such as a largely primordial spin, early satellite loss,\nor a retrograde secondary giant impact - must have influenced Quaoar's\nrotational evolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Weywot, Quaoar's small satellite, follows a nearly circular orbit at a\ndistance of 12.9 times Quaoar's diameter and coexists with a compact ring\nsystem. Nevertheless, Quaoar's flattening of 0.16, slow 17.7hr rotation and\nWeywot's low mass are difficult to reconcile with conventional tidal-evolution\ntheory. We assess whether standard tides can reproduce the present-day\narchitecture of the Quaoar-Weywot system and identify the initial conditions\nrequired. Orbit-averaged integrations spanning 4.5Gyr were carried out with two\nformalisms: (i) a constant phase-lag (CPL) and (ii) an Andrade creep-tide (ACT)\nframework. With the nominal Weywot mass, both tidal prescriptions converge on\nWeywot's observed orbital distance for a wide range of initial orbital\ndistances and eccentricities; eccentricity is damped and present-day tidal\ntorques are negligible, rendering the orbit quasi-stationary. Quaoar's spin,\nhowever, remains essentially unchanged from its inferred primordial period\nbased on its present-day flattening, and does not reproduce the observed value.\nA match is possible only if Weywot is 5-10x more massive than current estimates\nand if its initial eccentricity is finely tuned; such scenarios are\ninconsistent with occultation-derived masses and imply an implausibly dense\nsatellite. Based on the best fitting viscoelastic parameters, the most\nplausible composition for Quaoar is found to be a partially differentiated\ndwarf planet containing roughly equal masses of silicate rock and H2O-dominated\nwarm (150-180K) ices. Standard tidal models reproduce Weywot's semimajor axis\nbut cannot account for Quaoar's slow 17.7hr rotation without invoking an\nunrealistically massive satellite or external torques, suggesting that\nnon-tidal processes - such as a largely primordial spin, early satellite loss,\nor a retrograde secondary giant impact - must have influenced Quaoar's\nrotational evolution."
                },
                "authors": [
                    {
                        "name": "Zsolt Regaly"
                    },
                    {
                        "name": "Viktoria Frohlich"
                    },
                    {
                        "name": "Csaba Kiss"
                    }
                ],
                "author_detail": {
                    "name": "Csaba Kiss"
                },
                "author": "Csaba Kiss",
                "arxiv_comment": "Accepted for publication in PASP",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.12495v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.12495v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13434v1",
                "updated": "2025-10-15T11:30:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    30,
                    49,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:30:49Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    30,
                    49,
                    2,
                    288,
                    0
                ],
                "title": "Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference\n  Optimization for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference\n  Optimization for Machine Translation"
                },
                "summary": "Direct Preference Optimization (DPO) is a powerful paradigm for aligning\nLarge Language Models (LLMs) to human preferences in Machine Translation (MT),\nbut current methods are hindered by two fundamental challenges: (1) flawed\nreward signals from Quality Estimation (QE) models that overlook critical\nerrors like translation hallucination, and (2) inefficient data utilization\nthat discards valuable learning signals by selecting only a single win-loss\npair. To address these limitations, we introduce M^2PO: Multi-Pair,\nMulti-Perspective Preference Optimization. Our framework integrates a\nmulti-perspective reward engine that creates a more robust signal by combining\ntwo key viewpoints: a new hallucination penalty for factuality, and an\ninnovative dynamic quality score that adaptively fuses external evaluations\nwith the model's own evolving judgment. This is synergistically paired with a\nmulti-pair construction strategy that systematically creates a comprehensive\nset of preference pairs from the entire pool of translation candidates. This\nsynergistic approach ensures the model learns from a richer spectrum of quality\ntrade-offs, leading to more robust and faithful translations. On challenging\nWMT21-22 benchmarks, M^2PO substantially outperforms existing preference\noptimization methods and demonstrates highly competitive performance against\nleading proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is a powerful paradigm for aligning\nLarge Language Models (LLMs) to human preferences in Machine Translation (MT),\nbut current methods are hindered by two fundamental challenges: (1) flawed\nreward signals from Quality Estimation (QE) models that overlook critical\nerrors like translation hallucination, and (2) inefficient data utilization\nthat discards valuable learning signals by selecting only a single win-loss\npair. To address these limitations, we introduce M^2PO: Multi-Pair,\nMulti-Perspective Preference Optimization. Our framework integrates a\nmulti-perspective reward engine that creates a more robust signal by combining\ntwo key viewpoints: a new hallucination penalty for factuality, and an\ninnovative dynamic quality score that adaptively fuses external evaluations\nwith the model's own evolving judgment. This is synergistically paired with a\nmulti-pair construction strategy that systematically creates a comprehensive\nset of preference pairs from the entire pool of translation candidates. This\nsynergistic approach ensures the model learns from a richer spectrum of quality\ntrade-offs, leading to more robust and faithful translations. On challenging\nWMT21-22 benchmarks, M^2PO substantially outperforms existing preference\noptimization methods and demonstrates highly competitive performance against\nleading proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Linlong Xu"
                    },
                    {
                        "name": "Heng Liu"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Xiaohu Zhao"
                    },
                    {
                        "name": "Bo Zeng"
                    },
                    {
                        "name": "Liangying Shao"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13433v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13433v1",
                "updated": "2025-10-15T11:29:21Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    29,
                    21,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:29:21Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    29,
                    21,
                    2,
                    288,
                    0
                ],
                "title": "Beyond Pixels: A Differentiable Pipeline for Probing Neuronal\n  Selectivity in 3D",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Pixels: A Differentiable Pipeline for Probing Neuronal\n  Selectivity in 3D"
                },
                "summary": "Visual perception relies on inference of 3D scene properties such as shape,\npose, and lighting. To understand how visual sensory neurons enable robust\nperception, it is crucial to characterize their selectivity to such physically\ninterpretable factors. However, current approaches mainly operate on 2D pixels,\nmaking it difficult to isolate selectivity for physical scene properties. To\naddress this limitation, we introduce a differentiable rendering pipeline that\noptimizes deformable meshes to obtain MEIs directly in 3D. The method\nparameterizes mesh deformations with radial basis functions and learns offsets\nand scales that maximize neuronal responses while enforcing geometric\nregularity. Applied to models of monkey area V4, our approach enables probing\nneuronal selectivity to interpretable 3D factors such as pose and lighting.\nThis approach bridges inverse graphics with systems neuroscience, offering a\nway to probe neural selectivity with physically grounded, 3D stimuli beyond\nconventional pixel-based methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual perception relies on inference of 3D scene properties such as shape,\npose, and lighting. To understand how visual sensory neurons enable robust\nperception, it is crucial to characterize their selectivity to such physically\ninterpretable factors. However, current approaches mainly operate on 2D pixels,\nmaking it difficult to isolate selectivity for physical scene properties. To\naddress this limitation, we introduce a differentiable rendering pipeline that\noptimizes deformable meshes to obtain MEIs directly in 3D. The method\nparameterizes mesh deformations with radial basis functions and learns offsets\nand scales that maximize neuronal responses while enforcing geometric\nregularity. Applied to models of monkey area V4, our approach enables probing\nneuronal selectivity to interpretable 3D factors such as pose and lighting.\nThis approach bridges inverse graphics with systems neuroscience, offering a\nway to probe neural selectivity with physically grounded, 3D stimuli beyond\nconventional pixel-based methods."
                },
                "authors": [
                    {
                        "name": "Pavithra Elumalai"
                    },
                    {
                        "name": "Mohammad Bashiri"
                    },
                    {
                        "name": "Goirik Chakrabarty"
                    },
                    {
                        "name": "Suhas Shrinivasan"
                    },
                    {
                        "name": "Fabian H. Sinz"
                    }
                ],
                "author_detail": {
                    "name": "Fabian H. Sinz"
                },
                "author": "Fabian H. Sinz",
                "arxiv_comment": "Accepted in Symmetry and Geometry in Neural Representations 2025\n  (Extended Abstract Track)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13433v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13433v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13432v1",
                "updated": "2025-10-15T11:29:14Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    29,
                    14,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:29:14Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    29,
                    14,
                    2,
                    288,
                    0
                ],
                "title": "CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via\n  Domain Separation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CoDS: Enhancing Collaborative Perception in Heterogeneous Scenarios via\n  Domain Separation"
                },
                "summary": "Collaborative perception has been proven to improve individual perception in\nautonomous driving through multi-agent interaction. Nevertheless, most methods\noften assume identical encoders for all agents, which does not hold true when\nthese models are deployed in real-world applications. To realize collaborative\nperception in actual heterogeneous scenarios, existing methods usually align\nneighbor features to those of the ego vehicle, which is vulnerable to noise\nfrom domain gaps and thus fails to address feature discrepancies effectively.\nMoreover, they adopt transformer-based modules for domain adaptation, which\ncauses the model inference inefficiency on mobile devices. To tackle these\nissues, we propose CoDS, a Collaborative perception method that leverages\nDomain Separation to address feature discrepancies in heterogeneous scenarios.\nThe CoDS employs two feature alignment modules, i.e., Lightweight\nSpatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation\n(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)\nloss to ensure effective feature alignment. Specifically, the LSCR aligns the\nneighbor feature across spatial and channel dimensions using a lightweight\nconvolutional layer. Subsequently, the DADS mitigates feature distribution\ndiscrepancy with encoder-specific and encoder-agnostic domain separation\nmodules. The former removes domain-dependent information and the latter\ncaptures task-related information. During training, the DAMI loss maximizes the\nmutual information between aligned heterogeneous features to enhance the domain\nseparation process. The CoDS employs a fully convolutional architecture, which\nensures high inference efficiency. Extensive experiments demonstrate that the\nCoDS effectively mitigates feature discrepancies in heterogeneous scenarios and\nachieves a trade-off between detection accuracy and inference efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative perception has been proven to improve individual perception in\nautonomous driving through multi-agent interaction. Nevertheless, most methods\noften assume identical encoders for all agents, which does not hold true when\nthese models are deployed in real-world applications. To realize collaborative\nperception in actual heterogeneous scenarios, existing methods usually align\nneighbor features to those of the ego vehicle, which is vulnerable to noise\nfrom domain gaps and thus fails to address feature discrepancies effectively.\nMoreover, they adopt transformer-based modules for domain adaptation, which\ncauses the model inference inefficiency on mobile devices. To tackle these\nissues, we propose CoDS, a Collaborative perception method that leverages\nDomain Separation to address feature discrepancies in heterogeneous scenarios.\nThe CoDS employs two feature alignment modules, i.e., Lightweight\nSpatial-Channel Resizer (LSCR) and Distribution Alignment via Domain Separation\n(DADS). Besides, it utilizes the Domain Alignment Mutual Information (DAMI)\nloss to ensure effective feature alignment. Specifically, the LSCR aligns the\nneighbor feature across spatial and channel dimensions using a lightweight\nconvolutional layer. Subsequently, the DADS mitigates feature distribution\ndiscrepancy with encoder-specific and encoder-agnostic domain separation\nmodules. The former removes domain-dependent information and the latter\ncaptures task-related information. During training, the DAMI loss maximizes the\nmutual information between aligned heterogeneous features to enhance the domain\nseparation process. The CoDS employs a fully convolutional architecture, which\nensures high inference efficiency. Extensive experiments demonstrate that the\nCoDS effectively mitigates feature discrepancies in heterogeneous scenarios and\nachieves a trade-off between detection accuracy and inference efficiency."
                },
                "authors": [
                    {
                        "name": "Yushan Han"
                    },
                    {
                        "name": "Hui Zhang"
                    },
                    {
                        "name": "Honglei Zhang"
                    },
                    {
                        "name": "Chuntao Ding"
                    },
                    {
                        "name": "Yuanzhouhan Cao"
                    },
                    {
                        "name": "Yidong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yidong Li"
                },
                "author": "Yidong Li",
                "arxiv_comment": "Accepted by IEEE Transactions on Mobile Computing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07994v2",
                "updated": "2025-10-15T11:26:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    26,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2024-01-15T22:36:31Z",
                "published_parsed": [
                    2024,
                    1,
                    15,
                    22,
                    36,
                    31,
                    0,
                    15,
                    0
                ],
                "title": "Assessing the Latent Automated Program Repair Capabilities of Large\n  Language Models using Round-Trip Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Latent Automated Program Repair Capabilities of Large\n  Language Models using Round-Trip Translation"
                },
                "summary": "Research shows that errors in natural language can be corrected by\ntranslating texts to another language and back using language models. We\nexplore to what extent this latent correction capability extends to Automated\nProgram Repair (APR) by investigating Round-Trip Translation (RTT): translating\ncode from one programming language into another programming or natural language\nand back, using Large Language Models (LLMs). We hypothesize that RTT restores\npatterns most commonly seen in the LLM's training corpora through regression\ntoward the mean, replacing infrequent bugs with more frequent, natural,\nbug-free code. To test this hypothesis, we employ nine LLMs and four common APR\nbenchmarks in Java, and perform a detailed quantitative and qualitative\nanalysis of RTT-generated patches. We find that RTT through English generates\nplausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java\nbenchmark, and 97 are found to be correct in our manual assessment. Moreover,\nRTT uniquely generates plausible patches for 46 bugs that were missed by LLMs\nspecifically fine-tuned for APR. While this demonstrates the viability of RTT\nfor APR, we also observe limitations, such as a lower overall bug fix rate than\nthe state-of-the-art and diluting the original coding style. We analyze the\nimpact of these limitations and discuss the potential of using RTT as a\ncomplementary component in APR frameworks. A replication package is available\nfor download from https://doi.org/10.5281/zenodo.10500593.\n  Keywords: automated program repair, large language model, machine translation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research shows that errors in natural language can be corrected by\ntranslating texts to another language and back using language models. We\nexplore to what extent this latent correction capability extends to Automated\nProgram Repair (APR) by investigating Round-Trip Translation (RTT): translating\ncode from one programming language into another programming or natural language\nand back, using Large Language Models (LLMs). We hypothesize that RTT restores\npatterns most commonly seen in the LLM's training corpora through regression\ntoward the mean, replacing infrequent bugs with more frequent, natural,\nbug-free code. To test this hypothesis, we employ nine LLMs and four common APR\nbenchmarks in Java, and perform a detailed quantitative and qualitative\nanalysis of RTT-generated patches. We find that RTT through English generates\nplausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java\nbenchmark, and 97 are found to be correct in our manual assessment. Moreover,\nRTT uniquely generates plausible patches for 46 bugs that were missed by LLMs\nspecifically fine-tuned for APR. While this demonstrates the viability of RTT\nfor APR, we also observe limitations, such as a lower overall bug fix rate than\nthe state-of-the-art and diluting the original coding style. We analyze the\nimpact of these limitations and discuss the potential of using RTT as a\ncomplementary component in APR frameworks. A replication package is available\nfor download from https://doi.org/10.5281/zenodo.10500593.\n  Keywords: automated program repair, large language model, machine translation"
                },
                "authors": [
                    {
                        "name": "Fernando Vallecillos Ruiz"
                    },
                    {
                        "name": "Anastasiia Grishina"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Leon Moonen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Moonen"
                },
                "author": "Leon Moonen",
                "arxiv_doi": "10.1145/3771922",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771922",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.07994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in ACM Transactions on Software Engineering\n  and Methodology (TOSEM)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13430v1",
                "updated": "2025-10-15T11:25:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    25,
                    33,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:25:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    25,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "Evaluating Arabic Large Language Models: A Survey of Benchmarks,\n  Methods, and Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Arabic Large Language Models: A Survey of Benchmarks,\n  Methods, and Gaps"
                },
                "summary": "This survey provides the first systematic review of Arabic LLM benchmarks,\nanalyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,\ncultural understanding, and specialized capabilities. We propose a taxonomy\norganizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and\nDialects, and Target-Specific evaluations. Our analysis reveals significant\nprogress in benchmark diversity while identifying critical gaps: limited\ntemporal evaluation, insufficient multi-turn dialogue assessment, and cultural\nmisalignment in translated datasets. We examine three primary approaches:\nnative collection, translation, and synthetic generation discussing their\ntrade-offs regarding authenticity, scale, and cost. This work serves as a\ncomprehensive reference for Arabic NLP researchers, providing insights into\nbenchmark methodologies, reproducibility standards, and evaluation metrics\nwhile offering recommendations for future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey provides the first systematic review of Arabic LLM benchmarks,\nanalyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,\ncultural understanding, and specialized capabilities. We propose a taxonomy\norganizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and\nDialects, and Target-Specific evaluations. Our analysis reveals significant\nprogress in benchmark diversity while identifying critical gaps: limited\ntemporal evaluation, insufficient multi-turn dialogue assessment, and cultural\nmisalignment in translated datasets. We examine three primary approaches:\nnative collection, translation, and synthetic generation discussing their\ntrade-offs regarding authenticity, scale, and cost. This work serves as a\ncomprehensive reference for Arabic NLP researchers, providing insights into\nbenchmark methodologies, reproducibility standards, and evaluation metrics\nwhile offering recommendations for future development."
                },
                "authors": [
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Shaikha Alsuwaidi"
                    },
                    {
                        "name": "Basma El Amel Boussaha"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Omar Alkaabi"
                    },
                    {
                        "name": "Mohammed Alyafeai"
                    },
                    {
                        "name": "Hamza Alobeidli"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13417v1",
                "updated": "2025-10-15T11:15:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    15,
                    0,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:15:00Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    15,
                    0,
                    2,
                    288,
                    0
                ],
                "title": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in\n  Climate Discourse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in\n  Climate Discourse"
                },
                "summary": "How does a cause lead to an effect, and which intermediate causal steps\nexplain their connection? This work scrutinizes the mechanistic causal\nreasoning capabilities of large language models (LLMs) to answer these\nquestions through the task of implicit causal chain discovery. In a diagnostic\nevaluation framework, we instruct nine LLMs to generate all possible\nintermediate causal steps linking given cause-effect pairs in causal chain\nstructures. These pairs are drawn from recent resources in argumentation\nstudies featuring polarized discussion on climate change. Our analysis reveals\nthat LLMs vary in the number and granularity of causal steps they produce.\nAlthough they are generally self-consistent and confident about the\nintermediate causal connections in the generated chains, their judgments are\nmainly driven by associative pattern matching rather than genuine causal\nreasoning. Nonetheless, human evaluations confirmed the logical coherence and\nintegrity of the generated chains. Our baseline causal chain discovery\napproach, insights from our diagnostic evaluation, and benchmark dataset with\ncausal chains lay a solid foundation for advancing future work in implicit,\nmechanistic causal reasoning in argumentation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does a cause lead to an effect, and which intermediate causal steps\nexplain their connection? This work scrutinizes the mechanistic causal\nreasoning capabilities of large language models (LLMs) to answer these\nquestions through the task of implicit causal chain discovery. In a diagnostic\nevaluation framework, we instruct nine LLMs to generate all possible\nintermediate causal steps linking given cause-effect pairs in causal chain\nstructures. These pairs are drawn from recent resources in argumentation\nstudies featuring polarized discussion on climate change. Our analysis reveals\nthat LLMs vary in the number and granularity of causal steps they produce.\nAlthough they are generally self-consistent and confident about the\nintermediate causal connections in the generated chains, their judgments are\nmainly driven by associative pattern matching rather than genuine causal\nreasoning. Nonetheless, human evaluations confirmed the logical coherence and\nintegrity of the generated chains. Our baseline causal chain discovery\napproach, insights from our diagnostic evaluation, and benchmark dataset with\ncausal chains lay a solid foundation for advancing future work in implicit,\nmechanistic causal reasoning in argumentation settings."
                },
                "authors": [
                    {
                        "name": "Liesbeth Allein"
                    },
                    {
                        "name": "Nataly Pineda-Casta√±eda"
                    },
                    {
                        "name": "Andrea Rocci"
                    },
                    {
                        "name": "Marie-Francine Moens"
                    }
                ],
                "author_detail": {
                    "name": "Marie-Francine Moens"
                },
                "author": "Marie-Francine Moens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13405v1",
                "updated": "2025-10-15T11:03:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    3,
                    17,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:03:17Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    3,
                    17,
                    2,
                    288,
                    0
                ],
                "title": "Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile\n  Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile\n  Apps"
                },
                "summary": "Machine learning (ML) models are increasingly integrated into modern mobile\napps to enable personalized and intelligent services. These models typically\nrely on rich input features derived from historical user behaviors to capture\nuser intents. However, as ML-driven services become more prevalent, recording\nnecessary user behavior data imposes substantial storage cost on mobile apps,\nleading to lower system responsiveness and more app uninstalls. To address this\nstorage bottleneck, we present AdaLog, a lightweight and adaptive system\ndesigned to improve the storage efficiency of user behavior log in ML-embedded\nmobile apps, without compromising model inference accuracy or latency. We\nidentify two key inefficiencies in current industrial practices of user\nbehavior log: (i) redundant logging of overlapping behavior data across\ndifferent features and models, and (ii) sparse storage caused by storing\nbehaviors with heterogeneous attribute descriptions in a single log file. To\nsolve these issues, AdaLog first formulates the elimination of feature-level\nredundant data as a maximum weighted matching problem in hypergraphs, and\nproposes a hierarchical algorithm for efficient on-device deployment. Then,\nAdaLog employs a virtually hashed attribute design to distribute heterogeneous\nbehaviors into a few log files with physically dense storage. Finally, to\nensure scalability to dynamic user behavior patterns, AdaLog designs an\nincremental update mechanism to minimize the I/O operations needed for adapting\noutdated behavior log. We implement a prototype of AdaLog and deploy it into\npopular mobile apps in collaboration with our industry partner. Evaluations on\nreal-world user data show that AdaLog reduces behavior log size by 19% to 44%\nwith minimal system overhead (only 2 seconds latency and 15 MB memory usage),\nproviding a more efficient data foundation for broader adoption of on-device\nML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models are increasingly integrated into modern mobile\napps to enable personalized and intelligent services. These models typically\nrely on rich input features derived from historical user behaviors to capture\nuser intents. However, as ML-driven services become more prevalent, recording\nnecessary user behavior data imposes substantial storage cost on mobile apps,\nleading to lower system responsiveness and more app uninstalls. To address this\nstorage bottleneck, we present AdaLog, a lightweight and adaptive system\ndesigned to improve the storage efficiency of user behavior log in ML-embedded\nmobile apps, without compromising model inference accuracy or latency. We\nidentify two key inefficiencies in current industrial practices of user\nbehavior log: (i) redundant logging of overlapping behavior data across\ndifferent features and models, and (ii) sparse storage caused by storing\nbehaviors with heterogeneous attribute descriptions in a single log file. To\nsolve these issues, AdaLog first formulates the elimination of feature-level\nredundant data as a maximum weighted matching problem in hypergraphs, and\nproposes a hierarchical algorithm for efficient on-device deployment. Then,\nAdaLog employs a virtually hashed attribute design to distribute heterogeneous\nbehaviors into a few log files with physically dense storage. Finally, to\nensure scalability to dynamic user behavior patterns, AdaLog designs an\nincremental update mechanism to minimize the I/O operations needed for adapting\noutdated behavior log. We implement a prototype of AdaLog and deploy it into\npopular mobile apps in collaboration with our industry partner. Evaluations on\nreal-world user data show that AdaLog reduces behavior log size by 19% to 44%\nwith minimal system overhead (only 2 seconds latency and 15 MB memory usage),\nproviding a more efficient data foundation for broader adoption of on-device\nML."
                },
                "authors": [
                    {
                        "name": "Chen Gong"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Zhenzhe Zheng"
                    },
                    {
                        "name": "Yiliu Chen"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_doi": "10.1145/3771575",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771575",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.13405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.14896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14896v2",
                "updated": "2025-10-15T17:59:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    59,
                    6,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-20T17:59:51Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    17,
                    59,
                    51,
                    2,
                    232,
                    0
                ],
                "title": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization Meets dLLMs: A Systematic Study of Post-training\n  Quantization for Diffusion LLMs"
                },
                "summary": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. Our code is\npublicly available at https://github.com/FelixMessi/QDLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion large language models (dLLMs) have introduced a\npromising alternative to autoregressive (AR) LLMs for natural language\ngeneration tasks, leveraging full attention and denoising-based decoding\nstrategies. However, the deployment of these models on edge devices remains\nchallenging due to their massive parameter scale and high resource demands.\nWhile post-training quantization (PTQ) has emerged as a widely adopted\ntechnique for compressing AR LLMs, its applicability to dLLMs remains largely\nunexplored. In this work, we present the first systematic study on quantizing\ndiffusion-based language models. We begin by identifying the presence of\nactivation outliers, characterized by abnormally large activation values that\ndominate the dynamic range. These outliers pose a key challenge to low-bit\nquantization, as they make it difficult to preserve precision for the majority\nof values. More importantly, we implement state-of-the-art PTQ methods and\nconduct a comprehensive evaluation across multiple task types and model\nvariants. Our analysis is structured along four key dimensions: bit-width,\nquantization method, task category, and model type. Through this\nmulti-perspective evaluation, we offer practical insights into the quantization\nbehavior of dLLMs under different configurations. We hope our findings provide\na foundation for future research in efficient dLLM deployment. Our code is\npublicly available at https://github.com/FelixMessi/QDLM."
                },
                "authors": [
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Haobo Xu"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Ziyu Guo"
                    },
                    {
                        "name": "Renrui Zhang"
                    },
                    {
                        "name": "Zhichao Lu"
                    },
                    {
                        "name": "Ying Wei"
                    },
                    {
                        "name": "Qingfu Zhang"
                    },
                    {
                        "name": "Zhenan Sun"
                    }
                ],
                "author_detail": {
                    "name": "Zhenan Sun"
                },
                "author": "Zhenan Sun",
                "arxiv_comment": "Technical Report, Work in Progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13800v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13800v1",
                "updated": "2025-10-15T17:58:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    58,
                    8,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:58:08Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    58,
                    8,
                    2,
                    288,
                    0
                ],
                "title": "Reasoning in Space via Grounding in the World",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning in Space via Grounding in the World"
                },
                "summary": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we claim that 3D visual grounding is the cornerstone of\nspatial reasoning and introduce the Grounded-Spatial Reasoner (GS-Reasoner) to\nexplore the effective spatial representations that bridge the gap between them.\nExisting 3D LLMs suffer from the absence of a unified 3D representation capable\nof jointly capturing semantic and geometric information. This deficiency is\nmanifested either in poor performance on grounding or in an excessive reliance\non external modules, ultimately hindering the seamless integration of grounding\nand spatial reasoning. To address this, we propose a simple yet effective\ndual-path pooling mechanism that tightly aligns geometric features with both\nsemantic and positional cues, constructing a unified image patch-based 3D\nrepresentation that encapsulates all essential information without increasing\nthe number of input tokens. Leveraging this holistic representation,\nGS-Reasoner is the first 3D LLM that achieves autoregressive grounding entirely\nwithout external modules while delivering performance comparable to\nstate-of-the-art models, establishing a unified and self-contained framework\nfor 3D spatial reasoning. To further bridge grounding and spatial reasoning, we\nintroduce the Grounded Chain-of-Thought (GCoT) dataset. This dataset is\nmeticulously curated to include both 3D bounding box annotations for objects\nreferenced in reasoning questions and step-by-step reasoning paths that\nintegrate grounding as a core component of the problem-solving process.\nExtensive experiments demonstrate that GS-Reasoner achieves impressive results\non 3D visual grounding, which in turn significantly enhances its spatial\nreasoning capabilities, leading to state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Yiming Chen"
                    },
                    {
                        "name": "Zekun Qi"
                    },
                    {
                        "name": "Wenyao Zhang"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Li Zhang"
                    },
                    {
                        "name": "Peidong Liu"
                    }
                ],
                "author_detail": {
                    "name": "Peidong Liu"
                },
                "author": "Peidong Liu",
                "arxiv_comment": "20 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13800v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13800v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.07312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.07312v2",
                "updated": "2025-10-15T17:57:26Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    57,
                    26,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-08T17:58:41Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    17,
                    58,
                    41,
                    2,
                    281,
                    0
                ],
                "title": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "h1: Bootstrapping LLMs to Reason over Longer Horizons via Reinforcement\n  Learning"
                },
                "summary": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x. It\nalso transfers significantly to diverse out-of-distribution ReasoningGym\ndomains and long-context benchmarks, indicating broader generalization.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models excel at short-horizon reasoning tasks, but performance\ndrops as reasoning horizon lengths increase. Existing approaches to combat this\nrely on inference-time scaffolding or costly step-level supervision, neither of\nwhich scales easily. In this work, we introduce a scalable method to bootstrap\nlong-horizon reasoning capabilities using only existing, abundant short-horizon\ndata. Our approach synthetically composes simple problems into complex,\nmulti-step dependency chains of arbitrary length. We train models on this data\nusing outcome-only rewards under a curriculum that automatically increases in\ncomplexity, allowing RL training to be scaled much further without saturating.\nEmpirically, our method generalizes remarkably well: curriculum training on\ncomposed 6th-grade level math problems (GSM8K) boosts accuracy on longer,\ncompetition-level benchmarks (GSM-Symbolic, MATH-500, AIME) by up to 2.06x. It\nalso transfers significantly to diverse out-of-distribution ReasoningGym\ndomains and long-context benchmarks, indicating broader generalization.\nImportantly, our long-horizon improvements are significantly higher than\nbaselines even at high pass@k, showing that models can learn new reasoning\npaths under RL. Theoretically, we show that curriculum RL with outcome rewards\nachieves an exponential improvement in sample complexity over full-horizon\ntraining, providing training signal comparable to dense supervision. h1\ntherefore introduces an efficient path towards scaling RL for long-horizon\nproblems using only existing data."
                },
                "authors": [
                    {
                        "name": "Sumeet Ramesh Motwani"
                    },
                    {
                        "name": "Alesia Ivanova"
                    },
                    {
                        "name": "Ziyang Cai"
                    },
                    {
                        "name": "Philip Torr"
                    },
                    {
                        "name": "Riashat Islam"
                    },
                    {
                        "name": "Shital Shah"
                    },
                    {
                        "name": "Christian Schroeder de Witt"
                    },
                    {
                        "name": "Charles London"
                    }
                ],
                "author_detail": {
                    "name": "Charles London"
                },
                "author": "Charles London",
                "arxiv_comment": "Preprint, 31 pages, 8 figures, long-horizon reasoning",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.07312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.07312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14304v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14304v2",
                "updated": "2025-10-15T17:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    43,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-18T18:21:52Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    18,
                    21,
                    52,
                    4,
                    199,
                    0
                ],
                "title": "Aligning Large Language Models to Low-Resource Languages through\n  LLM-Based Selective Translation: A Systematic Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Large Language Models to Low-Resource Languages through\n  LLM-Based Selective Translation: A Systematic Study"
                },
                "summary": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multilingual large language models (LLMs) often demonstrate a performance gap\nbetween English and non-English languages, particularly in low-resource\nsettings. Aligning these models to low-resource languages is essential yet\nchallenging due to limited high-quality data. While English alignment datasets\nare readily available, curating equivalent data in other languages is expensive\nand time-consuming. A common workaround is to translate existing English\nalignment data; however, standard translation techniques often fail to preserve\ncritical elements such as code, mathematical expressions, and structured\nformats like JSON. In this work, we investigate LLM-based selective\ntranslation, a technique that selectively translates only the translatable\nparts of a text while preserving non-translatable content and sentence\nstructure. We conduct a systematic study to explore key questions around this\napproach, including its effectiveness compared to vanilla translation, the\nimportance of filtering noisy outputs, and the benefits of mixing translated\nsamples with original English data during alignment. Our experiments focus on\nthe low-resource Indic language Hindi and compare translations generated by\nGoogle Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the\npromise of selective translation as a practical and effective method for\nimproving multilingual alignment in LLMs."
                },
                "authors": [
                    {
                        "name": "Rakesh Paul"
                    },
                    {
                        "name": "Anusha Kamath"
                    },
                    {
                        "name": "Kanishk Singla"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Utkarsh Vaidya"
                    },
                    {
                        "name": "Sanjay Singh Chauhan"
                    },
                    {
                        "name": "Niranjan Wartikar"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Wartikar"
                },
                "author": "Niranjan Wartikar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14304v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14304v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13786v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13786v1",
                "updated": "2025-10-15T17:43:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    43,
                    3,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:43:03Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    43,
                    3,
                    2,
                    288,
                    0
                ],
                "title": "The Art of Scaling Reinforcement Learning Compute for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Art of Scaling Reinforcement Learning Compute for LLMs"
                },
                "summary": "Reinforcement learning (RL) has become central to training large language\nmodels (LLMs), yet the field lacks predictive scaling methodologies comparable\nto those established for pre-training. Despite rapidly rising compute budgets,\nthere is no principled understanding of how to evaluate algorithmic\nimprovements for scaling RL compute. We present the first large-scale\nsystematic study, amounting to more than 400,000 GPU-hours, that defines a\nprincipled framework for analyzing and predicting RL scaling in LLMs. We fit\nsigmoidal compute-performance curves for RL training and ablate a wide range of\ncommon design choices to analyze their effects on asymptotic performance and\ncompute efficiency. We observe: (1) Not all recipes yield similar asymptotic\nperformance, (2) Details such as loss aggregation, normalization, curriculum,\nand off-policy algorithm primarily modulate compute efficiency without\nmaterially shifting the asymptote, and (3) Stable, scalable recipes follow\npredictable scaling trajectories, enabling extrapolation from smaller-scale\nruns. Combining these insights, we propose a best-practice recipe, ScaleRL, and\ndemonstrate its effectiveness by successfully scaling and predicting validation\nperformance on a single RL run scaled up to 100,000 GPU-hours. Our work\nprovides both a scientific framework for analyzing scaling in RL and a\npractical recipe that brings RL training closer to the predictability long\nachieved in pre-training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become central to training large language\nmodels (LLMs), yet the field lacks predictive scaling methodologies comparable\nto those established for pre-training. Despite rapidly rising compute budgets,\nthere is no principled understanding of how to evaluate algorithmic\nimprovements for scaling RL compute. We present the first large-scale\nsystematic study, amounting to more than 400,000 GPU-hours, that defines a\nprincipled framework for analyzing and predicting RL scaling in LLMs. We fit\nsigmoidal compute-performance curves for RL training and ablate a wide range of\ncommon design choices to analyze their effects on asymptotic performance and\ncompute efficiency. We observe: (1) Not all recipes yield similar asymptotic\nperformance, (2) Details such as loss aggregation, normalization, curriculum,\nand off-policy algorithm primarily modulate compute efficiency without\nmaterially shifting the asymptote, and (3) Stable, scalable recipes follow\npredictable scaling trajectories, enabling extrapolation from smaller-scale\nruns. Combining these insights, we propose a best-practice recipe, ScaleRL, and\ndemonstrate its effectiveness by successfully scaling and predicting validation\nperformance on a single RL run scaled up to 100,000 GPU-hours. Our work\nprovides both a scientific framework for analyzing scaling in RL and a\npractical recipe that brings RL training closer to the predictability long\nachieved in pre-training."
                },
                "authors": [
                    {
                        "name": "Devvrit Khatri"
                    },
                    {
                        "name": "Lovish Madaan"
                    },
                    {
                        "name": "Rishabh Tiwari"
                    },
                    {
                        "name": "Rachit Bansal"
                    },
                    {
                        "name": "Sai Surya Duvvuri"
                    },
                    {
                        "name": "Manzil Zaheer"
                    },
                    {
                        "name": "Inderjit S. Dhillon"
                    },
                    {
                        "name": "David Brandfonbrener"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    }
                ],
                "author_detail": {
                    "name": "Rishabh Agarwal"
                },
                "author": "Rishabh Agarwal",
                "arxiv_comment": "28 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13786v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13786v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12470v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12470v3",
                "updated": "2025-10-15T17:36:19Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    36,
                    19,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-18T02:58:37Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    2,
                    58,
                    37,
                    1,
                    49,
                    0
                ],
                "title": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning on a Spectrum: Aligning LLMs to System 1 and System 2 Thinking"
                },
                "summary": "Large Language Models (LLMs) exhibit impressive reasoning abilities, yet\ntheir reliance on structured step-by-step processing reveals a critical\nlimitation. In contrast, human cognition fluidly adapts between intuitive,\nheuristic (System 1) and analytical, deliberative (System 2) reasoning\ndepending on the context. This difference between human cognitive flexibility\nand LLMs' reliance on a single reasoning style raises a critical question:\nwhile human fast heuristic reasoning evolved for its efficiency and\nadaptability, is a uniform reasoning approach truly optimal for LLMs, or does\nits inflexibility make them brittle and unreliable when faced with tasks\ndemanding more agile, intuitive responses? To answer these questions, we\nexplicitly align LLMs to these reasoning styles by curating a dataset with\nvalid System 1 and System 2 answers, and evaluate their performance across\nreasoning benchmarks. Our results reveal an accuracy-efficiency trade-off:\nSystem 2-aligned models excel in arithmetic and symbolic reasoning, while\nSystem 1-aligned models perform better in commonsense reasoning tasks. To\nanalyze the reasoning spectrum, we interpolated between the two extremes by\nvarying the proportion of alignment data, which resulted in a monotonic change\nin accuracy. A mechanistic analysis of model responses shows that System 1\nmodels employ more definitive outputs, whereas System 2 models demonstrate\ngreater uncertainty. Building on these findings, we further combine System 1-\nand System 2-aligned models based on the entropy of their generations, without\nadditional training, and obtain a dynamic model that outperforms across nearly\nall benchmarks. This work challenges the assumption that step-by-step reasoning\nis always optimal and highlights the need for adapting reasoning strategies\nbased on task demands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit impressive reasoning abilities, yet\ntheir reliance on structured step-by-step processing reveals a critical\nlimitation. In contrast, human cognition fluidly adapts between intuitive,\nheuristic (System 1) and analytical, deliberative (System 2) reasoning\ndepending on the context. This difference between human cognitive flexibility\nand LLMs' reliance on a single reasoning style raises a critical question:\nwhile human fast heuristic reasoning evolved for its efficiency and\nadaptability, is a uniform reasoning approach truly optimal for LLMs, or does\nits inflexibility make them brittle and unreliable when faced with tasks\ndemanding more agile, intuitive responses? To answer these questions, we\nexplicitly align LLMs to these reasoning styles by curating a dataset with\nvalid System 1 and System 2 answers, and evaluate their performance across\nreasoning benchmarks. Our results reveal an accuracy-efficiency trade-off:\nSystem 2-aligned models excel in arithmetic and symbolic reasoning, while\nSystem 1-aligned models perform better in commonsense reasoning tasks. To\nanalyze the reasoning spectrum, we interpolated between the two extremes by\nvarying the proportion of alignment data, which resulted in a monotonic change\nin accuracy. A mechanistic analysis of model responses shows that System 1\nmodels employ more definitive outputs, whereas System 2 models demonstrate\ngreater uncertainty. Building on these findings, we further combine System 1-\nand System 2-aligned models based on the entropy of their generations, without\nadditional training, and obtain a dynamic model that outperforms across nearly\nall benchmarks. This work challenges the assumption that step-by-step reasoning\nis always optimal and highlights the need for adapting reasoning strategies\nbased on task demands."
                },
                "authors": [
                    {
                        "name": "Alireza S. Ziabari"
                    },
                    {
                        "name": "Nona Ghazizadeh"
                    },
                    {
                        "name": "Zhivar Sourati"
                    },
                    {
                        "name": "Farzan Karimi-Malekabadi"
                    },
                    {
                        "name": "Payam Piray"
                    },
                    {
                        "name": "Morteza Dehghani"
                    }
                ],
                "author_detail": {
                    "name": "Morteza Dehghani"
                },
                "author": "Morteza Dehghani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12470v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12470v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.14058v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.14058v2",
                "updated": "2025-10-15T17:15:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    15,
                    3,
                    2,
                    288,
                    0
                ],
                "published": "2024-03-21T01:06:47Z",
                "published_parsed": [
                    2024,
                    3,
                    21,
                    1,
                    6,
                    47,
                    3,
                    81,
                    0
                ],
                "title": "Thinking in Groups: Permutation Tests Reveal Near-Out-of-Distribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thinking in Groups: Permutation Tests Reveal Near-Out-of-Distribution"
                },
                "summary": "Deep neural networks (DNNs) have the potential to power many biomedical\nworkflows, but training them on truly representative, IID datasets is often\ninfeasible. Most models instead rely on biased or incomplete data, making them\nprone to out-of-distribution (OoD) inputs that closely resemble in-distribution\nsamples. Such near-OoD cases are harder to detect than standard OOD benchmarks\nand can cause unreliable, even catastrophic, predictions. Biomedical assays,\nhowever, offer a unique opportunity: they often generate multiple correlated\nmeasurements per specimen through biological or technical replicates.\nExploiting this insight, we introduce Homogeneous OoD (HOoD), a novel OoD\ndetection framework for correlated data. HOoD projects groups of correlated\nmeasurements through a trained model and uses permutation-based hypothesis\ntests to compare them with known subpopulations. Each test yields an\ninterpretable p-value, quantifying how well a group matches a subpopulation. By\naggregating these p-values, HOoD reliably identifies OoD groups. In\nevaluations, HOoD consistently outperforms point-wise and ensemble-based OoD\ndetectors, demonstrating its promise for robust real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) have the potential to power many biomedical\nworkflows, but training them on truly representative, IID datasets is often\ninfeasible. Most models instead rely on biased or incomplete data, making them\nprone to out-of-distribution (OoD) inputs that closely resemble in-distribution\nsamples. Such near-OoD cases are harder to detect than standard OOD benchmarks\nand can cause unreliable, even catastrophic, predictions. Biomedical assays,\nhowever, offer a unique opportunity: they often generate multiple correlated\nmeasurements per specimen through biological or technical replicates.\nExploiting this insight, we introduce Homogeneous OoD (HOoD), a novel OoD\ndetection framework for correlated data. HOoD projects groups of correlated\nmeasurements through a trained model and uses permutation-based hypothesis\ntests to compare them with known subpopulations. Each test yields an\ninterpretable p-value, quantifying how well a group matches a subpopulation. By\naggregating these p-values, HOoD reliably identifies OoD groups. In\nevaluations, HOoD consistently outperforms point-wise and ensemble-based OoD\ndetectors, demonstrating its promise for robust real-world deployment."
                },
                "authors": [
                    {
                        "name": "Yasith Jayawardana"
                    },
                    {
                        "name": "Dineth Jayakody"
                    },
                    {
                        "name": "Sampath Jayarathna"
                    },
                    {
                        "name": "Dushan N. Wadduwage"
                    }
                ],
                "author_detail": {
                    "name": "Dushan N. Wadduwage"
                },
                "author": "Dushan N. Wadduwage",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.14058v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.14058v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.19831v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.19831v2",
                "updated": "2025-10-15T17:12:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    12,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-27T12:35:31Z",
                "published_parsed": [
                    2025,
                    8,
                    27,
                    12,
                    35,
                    31,
                    2,
                    239,
                    0
                ],
                "title": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Hindi LLMs: A New Suite of Datasets and a Comparative\n  Analysis"
                },
                "summary": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating instruction-tuned Large Language Models (LLMs) in Hindi is\nchallenging due to a lack of high-quality benchmarks, as direct translation of\nEnglish datasets fails to capture crucial linguistic and cultural nuances. To\naddress this, we introduce a suite of five Hindi LLM evaluation datasets:\nIFEval-Hi, MT-Bench-Hi, GSM8K-Hi, ChatRAG-Hi, and BFCL-Hi. These were created\nusing a methodology that combines from-scratch human annotation with a\ntranslate-and-verify process. We leverage this suite to conduct an extensive\nbenchmarking of open-source LLMs supporting Hindi, providing a detailed\ncomparative analysis of their current capabilities. Our curation process also\nserves as a replicable methodology for developing benchmarks in other\nlow-resource languages."
                },
                "authors": [
                    {
                        "name": "Anusha Kamath"
                    },
                    {
                        "name": "Kanishk Singla"
                    },
                    {
                        "name": "Rakesh Paul"
                    },
                    {
                        "name": "Raviraj Joshi"
                    },
                    {
                        "name": "Utkarsh Vaidya"
                    },
                    {
                        "name": "Sanjay Singh Chauhan"
                    },
                    {
                        "name": "Niranjan Wartikar"
                    }
                ],
                "author_detail": {
                    "name": "Niranjan Wartikar"
                },
                "author": "Niranjan Wartikar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.19831v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.19831v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13760v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13760v1",
                "updated": "2025-10-15T17:10:39Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    10,
                    39,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:10:39Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    10,
                    39,
                    2,
                    288,
                    0
                ],
                "title": "Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for\n  Medical AI Assistants on the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invited Paper: BitMedViT: Ternary-Quantized Vision Transformer for\n  Medical AI Assistants on the Edge"
                },
                "summary": "Vision Transformers (ViTs) have demonstrated strong capabilities in\ninterpreting complex medical imaging data. However, their significant\ncomputational and memory demands pose challenges for deployment in real-time,\nresource-constrained mobile and wearable devices used in clinical environments.\nWe introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI\nassistants that perform structured analysis of medical images directly on the\nedge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical\nimaging and com- bines a training procedure with multi-query attention,\npreserving stability under ternary weights with low-precision activations.\nFurthermore, BiTMedViT employs task-aware distillation from a high-capacity\nteacher to recover accuracy lost due to extreme quantization. Lastly, we also\npresent a pipeline that maps the ternarized ViTs to a custom CUDA kernel for\nefficient memory bandwidth utilization and latency reduction on the Jetson Orin\nNano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on\nMedMNIST across 12 datasets, while reducing model size by 43x, memory traffic\nby 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that\nof SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a\npractical and scientifically grounded route for extreme-precision medical\nimaging ViTs deployable on the edge, narrowing the gap between algorithmic\nadvances and deployable clinical tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Transformers (ViTs) have demonstrated strong capabilities in\ninterpreting complex medical imaging data. However, their significant\ncomputational and memory demands pose challenges for deployment in real-time,\nresource-constrained mobile and wearable devices used in clinical environments.\nWe introduce, BiTMedViT, a new class of Edge ViTs serving as medical AI\nassistants that perform structured analysis of medical images directly on the\nedge. BiTMedViT utilizes ternary- quantized linear layers tailored for medical\nimaging and com- bines a training procedure with multi-query attention,\npreserving stability under ternary weights with low-precision activations.\nFurthermore, BiTMedViT employs task-aware distillation from a high-capacity\nteacher to recover accuracy lost due to extreme quantization. Lastly, we also\npresent a pipeline that maps the ternarized ViTs to a custom CUDA kernel for\nefficient memory bandwidth utilization and latency reduction on the Jetson Orin\nNano. Finally, BiTMedViT achieves 86% diagnostic accuracy (89% SOTA) on\nMedMNIST across 12 datasets, while reducing model size by 43x, memory traffic\nby 39x, and enabling 16.8 ms inference at an energy efficiency up to 41x that\nof SOTA models at 183.62 GOPs/J on the Orin Nano. Our results demonstrate a\npractical and scientifically grounded route for extreme-precision medical\nimaging ViTs deployable on the edge, narrowing the gap between algorithmic\nadvances and deployable clinical tools."
                },
                "authors": [
                    {
                        "name": "Mikolaj Walczak"
                    },
                    {
                        "name": "Uttej Kallakuri"
                    },
                    {
                        "name": "Edward Humes"
                    },
                    {
                        "name": "Xiaomin Lin"
                    },
                    {
                        "name": "Tinoosh Mohsenin"
                    }
                ],
                "author_detail": {
                    "name": "Tinoosh Mohsenin"
                },
                "author": "Tinoosh Mohsenin",
                "arxiv_comment": "Accepted at 2025 IEEE/ACM International Conf. on Computer-Aided\n  Design (ICCAD) Oct. 26-30 2025, Munich, DE",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13760v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13760v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12400v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12400v3",
                "updated": "2025-10-15T17:09:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    9,
                    41,
                    2,
                    288,
                    0
                ],
                "published": "2024-10-16T09:28:58Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    9,
                    28,
                    58,
                    2,
                    290,
                    0
                ],
                "title": "QUIDS: Query Intent Description for Exploratory Search via Dual Space\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QUIDS: Query Intent Description for Exploratory Search via Dual Space\n  Modeling"
                },
                "summary": "In exploratory search, users often submit vague queries to investigate\nunfamiliar topics, but receive limited feedback about how the search engine\nunderstood their input. This leads to a self-reinforcing cycle of mismatched\nresults and trial-and-error reformulation. To address this, we study the task\nof generating user-facing natural language query intent descriptions that\nsurface what the system likely inferred the query to mean, based on\npost-retrieval evidence. We propose QUIDS, a method that leverages dual-space\ncontrastive learning to isolate intent-relevant information while suppressing\nirrelevant content. QUIDS combines a dual-encoder representation space with a\ndisentangling decoder that works together to produce concise and accurate\nintent descriptions. Enhanced by intent-driven hard negative sampling, the\nmodel significantly outperforms state-of-the-art baselines across ROUGE,\nBERTScore, and human/LLM evaluations. Our qualitative analysis confirms QUIDS'\neffectiveness in generating accurate intent descriptions for exploratory\nsearch. Our work contributes to improving the interaction between users and\nsearch engines by providing feedback to the user in exploratory search\nsettings. Our code is available at https://github.com/menauwy/QUIDS",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In exploratory search, users often submit vague queries to investigate\nunfamiliar topics, but receive limited feedback about how the search engine\nunderstood their input. This leads to a self-reinforcing cycle of mismatched\nresults and trial-and-error reformulation. To address this, we study the task\nof generating user-facing natural language query intent descriptions that\nsurface what the system likely inferred the query to mean, based on\npost-retrieval evidence. We propose QUIDS, a method that leverages dual-space\ncontrastive learning to isolate intent-relevant information while suppressing\nirrelevant content. QUIDS combines a dual-encoder representation space with a\ndisentangling decoder that works together to produce concise and accurate\nintent descriptions. Enhanced by intent-driven hard negative sampling, the\nmodel significantly outperforms state-of-the-art baselines across ROUGE,\nBERTScore, and human/LLM evaluations. Our qualitative analysis confirms QUIDS'\neffectiveness in generating accurate intent descriptions for exploratory\nsearch. Our work contributes to improving the interaction between users and\nsearch engines by providing feedback to the user in exploratory search\nsettings. Our code is available at https://github.com/menauwy/QUIDS"
                },
                "authors": [
                    {
                        "name": "Yumeng Wang"
                    },
                    {
                        "name": "Xiuying Chen"
                    },
                    {
                        "name": "Suzan Verberne"
                    }
                ],
                "author_detail": {
                    "name": "Suzan Verberne"
                },
                "author": "Suzan Verberne",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12400v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12400v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13757v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13757v1",
                "updated": "2025-10-15T17:05:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    5,
                    55,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T17:05:55Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    17,
                    5,
                    55,
                    2,
                    288,
                    0
                ],
                "title": "A Complete Pipeline for deploying SNNs with Synaptic Delays on Loihi 2",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Complete Pipeline for deploying SNNs with Synaptic Delays on Loihi 2"
                },
                "summary": "Spiking Neural Networks are attracting increased attention as a more\nenergy-efficient alternative to traditional Artificial Neural Networks for edge\ncomputing. Neuromorphic computing can significantly reduce energy requirements.\nHere, we present a complete pipeline: efficient event-based training of SNNs\nwith synaptic delays on GPUs and deployment on Intel's Loihi 2 neuromorphic\nchip. We evaluate our approach on keyword recognition tasks using the Spiking\nHeidelberg Digits and Spiking Speech Commands datasets, demonstrating that our\nalgorithm can enhance classification accuracy compared to architectures without\ndelays. Our benchmarking indicates almost no accuracy loss between GPU and\nLoihi 2 implementations, while classification on Loihi 2 is up to 18x faster\nand uses 250x less energy than on an NVIDIA Jetson Orin Nano.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks are attracting increased attention as a more\nenergy-efficient alternative to traditional Artificial Neural Networks for edge\ncomputing. Neuromorphic computing can significantly reduce energy requirements.\nHere, we present a complete pipeline: efficient event-based training of SNNs\nwith synaptic delays on GPUs and deployment on Intel's Loihi 2 neuromorphic\nchip. We evaluate our approach on keyword recognition tasks using the Spiking\nHeidelberg Digits and Spiking Speech Commands datasets, demonstrating that our\nalgorithm can enhance classification accuracy compared to architectures without\ndelays. Our benchmarking indicates almost no accuracy loss between GPU and\nLoihi 2 implementations, while classification on Loihi 2 is up to 18x faster\nand uses 250x less energy than on an NVIDIA Jetson Orin Nano."
                },
                "authors": [
                    {
                        "name": "Bal√°zs M√©sz√°ros"
                    },
                    {
                        "name": "James C. Knight"
                    },
                    {
                        "name": "Jonathan Timcheck"
                    },
                    {
                        "name": "Thomas Nowotny"
                    }
                ],
                "author_detail": {
                    "name": "Thomas Nowotny"
                },
                "author": "Thomas Nowotny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13757v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13757v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09653v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09653v2",
                "updated": "2025-10-15T16:57:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    57,
                    20,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-06T23:28:44Z",
                "published_parsed": [
                    2025,
                    10,
                    6,
                    23,
                    28,
                    44,
                    0,
                    279,
                    0
                ],
                "title": "Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and\n  YOLOv5 Object Detectors for Computer Vision and Pattern Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ultralytics YOLO Evolution: An Overview of YOLO26, YOLO11, YOLOv8 and\n  YOLOv5 Object Detectors for Computer Vision and Pattern Recognition"
                },
                "summary": "This paper presents a comprehensive overview of the Ultralytics YOLO(You Only\nLook Once) family of object detectors, focusing the architectural evolution,\nbenchmarking, deployment perspectives, and future challenges. The review begins\nwith the most recent release, YOLO26 (or YOLOv26), which introduces key\ninnovations including Distribution Focal Loss (DFL) removal, native NMS-free\ninference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label\nAssignment (STAL), and the MuSGD optimizer for stable training. The progression\nis then traced through YOLO11, with its hybrid task assignment and\nefficiency-focused modules; YOLOv8, which advanced with a decoupled detection\nhead and anchor-free predictions; and YOLOv5, which established the modular\nPyTorch foundation that enabled modern YOLO development. Benchmarking on the MS\nCOCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,\nYOLO11, and YOLO26 (YOLOv26), alongside cross-comparisons with YOLOv12,\nYOLOv13, RT-DETR, and DEIM(DETR with Improved Matching). Metrics including\nprecision, recall, F1 score, mean Average Precision, and inference speed are\nanalyzed to highlight trade-offs between accuracy and efficiency. Deployment\nand application perspectives are further discussed, covering export formats,\nquantization strategies, and real-world use in robotics, agriculture,\nsurveillance, and manufacturing. Finally, the paper identifies challenges and\nfuture directions, including dense-scene limitations, hybrid CNN-Transformer\nintegration, open-vocabulary detection, and edge-aware training approaches.\n(Object Detection, YOLOv26, YOLO)",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive overview of the Ultralytics YOLO(You Only\nLook Once) family of object detectors, focusing the architectural evolution,\nbenchmarking, deployment perspectives, and future challenges. The review begins\nwith the most recent release, YOLO26 (or YOLOv26), which introduces key\ninnovations including Distribution Focal Loss (DFL) removal, native NMS-free\ninference, Progressive Loss Balancing (ProgLoss), Small-Target-Aware Label\nAssignment (STAL), and the MuSGD optimizer for stable training. The progression\nis then traced through YOLO11, with its hybrid task assignment and\nefficiency-focused modules; YOLOv8, which advanced with a decoupled detection\nhead and anchor-free predictions; and YOLOv5, which established the modular\nPyTorch foundation that enabled modern YOLO development. Benchmarking on the MS\nCOCO dataset provides a detailed quantitative comparison of YOLOv5, YOLOv8,\nYOLO11, and YOLO26 (YOLOv26), alongside cross-comparisons with YOLOv12,\nYOLOv13, RT-DETR, and DEIM(DETR with Improved Matching). Metrics including\nprecision, recall, F1 score, mean Average Precision, and inference speed are\nanalyzed to highlight trade-offs between accuracy and efficiency. Deployment\nand application perspectives are further discussed, covering export formats,\nquantization strategies, and real-world use in robotics, agriculture,\nsurveillance, and manufacturing. Finally, the paper identifies challenges and\nfuture directions, including dense-scene limitations, hybrid CNN-Transformer\nintegration, open-vocabulary detection, and edge-aware training approaches.\n(Object Detection, YOLOv26, YOLO)"
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09653v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09653v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13750v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13750v1",
                "updated": "2025-10-15T16:55:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    55,
                    56,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:55:56Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    55,
                    56,
                    2,
                    288,
                    0
                ],
                "title": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence-Based Response Abstinence: Improving LLM Trustworthiness via\n  Activation-Based Uncertainty Estimation"
                },
                "summary": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method for confidence estimation in retrieval-augmented\ngeneration (RAG) systems that aligns closely with the correctness of large\nlanguage model (LLM) outputs. Confidence estimation is especially critical in\nhigh-stakes domains such as finance and healthcare, where the cost of an\nincorrect answer outweighs that of not answering the question. Our approach\nextends prior uncertainty quantification methods by leveraging raw feed-forward\nnetwork (FFN) activations as auto-regressive signals, avoiding the information\nloss inherent in token logits and probabilities after projection and softmax\nnormalization. We model confidence prediction as a sequence classification\ntask, and regularize training with a Huber loss term to improve robustness\nagainst noisy supervision. Applied in a real-world financial industry\ncustomer-support setting with complex knowledge bases, our method outperforms\nstrong baselines and maintains high accuracy under strict latency constraints.\nExperiments on Llama 3.1 8B model show that using activations from only the\n16th layer preserves accuracy while reducing response latency. Our results\ndemonstrate that activation-based confidence modeling offers a scalable,\narchitecture-aware path toward trustworthy RAG deployment."
                },
                "authors": [
                    {
                        "name": "Zhiqi Huang"
                    },
                    {
                        "name": "Vivek Datla"
                    },
                    {
                        "name": "Chenyang Zhu"
                    },
                    {
                        "name": "Alfy Samuel"
                    },
                    {
                        "name": "Daben Liu"
                    },
                    {
                        "name": "Anoop Kumar"
                    },
                    {
                        "name": "Ritesh Soni"
                    }
                ],
                "author_detail": {
                    "name": "Ritesh Soni"
                },
                "author": "Ritesh Soni",
                "arxiv_comment": "UncertaiNLP at EMNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13750v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13750v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13744v1",
                "updated": "2025-10-15T16:50:54Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    50,
                    54,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:50:54Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    50,
                    54,
                    2,
                    288,
                    0
                ],
                "title": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hard2Verify: A Step-Level Verification Benchmark for Open-Ended Frontier\n  Math"
                },
                "summary": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM)-based reasoning systems have recently achieved\ngold medal-level performance in the IMO 2025 competition, writing mathematical\nproofs where, to receive full credit, each step must be not only correct but\nalso sufficiently supported. To train LLM-based reasoners in such challenging,\nopen-ended settings, strong verifiers capable of catching step-level mistakes\nare necessary prerequisites. We introduce Hard2Verify, a human-annotated,\nstep-level verification benchmark produced with over 500 hours of human labor.\nHard2Verify is designed to rigorously assess step-level verifiers at the\nfrontier: Verifiers must provide step-level annotations or identify the first\nerror in responses generated by frontier LLMs for very recent, challenging, and\nopen-ended math questions. We evaluate 29 generative critics and process reward\nmodels, demonstrating that, beyond a few standouts, open-source verifiers lag\nclosed source models. We subsequently analyze what drives poor performance in\nstep-level verification, the impacts of scaling verifier compute, as well as\nfundamental questions such as self-verification and verification-generation\ndynamics."
                },
                "authors": [
                    {
                        "name": "Shrey Pandit"
                    },
                    {
                        "name": "Austin Xu"
                    },
                    {
                        "name": "Xuan-Phi Nguyen"
                    },
                    {
                        "name": "Yifei Ming"
                    },
                    {
                        "name": "Caiming Xiong"
                    },
                    {
                        "name": "Shafiq Joty"
                    }
                ],
                "author_detail": {
                    "name": "Shafiq Joty"
                },
                "author": "Shafiq Joty",
                "arxiv_comment": "21 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13738v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13738v1",
                "updated": "2025-10-15T16:45:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    45,
                    59,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:45:59Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    45,
                    59,
                    2,
                    288,
                    0
                ],
                "title": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyMiRec: A Hybrid Multi-interest Learning Framework for LLM-based\n  Sequential Recommendation"
                },
                "summary": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have recently demonstrated strong potential for\nsequential recommendation. However, current LLM-based approaches face critical\nlimitations in modeling users' long-term and diverse interests. First, due to\ninference latency and feature fetching bandwidth constraints, existing methods\ntypically truncate user behavior sequences to include only the most recent\ninteractions, resulting in the loss of valuable long-range preference signals.\nSecond, most current methods rely on next-item prediction with a single\npredicted embedding, overlooking the multifaceted nature of user interests and\nlimiting recommendation diversity. To address these challenges, we propose\nHyMiRec, a hybrid multi-interest sequential recommendation framework, which\nleverages a lightweight recommender to extracts coarse interest embeddings from\nlong user sequences and an LLM-based recommender to captures refined interest\nembeddings. To alleviate the overhead of fetching features, we introduce a\nresidual codebook based on cosine similarity, enabling efficient compression\nand reuse of user history embeddings. To model the diverse preferences of\nusers, we design a disentangled multi-interest learning module, which leverages\nmultiple interest queries to learn disentangles multiple interest signals\nadaptively, allowing the model to capture different facets of user intent.\nExtensive experiments are conducted on both benchmark datasets and a collected\nindustrial dataset, demonstrating our effectiveness over existing\nstate-of-the-art methods. Furthermore, online A/B testing shows that HyMiRec\nbrings consistent improvements in real-world recommendation systems."
                },
                "authors": [
                    {
                        "name": "Jingyi Zhou"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Kai Zuo"
                    },
                    {
                        "name": "Manjie Xu"
                    },
                    {
                        "name": "Zhendong Fu"
                    },
                    {
                        "name": "Yibo Chen"
                    },
                    {
                        "name": "Xu Tang"
                    },
                    {
                        "name": "Yao Hu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Hu"
                },
                "author": "Yao Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13738v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13738v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.15225v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.15225v2",
                "updated": "2025-10-15T16:43:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    43,
                    12,
                    2,
                    288,
                    0
                ],
                "published": "2025-03-19T14:03:20Z",
                "published_parsed": [
                    2025,
                    3,
                    19,
                    14,
                    3,
                    20,
                    2,
                    78,
                    0
                ],
                "title": "A Personalized Data-Driven Generative Model of Human Repetitive Motion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Personalized Data-Driven Generative Model of Human Repetitive Motion"
                },
                "summary": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities -- such as rehabilitation therapy, sports, and\nmanufacturing -- is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. Furthermore, recent\nresearch has shown that each person exhibits a unique velocity signature,\nhighlighting how individual motor behaviors are both rich in variability and\ninternally consistent. However, existing models only provide simplified\ndescriptions of human motor behavior, hindering the development of effective\ncognitive architectures. In this work, we first show that motion amplitude\nprovides a valid and complementary characterization of individual motor\nsignatures. Then, we propose a fully data-driven approach, based on long\nshort-term memory neural networks, to generate original motion that captures\nthe unique features of specific individuals. We validate the architecture using\nreal human data from participants performing spontaneous oscillatory motion.\nExtensive analyses show that state-of-the-art Kuramoto-like models fail to\nreplicate individual motor signatures, whereas our model accurately reproduces\nthe velocity distribution and amplitude envelopes of the individual it was\ntrained on, while remaining distinct from others.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The deployment of autonomous virtual avatars (in extended reality) and robots\nin human group activities -- such as rehabilitation therapy, sports, and\nmanufacturing -- is expected to increase as these technologies become more\npervasive. Designing cognitive architectures and control strategies to drive\nthese agents requires realistic models of human motion. Furthermore, recent\nresearch has shown that each person exhibits a unique velocity signature,\nhighlighting how individual motor behaviors are both rich in variability and\ninternally consistent. However, existing models only provide simplified\ndescriptions of human motor behavior, hindering the development of effective\ncognitive architectures. In this work, we first show that motion amplitude\nprovides a valid and complementary characterization of individual motor\nsignatures. Then, we propose a fully data-driven approach, based on long\nshort-term memory neural networks, to generate original motion that captures\nthe unique features of specific individuals. We validate the architecture using\nreal human data from participants performing spontaneous oscillatory motion.\nExtensive analyses show that state-of-the-art Kuramoto-like models fail to\nreplicate individual motor signatures, whereas our model accurately reproduces\nthe velocity distribution and amplitude envelopes of the individual it was\ntrained on, while remaining distinct from others."
                },
                "authors": [
                    {
                        "name": "Angelo Di Porzio"
                    },
                    {
                        "name": "Marco Coraggio"
                    }
                ],
                "author_detail": {
                    "name": "Marco Coraggio"
                },
                "author": "Marco Coraggio",
                "arxiv_comment": "12 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.15225v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.15225v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13734v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13734v1",
                "updated": "2025-10-15T16:40:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    40,
                    28,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:40:28Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    40,
                    28,
                    2,
                    288,
                    0
                ],
                "title": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI\n  Clinicians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GAPS: A Clinically Grounded, Automated Benchmark for Evaluating AI\n  Clinicians"
                },
                "summary": "Current benchmarks for AI clinician systems, often based on multiple-choice\nexams or manual rubrics, fail to capture the depth, robustness, and safety\nrequired for real-world clinical practice. To address this, we introduce the\nGAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding\n(cognitive depth), \\textbf{A}dequacy (answer completeness),\n\\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we\ndeveloped a fully automated, guideline-anchored pipeline to construct a\nGAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity\nlimitations of prior work. Our pipeline assembles an evidence neighborhood,\ncreates dual graph and tree representations, and automatically generates\nquestions across G-levels. Rubrics are synthesized by a DeepResearch agent that\nmimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring\nis performed by an ensemble of large language model (LLM) judges. Validation\nconfirmed our automated questions are high-quality and align with clinician\njudgment. Evaluating state-of-the-art models on the benchmark revealed key\nfailure modes: performance degrades sharply with increased reasoning depth\n(G-axis), models struggle with answer completeness (A-axis), and they are\nhighly vulnerable to adversarial perturbations (P-axis) as well as certain\nsafety issues (S-axis). This automated, clinically-grounded approach provides a\nreproducible and scalable method for rigorously evaluating AI clinician systems\nand guiding their development toward safer, more reliable clinical practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current benchmarks for AI clinician systems, often based on multiple-choice\nexams or manual rubrics, fail to capture the depth, robustness, and safety\nrequired for real-world clinical practice. To address this, we introduce the\nGAPS framework, a multidimensional paradigm for evaluating \\textbf{G}rounding\n(cognitive depth), \\textbf{A}dequacy (answer completeness),\n\\textbf{P}erturbation (robustness), and \\textbf{S}afety. Critically, we\ndeveloped a fully automated, guideline-anchored pipeline to construct a\nGAPS-aligned benchmark end-to-end, overcoming the scalability and subjectivity\nlimitations of prior work. Our pipeline assembles an evidence neighborhood,\ncreates dual graph and tree representations, and automatically generates\nquestions across G-levels. Rubrics are synthesized by a DeepResearch agent that\nmimics GRADE-consistent, PICO-driven evidence review in a ReAct loop. Scoring\nis performed by an ensemble of large language model (LLM) judges. Validation\nconfirmed our automated questions are high-quality and align with clinician\njudgment. Evaluating state-of-the-art models on the benchmark revealed key\nfailure modes: performance degrades sharply with increased reasoning depth\n(G-axis), models struggle with answer completeness (A-axis), and they are\nhighly vulnerable to adversarial perturbations (P-axis) as well as certain\nsafety issues (S-axis). This automated, clinically-grounded approach provides a\nreproducible and scalable method for rigorously evaluating AI clinician systems\nand guiding their development toward safer, more reliable clinical practice."
                },
                "authors": [
                    {
                        "name": "Xiuyuan Chen"
                    },
                    {
                        "name": "Tao Sun"
                    },
                    {
                        "name": "Dexin Su"
                    },
                    {
                        "name": "Ailing Yu"
                    },
                    {
                        "name": "Junwei Liu"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Gangzeng Jin"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Jingnan Liu"
                    },
                    {
                        "name": "Hansong Xiao"
                    },
                    {
                        "name": "Hualei Zhou"
                    },
                    {
                        "name": "Dongjie Tao"
                    },
                    {
                        "name": "Chunxiao Guo"
                    },
                    {
                        "name": "Minghui Yang"
                    },
                    {
                        "name": "Yuan Xia"
                    },
                    {
                        "name": "Jing Zhao"
                    },
                    {
                        "name": "Qianrui Fan"
                    },
                    {
                        "name": "Yanyun Wang"
                    },
                    {
                        "name": "Shuai Zhen"
                    },
                    {
                        "name": "Kezhong Chen"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Zewen Sun"
                    },
                    {
                        "name": "Heng Zhao"
                    },
                    {
                        "name": "Tian Guan"
                    },
                    {
                        "name": "Shaodong Wang"
                    },
                    {
                        "name": "Geyun Chang"
                    },
                    {
                        "name": "Jiaming Deng"
                    },
                    {
                        "name": "Hongchengcheng Chen"
                    },
                    {
                        "name": "Kexin Feng"
                    },
                    {
                        "name": "Ruzhen Li"
                    },
                    {
                        "name": "Jiayi Geng"
                    },
                    {
                        "name": "Changtai Zhao"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Guihu Lin"
                    },
                    {
                        "name": "Peihao Li"
                    },
                    {
                        "name": "Liqi Liu"
                    },
                    {
                        "name": "Peng Wei"
                    },
                    {
                        "name": "Jian Wang"
                    },
                    {
                        "name": "Jinjie Gu"
                    },
                    {
                        "name": "Ping Wang"
                    },
                    {
                        "name": "Fan Yang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Yang"
                },
                "author": "Fan Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13734v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13734v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13732v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13732v1",
                "updated": "2025-10-15T16:36:06Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    36,
                    6,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:36:06Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    36,
                    6,
                    2,
                    288,
                    0
                ],
                "title": "Scalable Pilot Assignment for Distributed Massive MIMO using Channel\n  Estimation Error",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scalable Pilot Assignment for Distributed Massive MIMO using Channel\n  Estimation Error"
                },
                "summary": "Pilot contamination remains a major bottleneck in realizing the full\npotential of distributed massive MIMO systems. We propose two dynamic and\nscalable pilot assignment strategies designed for practical deployment in such\nnetworks. First, we present a low complexity centralized algorithm that\nsequentially assigns pilots to user equipments (UEs) to minimize the global\nchannel estimation errors across serving access points (APs). This improves the\nchannel estimation quality and reduces interference among UEs, enhancing the\nspectral efficiency. Second, we develop a fully distributed algorithm that uses\na priority-based pilot selection approach. In this algorithm, each selected AP\nminimizes estimation error using only local information and offers candidate\npilots to the UEs. Every UE then selects a suitable pilot based on AP priority.\nThis approach ensures consistency and minimizes interference while\nsignificantly reducing pilot contamination. The method requires no global\ncoordination, maintains low signaling overhead, and adapts dynamically to the\nUE deployment. Numerical simulations demonstrate the superiority of our\nproposed schemes in terms of network throughput when compared to other\nstate-of-the-art benchmark schemes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pilot contamination remains a major bottleneck in realizing the full\npotential of distributed massive MIMO systems. We propose two dynamic and\nscalable pilot assignment strategies designed for practical deployment in such\nnetworks. First, we present a low complexity centralized algorithm that\nsequentially assigns pilots to user equipments (UEs) to minimize the global\nchannel estimation errors across serving access points (APs). This improves the\nchannel estimation quality and reduces interference among UEs, enhancing the\nspectral efficiency. Second, we develop a fully distributed algorithm that uses\na priority-based pilot selection approach. In this algorithm, each selected AP\nminimizes estimation error using only local information and offers candidate\npilots to the UEs. Every UE then selects a suitable pilot based on AP priority.\nThis approach ensures consistency and minimizes interference while\nsignificantly reducing pilot contamination. The method requires no global\ncoordination, maintains low signaling overhead, and adapts dynamically to the\nUE deployment. Numerical simulations demonstrate the superiority of our\nproposed schemes in terms of network throughput when compared to other\nstate-of-the-art benchmark schemes."
                },
                "authors": [
                    {
                        "name": "Mohd Saif Ali Khan"
                    },
                    {
                        "name": "Karthik RM"
                    },
                    {
                        "name": "Samar Agnihotri"
                    }
                ],
                "author_detail": {
                    "name": "Samar Agnihotri"
                },
                "author": "Samar Agnihotri",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13732v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13732v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13727v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13727v1",
                "updated": "2025-10-15T16:30:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    30,
                    57,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:30:57Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    30,
                    57,
                    2,
                    288,
                    0
                ],
                "title": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI\n  Guardrails",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Refusal to Recovery: A Control-Theoretic Approach to Generative AI\n  Guardrails"
                },
                "summary": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI systems are increasingly assisting and acting on behalf of end\nusers in practical settings, from digital shopping assistants to\nnext-generation autonomous cars. In this context, safety is no longer about\nblocking harmful content, but about preempting downstream hazards like\nfinancial or physical harm. Yet, most AI guardrails continue to rely on output\nclassification based on labeled datasets and human-specified criteria,making\nthem brittle to new hazardous situations. Even when unsafe conditions are\nflagged, this detection offers no path to recovery: typically, the AI system\nsimply refuses to act--which is not always a safe choice. In this work, we\nargue that agentic AI safety is fundamentally a sequential decision problem:\nharmful outcomes arise from the AI system's continually evolving interactions\nand their downstream consequences on the world. We formalize this through the\nlens of safety-critical control theory, but within the AI model's latent\nrepresentation of the world. This enables us to build predictive guardrails\nthat (i) monitor an AI system's outputs (actions) in real time and (ii)\nproactively correct risky outputs to safe ones, all in a model-agnostic manner\nso the same guardrail can be wrapped around any AI model. We also offer a\npractical training recipe for computing such guardrails at scale via\nsafety-critical reinforcement learning. Our experiments in simulated driving\nand e-commerce settings demonstrate that control-theoretic guardrails can\nreliably steer LLM agents clear of catastrophic outcomes (from collisions to\nbankruptcy) while preserving task performance, offering a principled dynamic\nalternative to today's flag-and-block guardrails."
                },
                "authors": [
                    {
                        "name": "Ravi Pandya"
                    },
                    {
                        "name": "Madison Bland"
                    },
                    {
                        "name": "Duy P. Nguyen"
                    },
                    {
                        "name": "Changliu Liu"
                    },
                    {
                        "name": "Jaime Fern√°ndez Fisac"
                    },
                    {
                        "name": "Andrea Bajcsy"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bajcsy"
                },
                "author": "Andrea Bajcsy",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13727v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13727v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13724v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13724v1",
                "updated": "2025-10-15T16:28:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    28,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:28:34Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    28,
                    34,
                    2,
                    288,
                    0
                ],
                "title": "FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI\n  Model Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FIRST: Federated Inference Resource Scheduling Toolkit for Scientific AI\n  Model Access"
                },
                "summary": "We present the Federated Inference Resource Scheduling Toolkit (FIRST), a\nframework enabling Inference-as-a-Service across distributed High-Performance\nComputing (HPC) clusters. FIRST provides cloud-like access to diverse AI\nmodels, like Large Language Models (LLMs), on existing HPC infrastructure.\nLeveraging Globus Auth and Globus Compute, the system allows researchers to run\nparallel inference workloads via an OpenAI-compliant API on private, secure\nenvironments. This cluster-agnostic API allows requests to be distributed\nacross federated clusters, targeting numerous hosted models. FIRST supports\nmultiple inference backends (e.g., vLLM), auto-scales resources, maintains\n\"hot\" nodes for low-latency execution, and offers both high-throughput batch\nand interactive modes. The framework addresses the growing demand for private,\nsecure, and scalable AI inference in scientific workflows, allowing researchers\nto generate billions of tokens daily on-premises without relying on commercial\ncloud infrastructure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present the Federated Inference Resource Scheduling Toolkit (FIRST), a\nframework enabling Inference-as-a-Service across distributed High-Performance\nComputing (HPC) clusters. FIRST provides cloud-like access to diverse AI\nmodels, like Large Language Models (LLMs), on existing HPC infrastructure.\nLeveraging Globus Auth and Globus Compute, the system allows researchers to run\nparallel inference workloads via an OpenAI-compliant API on private, secure\nenvironments. This cluster-agnostic API allows requests to be distributed\nacross federated clusters, targeting numerous hosted models. FIRST supports\nmultiple inference backends (e.g., vLLM), auto-scales resources, maintains\n\"hot\" nodes for low-latency execution, and offers both high-throughput batch\nand interactive modes. The framework addresses the growing demand for private,\nsecure, and scalable AI inference in scientific workflows, allowing researchers\nto generate billions of tokens daily on-premises without relying on commercial\ncloud infrastructure."
                },
                "authors": [
                    {
                        "name": "Aditya Tanikanti"
                    },
                    {
                        "name": "Benoit C√¥t√©"
                    },
                    {
                        "name": "Yanfei Guo"
                    },
                    {
                        "name": "Le Chen"
                    },
                    {
                        "name": "Nickolaus Saint"
                    },
                    {
                        "name": "Ryan Chard"
                    },
                    {
                        "name": "Ken Raffenetti"
                    },
                    {
                        "name": "Rajeev Thakur"
                    },
                    {
                        "name": "Thomas Uram"
                    },
                    {
                        "name": "Ian Foster"
                    },
                    {
                        "name": "Michael E. Papka"
                    },
                    {
                        "name": "Venkatram Vishwanath"
                    }
                ],
                "author_detail": {
                    "name": "Venkatram Vishwanath"
                },
                "author": "Venkatram Vishwanath",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13724v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13724v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13713v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13713v1",
                "updated": "2025-10-15T16:13:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T16:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Don't Be Greedy, Just Relax! Pruning LLMs via Frank-Wolfe"
                },
                "summary": "Pruning is a common technique to reduce the compute and storage requirements\nof Neural Networks. While conventional approaches typically retrain the model\nto recover pruning-induced performance degradation, state-of-the-art Large\nLanguage Model (LLM) pruning methods operate layer-wise, minimizing the\nper-layer pruning error on a small calibration dataset to avoid full\nretraining, which is considered computationally prohibitive for LLMs. However,\nfinding the optimal pruning mask is a hard combinatorial problem and solving it\nto optimality is intractable. Existing methods hence rely on greedy heuristics\nthat ignore the weight interactions in the pruning objective. In this work, we\ninstead consider the convex relaxation of these combinatorial constraints and\nsolve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method\ndrastically reduces the per-layer pruning error, outperforms strong baselines\non state-of-the-art GPT architectures, and remains memory-efficient. We provide\ntheoretical justification by showing that, combined with the convergence\nguarantees of the FW algorithm, we obtain an approximate solution to the\noriginal combinatorial problem upon rounding the relaxed solution to\nintegrality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pruning is a common technique to reduce the compute and storage requirements\nof Neural Networks. While conventional approaches typically retrain the model\nto recover pruning-induced performance degradation, state-of-the-art Large\nLanguage Model (LLM) pruning methods operate layer-wise, minimizing the\nper-layer pruning error on a small calibration dataset to avoid full\nretraining, which is considered computationally prohibitive for LLMs. However,\nfinding the optimal pruning mask is a hard combinatorial problem and solving it\nto optimality is intractable. Existing methods hence rely on greedy heuristics\nthat ignore the weight interactions in the pruning objective. In this work, we\ninstead consider the convex relaxation of these combinatorial constraints and\nsolve the resulting problem using the Frank-Wolfe (FW) algorithm. Our method\ndrastically reduces the per-layer pruning error, outperforms strong baselines\non state-of-the-art GPT architectures, and remains memory-efficient. We provide\ntheoretical justification by showing that, combined with the convergence\nguarantees of the FW algorithm, we obtain an approximate solution to the\noriginal combinatorial problem upon rounding the relaxed solution to\nintegrality."
                },
                "authors": [
                    {
                        "name": "Christophe Roux"
                    },
                    {
                        "name": "Max Zimmer"
                    },
                    {
                        "name": "Alexandre d'Aspremont"
                    },
                    {
                        "name": "Sebastian Pokutta"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Pokutta"
                },
                "author": "Sebastian Pokutta",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13713v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13713v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13709v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13709v2",
                "updated": "2025-10-16T03:39:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    3,
                    39,
                    31,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T16:09:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    9,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "Training LLM Agents to Empower Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training LLM Agents to Empower Humans"
                },
                "summary": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assistive agents should not only take actions on behalf of a human, but also\nstep out of the way and cede control when there are important decisions to be\nmade. However, current methods for building assistive agents, whether via\nmimicking expert humans or via RL finetuning on an inferred reward, often\nencourage agents to complete tasks on their own rather than truly assisting the\nhuman attain their objectives. Additionally, these methods often require costly\nexplicit human feedback to provide a training signal. We propose a new approach\nto tuning assistive language models based on maximizing the human's\nempowerment, their ability to effect desired changes in the environment. Our\nempowerment-maximizing method, Empower, only requires offline text data,\nproviding a self-supervised method for fine-tuning language models to better\nassist humans. To study the efficacy of our approach, we conducted an 18-person\nuser study comparing our empowerment assistant with a strong baseline.\nParticipants preferred our assistant 78% of the time (p=0.015), with a 31%\nhigher acceptance rate and 38% fewer suggestions. Additionally, we introduce a\nnew environment for evaluating multi-turn code assistance using simulated\nhumans. Using this environment, we show that agents trained with Empower\nincrease the success rate of a simulated human programmer on challenging coding\nquestions by an average of 192% over an SFT baseline. With this empowerment\nobjective, we provide a framework for useful aligned AI agents at scale using\nonly offline data without the need for any additional human feedback or\nverifiable rewards."
                },
                "authors": [
                    {
                        "name": "Evan Ellis"
                    },
                    {
                        "name": "Vivek Myers"
                    },
                    {
                        "name": "Jens Tuyls"
                    },
                    {
                        "name": "Sergey Levine"
                    },
                    {
                        "name": "Anca Dragan"
                    },
                    {
                        "name": "Benjamin Eysenbach"
                    }
                ],
                "author_detail": {
                    "name": "Benjamin Eysenbach"
                },
                "author": "Benjamin Eysenbach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13709v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13709v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08615v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08615v3",
                "updated": "2025-10-16T03:40:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    3,
                    40,
                    22,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-08T01:26:48Z",
                "published_parsed": [
                    2025,
                    10,
                    8,
                    1,
                    26,
                    48,
                    2,
                    281,
                    0
                ],
                "title": "Iterative LLM-Based Generation and Refinement of Distracting Conditions\n  in Math Word Problems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iterative LLM-Based Generation and Refinement of Distracting Conditions\n  in Math Word Problems"
                },
                "summary": "Mathematical reasoning serves as a crucial testbed for the intelligence of\nlarge language models (LLMs), and math word problems (MWPs) are a popular type\nof math problems. Most MWP datasets consist of problems containing only the\nnecessary information, while problems with distracting and excessive conditions\nare often overlooked. Prior works have tested popular LLMs and found a dramatic\nperformance drop in the presence of distracting conditions. However, datasets\nof MWPs with distracting conditions are limited, and most suffer from lower\nlevels of difficulty and out-of-context expressions. This makes distracting\nconditions easy to identify and exclude, thus reducing the credibility of\nbenchmarking on them. Moreover, when adding distracting conditions, the\nreasoning and answers may also change, requiring intensive labor to check and\nwrite the solutions. To address these issues, we design an iterative framework\nto generate distracting conditions using LLMs. We develop a set of prompts to\nrevise MWPs from different perspectives and cognitive levels, encouraging the\ngeneration of distracting conditions as well as suggestions for further\nrevision. Another advantage is the shared solutions between original and\nrevised problems: we explicitly guide the LLMs to generate distracting\nconditions that do not alter the original solutions, thus avoiding the need to\ngenerate new solutions. This framework is efficient and easy to deploy,\nreducing the overhead of generating MWPs with distracting conditions while\nmaintaining data quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mathematical reasoning serves as a crucial testbed for the intelligence of\nlarge language models (LLMs), and math word problems (MWPs) are a popular type\nof math problems. Most MWP datasets consist of problems containing only the\nnecessary information, while problems with distracting and excessive conditions\nare often overlooked. Prior works have tested popular LLMs and found a dramatic\nperformance drop in the presence of distracting conditions. However, datasets\nof MWPs with distracting conditions are limited, and most suffer from lower\nlevels of difficulty and out-of-context expressions. This makes distracting\nconditions easy to identify and exclude, thus reducing the credibility of\nbenchmarking on them. Moreover, when adding distracting conditions, the\nreasoning and answers may also change, requiring intensive labor to check and\nwrite the solutions. To address these issues, we design an iterative framework\nto generate distracting conditions using LLMs. We develop a set of prompts to\nrevise MWPs from different perspectives and cognitive levels, encouraging the\ngeneration of distracting conditions as well as suggestions for further\nrevision. Another advantage is the shared solutions between original and\nrevised problems: we explicitly guide the LLMs to generate distracting\nconditions that do not alter the original solutions, thus avoiding the need to\ngenerate new solutions. This framework is efficient and easy to deploy,\nreducing the overhead of generating MWPs with distracting conditions while\nmaintaining data quality."
                },
                "authors": [
                    {
                        "name": "Kaiqi Yang"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Yucheng Chu"
                    },
                    {
                        "name": "Zitao Liu"
                    },
                    {
                        "name": "Mi Tian"
                    },
                    {
                        "name": "Hui Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hui Liu"
                },
                "author": "Hui Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08615v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08615v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15075v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15075v3",
                "updated": "2025-10-15T16:03:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    16,
                    3,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-02-20T22:24:27Z",
                "published_parsed": [
                    2025,
                    2,
                    20,
                    22,
                    24,
                    27,
                    3,
                    51,
                    0
                ],
                "title": "Quantize What Counts: More for Keys, Less for Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize What Counts: More for Keys, Less for Values"
                },
                "summary": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) suffer inference-time memory bottlenecks\ndominated by the attention Key-Value (KV) cache, which scales with model size\nand context length. While KV-cache quantization alleviates this cost, bit\nallocation between keys and values is often tuned heuristically, lacking\ntheoretical grounding and generalizability. This paper proposes two theorems\nthat anchor mixed-precision KV quantization in the intrinsic geometry of\nTransformer models. First, key projections systematically have larger spectral\nand Frobenius norms than value matrices, implying higher information density\nalong the key path. Second, for any given memory budget, prioritizing precision\nfor keys over values strictly reduces quantization error and better preserves\naccuracy. Empirical evaluations across various prominent LLMs and benchmarks\nshow that key-favored allocations (e.g., 4-bit keys, 2-bit values) retain up to\n98.3\\% accuracy compared to uniform allocations (e.g., 4-bit for both), while\nconserving memory. These results transform bit allocation from ad hoc tuning\ninto a theoretically grounded, geometry-driven design principle for efficient\nLLM inference. Source code is available at\nhttps://github.com/mohsenhariri/spectral-kv."
                },
                "authors": [
                    {
                        "name": "Mohsen Hariri"
                    },
                    {
                        "name": "Alan Luo"
                    },
                    {
                        "name": "Weicong Chen"
                    },
                    {
                        "name": "Shaochen Zhong"
                    },
                    {
                        "name": "Tianyi Zhang"
                    },
                    {
                        "name": "Qifan Wang"
                    },
                    {
                        "name": "Xia Hu"
                    },
                    {
                        "name": "Xiaotian Han"
                    },
                    {
                        "name": "Vipin Chaudhary"
                    }
                ],
                "author_detail": {
                    "name": "Vipin Chaudhary"
                },
                "author": "Vipin Chaudhary",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15075v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15075v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14556v2",
                "updated": "2025-10-15T15:59:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    59,
                    15,
                    2,
                    288,
                    0
                ],
                "published": "2025-04-20T10:05:07Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    10,
                    5,
                    7,
                    6,
                    110,
                    0
                ],
                "title": "LLM-Enabled In-Context Learning for Data Collection Scheduling in\n  UAV-assisted Sensor Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Enabled In-Context Learning for Data Collection Scheduling in\n  UAV-assisted Sensor Networks"
                },
                "summary": "Unmanned Aerial Vehicles (UAVs) are increasingly being utilized in various\nprivate and commercial applications, e.g., traffic control, parcel delivery,\nand Search and Rescue (SAR) missions. Machine Learning (ML) methods used in\nUAV-Assisted Sensor Networks (UASNETs) and, especially, in Deep Reinforcement\nLearning (DRL) face challenges such as complex and lengthy model training, gaps\nbetween simulation and reality, and low sampling efficiency, which conflict\nwith the urgency of emergencies, such as SAR missions. In this paper, an\nIn-Context Learning (ICL)-Data Collection Scheduling (ICLDC) system is proposed\nas an alternative to DRL in emergencies. The UAV collects sensory data and\ntransmits it to a Large Language Model (LLM), which creates a task description\nin natural language. From this description, the UAV receives a data collection\nschedule that must be executed. A verifier ensures safe UAV operations by\nevaluating the schedules generated by the LLM and overriding unsafe schedules\nbased on predefined rules. The system continuously adapts by incorporating\nfeedback into the task descriptions and using this for future decisions. This\nmethod is tested against jailbreaking attacks, where the task description is\nmanipulated to undermine network performance, highlighting the vulnerability of\nLLMs to such attacks. The proposed ICLDC significantly reduces cumulative\npacket loss compared to both the DQN and Maximum Channel Gain baselines. ICLDC\npresents a promising direction for intelligent scheduling and control in\nUASNETs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmanned Aerial Vehicles (UAVs) are increasingly being utilized in various\nprivate and commercial applications, e.g., traffic control, parcel delivery,\nand Search and Rescue (SAR) missions. Machine Learning (ML) methods used in\nUAV-Assisted Sensor Networks (UASNETs) and, especially, in Deep Reinforcement\nLearning (DRL) face challenges such as complex and lengthy model training, gaps\nbetween simulation and reality, and low sampling efficiency, which conflict\nwith the urgency of emergencies, such as SAR missions. In this paper, an\nIn-Context Learning (ICL)-Data Collection Scheduling (ICLDC) system is proposed\nas an alternative to DRL in emergencies. The UAV collects sensory data and\ntransmits it to a Large Language Model (LLM), which creates a task description\nin natural language. From this description, the UAV receives a data collection\nschedule that must be executed. A verifier ensures safe UAV operations by\nevaluating the schedules generated by the LLM and overriding unsafe schedules\nbased on predefined rules. The system continuously adapts by incorporating\nfeedback into the task descriptions and using this for future decisions. This\nmethod is tested against jailbreaking attacks, where the task description is\nmanipulated to undermine network performance, highlighting the vulnerability of\nLLMs to such attacks. The proposed ICLDC significantly reduces cumulative\npacket loss compared to both the DQN and Maximum Channel Gain baselines. ICLDC\npresents a promising direction for intelligent scheduling and control in\nUASNETs."
                },
                "authors": [
                    {
                        "name": "Yousef Emami"
                    },
                    {
                        "name": "Hao Zhou"
                    },
                    {
                        "name": "SeyedSina Nabavirazani"
                    },
                    {
                        "name": "Luis Almeida"
                    }
                ],
                "author_detail": {
                    "name": "Luis Almeida"
                },
                "author": "Luis Almeida",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13694v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13694v1",
                "updated": "2025-10-15T15:51:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    51,
                    59,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:51:59Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    51,
                    59,
                    2,
                    288,
                    0
                ],
                "title": "Information-Theoretic Reward Modeling for Stable RLHF: Detecting and\n  Mitigating Reward Hacking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Information-Theoretic Reward Modeling for Stable RLHF: Detecting and\n  Mitigating Reward Hacking"
                },
                "summary": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in\naligning language models with human values, reward hacking-or reward\nover-optimization-remains a major challenge. We identify two key obstacles to\nits mitigation: (1) reward misgeneralization in reward modeling, where reward\nmodels overfit to spurious, preference-irrelevant features; and (2) the lack of\nsuitable regularization during RL optimization, as existing token-level\nconstraints often over-restrict the policy space. To address these issues, we\npropose InfoRM, an information-theoretic reward modeling framework based on the\nInformation Bottleneck (IB) principle, which filters out preference-irrelevant\ninformation to alleviate reward misgeneralization. We further observe that\nreward-hacked responses manifest as pronounced outliers in InfoRM's IB latent\nspace, measured by Mahalanobis distance from the SFT-induced distribution.\nMotivated by this, we introduce IBL, a distribution-level regularization that\npenalizes such deviations, effectively expanding the optimization landscape\nwhile maintaining alignment. We prove that IBL is theoretically equivalent to\nthe pessimistic RL objective within the IB latent space. Finally, we present\nMahalanobis Outlier Probability (MOP), a statistical metric for quantifying\nreward hacking severity, enabling principled hyperparameter tuning and online\nmitigation such as early stopping. Extensive experiments across diverse LLMs\nand datasets confirm the generality of our findings, the effectiveness of\nInfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively\nadvancing the state of RLHF.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the success of Reinforcement Learning from Human Feedback (RLHF) in\naligning language models with human values, reward hacking-or reward\nover-optimization-remains a major challenge. We identify two key obstacles to\nits mitigation: (1) reward misgeneralization in reward modeling, where reward\nmodels overfit to spurious, preference-irrelevant features; and (2) the lack of\nsuitable regularization during RL optimization, as existing token-level\nconstraints often over-restrict the policy space. To address these issues, we\npropose InfoRM, an information-theoretic reward modeling framework based on the\nInformation Bottleneck (IB) principle, which filters out preference-irrelevant\ninformation to alleviate reward misgeneralization. We further observe that\nreward-hacked responses manifest as pronounced outliers in InfoRM's IB latent\nspace, measured by Mahalanobis distance from the SFT-induced distribution.\nMotivated by this, we introduce IBL, a distribution-level regularization that\npenalizes such deviations, effectively expanding the optimization landscape\nwhile maintaining alignment. We prove that IBL is theoretically equivalent to\nthe pessimistic RL objective within the IB latent space. Finally, we present\nMahalanobis Outlier Probability (MOP), a statistical metric for quantifying\nreward hacking severity, enabling principled hyperparameter tuning and online\nmitigation such as early stopping. Extensive experiments across diverse LLMs\nand datasets confirm the generality of our findings, the effectiveness of\nInfoRM and IBL, and the reliability of MOP as a diagnostic tool-collectively\nadvancing the state of RLHF."
                },
                "authors": [
                    {
                        "name": "Yuchun Miao"
                    },
                    {
                        "name": "Liang Ding"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Rong Bao"
                    },
                    {
                        "name": "Lefei Zhang"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "arxiv_comment": "46 pages, 36 figures, submitted to IEEE Transactions on Pattern\n  Analysis and Machine Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13694v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13694v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.13313v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.13313v2",
                "updated": "2025-10-15T15:51:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    51,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-16T17:57:22Z",
                "published_parsed": [
                    2025,
                    9,
                    16,
                    17,
                    57,
                    22,
                    1,
                    259,
                    0
                ],
                "title": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReSum: Unlocking Long-Horizon Search Intelligence via Context\n  Summarization"
                },
                "summary": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching solutions. To overcome this challenge,\nwe introduce ReSum, a novel paradigm that enables indefinite exploration\nthrough periodic context summarization. ReSum converts growing interaction\nhistories into compact reasoning states, maintaining awareness of prior\ndiscoveries while bypassing context constraints. For paradigm adaptation, we\npropose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents across three benchmarks\ndemonstrate that ReSum delivers an average absolute improvement of 4.5% over\nReAct, with further gains of 8.2% following ReSum-GRPO training. Notably, with\nonly 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of\nWebSailor-30B) achieves 33.3% Pass@1 on BrowseComp-zh and 18.3% on\nBrowseComp-en, surpassing most open-source web agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM)-based web agents demonstrate strong performance on\nknowledge-intensive tasks but are hindered by context window limitations in\nparadigms like ReAct. Complex queries involving multiple entities, intertwined\nrelationships, and high uncertainty demand extensive search cycles that rapidly\nexhaust context budgets before reaching solutions. To overcome this challenge,\nwe introduce ReSum, a novel paradigm that enables indefinite exploration\nthrough periodic context summarization. ReSum converts growing interaction\nhistories into compact reasoning states, maintaining awareness of prior\ndiscoveries while bypassing context constraints. For paradigm adaptation, we\npropose ReSum-GRPO, integrating GRPO with segmented trajectory training and\nadvantage broadcasting to familiarize agents with summary-conditioned\nreasoning. Extensive experiments on web agents across three benchmarks\ndemonstrate that ReSum delivers an average absolute improvement of 4.5% over\nReAct, with further gains of 8.2% following ReSum-GRPO training. Notably, with\nonly 1K training samples, our WebResummer-30B (a ReSum-GRPO-trained version of\nWebSailor-30B) achieves 33.3% Pass@1 on BrowseComp-zh and 18.3% on\nBrowseComp-en, surpassing most open-source web agents."
                },
                "authors": [
                    {
                        "name": "Xixi Wu"
                    },
                    {
                        "name": "Kuan Li"
                    },
                    {
                        "name": "Yida Zhao"
                    },
                    {
                        "name": "Liwen Zhang"
                    },
                    {
                        "name": "Litu Ou"
                    },
                    {
                        "name": "Huifeng Yin"
                    },
                    {
                        "name": "Zhongwang Zhang"
                    },
                    {
                        "name": "Xinmiao Yu"
                    },
                    {
                        "name": "Dingchu Zhang"
                    },
                    {
                        "name": "Yong Jiang"
                    },
                    {
                        "name": "Pengjun Xie"
                    },
                    {
                        "name": "Fei Huang"
                    },
                    {
                        "name": "Minhao Cheng"
                    },
                    {
                        "name": "Shuai Wang"
                    },
                    {
                        "name": "Hong Cheng"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "arxiv_comment": "https://tongyi-agent.github.io/blog/introducing-tongyi-deep-research/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.13313v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.13313v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13681v1",
                "updated": "2025-10-15T15:36:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    36,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:36:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    36,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "How Sampling Affects the Detectability of Machine-written texts: A\n  Comprehensive Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How Sampling Affects the Detectability of Machine-written texts: A\n  Comprehensive Study"
                },
                "summary": "As texts generated by Large Language Models (LLMs) are ever more common and\noften indistinguishable from human-written content, research on automatic text\ndetection has attracted growing attention. Many recent detectors report\nnear-perfect accuracy, often boasting AUROC scores above 99\\%. However, these\nclaims typically assume fixed generation settings, leaving open the question of\nhow robust such systems are to changes in decoding strategies. In this work, we\nsystematically examine how sampling-based decoding impacts detectability, with\na focus on how subtle variations in a model's (sub)word-level distribution\naffect detection performance. We find that even minor adjustments to decoding\nparameters - such as temperature, top-p, or nucleus sampling - can severely\nimpair detector accuracy, with AUROC dropping from near-perfect levels to 1\\%\nin some settings. Our findings expose critical blind spots in current detection\nmethods and emphasize the need for more comprehensive evaluation protocols. To\nfacilitate future research, we release a large-scale dataset encompassing 37\ndecoding configurations, along with our code and evaluation framework\nhttps://github.com/BaggerOfWords/Sampling-and-Detection",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As texts generated by Large Language Models (LLMs) are ever more common and\noften indistinguishable from human-written content, research on automatic text\ndetection has attracted growing attention. Many recent detectors report\nnear-perfect accuracy, often boasting AUROC scores above 99\\%. However, these\nclaims typically assume fixed generation settings, leaving open the question of\nhow robust such systems are to changes in decoding strategies. In this work, we\nsystematically examine how sampling-based decoding impacts detectability, with\na focus on how subtle variations in a model's (sub)word-level distribution\naffect detection performance. We find that even minor adjustments to decoding\nparameters - such as temperature, top-p, or nucleus sampling - can severely\nimpair detector accuracy, with AUROC dropping from near-perfect levels to 1\\%\nin some settings. Our findings expose critical blind spots in current detection\nmethods and emphasize the need for more comprehensive evaluation protocols. To\nfacilitate future research, we release a large-scale dataset encompassing 37\ndecoding configurations, along with our code and evaluation framework\nhttps://github.com/BaggerOfWords/Sampling-and-Detection"
                },
                "authors": [
                    {
                        "name": "Matthieu Dubois"
                    },
                    {
                        "name": "Fran√ßois Yvon"
                    },
                    {
                        "name": "Pablo Piantanida"
                    }
                ],
                "author_detail": {
                    "name": "Pablo Piantanida"
                },
                "author": "Pablo Piantanida",
                "arxiv_comment": "EMNLP 2025 Findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.09558v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.09558v2",
                "updated": "2025-10-15T15:32:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    32,
                    50,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-10T17:08:36Z",
                "published_parsed": [
                    2025,
                    10,
                    10,
                    17,
                    8,
                    36,
                    4,
                    283,
                    0
                ],
                "title": "AutoPR: Let's Automate Your Academic Promotion!",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPR: Let's Automate Your Academic Promotion!"
                },
                "summary": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the volume of peer-reviewed research surges, scholars increasingly rely on\nsocial platforms for discovery, while authors invest considerable effort in\npromoting their work to ensure visibility and citations. To streamline this\nprocess and reduce the reliance on human effort, we introduce Automatic\nPromotion (AutoPR), a novel task that transforms research papers into accurate,\nengaging, and timely public content. To enable rigorous evaluation, we release\nPRBench, a multimodal benchmark that links 512 peer-reviewed articles to\nhigh-quality promotional posts, assessing systems along three axes: Fidelity\n(accuracy and tone), Engagement (audience targeting and appeal), and Alignment\n(timing and channel optimization). We also introduce PRAgent, a multi-agent\nframework that automates AutoPR in three stages: content extraction with\nmultimodal preparation, collaborative synthesis for polished outputs, and\nplatform-specific adaptation to optimize norms, tone, and tagging for maximum\nreach. When compared to direct LLM pipelines on PRBench, PRAgent demonstrates\nsubstantial improvements, including a 604% increase in total watch time, a 438%\nrise in likes, and at least a 2.9x boost in overall engagement. Ablation\nstudies show that platform modeling and targeted promotion contribute the most\nto these gains. Our results position AutoPR as a tractable, measurable research\nproblem and provide a roadmap for scalable, impactful automated scholarly\ncommunication."
                },
                "authors": [
                    {
                        "name": "Qiguang Chen"
                    },
                    {
                        "name": "Zheng Yan"
                    },
                    {
                        "name": "Mingda Yang"
                    },
                    {
                        "name": "Libo Qin"
                    },
                    {
                        "name": "Yixin Yuan"
                    },
                    {
                        "name": "Hanjing Li"
                    },
                    {
                        "name": "Jinhao Liu"
                    },
                    {
                        "name": "Yiyan Ji"
                    },
                    {
                        "name": "Dengyun Peng"
                    },
                    {
                        "name": "Jiannan Guan"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Yantao Du"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "Preprint. Code: https://github.com/LightChen233/AutoPR . Benchmark:\n  https://huggingface.co/datasets/yzweak/PRBench",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.09558v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.09558v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13668v1",
                "updated": "2025-10-15T15:29:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    29,
                    8,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:29:08Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    29,
                    8,
                    2,
                    288,
                    0
                ],
                "title": "Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Rescheduling in Prefill-Decode Disaggregated LLM Inference"
                },
                "summary": "Large Language Model (LLM) inference has emerged as a fundamental paradigm.\nIn real-world scenarios, variations in output length cause severe workload\nimbalance in the decode phase, particularly for long-output reasoning tasks.\nExisting systems, such as PD disaggregation architectures, rely on static\nprefill-to-decode scheduling, which often results in SLO violations and OOM\nfailures under evolving decode workloads.\n  In this paper, we propose ARES, an adaptive decoding rescheduling system\npowered by length prediction to anticipate future workloads. Our core\ncontributions include: (1) A lightweight and continuous LLM-native prediction\nmethod that leverages LLM hidden state to model remaining generation length\nwith high precision (reducing MAE by 49.42%) and low overhead (cutting\npredictor parameters by 93.28%); (2) A rescheduling solution in decode phase\nwith : A dynamic balancing mechanism that integrates current and predicted\nworkloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher\ngoodput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference has emerged as a fundamental paradigm.\nIn real-world scenarios, variations in output length cause severe workload\nimbalance in the decode phase, particularly for long-output reasoning tasks.\nExisting systems, such as PD disaggregation architectures, rely on static\nprefill-to-decode scheduling, which often results in SLO violations and OOM\nfailures under evolving decode workloads.\n  In this paper, we propose ARES, an adaptive decoding rescheduling system\npowered by length prediction to anticipate future workloads. Our core\ncontributions include: (1) A lightweight and continuous LLM-native prediction\nmethod that leverages LLM hidden state to model remaining generation length\nwith high precision (reducing MAE by 49.42%) and low overhead (cutting\npredictor parameters by 93.28%); (2) A rescheduling solution in decode phase\nwith : A dynamic balancing mechanism that integrates current and predicted\nworkloads, reducing P99 TPOT by 74.77% and achieving up to 2.24 times higher\ngoodput."
                },
                "authors": [
                    {
                        "name": "Zhibin Wang"
                    },
                    {
                        "name": "Zetao Hong"
                    },
                    {
                        "name": "Xue Li"
                    },
                    {
                        "name": "Zibo Wang"
                    },
                    {
                        "name": "Shipeng Li"
                    },
                    {
                        "name": "Qingkai Meng"
                    },
                    {
                        "name": "Qing Wang"
                    },
                    {
                        "name": "Chengying Huan"
                    },
                    {
                        "name": "Rong Gu"
                    },
                    {
                        "name": "Sheng Zhong"
                    },
                    {
                        "name": "Chen Tian"
                    }
                ],
                "author_detail": {
                    "name": "Chen Tian"
                },
                "author": "Chen Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19234v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19234v2",
                "updated": "2025-10-15T15:21:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    21,
                    41,
                    2,
                    288,
                    0
                ],
                "published": "2025-05-25T17:15:55Z",
                "published_parsed": [
                    2025,
                    5,
                    25,
                    17,
                    15,
                    55,
                    6,
                    145,
                    0
                ],
                "title": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal\n  Graph Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUARDIAN: Safeguarding LLM Multi-Agent Collaborations with Temporal\n  Graph Modeling"
                },
                "summary": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration faces critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization. The code is\navailable at https://github.com/JialongZhou666/GUARDIAN",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) enables the development of\nintelligent agents capable of engaging in complex and multi-turn dialogues.\nHowever, multi-agent collaboration faces critical safety challenges, such as\nhallucination amplification and error injection and propagation. This paper\npresents GUARDIAN, a unified method for detecting and mitigating multiple\nsafety concerns in GUARDing Intelligent Agent collaboratioNs. By modeling the\nmulti-agent collaboration process as a discrete-time temporal attributed graph,\nGUARDIAN explicitly captures the propagation dynamics of hallucinations and\nerrors. The unsupervised encoder-decoder architecture incorporating an\nincremental training paradigm learns to reconstruct node attributes and graph\nstructures from latent embeddings, enabling the identification of anomalous\nnodes and edges with unparalleled precision. Moreover, we introduce a graph\nabstraction mechanism based on the Information Bottleneck Theory, which\ncompresses temporal interaction graphs while preserving essential patterns.\nExtensive experiments demonstrate GUARDIAN's effectiveness in safeguarding LLM\nmulti-agent collaborations against diverse safety vulnerabilities, achieving\nstate-of-the-art accuracy with efficient resource utilization. The code is\navailable at https://github.com/JialongZhou666/GUARDIAN"
                },
                "authors": [
                    {
                        "name": "Jialong Zhou"
                    },
                    {
                        "name": "Lichao Wang"
                    },
                    {
                        "name": "Xiao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Yang"
                },
                "author": "Xiao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19234v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19234v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13654v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13654v1",
                "updated": "2025-10-15T15:15:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    15,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:15:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    15,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Time Series Foundation Models: Benchmarking Challenges and Requirements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Foundation Models: Benchmarking Challenges and Requirements"
                },
                "summary": "Time Series Foundation Models (TSFMs) represent a new paradigm for time\nseries forecasting, offering zero-shot forecasting capabilities without the\nneed for domain-specific pre-training or fine-tuning. However, as with Large\nLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive\ntraining sets, it becomes more and more challenging to ensure the integrity of\nbenchmarking data. Our investigation of existing TSFM evaluation highlights\nmultiple challenges, ranging from the representativeness of the benchmark\ndatasets, over the lack of spatiotemporal evaluation, to risks of information\nleakage due to overlapping and obscure datasets, and the memorization of global\npatterns caused by external shocks like economic crises or pandemics. Our\nfindings reveal widespread confusion regarding data partitions, risking\ninflated performance estimates and incorrect transfer of global knowledge to\nlocal time series. We argue for the development of robust evaluation\nmethodologies to prevent pitfalls already observed in LLM and classical time\nseries benchmarking, and call upon the research community to design new,\nprincipled approaches, such as evaluations on truly out-of-sample future data,\nto safeguard the integrity of TSFM assessment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Series Foundation Models (TSFMs) represent a new paradigm for time\nseries forecasting, offering zero-shot forecasting capabilities without the\nneed for domain-specific pre-training or fine-tuning. However, as with Large\nLanguage Models (LLMs), evaluating TSFMs is tricky, as with ever more extensive\ntraining sets, it becomes more and more challenging to ensure the integrity of\nbenchmarking data. Our investigation of existing TSFM evaluation highlights\nmultiple challenges, ranging from the representativeness of the benchmark\ndatasets, over the lack of spatiotemporal evaluation, to risks of information\nleakage due to overlapping and obscure datasets, and the memorization of global\npatterns caused by external shocks like economic crises or pandemics. Our\nfindings reveal widespread confusion regarding data partitions, risking\ninflated performance estimates and incorrect transfer of global knowledge to\nlocal time series. We argue for the development of robust evaluation\nmethodologies to prevent pitfalls already observed in LLM and classical time\nseries benchmarking, and call upon the research community to design new,\nprincipled approaches, such as evaluations on truly out-of-sample future data,\nto safeguard the integrity of TSFM assessment."
                },
                "authors": [
                    {
                        "name": "Marcel Meyer"
                    },
                    {
                        "name": "Sascha Kaltenpoth"
                    },
                    {
                        "name": "Kevin Zalipski"
                    },
                    {
                        "name": "Oliver M√ºller"
                    }
                ],
                "author_detail": {
                    "name": "Oliver M√ºller"
                },
                "author": "Oliver M√ºller",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13654v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13654v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.14755v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.14755v2",
                "updated": "2025-10-15T15:13:01Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    13,
                    1,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-20T14:58:05Z",
                "published_parsed": [
                    2025,
                    8,
                    20,
                    14,
                    58,
                    5,
                    2,
                    232,
                    0
                ],
                "title": "Reliable generation of isomorphic physics problems using Generative AI\n  with prompt-chaining and tool use",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable generation of isomorphic physics problems using Generative AI\n  with prompt-chaining and tool use"
                },
                "summary": "We present a method for generating large numbers of isomorphic physics\nproblems using generative AI services such as ChatGPT, through prompt chaining\nand tool use. This approach enables precise control over structural\nvariations-such as numeric values and spatial relations-while supporting\ndiverse contextual variations in the problem body. By utilizing the Python code\ninterpreter, the method supports automatic solution validation and simple\ndiagram generation, addressing key limitations in existing LLM-based methods.\nWe generated two example isomorphic problem banks and compared the outcome\nagainst two simpler prompt-based approaches. Results show that prompt-chaining\nproduces significantly higher quality and more consistent outputs than simpler,\nnon-chaining prompts. We also show that GenAI services can be used to validate\nthe quality of the generated isomorphic problems. This work demonstrates a\npromising method for efficient and scalable problem creation accessible to the\naverage instructor, which opens new possibilities for personalized adaptive\ntesting and automated content development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a method for generating large numbers of isomorphic physics\nproblems using generative AI services such as ChatGPT, through prompt chaining\nand tool use. This approach enables precise control over structural\nvariations-such as numeric values and spatial relations-while supporting\ndiverse contextual variations in the problem body. By utilizing the Python code\ninterpreter, the method supports automatic solution validation and simple\ndiagram generation, addressing key limitations in existing LLM-based methods.\nWe generated two example isomorphic problem banks and compared the outcome\nagainst two simpler prompt-based approaches. Results show that prompt-chaining\nproduces significantly higher quality and more consistent outputs than simpler,\nnon-chaining prompts. We also show that GenAI services can be used to validate\nthe quality of the generated isomorphic problems. This work demonstrates a\npromising method for efficient and scalable problem creation accessible to the\naverage instructor, which opens new possibilities for personalized adaptive\ntesting and automated content development."
                },
                "authors": [
                    {
                        "name": "Zhongzhou Chen"
                    }
                ],
                "author_detail": {
                    "name": "Zhongzhou Chen"
                },
                "author": "Zhongzhou Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.14755v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.14755v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ed-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ed-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13643v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13643v1",
                "updated": "2025-10-15T15:06:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    6,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T15:06:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    6,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "Towards Adversarial Robustness and Uncertainty Quantification in\n  DINOv2-based Few-Shot Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Adversarial Robustness and Uncertainty Quantification in\n  DINOv2-based Few-Shot Anomaly Detection"
                },
                "summary": "Foundation models such as DINOv2 have shown strong performance in few-shot\nanomaly detection, yet two key questions remain unexamined: (i) how susceptible\nare these detectors to adversarial perturbations; and (ii) how well do their\nanomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a\ntraining-free deep nearest-neighbor detector over DINOv2 features, we present\none of the first systematic studies of adversarial attacks and uncertainty\nestimation in this setting. To enable white-box gradient attacks while\npreserving test-time behavior, we attach a lightweight linear head to frozen\nDINOv2 features only for crafting perturbations. Using this heuristic, we\nevaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe\nconsistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible\nperturbations can flip nearest-neighbor relations in feature space to induce\nconfident misclassification. Complementing robustness, we probe reliability and\nfind that raw anomaly scores are poorly calibrated, revealing a gap between\nconfidence and correctness that limits safety-critical use. As a simple, strong\nbaseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly\nscores for uncertainty estimation. The resulting calibrated posteriors yield\nsignificantly higher predictive entropy on adversarially perturbed inputs than\non clean ones, enabling a practical flagging mechanism for attack detection\nwhile reducing calibration error (ECE). Our findings surface concrete\nvulnerabilities in DINOv2-based few-shot anomaly detectors and establish an\nevaluation protocol and baseline for robust, uncertainty-aware anomaly\ndetection. We argue that adversarial robustness and principled uncertainty\nquantification are not optional add-ons but essential capabilities if anomaly\ndetection systems are to be trustworthy and ready for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models such as DINOv2 have shown strong performance in few-shot\nanomaly detection, yet two key questions remain unexamined: (i) how susceptible\nare these detectors to adversarial perturbations; and (ii) how well do their\nanomaly scores reflect calibrated uncertainty? Building on AnomalyDINO, a\ntraining-free deep nearest-neighbor detector over DINOv2 features, we present\none of the first systematic studies of adversarial attacks and uncertainty\nestimation in this setting. To enable white-box gradient attacks while\npreserving test-time behavior, we attach a lightweight linear head to frozen\nDINOv2 features only for crafting perturbations. Using this heuristic, we\nevaluate the impact of FGSM across the MVTec-AD and VisA datasets and observe\nconsistent drops in F1, AUROC, AP, and G-mean, indicating that imperceptible\nperturbations can flip nearest-neighbor relations in feature space to induce\nconfident misclassification. Complementing robustness, we probe reliability and\nfind that raw anomaly scores are poorly calibrated, revealing a gap between\nconfidence and correctness that limits safety-critical use. As a simple, strong\nbaseline toward trustworthiness, we apply post-hoc Platt scaling to the anomaly\nscores for uncertainty estimation. The resulting calibrated posteriors yield\nsignificantly higher predictive entropy on adversarially perturbed inputs than\non clean ones, enabling a practical flagging mechanism for attack detection\nwhile reducing calibration error (ECE). Our findings surface concrete\nvulnerabilities in DINOv2-based few-shot anomaly detectors and establish an\nevaluation protocol and baseline for robust, uncertainty-aware anomaly\ndetection. We argue that adversarial robustness and principled uncertainty\nquantification are not optional add-ons but essential capabilities if anomaly\ndetection systems are to be trustworthy and ready for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Akib Mohammed Khan"
                    },
                    {
                        "name": "Bartosz Krawczyk"
                    }
                ],
                "author_detail": {
                    "name": "Bartosz Krawczyk"
                },
                "author": "Bartosz Krawczyk",
                "arxiv_comment": "10 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13643v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13643v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.20245v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.20245v5",
                "updated": "2025-10-15T15:02:20Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    15,
                    2,
                    20,
                    2,
                    288,
                    0
                ],
                "published": "2024-09-30T12:34:04Z",
                "published_parsed": [
                    2024,
                    9,
                    30,
                    12,
                    34,
                    4,
                    0,
                    274,
                    0
                ],
                "title": "A Framework for Holistic KLD-based Waveform Design for\n  Multi-User-Multi-Target ISAC Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Framework for Holistic KLD-based Waveform Design for\n  Multi-User-Multi-Target ISAC Systems"
                },
                "summary": "This paper introduces a novel framework aimed at designing integrated\nwaveforms for robust integrated sensing and communication (ISAC) systems. The\nsystem model consists of a multiple-input multiple-output (MIMO) base station\nthat simultaneously serves communication user equipments (UEs) and detects\nmultiple targets using a shared-antenna deployment scenario. By leveraging\nKullback-Leibler divergence (KLD) to holistically characterise both\ncommunication and sensing subsystems, three optimisation problems are\nformulated: (i) radar waveform KLD maximisation under communication\nconstraints, (ii) communication waveform KLD maximisation subject to radar KLD\nrequirements, and (iii) an integrated waveform KLD-based optimisation for ISAC\nthat jointly balances both subsystems. The first two problems are solved using\na projected gradient method with adaptive penalties for the radar waveforms and\na gradient-assisted interior point method (IPM) for the communication\nwaveforms. The third, integrated waveform optimisation approach adopts an\nalternating direction method of multipliers (ADMM) framework to unify radar and\ncommunication waveform designs into a single integrated optimisation, thereby\nsynergising sensing and communication objectives and achieving higher overall\nperformance than either radar- or communication-only techniques. Unlike most\nexisting ISAC waveform designs that regard communication signals solely as\ninterference for sensing, the proposed framework utilises the holistic ISAC\nwaveform-that is, the superimposed communication and sensing signals-to boost\ndetection performance in the radar subsystem. Simulation results show\nsignificant improvements in both radar detection and communication reliability\ncompared with conventional zero-forcing beamforming, identity-covariance radar\nbaselines, and traditional optimisation approaches,..",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces a novel framework aimed at designing integrated\nwaveforms for robust integrated sensing and communication (ISAC) systems. The\nsystem model consists of a multiple-input multiple-output (MIMO) base station\nthat simultaneously serves communication user equipments (UEs) and detects\nmultiple targets using a shared-antenna deployment scenario. By leveraging\nKullback-Leibler divergence (KLD) to holistically characterise both\ncommunication and sensing subsystems, three optimisation problems are\nformulated: (i) radar waveform KLD maximisation under communication\nconstraints, (ii) communication waveform KLD maximisation subject to radar KLD\nrequirements, and (iii) an integrated waveform KLD-based optimisation for ISAC\nthat jointly balances both subsystems. The first two problems are solved using\na projected gradient method with adaptive penalties for the radar waveforms and\na gradient-assisted interior point method (IPM) for the communication\nwaveforms. The third, integrated waveform optimisation approach adopts an\nalternating direction method of multipliers (ADMM) framework to unify radar and\ncommunication waveform designs into a single integrated optimisation, thereby\nsynergising sensing and communication objectives and achieving higher overall\nperformance than either radar- or communication-only techniques. Unlike most\nexisting ISAC waveform designs that regard communication signals solely as\ninterference for sensing, the proposed framework utilises the holistic ISAC\nwaveform-that is, the superimposed communication and sensing signals-to boost\ndetection performance in the radar subsystem. Simulation results show\nsignificant improvements in both radar detection and communication reliability\ncompared with conventional zero-forcing beamforming, identity-covariance radar\nbaselines, and traditional optimisation approaches,.."
                },
                "authors": [
                    {
                        "name": "Yousef Kloob"
                    },
                    {
                        "name": "Mohammad Al-Jarrah"
                    },
                    {
                        "name": "Emad Alsusa"
                    }
                ],
                "author_detail": {
                    "name": "Emad Alsusa"
                },
                "author": "Emad Alsusa",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.20245v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.20245v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13632v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13632v1",
                "updated": "2025-10-15T14:57:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    57,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:57:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    57,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "Closing the Gap Between Text and Speech Understanding in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closing the Gap Between Text and Speech Understanding in LLMs"
                },
                "summary": "Large Language Models (LLMs) can be adapted to extend their text capabilities\nto speech inputs. However, these speech-adapted LLMs consistently underperform\ntheir text-based counterparts--and even cascaded pipelines--on language\nunderstanding tasks. We term this shortfall the text-speech understanding gap:\nthe performance drop observed when a speech-adapted LLM processes spoken inputs\nrelative to when the original text-based LLM processes the equivalent text.\nRecent approaches to narrowing this gap either rely on large-scale speech\nsynthesis of text corpora, which is costly and heavily dependent on synthetic\ndata, or on large-scale proprietary speech datasets, which are not\nreproducible. As a result, there remains a need for more data-efficient\nalternatives for closing the text-speech understanding gap. In this work, we\nanalyze the gap as driven by two factors: (i) forgetting of text capabilities\nduring adaptation, and (ii) cross-modal misalignment between speech and text.\nBased on this analysis, we introduce SALAD--Sample-efficient Alignment with\nLearning through Active selection and cross-modal Distillation--which combines\ncross-modal distillation with targeted synthetic data to improve alignment\nwhile mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves\ncompetitive performance with a strong open-weight model across broad-domain\nbenchmarks in knowledge, language understanding, and reasoning, while training\non over an order of magnitude less speech data from public corpora.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can be adapted to extend their text capabilities\nto speech inputs. However, these speech-adapted LLMs consistently underperform\ntheir text-based counterparts--and even cascaded pipelines--on language\nunderstanding tasks. We term this shortfall the text-speech understanding gap:\nthe performance drop observed when a speech-adapted LLM processes spoken inputs\nrelative to when the original text-based LLM processes the equivalent text.\nRecent approaches to narrowing this gap either rely on large-scale speech\nsynthesis of text corpora, which is costly and heavily dependent on synthetic\ndata, or on large-scale proprietary speech datasets, which are not\nreproducible. As a result, there remains a need for more data-efficient\nalternatives for closing the text-speech understanding gap. In this work, we\nanalyze the gap as driven by two factors: (i) forgetting of text capabilities\nduring adaptation, and (ii) cross-modal misalignment between speech and text.\nBased on this analysis, we introduce SALAD--Sample-efficient Alignment with\nLearning through Active selection and cross-modal Distillation--which combines\ncross-modal distillation with targeted synthetic data to improve alignment\nwhile mitigating forgetting. Applied to 3B and 7B LLMs, SALAD achieves\ncompetitive performance with a strong open-weight model across broad-domain\nbenchmarks in knowledge, language understanding, and reasoning, while training\non over an order of magnitude less speech data from public corpora."
                },
                "authors": [
                    {
                        "name": "Santiago Cuervo"
                    },
                    {
                        "name": "Skyler Seto"
                    },
                    {
                        "name": "Maureen de Seyssel"
                    },
                    {
                        "name": "Richard He Bai"
                    },
                    {
                        "name": "Zijin Gu"
                    },
                    {
                        "name": "Tatiana Likhomanenko"
                    },
                    {
                        "name": "Navdeep Jaitly"
                    },
                    {
                        "name": "Zakaria Aldeneh"
                    }
                ],
                "author_detail": {
                    "name": "Zakaria Aldeneh"
                },
                "author": "Zakaria Aldeneh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13632v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13632v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13624v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13624v1",
                "updated": "2025-10-15T14:51:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    51,
                    28,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:51:28Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    51,
                    28,
                    2,
                    288,
                    0
                ],
                "title": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of\n  German Tumor Diagnoses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unlocking Public Catalogues: Instruction-Tuning LLMs for ICD Coding of\n  German Tumor Diagnoses"
                },
                "summary": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential\nfor structured cancer documentation in Germany. Smaller open-weight LLMs are\nappealing for privacy-preserving automation but often struggle with coding\naccuracy in German-language contexts. This study investigates whether\ninstruction-based fine-tuning on public datasets improves the coding accuracy\nof open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded\ndiagnoses from the local tumor documentation system as test data. In a\nsystematic data quality assessment, the upper limit for ICD-10 coding\nperformance was estimated at 60-79% for exact and 81-94% for partial\n(three-character codes only) derivation. As training data, over 500,000\nquestion-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS\ncatalogues. Eight open-weight models from the Qwen, Llama, and Mistral families\n(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to\n41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3\ntopography coding also improved but started and remained considerably lower\nwith an exact accuracy of 22-40% and a partial accuracy of 56-67% after\nfine-tuning. Malformed code outputs dropped to 0% for all models.\nTumor-diagnosis recognition reached 99%. Accuracy correlated positively with\nmodel size, but gaps between small and large models narrowed after fine-tuning.\nThe reasoning mode in Qwen3 generally yielded a lower performance than\nfine-tuning and was over 100 times slower. Our findings highlight the potential\nof leveraging public catalogues to build instruction datasets that improve LLMs\nin medical documentation tasks. The complete training dataset and the\nbest-performing checkpoints of the fine-tuned models are available from\nhttps://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate coding of tumor diagnoses with ICD-10-GM and ICD-O-3 is essential\nfor structured cancer documentation in Germany. Smaller open-weight LLMs are\nappealing for privacy-preserving automation but often struggle with coding\naccuracy in German-language contexts. This study investigates whether\ninstruction-based fine-tuning on public datasets improves the coding accuracy\nof open-weight LLMs for German tumor diagnosis texts. The evaluation uses coded\ndiagnoses from the local tumor documentation system as test data. In a\nsystematic data quality assessment, the upper limit for ICD-10 coding\nperformance was estimated at 60-79% for exact and 81-94% for partial\n(three-character codes only) derivation. As training data, over 500,000\nquestion-answer pairs were created based on the ICD-10-GM, ICD-O-3, and OPS\ncatalogues. Eight open-weight models from the Qwen, Llama, and Mistral families\n(7-70 B parameters) were fine-tuned. ICD-10-GM accuracy rose from 1.4-24% to\n41-58%, and partial accuracy from 31-74% to 73-83%. The accuracy of ICD-O-3\ntopography coding also improved but started and remained considerably lower\nwith an exact accuracy of 22-40% and a partial accuracy of 56-67% after\nfine-tuning. Malformed code outputs dropped to 0% for all models.\nTumor-diagnosis recognition reached 99%. Accuracy correlated positively with\nmodel size, but gaps between small and large models narrowed after fine-tuning.\nThe reasoning mode in Qwen3 generally yielded a lower performance than\nfine-tuning and was over 100 times slower. Our findings highlight the potential\nof leveraging public catalogues to build instruction datasets that improve LLMs\nin medical documentation tasks. The complete training dataset and the\nbest-performing checkpoints of the fine-tuned models are available from\nhttps://huggingface.co/datasets/stefan-m-lenz/ICDOPS-QA-2024."
                },
                "authors": [
                    {
                        "name": "Stefan Lenz"
                    },
                    {
                        "name": "Lakisha Ortiz Rosario"
                    },
                    {
                        "name": "Georg Vollmar"
                    },
                    {
                        "name": "Arsenij Ustjanzew"
                    },
                    {
                        "name": "Fatma Alickovic"
                    },
                    {
                        "name": "Thomas Kindler"
                    },
                    {
                        "name": "Torsten Panholzer"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Panholzer"
                },
                "author": "Torsten Panholzer",
                "arxiv_comment": "19 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13624v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13624v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2308.08728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2308.08728v2",
                "updated": "2025-10-15T14:49:35Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    49,
                    35,
                    2,
                    288,
                    0
                ],
                "published": "2023-08-17T01:58:04Z",
                "published_parsed": [
                    2023,
                    8,
                    17,
                    1,
                    58,
                    4,
                    3,
                    229,
                    0
                ],
                "title": "Translating Regulatory Clauses into Executable Codes for Building Design\n  Checking via Large Language Model Driven Function Matching and Composing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating Regulatory Clauses into Executable Codes for Building Design\n  Checking via Large Language Model Driven Function Matching and Composing"
                },
                "summary": "Translating clauses into executable code is a vital stage of automated rule\nchecking (ARC) and is essential for effective building design compliance\nchecking, particularly for rules with implicit properties or complex logic\nrequiring domain knowledge. Thus, by systematically analyzing building clauses,\n66 atomic functions are defined first to encapsulate common computational\nlogics. Then, LLM-FuncMapper is proposed, a large language model (LLM)-based\napproach with rule-based adaptive prompts that match clauses to atomic\nfunctions. Finally, executable code is generated by composing functions through\nthe LLMs. Experiments show LLM-FuncMapper outperforms fine-tuning methods by\n19% in function matching while significantly reducing manual annotation\nefforts. Case study demonstrates that LLM-FuncMapper can automatically compose\nmultiple atomic functions to generate executable code, boosting rule-checking\nefficiency. To our knowledge, this research represents the first application of\nLLMs for interpreting complex design clauses into executable code, which may\nshed light on further adoption of LLMs in the construction domain.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Translating clauses into executable code is a vital stage of automated rule\nchecking (ARC) and is essential for effective building design compliance\nchecking, particularly for rules with implicit properties or complex logic\nrequiring domain knowledge. Thus, by systematically analyzing building clauses,\n66 atomic functions are defined first to encapsulate common computational\nlogics. Then, LLM-FuncMapper is proposed, a large language model (LLM)-based\napproach with rule-based adaptive prompts that match clauses to atomic\nfunctions. Finally, executable code is generated by composing functions through\nthe LLMs. Experiments show LLM-FuncMapper outperforms fine-tuning methods by\n19% in function matching while significantly reducing manual annotation\nefforts. Case study demonstrates that LLM-FuncMapper can automatically compose\nmultiple atomic functions to generate executable code, boosting rule-checking\nefficiency. To our knowledge, this research represents the first application of\nLLMs for interpreting complex design clauses into executable code, which may\nshed light on further adoption of LLMs in the construction domain."
                },
                "authors": [
                    {
                        "name": "Zhe Zheng"
                    },
                    {
                        "name": "Jin Han"
                    },
                    {
                        "name": "Ke-Yin Chen"
                    },
                    {
                        "name": "Xin-Yu Cao"
                    },
                    {
                        "name": "Xin-Zheng Lu"
                    },
                    {
                        "name": "Jia-Rui Lin"
                    }
                ],
                "author_detail": {
                    "name": "Jia-Rui Lin"
                },
                "author": "Jia-Rui Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2308.08728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2308.08728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13614v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13614v1",
                "updated": "2025-10-15T14:43:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    43,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:43:31Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    43,
                    31,
                    2,
                    288,
                    0
                ],
                "title": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large\n  Language Model Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemoTime: Memory-Augmented Temporal Knowledge Graph Enhanced Large\n  Language Model Reasoning"
                },
                "summary": "Large Language Models (LLMs) have achieved impressive reasoning abilities,\nbut struggle with temporal understanding, especially when questions involve\nmultiple entities, compound operators, and evolving event sequences. Temporal\nKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in a\nstructured format, offer a reliable source for temporal reasoning. However,\nexisting TKG-based LLM reasoning methods still struggle with four major\nchallenges: maintaining temporal faithfulness in multi-hop reasoning, achieving\nmulti-entity temporal synchronization, adapting retrieval to diverse temporal\noperators, and reusing prior reasoning experience for stability and efficiency.\nTo address these issues, we propose MemoTime, a memory-augmented temporal\nknowledge graph framework that enhances LLM reasoning through structured\ngrounding, recursive reasoning, and continual experience learning. MemoTime\ndecomposes complex temporal questions into a hierarchical Tree of Time,\nenabling operator-aware reasoning that enforces monotonic timestamps and\nco-constrains multiple entities under unified temporal bounds. A dynamic\nevidence retrieval layer adaptively selects operator-specific retrieval\nstrategies, while a self-evolving experience memory stores verified reasoning\ntraces, toolkit decisions, and sub-question embeddings for cross-type reuse.\nComprehensive experiments on multiple temporal QA benchmarks show that MemoTime\nachieves overall state-of-the-art results, outperforming the strong baseline by\nup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to\nachieve reasoning performance comparable to that of GPT-4-Turbo.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved impressive reasoning abilities,\nbut struggle with temporal understanding, especially when questions involve\nmultiple entities, compound operators, and evolving event sequences. Temporal\nKnowledge Graphs (TKGs), which capture vast amounts of temporal facts in a\nstructured format, offer a reliable source for temporal reasoning. However,\nexisting TKG-based LLM reasoning methods still struggle with four major\nchallenges: maintaining temporal faithfulness in multi-hop reasoning, achieving\nmulti-entity temporal synchronization, adapting retrieval to diverse temporal\noperators, and reusing prior reasoning experience for stability and efficiency.\nTo address these issues, we propose MemoTime, a memory-augmented temporal\nknowledge graph framework that enhances LLM reasoning through structured\ngrounding, recursive reasoning, and continual experience learning. MemoTime\ndecomposes complex temporal questions into a hierarchical Tree of Time,\nenabling operator-aware reasoning that enforces monotonic timestamps and\nco-constrains multiple entities under unified temporal bounds. A dynamic\nevidence retrieval layer adaptively selects operator-specific retrieval\nstrategies, while a self-evolving experience memory stores verified reasoning\ntraces, toolkit decisions, and sub-question embeddings for cross-type reuse.\nComprehensive experiments on multiple temporal QA benchmarks show that MemoTime\nachieves overall state-of-the-art results, outperforming the strong baseline by\nup to 24.0%. Furthermore, MemoTime enables smaller models (e.g., Qwen3-4B) to\nachieve reasoning performance comparable to that of GPT-4-Turbo."
                },
                "authors": [
                    {
                        "name": "Xingyu Tan"
                    },
                    {
                        "name": "Xiaoyang Wang"
                    },
                    {
                        "name": "Qing Liu"
                    },
                    {
                        "name": "Xiwei Xu"
                    },
                    {
                        "name": "Xin Yuan"
                    },
                    {
                        "name": "Liming Zhu"
                    },
                    {
                        "name": "Wenjie Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjie Zhang"
                },
                "author": "Wenjie Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13614v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13614v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04329v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04329v3",
                "updated": "2025-10-15T14:42:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    42,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-06T10:32:32Z",
                "published_parsed": [
                    2025,
                    7,
                    6,
                    10,
                    32,
                    32,
                    6,
                    187,
                    0
                ],
                "title": "No Language Data Left Behind: A Comparative Study of CJK Language\n  Datasets in the Hugging Face Ecosystem",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "No Language Data Left Behind: A Comparative Study of CJK Language\n  Datasets in the Hugging Face Ecosystem"
                },
                "summary": "Recent advances in Natural Language Processing (NLP) have underscored the\ncrucial role of high-quality datasets in building large language models (LLMs).\nHowever, while extensive resources and analyses exist for English, the\nlandscape for East Asian languages - particularly Chinese, Japanese, and Korean\n(CJK) - remains fragmented and underexplored, despite these languages together\nserving over 1.6 billion speakers. To address this gap, we investigate the\nHuggingFace ecosystem from a cross-linguistic perspective, focusing on how\ncultural norms, research environments, and institutional practices shape\ndataset availability and quality. Drawing on more than 3,300 datasets, we\nemploy quantitative and qualitative methods to examine how these factors drive\ndistinct creation and curation patterns across Chinese, Japanese, and Korean\nNLP communities. Our findings highlight the large-scale and often\ninstitution-driven nature of Chinese datasets, grassroots community-led\ndevelopment in Korean NLP, and an entertainment- and subculture-focused\nemphasis on Japanese collections. By uncovering these patterns, we reveal\npractical strategies for enhancing dataset documentation, licensing clarity,\nand cross-lingual resource sharing - ultimately guiding more effective and\nculturally attuned LLM development in East Asia. We conclude by discussing best\npractices for future dataset curation and collaboration, aiming to strengthen\nresource development across all three languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in Natural Language Processing (NLP) have underscored the\ncrucial role of high-quality datasets in building large language models (LLMs).\nHowever, while extensive resources and analyses exist for English, the\nlandscape for East Asian languages - particularly Chinese, Japanese, and Korean\n(CJK) - remains fragmented and underexplored, despite these languages together\nserving over 1.6 billion speakers. To address this gap, we investigate the\nHuggingFace ecosystem from a cross-linguistic perspective, focusing on how\ncultural norms, research environments, and institutional practices shape\ndataset availability and quality. Drawing on more than 3,300 datasets, we\nemploy quantitative and qualitative methods to examine how these factors drive\ndistinct creation and curation patterns across Chinese, Japanese, and Korean\nNLP communities. Our findings highlight the large-scale and often\ninstitution-driven nature of Chinese datasets, grassroots community-led\ndevelopment in Korean NLP, and an entertainment- and subculture-focused\nemphasis on Japanese collections. By uncovering these patterns, we reveal\npractical strategies for enhancing dataset documentation, licensing clarity,\nand cross-lingual resource sharing - ultimately guiding more effective and\nculturally attuned LLM development in East Asia. We conclude by discussing best\npractices for future dataset curation and collaboration, aiming to strengthen\nresource development across all three languages."
                },
                "authors": [
                    {
                        "name": "Dasol Choi"
                    },
                    {
                        "name": "Woomyoung Park"
                    },
                    {
                        "name": "Youngsook Song"
                    }
                ],
                "author_detail": {
                    "name": "Youngsook Song"
                },
                "author": "Youngsook Song",
                "arxiv_comment": "Accepted to EMNLP 2025 MRL Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04329v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04329v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13602v1",
                "updated": "2025-10-15T14:33:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:33:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    33,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "NOSA: Native and Offloadable Sparse Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NOSA: Native and Offloadable Sparse Attention"
                },
                "summary": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trainable sparse attention has emerged as a promising solution to address the\ndecoding efficiency bottleneck of LLMs in long-context processing,\nsignificantly saving memory accesses while minimally impacting task\nperformance. However, existing sparse attention methods leave a crucial\nlimitation unresolved: the size of the key-value (KV) cache remains unreduced,\nwhich constrains on-GPU batch sizes and throttles decoding throughput,\nespecially in large-scale batched inference. In this paper, we show that\ntrainable sparse attention naturally exhibits strong locality in token\nselection across adjacent decoding steps, thereby enabling KV cache offloading\nwithout altering the underlying attention computation. However, the inherent\nlocality remains insufficient to achieve efficient offloading, as the transfer\nof selected KV pairs between the CPU and GPU continues to dominate the overall\ndecoding cost. Building on this insight, we present NOSA, a trainable sparse\nattention framework designed to natively support KV cache offloading. NOSA\nintroduces explicit locality constraints by decomposing token selection into\nquery-aware and query-agnostic components, thereby reducing KV transfers while\npreserving the same attention computation as used during training. We pretrain\na 1B-parameter model with NOSA and conduct extensive benchmarks, showing that\nit preserves near-lossless performance while achieving up to a 2.3x improvement\nin decoding throughput compared with the vanilla trainable sparse attention\nbaseline (InfLLM-V2)."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13598v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13598v1",
                "updated": "2025-10-15T14:31:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    31,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:31:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    31,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreshTab: Sourcing Fresh Data for Table-to-Text Generation Evaluation"
                },
                "summary": "Table-to-text generation (insight generation from tables) is a challenging\ntask that requires precision in analyzing the data. In addition, the evaluation\nof existing benchmarks is affected by contamination of Large Language Model\n(LLM) training data as well as domain imbalance. We introduce FreshTab, an\non-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM\ndata contamination problem and enable domain-sensitive evaluation. While\nnon-English table-to-text datasets are limited, FreshTab collects datasets in\ndifferent languages on demand (we experiment with German, Russian and French in\naddition to English). We find that insights generated by LLMs from recent\ntables collected by our method appear clearly worse by automatic metrics, but\nthis does not translate into LLM and human evaluations. Domain effects are\nvisible in all evaluations, showing that a~domain-balanced benchmark is more\nchallenging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table-to-text generation (insight generation from tables) is a challenging\ntask that requires precision in analyzing the data. In addition, the evaluation\nof existing benchmarks is affected by contamination of Large Language Model\n(LLM) training data as well as domain imbalance. We introduce FreshTab, an\non-the-fly table-to-text benchmark generation from Wikipedia, to combat the LLM\ndata contamination problem and enable domain-sensitive evaluation. While\nnon-English table-to-text datasets are limited, FreshTab collects datasets in\ndifferent languages on demand (we experiment with German, Russian and French in\naddition to English). We find that insights generated by LLMs from recent\ntables collected by our method appear clearly worse by automatic metrics, but\nthis does not translate into LLM and human evaluations. Domain effects are\nvisible in all evaluations, showing that a~domain-balanced benchmark is more\nchallenging."
                },
                "authors": [
                    {
                        "name": "Krist√Ωna Onderkov√°"
                    },
                    {
                        "name": "Ond≈ôej Pl√°tek"
                    },
                    {
                        "name": "Zdenƒõk Kasner"
                    },
                    {
                        "name": "Ond≈ôej Du≈°ek"
                    }
                ],
                "author_detail": {
                    "name": "Ond≈ôej Du≈°ek"
                },
                "author": "Ond≈ôej Du≈°ek",
                "arxiv_comment": "To be published in INLG 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13598v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13598v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20957v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20957v3",
                "updated": "2025-10-15T14:31:15Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    31,
                    15,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-28T16:09:38Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    9,
                    38,
                    0,
                    209,
                    0
                ],
                "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis"
                },
                "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\narising from discrepancies between their pre-trained parametric knowledge and\nreal-time market data. These conflicts are especially problematic in real-world\ninvestment services, where a model's inherent biases can misalign with\ninstitutional objectives, leading to unreliable recommendations. Despite this\nrisk, the intrinsic investment biases of LLMs remain underexplored. We propose\nan experimental framework to investigate emergent behaviors in such conflict\nscenarios, offering a quantitative analysis of bias in LLM-based investment\nanalysis. Using hypothetical scenarios with balanced and imbalanced arguments,\nwe extract the latent biases of models and measure their persistence. Our\nanalysis, centered on sector, size, and momentum, reveals distinct,\nmodel-specific biases. Across most models, a tendency to prefer technology\nstocks, large-cap stocks, and contrarian strategies is observed. These\nfoundational biases often escalate into confirmation bias, causing models to\ncling to initial judgments even when faced with increasing counter-evidence. A\npublic leaderboard benchmarking bias across a broader set of models is\navailable at https://linqalpha.com/leaderboard",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\narising from discrepancies between their pre-trained parametric knowledge and\nreal-time market data. These conflicts are especially problematic in real-world\ninvestment services, where a model's inherent biases can misalign with\ninstitutional objectives, leading to unreliable recommendations. Despite this\nrisk, the intrinsic investment biases of LLMs remain underexplored. We propose\nan experimental framework to investigate emergent behaviors in such conflict\nscenarios, offering a quantitative analysis of bias in LLM-based investment\nanalysis. Using hypothetical scenarios with balanced and imbalanced arguments,\nwe extract the latent biases of models and measure their persistence. Our\nanalysis, centered on sector, size, and momentum, reveals distinct,\nmodel-specific biases. Across most models, a tendency to prefer technology\nstocks, large-cap stocks, and contrarian strategies is observed. These\nfoundational biases often escalate into confirmation bias, causing models to\ncling to initial judgments even when faced with increasing counter-evidence. A\npublic leaderboard benchmarking bias across a broader set of models is\navailable at https://linqalpha.com/leaderboard"
                },
                "authors": [
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Junhyuk Seo"
                    },
                    {
                        "name": "Suhwan Park"
                    },
                    {
                        "name": "Junhyeong Lee"
                    },
                    {
                        "name": "Wonbin Ahn"
                    },
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "arxiv_comment": "Accepted at ACM International Conference on AI in Finance (ICAIF)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20957v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20957v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13591v1",
                "updated": "2025-10-15T14:21:30Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    21,
                    30,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:21:30Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    21,
                    30,
                    2,
                    288,
                    0
                ],
                "title": "Subject Roles in the EU AI Act: Mapping and Regulatory Implications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subject Roles in the EU AI Act: Mapping and Regulatory Implications"
                },
                "summary": "The European Union's Artificial Intelligence Act (Regulation (EU) 2024/1689)\nestablishes the world's first comprehensive regulatory framework for AI systems\nthrough a sophisticated ecosystem of interconnected subjects defined in Article\n3. This paper provides a structured examination of the six main categories of\nactors - providers, deployers, authorized representatives, importers,\ndistributors, and product manufacturers - collectively referred to as\n\"operators\" within the regulation. Through examination of these Article 3\ndefinitions and their elaboration across the regulation's 113 articles, 180\nrecitals, and 13 annexes, we map the complete governance structure and analyze\nhow the AI Act regulates these subjects. Our analysis reveals critical\ntransformation mechanisms whereby subjects can assume different roles under\nspecific conditions, particularly through Article 25 provisions ensuring\naccountability follows control. We identify how obligations cascade through the\nsupply chain via mandatory information flows and cooperation requirements,\ncreating a distributed yet coordinated governance system. The findings\ndemonstrate how the regulation balances innovation with the protection of\nfundamental rights through risk-based obligations that scale with the\ncapabilities and deployment contexts of AI systems, providing essential\nguidance for stakeholders implementing the AI Act's requirements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The European Union's Artificial Intelligence Act (Regulation (EU) 2024/1689)\nestablishes the world's first comprehensive regulatory framework for AI systems\nthrough a sophisticated ecosystem of interconnected subjects defined in Article\n3. This paper provides a structured examination of the six main categories of\nactors - providers, deployers, authorized representatives, importers,\ndistributors, and product manufacturers - collectively referred to as\n\"operators\" within the regulation. Through examination of these Article 3\ndefinitions and their elaboration across the regulation's 113 articles, 180\nrecitals, and 13 annexes, we map the complete governance structure and analyze\nhow the AI Act regulates these subjects. Our analysis reveals critical\ntransformation mechanisms whereby subjects can assume different roles under\nspecific conditions, particularly through Article 25 provisions ensuring\naccountability follows control. We identify how obligations cascade through the\nsupply chain via mandatory information flows and cooperation requirements,\ncreating a distributed yet coordinated governance system. The findings\ndemonstrate how the regulation balances innovation with the protection of\nfundamental rights through risk-based obligations that scale with the\ncapabilities and deployment contexts of AI systems, providing essential\nguidance for stakeholders implementing the AI Act's requirements."
                },
                "authors": [
                    {
                        "name": "Nicola Fabiano"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Fabiano"
                },
                "author": "Nicola Fabiano",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13590v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13590v1",
                "updated": "2025-10-15T14:21:08Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    21,
                    8,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:21:08Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    21,
                    8,
                    2,
                    288,
                    0
                ],
                "title": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for\n  Evolving Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAG Meets Temporal Graphs: Time-Sensitive Modeling and Retrieval for\n  Evolving Knowledge"
                },
                "summary": "Knowledge is inherently time-sensitive and continuously evolves over time.\nAlthough current Retrieval-Augmented Generation (RAG) systems enrich LLMs with\nexternal knowledge, they largely ignore this temporal nature. This raises two\nchallenges for RAG. First, current RAG methods lack effective time-aware\nrepresentations. Same facts of different time are difficult to distinguish with\nvector embeddings or conventional knowledge graphs. Second, most RAG\nevaluations assume a static corpus, leaving a blind spot regarding update costs\nand retrieval stability as knowledge evolves. To make RAG time-aware, we\npropose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level\ntemporal graph consisting of a temporal knowledge graph with timestamped\nrelations and a hierarchical time graph. Multi-granularity temporal summaries\nare generated for each time node to capture both key events and broader trends\nat that time. The design supports incremental updates by extracting new\ntemporal facts from the incoming corpus and merging them into the existing\ngraph. The temporal graph explicitly represents identical facts at different\ntimes as distinct edges to avoid ambiguity, and the time hierarchy graph allows\nonly generating reports for new leaf time nodes and their ancestors, ensuring\neffective and efficient updates. During inference, TG-RAG dynamically retrieves\na subgraph within the temporal and semantic scope of the query, enabling\nprecise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive\nquestion-answering dataset featuring both specific and abstract queries, along\nwith a comprehensive evaluation protocol designed to assess incremental update\ncapabilities of RAG systems. Extensive experiments show that TG-RAG\nsignificantly outperforms existing baselines, demonstrating the effectiveness\nof our method in handling temporal knowledge and incremental updates.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge is inherently time-sensitive and continuously evolves over time.\nAlthough current Retrieval-Augmented Generation (RAG) systems enrich LLMs with\nexternal knowledge, they largely ignore this temporal nature. This raises two\nchallenges for RAG. First, current RAG methods lack effective time-aware\nrepresentations. Same facts of different time are difficult to distinguish with\nvector embeddings or conventional knowledge graphs. Second, most RAG\nevaluations assume a static corpus, leaving a blind spot regarding update costs\nand retrieval stability as knowledge evolves. To make RAG time-aware, we\npropose Temporal GraphRAG (TG-RAG), which models external corpora as a bi-level\ntemporal graph consisting of a temporal knowledge graph with timestamped\nrelations and a hierarchical time graph. Multi-granularity temporal summaries\nare generated for each time node to capture both key events and broader trends\nat that time. The design supports incremental updates by extracting new\ntemporal facts from the incoming corpus and merging them into the existing\ngraph. The temporal graph explicitly represents identical facts at different\ntimes as distinct edges to avoid ambiguity, and the time hierarchy graph allows\nonly generating reports for new leaf time nodes and their ancestors, ensuring\neffective and efficient updates. During inference, TG-RAG dynamically retrieves\na subgraph within the temporal and semantic scope of the query, enabling\nprecise evidence gathering. Moreover, we introduce ECT-QA, a time-sensitive\nquestion-answering dataset featuring both specific and abstract queries, along\nwith a comprehensive evaluation protocol designed to assess incremental update\ncapabilities of RAG systems. Extensive experiments show that TG-RAG\nsignificantly outperforms existing baselines, demonstrating the effectiveness\nof our method in handling temporal knowledge and incremental updates."
                },
                "authors": [
                    {
                        "name": "Jiale Han"
                    },
                    {
                        "name": "Austin Cheung"
                    },
                    {
                        "name": "Yubai Wei"
                    },
                    {
                        "name": "Zheng Yu"
                    },
                    {
                        "name": "Xusheng Wang"
                    },
                    {
                        "name": "Bing Zhu"
                    },
                    {
                        "name": "Yi Yang"
                    }
                ],
                "author_detail": {
                    "name": "Yi Yang"
                },
                "author": "Yi Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13590v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13590v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13586v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13586v1",
                "updated": "2025-10-15T14:17:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    17,
                    23,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:17:23Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    17,
                    23,
                    2,
                    288,
                    0
                ],
                "title": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deflanderization for Game Dialogue: Balancing Character Authenticity\n  with Task Execution in LLM-based NPCs"
                },
                "summary": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of large language models (LLMs) has opened new opportunities\nfor cre- ating dynamic non-player characters (NPCs) in gaming environments,\nenabling both func- tional task execution and persona-consistent dialogue\ngeneration. In this paper, we (Tu_Character_lab) report our participation in\nthe Commonsense Persona-Grounded Dialogue Challenge (CPDC) 2025 Round 2, which\neval- uates agents across three tracks: task-oriented dialogue, context-aware\ndialogue, and their integration. Our approach combines two complementary\nstrategies: (i) lightweight prompting techniques in the API track, including a\nDeflanderization prompting method to suppress excessive role-play and improve\ntask fidelity, and (ii) fine-tuned large models in the GPU track, leveraging\nQwen3-14B with supervisedfinetuning (SFT) and Low-Rank Adaptation(LoRA). Our\nbest submissions ranked 2nd on Task 1, 2nd on Task 3 (API track), and 4th on\nTask 3 (GPU track)."
                },
                "authors": [
                    {
                        "name": "Pasin Buakhaw"
                    },
                    {
                        "name": "Kun Kerdthaisong"
                    },
                    {
                        "name": "Phuree Phenhiran"
                    },
                    {
                        "name": "Pitikorn Khlaisamniang"
                    },
                    {
                        "name": "Supasate Vorathammathorn"
                    },
                    {
                        "name": "Piyalitt Ittichaiwong"
                    },
                    {
                        "name": "Nutchanon Yongsatianchot"
                    }
                ],
                "author_detail": {
                    "name": "Nutchanon Yongsatianchot"
                },
                "author": "Nutchanon Yongsatianchot",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13586v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13586v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13580v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13580v1",
                "updated": "2025-10-15T14:14:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    14,
                    49,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:14:49Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    14,
                    49,
                    2,
                    288,
                    0
                ],
                "title": "Sparse Subnetwork Enhancement for Underrepresented Languages in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Subnetwork Enhancement for Underrepresented Languages in Large\n  Language Models"
                },
                "summary": "Large language models exhibit uneven performance across languages, with\nsubstantial gaps between high- and low-resource languages. We present a\nframework for enhancing monolingual capabilities of LLMs in underrepresented\nlanguages while preserving their general-purpose performance through targeted\nfine-tuning of language-specific subnetworks. Our approach identifies\nlanguage-specific neurons using Language Activation Probability Entropy and\nfine-tunes only the weights associated with these neurons, a dedicated\nsubnetwork, on target-language data. Experiments on Llama-3.1-8B and\nMistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our\nmethod consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA\nadaptation, and random subset fine-tuning baselines while efficiently updating\nonly up to 1% of model parameters. Beyond performance improvements, we observe\nenhanced favorable training dynamics, cross-lingual representational alignment,\nand systematic weight update changes. To facilitate future research, we release\nlanguage-specific neuron identifications for over 100 languages as well as our\nadaptation pipeline, offering a cost-effective pathway for adapting\nstate-of-the-art models to underrepresented languages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models exhibit uneven performance across languages, with\nsubstantial gaps between high- and low-resource languages. We present a\nframework for enhancing monolingual capabilities of LLMs in underrepresented\nlanguages while preserving their general-purpose performance through targeted\nfine-tuning of language-specific subnetworks. Our approach identifies\nlanguage-specific neurons using Language Activation Probability Entropy and\nfine-tunes only the weights associated with these neurons, a dedicated\nsubnetwork, on target-language data. Experiments on Llama-3.1-8B and\nMistral-Nemo-12B across 12 mid- and low-resource languages demonstrate that our\nmethod consistently outperforms full fine-tuning, FFN-only fine-tuning, LoRA\nadaptation, and random subset fine-tuning baselines while efficiently updating\nonly up to 1% of model parameters. Beyond performance improvements, we observe\nenhanced favorable training dynamics, cross-lingual representational alignment,\nand systematic weight update changes. To facilitate future research, we release\nlanguage-specific neuron identifications for over 100 languages as well as our\nadaptation pipeline, offering a cost-effective pathway for adapting\nstate-of-the-art models to underrepresented languages."
                },
                "authors": [
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13580v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13580v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13575v1",
                "updated": "2025-10-15T14:13:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    13,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:13:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    13,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "Auto-repair without test cases: How LLMs fix compilation errors in large\n  industrial embedded code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Auto-repair without test cases: How LLMs fix compilation errors in large\n  industrial embedded code"
                },
                "summary": "The co-development of hardware and software in industrial embedded systems\nfrequently leads to compilation errors during continuous integration (CI).\nAutomated repair of such failures is promising, but existing techniques rely on\ntest cases, which are not available for non-compilable code.\n  We employ an automated repair approach for compilation errors driven by large\nlanguage models (LLMs). Our study encompasses the collection of more than 40000\ncommits from the product's source code. We assess the performance of an\nindustrial CI system enhanced by four state-of-the-art LLMs, comparing their\noutcomes with manual corrections provided by human programmers. LLM-equipped CI\nsystems can resolve up to 63 % of the compilation errors in our baseline\ndataset. Among the fixes associated with successful CI builds, 83 % are deemed\nreasonable. Moreover, LLMs significantly reduce debugging time, with the\nmajority of successful cases completed within 8 minutes, compared to hours\ntypically required for manual debugging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-development of hardware and software in industrial embedded systems\nfrequently leads to compilation errors during continuous integration (CI).\nAutomated repair of such failures is promising, but existing techniques rely on\ntest cases, which are not available for non-compilable code.\n  We employ an automated repair approach for compilation errors driven by large\nlanguage models (LLMs). Our study encompasses the collection of more than 40000\ncommits from the product's source code. We assess the performance of an\nindustrial CI system enhanced by four state-of-the-art LLMs, comparing their\noutcomes with manual corrections provided by human programmers. LLM-equipped CI\nsystems can resolve up to 63 % of the compilation errors in our baseline\ndataset. Among the fixes associated with successful CI builds, 83 % are deemed\nreasonable. Moreover, LLMs significantly reduce debugging time, with the\nmajority of successful cases completed within 8 minutes, compared to hours\ntypically required for manual debugging."
                },
                "authors": [
                    {
                        "name": "Han Fu"
                    },
                    {
                        "name": "Sigrid Eldh"
                    },
                    {
                        "name": "Kristian Wiklund"
                    },
                    {
                        "name": "Andreas Ermedahl"
                    },
                    {
                        "name": "Philipp Haller"
                    },
                    {
                        "name": "Cyrille Artho"
                    }
                ],
                "author_detail": {
                    "name": "Cyrille Artho"
                },
                "author": "Cyrille Artho",
                "arxiv_comment": "9 pages, 4 figures, conference: 2025 28th Euromicro Conference on\n  Digital System Design (DSD)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13570v1",
                "updated": "2025-10-15T14:08:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    8,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T14:08:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    14,
                    8,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "Selective Adversarial Attacks on LLM Benchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Selective Adversarial Attacks on LLM Benchmarks"
                },
                "summary": "Benchmarking outcomes increasingly govern trust, selection, and deployment of\nLLMs, yet these evaluations remain vulnerable to semantically equivalent\nadversarial perturbations. Prior work on adversarial robustness in NLP has\nemphasized text attacks that affect many models equally, leaving open the\nquestion of whether it is possible to selectively degrade or enhance\nperformance while minimally affecting other models. We formalize this problem\nand study selective adversarial attacks on MMLU - a widely used benchmark\ndesigned to measure a language model's broad general knowledge and reasoning\nability across different subjects. Using canonical attacks integrated into\nTextAttack framework, we introduce a protocol for selectivity assessment,\ndevelop a custom constraint to increase selectivity of attacks and propose a\nsurrogate-LLM pipeline that generates selective perturbations. Empirically, we\nfind that selective adversarial attacks exist and can materially alter relative\nrankings, challenging the fairness, reproducibility, and transparency of\nleaderboard-driven evaluation. Our results motivate perturbation-aware\nreporting and robustness diagnostics for LLM evaluation and demonstrate that\neven subtle edits can shift comparative judgments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking outcomes increasingly govern trust, selection, and deployment of\nLLMs, yet these evaluations remain vulnerable to semantically equivalent\nadversarial perturbations. Prior work on adversarial robustness in NLP has\nemphasized text attacks that affect many models equally, leaving open the\nquestion of whether it is possible to selectively degrade or enhance\nperformance while minimally affecting other models. We formalize this problem\nand study selective adversarial attacks on MMLU - a widely used benchmark\ndesigned to measure a language model's broad general knowledge and reasoning\nability across different subjects. Using canonical attacks integrated into\nTextAttack framework, we introduce a protocol for selectivity assessment,\ndevelop a custom constraint to increase selectivity of attacks and propose a\nsurrogate-LLM pipeline that generates selective perturbations. Empirically, we\nfind that selective adversarial attacks exist and can materially alter relative\nrankings, challenging the fairness, reproducibility, and transparency of\nleaderboard-driven evaluation. Our results motivate perturbation-aware\nreporting and robustness diagnostics for LLM evaluation and demonstrate that\neven subtle edits can shift comparative judgments."
                },
                "authors": [
                    {
                        "name": "Ivan Dubrovsky"
                    },
                    {
                        "name": "Anastasia Orlova"
                    },
                    {
                        "name": "Illarion Iov"
                    },
                    {
                        "name": "Nina Gubina"
                    },
                    {
                        "name": "Irena Gureeva"
                    },
                    {
                        "name": "Alexey Zaytsev"
                    }
                ],
                "author_detail": {
                    "name": "Alexey Zaytsev"
                },
                "author": "Alexey Zaytsev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13561v1",
                "updated": "2025-10-15T13:59:58Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    59,
                    58,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:59:58Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    59,
                    58,
                    2,
                    288,
                    0
                ],
                "title": "OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design,\n  Implementation, and Case Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenDerisk: An Industrial Framework for AI-Driven SRE, with Design,\n  Implementation, and Case Studies"
                },
                "summary": "The escalating complexity of modern software imposes an unsustainable\noperational burden on Site Reliability Engineering (SRE) teams, demanding\nAI-driven automation that can emulate expert diagnostic reasoning. Existing\nsolutions, from traditional AI methods to general-purpose multi-agent systems,\nfall short: they either lack deep causal reasoning or are not tailored for the\nspecialized, investigative workflows unique to SRE. To address this gap, we\npresent OpenDerisk, a specialized, open-source multi-agent framework\narchitected for SRE. OpenDerisk integrates a diagnostic-native collaboration\nmodel, a pluggable reasoning engine, a knowledge engine, and a standardized\nprotocol (MCP) to enable specialist agents to collectively solve complex,\nmulti-domain problems. Our comprehensive evaluation demonstrates that\nOpenDerisk significantly outperforms state-of-the-art baselines in both\naccuracy and efficiency. This effectiveness is validated by its large-scale\nproduction deployment at Ant Group, where it serves over 3,000 daily users\nacross diverse scenarios, confirming its industrial-grade scalability and\npractical impact. OpenDerisk is open source and available at\nhttps://github.com/derisk-ai/OpenDerisk/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The escalating complexity of modern software imposes an unsustainable\noperational burden on Site Reliability Engineering (SRE) teams, demanding\nAI-driven automation that can emulate expert diagnostic reasoning. Existing\nsolutions, from traditional AI methods to general-purpose multi-agent systems,\nfall short: they either lack deep causal reasoning or are not tailored for the\nspecialized, investigative workflows unique to SRE. To address this gap, we\npresent OpenDerisk, a specialized, open-source multi-agent framework\narchitected for SRE. OpenDerisk integrates a diagnostic-native collaboration\nmodel, a pluggable reasoning engine, a knowledge engine, and a standardized\nprotocol (MCP) to enable specialist agents to collectively solve complex,\nmulti-domain problems. Our comprehensive evaluation demonstrates that\nOpenDerisk significantly outperforms state-of-the-art baselines in both\naccuracy and efficiency. This effectiveness is validated by its large-scale\nproduction deployment at Ant Group, where it serves over 3,000 daily users\nacross diverse scenarios, confirming its industrial-grade scalability and\npractical impact. OpenDerisk is open source and available at\nhttps://github.com/derisk-ai/OpenDerisk/"
                },
                "authors": [
                    {
                        "name": "Peng Di"
                    },
                    {
                        "name": "Faqiang Chen"
                    },
                    {
                        "name": "Xiao Bai"
                    },
                    {
                        "name": "Hongjun Yang"
                    },
                    {
                        "name": "Qingfeng Li"
                    },
                    {
                        "name": "Ganglin Wei"
                    },
                    {
                        "name": "Jian Mou"
                    },
                    {
                        "name": "Feng Shi"
                    },
                    {
                        "name": "Keting Chen"
                    },
                    {
                        "name": "Peng Tang"
                    },
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Zheng Li"
                    },
                    {
                        "name": "Wenhui Shi"
                    },
                    {
                        "name": "Junwei Guo"
                    },
                    {
                        "name": "Hang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Hang Yu"
                },
                "author": "Hang Yu",
                "arxiv_comment": "23 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.00893v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.00893v2",
                "updated": "2025-10-15T13:56:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    56,
                    3,
                    2,
                    288,
                    0
                ],
                "published": "2025-08-31T15:12:51Z",
                "published_parsed": [
                    2025,
                    8,
                    31,
                    15,
                    12,
                    51,
                    6,
                    243,
                    0
                ],
                "title": "SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SeLeRoSa: Sentence-Level Romanian Satire Detection Dataset"
                },
                "summary": "Satire, irony, and sarcasm are techniques typically used to express humor and\ncritique, rather than deceive; however, they can occasionally be mistaken for\nfactual reporting, akin to fake news. These techniques can be applied at a more\ngranular level, allowing satirical information to be incorporated into news\narticles. In this paper, we introduce the first sentence-level dataset for\nRomanian satire detection for news articles, called SeLeRoSa. The dataset\ncomprises 13,873 manually annotated sentences spanning various domains,\nincluding social issues, IT, science, and movies. With the rise and recent\nprogress of large language models (LLMs) in the natural language processing\nliterature, LLMs have demonstrated enhanced capabilities to tackle various\ntasks in zero-shot settings. We evaluate multiple baseline models based on LLMs\nin both zero-shot and fine-tuning settings, as well as baseline\ntransformer-based models. Our findings reveal the current limitations of these\nmodels in the sentence-level satire detection task, paving the way for new\nresearch directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Satire, irony, and sarcasm are techniques typically used to express humor and\ncritique, rather than deceive; however, they can occasionally be mistaken for\nfactual reporting, akin to fake news. These techniques can be applied at a more\ngranular level, allowing satirical information to be incorporated into news\narticles. In this paper, we introduce the first sentence-level dataset for\nRomanian satire detection for news articles, called SeLeRoSa. The dataset\ncomprises 13,873 manually annotated sentences spanning various domains,\nincluding social issues, IT, science, and movies. With the rise and recent\nprogress of large language models (LLMs) in the natural language processing\nliterature, LLMs have demonstrated enhanced capabilities to tackle various\ntasks in zero-shot settings. We evaluate multiple baseline models based on LLMs\nin both zero-shot and fine-tuning settings, as well as baseline\ntransformer-based models. Our findings reveal the current limitations of these\nmodels in the sentence-level satire detection task, paving the way for new\nresearch directions."
                },
                "authors": [
                    {
                        "name": "RƒÉzvan-Alexandru SmƒÉdu"
                    },
                    {
                        "name": "Andreea Iuga"
                    },
                    {
                        "name": "Dumitru-Clementin Cercel"
                    },
                    {
                        "name": "Florin Pop"
                    }
                ],
                "author_detail": {
                    "name": "Florin Pop"
                },
                "author": "Florin Pop",
                "arxiv_doi": "10.1145/3746252.3761632",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746252.3761632",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2509.00893v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.00893v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 2 Figures",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13558v1",
                "updated": "2025-10-15T13:54:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    54,
                    42,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:54:42Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    54,
                    42,
                    2,
                    288,
                    0
                ],
                "title": "Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts\n  Steering Module",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steer-MoE: Efficient Audio-Language Alignment with a Mixture-of-Experts\n  Steering Module"
                },
                "summary": "Aligning pretrained audio encoders and Large Language Models (LLMs) offers a\npromising, parameter-efficient path to building powerful multimodal agents.\nHowever, existing methods often require costly full-model finetuning or rely on\nstatic adapters that may lack expressive power. Drawing inspiration from the\nPlatonic Representation Hypothesis, we introduce SteerMoE, a novel and modular\nframework for audio-language alignment. SteerMoE freezes both the audio encoder\nand the LLM decoder, training only a lightweight steering module integrated\nwithin the encoder's layers. This module uses a Mixture-of-Experts (MoE) router\nto dynamically select and apply learned steering vectors, progressively\ntransforming continuous audio representations into a space comprehensible to\nthe LLM. By operating entirely in the continuous embedding space, our approach\nrequires no modifications to the LLM's vocabulary and preserves its advanced\nreasoning and agentic capabilities. We demonstrate through experiments on ASR,\naudio understanding, and a qualitative function-calling task that SteerMoE\nachieves strong performance while remaining highly modular and computationally\nefficient, offering a robust new paradigm for developing sophisticated\naudio-language systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning pretrained audio encoders and Large Language Models (LLMs) offers a\npromising, parameter-efficient path to building powerful multimodal agents.\nHowever, existing methods often require costly full-model finetuning or rely on\nstatic adapters that may lack expressive power. Drawing inspiration from the\nPlatonic Representation Hypothesis, we introduce SteerMoE, a novel and modular\nframework for audio-language alignment. SteerMoE freezes both the audio encoder\nand the LLM decoder, training only a lightweight steering module integrated\nwithin the encoder's layers. This module uses a Mixture-of-Experts (MoE) router\nto dynamically select and apply learned steering vectors, progressively\ntransforming continuous audio representations into a space comprehensible to\nthe LLM. By operating entirely in the continuous embedding space, our approach\nrequires no modifications to the LLM's vocabulary and preserves its advanced\nreasoning and agentic capabilities. We demonstrate through experiments on ASR,\naudio understanding, and a qualitative function-calling task that SteerMoE\nachieves strong performance while remaining highly modular and computationally\nefficient, offering a robust new paradigm for developing sophisticated\naudio-language systems."
                },
                "authors": [
                    {
                        "name": "Ruitao Feng"
                    },
                    {
                        "name": "Bixi Zhang"
                    },
                    {
                        "name": "Sheng Liang"
                    },
                    {
                        "name": "Zheng Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yuan"
                },
                "author": "Zheng Yuan",
                "arxiv_comment": "5 pages, 1 figures. Code is available at:\n  https://github.com/forfrt/SteerMoE. Submitted to ICASSP 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.08141v3",
                "updated": "2025-10-15T13:54:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    54,
                    25,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-09T12:24:08Z",
                "published_parsed": [
                    2025,
                    10,
                    9,
                    12,
                    24,
                    8,
                    3,
                    282,
                    0
                ],
                "title": "Arbitrary Entropy Policy Optimization: Entropy Is Controllable in\n  Reinforcement Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arbitrary Entropy Policy Optimization: Entropy Is Controllable in\n  Reinforcement Fine-tuning"
                },
                "summary": "Reinforcement fine-tuning (RFT) is essential for enhancing the reasoning\ncapabilities of large language models (LLM), yet the widely adopted Group\nRelative Policy Optimization (GRPO) suffers from entropy collapse, where\nentropy monotonically decreases, exploration vanishes, and policies converge\nprematurely. Existing entropy-regularized methods only partially alleviate this\nissue while introducing bias and instability, leaving entropy control\nunresolved and the connection between entropy, exploration, and performance\nunclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which\neliminates entropy collapse by replacing entropy bonuses with REINFORCE policy\ngradient on temperature-adjusted distributions and stabilizing entropy through\ntemperature regulation. AEPO integrates three key designs: policy gradient as\nregularization, distribution as regularization, and REINFORCE as\nregularization, enabling precise entropy control without distorting\noptimization. Experiments demonstrate three major contributions: AEPO (1)\nstabilizes entropy at arbitrary target levels, effectively removing collapse in\nGRPO; (2) reveals a non-monotonic relation where performance first improves\nthen declines with increasing entropy, clarifying the link between entropy,\nexploration, and reasoning; and (3) generalizes beyond entropy, providing a\nbroader RFT paradigm where superior target distributions can serve as REINFORCE\nregularizers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement fine-tuning (RFT) is essential for enhancing the reasoning\ncapabilities of large language models (LLM), yet the widely adopted Group\nRelative Policy Optimization (GRPO) suffers from entropy collapse, where\nentropy monotonically decreases, exploration vanishes, and policies converge\nprematurely. Existing entropy-regularized methods only partially alleviate this\nissue while introducing bias and instability, leaving entropy control\nunresolved and the connection between entropy, exploration, and performance\nunclear. We propose Arbitrary Entropy Policy Optimization (AEPO), which\neliminates entropy collapse by replacing entropy bonuses with REINFORCE policy\ngradient on temperature-adjusted distributions and stabilizing entropy through\ntemperature regulation. AEPO integrates three key designs: policy gradient as\nregularization, distribution as regularization, and REINFORCE as\nregularization, enabling precise entropy control without distorting\noptimization. Experiments demonstrate three major contributions: AEPO (1)\nstabilizes entropy at arbitrary target levels, effectively removing collapse in\nGRPO; (2) reveals a non-monotonic relation where performance first improves\nthen declines with increasing entropy, clarifying the link between entropy,\nexploration, and reasoning; and (3) generalizes beyond entropy, providing a\nbroader RFT paradigm where superior target distributions can serve as REINFORCE\nregularizers."
                },
                "authors": [
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Zhaochun Li"
                    },
                    {
                        "name": "Jionghao Bai"
                    },
                    {
                        "name": "Yuzhi Zhang"
                    },
                    {
                        "name": "Shisheng Cui"
                    },
                    {
                        "name": "Zhou Zhao"
                    },
                    {
                        "name": "Yue Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yue Wang"
                },
                "author": "Yue Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13554v1",
                "updated": "2025-10-15T13:49:51Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    49,
                    51,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:49:51Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    49,
                    51,
                    2,
                    288,
                    0
                ],
                "title": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention Illuminates LLM Reasoning: The Preplan-and-Anchor Rhythm\n  Enables Fine-Grained Policy Optimization"
                },
                "summary": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning pattern of Large language models (LLMs) remains opaque, and\nReinforcement learning (RL) typically applies uniform credit across an entire\ngeneration, blurring the distinction between pivotal and routine steps. This\nwork positions attention as a privileged substrate that renders the internal\nlogic of LLMs legible, not merely as a byproduct of computation, but as a\nmechanistic blueprint of reasoning itself. We first distinguish attention heads\nbetween locally and globally focused information processing and reveal that\nlocally focused heads produce a sawtooth pattern near the diagonal indicating\nphrasal chunks, while globally focused heads expose tokens that exert broad\ndownstream influence over future tokens. We formalize these with two metrics:\n1) Windowed Average Attention Distance, which measures the extent of backward\nattention within a clipped window; 2) Future Attention Influence, which\nquantifies a token's global importance as the average attention it receives\nfrom subsequent tokens. Taken together, these signals reveal a recurring\npreplan-and-anchor mechanism, where the model first performs a long-range\ncontextual reference to generate an introductory token, which is immediately\nfollowed by or coincides with a semantic anchor token that organizes subsequent\nreasoning. Leveraging these insights, we introduce three novel RL strategies\nthat dynamically perform targeted credit assignment to critical nodes (preplan\ntokens, anchor tokens, and their temporal coupling) and show consistent\nperformance gains across various reasoning tasks. By aligning optimization with\nthe model's intrinsic reasoning rhythm, we aim to transform opaque optimization\ninto an actionable structure-aware process, hoping to offer a potential step\ntoward more transparent and effective optimization of LLM reasoning."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Zhichen Dong"
                    },
                    {
                        "name": "Yuhan Sun"
                    },
                    {
                        "name": "Weixun Wang"
                    },
                    {
                        "name": "Shaopan Xiong"
                    },
                    {
                        "name": "Yijia Luo"
                    },
                    {
                        "name": "Jiashun Liu"
                    },
                    {
                        "name": "Han Lu"
                    },
                    {
                        "name": "Jiamang Wang"
                    },
                    {
                        "name": "Wenbo Su"
                    },
                    {
                        "name": "Bo Zheng"
                    },
                    {
                        "name": "Junchi Yan"
                    }
                ],
                "author_detail": {
                    "name": "Junchi Yan"
                },
                "author": "Junchi Yan",
                "arxiv_comment": "23 pages, 8 figures, 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.04886v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.04886v4",
                "updated": "2025-10-15T13:46:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    46,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-07T11:17:32Z",
                "published_parsed": [
                    2025,
                    7,
                    7,
                    11,
                    17,
                    32,
                    0,
                    188,
                    0
                ],
                "title": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Emergent Semantics Beyond Token Embeddings: Transformer LMs with Frozen\n  Visual Unicode Representations"
                },
                "summary": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the locus of semantic representation in large language models\n(LLMs) is crucial for interpretability and architectural innovation. The\ndominant paradigm posits that trainable input embeddings serve as foundational\n\"meaning vectors.\" This paper challenges that view. We construct Transformer\nmodels where the embedding layer is entirely frozen, with vectors derived not\nfrom data, but from the visual structure of Unicode glyphs. These non-semantic,\nprecomputed visual embeddings are fixed throughout training. Our method is\ncompatible with any tokenizer, including a novel Unicode-centric tokenizer we\nintroduce to ensure universal text coverage. Despite the absence of trainable,\nsemantically initialized embeddings, our models converge, generate coherent\ntext, and, critically, outperform architecturally identical models with\ntrainable embeddings on the MMLU reasoning benchmark. We attribute this to\n\"representational interference\" in conventional models, where the embedding\nlayer is burdened with learning both structural and semantic features. Our\nresults indicate that high-level semantics are not inherent to input embeddings\nbut are an emergent property of the Transformer's compositional architecture\nand data scale. This reframes the role of embeddings from meaning containers to\nstructural primitives. We release all code and models to foster further\nresearch."
                },
                "authors": [
                    {
                        "name": "A. Bochkov"
                    }
                ],
                "author_detail": {
                    "name": "A. Bochkov"
                },
                "author": "A. Bochkov",
                "arxiv_comment": "Published in Transactions on Machine Learning Research (10/2025).\n  OpenReview: https://openreview.net/forum?id=Odh8IynO1o",
                "arxiv_journal_ref": "Transactions on Machine Learning Research (TMLR), 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.04886v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.04886v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02477v3",
                "updated": "2025-10-15T13:42:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    42,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-04-03T10:53:07Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    10,
                    53,
                    7,
                    3,
                    93,
                    0
                ],
                "title": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal Fusion and Vision-Language Models: A Survey for Robot Vision"
                },
                "summary": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We adopt a task-oriented\nperspective to systematically review the applications and advancements of\nmultimodal fusion methods and VLMs in the field of robot vision. For semantic\nscene understanding tasks, we categorize fusion approaches into encoder-decoder\nframeworks, attention-based architectures, and graph neural networks.\nMeanwhile, we also analyze the architectural characteristics and practical\nimplementations of these fusion strategies in key tasks such as simultaneous\nlocalization and mapping (SLAM), 3D object detection, navigation, and\nmanipulation. We compare the evolutionary paths and applicability of VLMs based\non large language models (LLMs) with traditional multimodal fusion\nmethods.Additionally, we conduct an in-depth analysis of commonly used\ndatasets, evaluating their applicability and challenges in real-world robotic\nscenarios. Building on this analysis, we identify key challenges in current\nresearch, including cross-modal alignment, efficient fusion, real-time\ndeployment, and domain adaptation. We propose future directions such as\nself-supervised learning for robust multimodal representations, structured\nspatial memory and environment modeling to enhance spatial intelligence, and\nthe integration of adversarial robustness and human feedback mechanisms to\nenable ethically aligned system deployment. Through a comprehensive review,\ncomparative analysis, and forward-looking discussion, we provide a valuable\nreference for advancing multimodal perception and interaction in robotic\nvision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robot vision has greatly benefited from advancements in multimodal fusion\ntechniques and vision-language models (VLMs). We adopt a task-oriented\nperspective to systematically review the applications and advancements of\nmultimodal fusion methods and VLMs in the field of robot vision. For semantic\nscene understanding tasks, we categorize fusion approaches into encoder-decoder\nframeworks, attention-based architectures, and graph neural networks.\nMeanwhile, we also analyze the architectural characteristics and practical\nimplementations of these fusion strategies in key tasks such as simultaneous\nlocalization and mapping (SLAM), 3D object detection, navigation, and\nmanipulation. We compare the evolutionary paths and applicability of VLMs based\non large language models (LLMs) with traditional multimodal fusion\nmethods.Additionally, we conduct an in-depth analysis of commonly used\ndatasets, evaluating their applicability and challenges in real-world robotic\nscenarios. Building on this analysis, we identify key challenges in current\nresearch, including cross-modal alignment, efficient fusion, real-time\ndeployment, and domain adaptation. We propose future directions such as\nself-supervised learning for robust multimodal representations, structured\nspatial memory and environment modeling to enhance spatial intelligence, and\nthe integration of adversarial robustness and human feedback mechanisms to\nenable ethically aligned system deployment. Through a comprehensive review,\ncomparative analysis, and forward-looking discussion, we provide a valuable\nreference for advancing multimodal perception and interaction in robotic\nvision. A comprehensive list of studies in this survey is available at\nhttps://github.com/Xiaofeng-Han-Res/MF-RV."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Han"
                    },
                    {
                        "name": "Shunpeng Chen"
                    },
                    {
                        "name": "Zenghuang Fu"
                    },
                    {
                        "name": "Zhe Feng"
                    },
                    {
                        "name": "Lue Fan"
                    },
                    {
                        "name": "Dong An"
                    },
                    {
                        "name": "Changwei Wang"
                    },
                    {
                        "name": "Li Guo"
                    },
                    {
                        "name": "Weiliang Meng"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    },
                    {
                        "name": "Rongtao Xu"
                    },
                    {
                        "name": "Shibiao Xu"
                    }
                ],
                "author_detail": {
                    "name": "Shibiao Xu"
                },
                "author": "Shibiao Xu",
                "arxiv_doi": "10.1016/j.inffus.2025.103652",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.inffus.2025.103652",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.02477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 11 figures. Accepted to Information Fusion. Final journal\n  version: volume 126 (Part B), February 2026",
                "arxiv_journal_ref": "Information Fusion, 126 (Part B), February 2026, 103652",
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.20712v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.20712v4",
                "updated": "2025-10-15T13:41:22Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    41,
                    22,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-25T03:22:04Z",
                "published_parsed": [
                    2025,
                    9,
                    25,
                    3,
                    22,
                    4,
                    3,
                    268,
                    0
                ],
                "title": "CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CE-GPPO: Coordinating Entropy via Gradient-Preserving Clipping Policy\n  Optimization in Reinforcement Learning"
                },
                "summary": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}oordinating \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has become a powerful paradigm for optimizing\nlarge language models (LLMs) to handle complex reasoning tasks. A core\nchallenge in this process lies in managing policy entropy, which reflects the\nbalance between exploration and exploitation during training. Existing methods,\nsuch as proximal policy optimization (PPO) and its variants, discard valuable\ngradient signals from low-probability tokens due to the clipping mechanism. We\nsystematically analyze the entropy dynamics and reveal that these clipped\ntokens play a critical yet overlooked role in regulating entropy evolution. We\npropose \\textbf{C}oordinating \\textbf{E}ntropy via\n\\textbf{G}radient-\\textbf{P}reserving \\textbf{P}olicy \\textbf{O}ptimization\n(CE-GPPO), a novel algorithm that reintroduces gradients from clipped tokens in\nnative PPO in a gentle and bounded manner. By controlling the magnitude of\ngradients from tokens outside the clipping interval, CE-GPPO is able to achieve\nan exploration-exploitation trade-off. We provide theoretical justification and\nempirical evidence showing that CE-GPPO effectively mitigates entropy\ninstability. Extensive experiments on mathematical reasoning benchmarks show\nthat CE-GPPO consistently outperforms strong baselines across different model\nscales."
                },
                "authors": [
                    {
                        "name": "Zhenpeng Su"
                    },
                    {
                        "name": "Leiyu Pan"
                    },
                    {
                        "name": "Minxuan Lv"
                    },
                    {
                        "name": "Yuntao Li"
                    },
                    {
                        "name": "Wenping Hu"
                    },
                    {
                        "name": "Fuzheng Zhang"
                    },
                    {
                        "name": "Kun Gai"
                    },
                    {
                        "name": "Guorui Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Guorui Zhou"
                },
                "author": "Guorui Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.20712v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.20712v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13543v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13543v1",
                "updated": "2025-10-15T13:39:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    39,
                    13,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:39:13Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    39,
                    13,
                    2,
                    288,
                    0
                ],
                "title": "In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in\n  Agentic AI Browsers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-Browser LLM-Guided Fuzzing for Real-Time Prompt Injection Testing in\n  Agentic AI Browsers"
                },
                "summary": "Large Language Model (LLM) based agents integrated into web browsers (often\ncalled agentic AI browsers) offer powerful automation of web tasks. However,\nthey are vulnerable to indirect prompt injection attacks, where malicious\ninstructions hidden in a webpage deceive the agent into unwanted actions. These\nattacks can bypass traditional web security boundaries, as the AI agent\noperates with the user privileges across sites. In this paper, we present a\nnovel fuzzing framework that runs entirely in the browser and is guided by an\nLLM to automatically discover such prompt injection vulnerabilities in real\ntime.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) based agents integrated into web browsers (often\ncalled agentic AI browsers) offer powerful automation of web tasks. However,\nthey are vulnerable to indirect prompt injection attacks, where malicious\ninstructions hidden in a webpage deceive the agent into unwanted actions. These\nattacks can bypass traditional web security boundaries, as the AI agent\noperates with the user privileges across sites. In this paper, we present a\nnovel fuzzing framework that runs entirely in the browser and is guided by an\nLLM to automatically discover such prompt injection vulnerabilities in real\ntime."
                },
                "authors": [
                    {
                        "name": "Avihay Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Avihay Cohen"
                },
                "author": "Avihay Cohen",
                "arxiv_comment": "37 pages , 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13543v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13543v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20554v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20554v3",
                "updated": "2025-10-15T13:36:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    36,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-03-26T13:49:26Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    13,
                    49,
                    26,
                    2,
                    85,
                    0
                ],
                "title": "LACeS: An Open, Fast, Responsible, and Efficient Longitudinal Anycast\n  Census System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LACeS: An Open, Fast, Responsible, and Efficient Longitudinal Anycast\n  Census System"
                },
                "summary": "IP anycast replicates an address at multiple locations to reduce latency and\nenhance resilience. Due to anycast's crucial role in the modern Internet,\nearlier research introduced tools to perform anycast censuses. The first,\niGreedy, uses latency measurements from geographically dispersed locations to\nmap anycast deployments. The second, MAnycast2, uses anycast to perform a\ncensus of other anycast networks. MAnycast2's advantage is speed and coverage\nbut suffers from problems with accuracy, while iGreedy is highly accurate but\nslower using author-defined probing rates and costlier. In this paper we\naddress the shortcomings of both systems and present LACeS (Longitudinal\nAnycast Census System). Taking MAnycast2 as a basis, we completely redesign its\nmeasurement pipeline, and add support for distributed probing, additional\nprotocols (DNS over UDP, TCP SYN/ACK, and IPv6) and latency measurements\nsimilar to iGreedy. We validate LACeS on an anycast testbed with 32 globally\ndistributed nodes, compare against an external anycast production deployment,\nextensive latency measurements with RIPE Atlas and cross-check over 60% of\ndetected anycast using operator ground truth that shows LACeS achieves high\naccuracy. Finally, we provide a longitudinal analysis of anycast, covering 17+\nmonths, showing LACeS achieves high precision. We make continual daily LACeS\ncensuses available to the community and release the source code of the tool\nunder a permissive open source license.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IP anycast replicates an address at multiple locations to reduce latency and\nenhance resilience. Due to anycast's crucial role in the modern Internet,\nearlier research introduced tools to perform anycast censuses. The first,\niGreedy, uses latency measurements from geographically dispersed locations to\nmap anycast deployments. The second, MAnycast2, uses anycast to perform a\ncensus of other anycast networks. MAnycast2's advantage is speed and coverage\nbut suffers from problems with accuracy, while iGreedy is highly accurate but\nslower using author-defined probing rates and costlier. In this paper we\naddress the shortcomings of both systems and present LACeS (Longitudinal\nAnycast Census System). Taking MAnycast2 as a basis, we completely redesign its\nmeasurement pipeline, and add support for distributed probing, additional\nprotocols (DNS over UDP, TCP SYN/ACK, and IPv6) and latency measurements\nsimilar to iGreedy. We validate LACeS on an anycast testbed with 32 globally\ndistributed nodes, compare against an external anycast production deployment,\nextensive latency measurements with RIPE Atlas and cross-check over 60% of\ndetected anycast using operator ground truth that shows LACeS achieves high\naccuracy. Finally, we provide a longitudinal analysis of anycast, covering 17+\nmonths, showing LACeS achieves high precision. We make continual daily LACeS\ncensuses available to the community and release the source code of the tool\nunder a permissive open source license."
                },
                "authors": [
                    {
                        "name": "Remi Hendriks"
                    },
                    {
                        "name": "Matthew Luckie"
                    },
                    {
                        "name": "Mattijs Jonker"
                    },
                    {
                        "name": "Raffaele Sommese"
                    },
                    {
                        "name": "Roland van Rijswijk-Deij"
                    }
                ],
                "author_detail": {
                    "name": "Roland van Rijswijk-Deij"
                },
                "author": "Roland van Rijswijk-Deij",
                "arxiv_comment": "14 pages + 3 pages appendix, 14 figures, 6 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20554v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20554v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13537v1",
                "updated": "2025-10-15T13:32:25Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    32,
                    25,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:32:25Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    32,
                    25,
                    2,
                    288,
                    0
                ],
                "title": "K-Merge: Online Continual Merging of Adapters for On-device Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "K-Merge: Online Continual Merging of Adapters for On-device Large\n  Language Models"
                },
                "summary": "On-device deployment of Large Language Models (LLMs) frequently leverages\nLow-Rank Adapters (LoRAs) to support diverse downstream tasks under tight\nresource constraints. To address the limited storage capacity of mobile\ndevices, recent works have explored model merging techniques to fuse multiple\nLoRAs into a single one. In practice, however, LoRAs are often delivered\nincrementally, as users request support for new tasks (e.g., novel problem\ntypes or languages). This scenario introduces a new challenge: on-device online\ncontinual merging, where the objective is to incorporate new LoRAs while\npreserving the performance on previously supported tasks. In this paper, we\npropose a data-free and computationally efficient strategy for selecting and\nmerging LoRAs when a new one becomes available, assuming the device can store\nonly a limited number of adapters. Extensive experiments across real-world\ntasks demonstrate the superiority of our approach compared to alternative\nstrategies while adhering to the storage budget and compute limitations of\non-device settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device deployment of Large Language Models (LLMs) frequently leverages\nLow-Rank Adapters (LoRAs) to support diverse downstream tasks under tight\nresource constraints. To address the limited storage capacity of mobile\ndevices, recent works have explored model merging techniques to fuse multiple\nLoRAs into a single one. In practice, however, LoRAs are often delivered\nincrementally, as users request support for new tasks (e.g., novel problem\ntypes or languages). This scenario introduces a new challenge: on-device online\ncontinual merging, where the objective is to incorporate new LoRAs while\npreserving the performance on previously supported tasks. In this paper, we\npropose a data-free and computationally efficient strategy for selecting and\nmerging LoRAs when a new one becomes available, assuming the device can store\nonly a limited number of adapters. Extensive experiments across real-world\ntasks demonstrate the superiority of our approach compared to alternative\nstrategies while adhering to the storage budget and compute limitations of\non-device settings."
                },
                "authors": [
                    {
                        "name": "Donald Shenaj"
                    },
                    {
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "name": "Taha Ceritli"
                    },
                    {
                        "name": "Mete Ozay"
                    },
                    {
                        "name": "Pietro Zanuttigh"
                    },
                    {
                        "name": "Umberto Michieli"
                    }
                ],
                "author_detail": {
                    "name": "Umberto Michieli"
                },
                "author": "Umberto Michieli",
                "arxiv_comment": "15 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24852v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24852v2",
                "updated": "2025-10-15T13:26:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    26,
                    28,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-29T14:38:57Z",
                "published_parsed": [
                    2025,
                    9,
                    29,
                    14,
                    38,
                    57,
                    0,
                    272,
                    0
                ],
                "title": "DelRec: learning delays in recurrent spiking neural networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DelRec: learning delays in recurrent spiking neural networks"
                },
                "summary": "Spiking neural networks (SNNs) are a bio-inspired alternative to conventional\nreal-valued deep learning models, with the potential for substantially higher\nenergy efficiency. Interest in SNNs has recently exploded due to a major\nbreakthrough: surrogate gradient learning (SGL), which allows training SNNs\nwith backpropagation, strongly outperforming other approaches. In SNNs, each\nsynapse is characterized not only by a weight but also by a transmission delay.\nWhile theoretical works have long suggested that trainable delays significantly\nenhance expressivity, practical methods for learning them have only recently\nemerged. Here, we introduce ``DelRec'', the first SGL-based method to train\naxonal or synaptic delays in recurrent spiking layers, compatible with any\nspiking neuron model. DelRec leverages a differentiable interpolation technique\nto handle non-integer delays with well-defined gradients at training time. We\nshow that SNNs with trainable recurrent delays outperform feedforward ones,\nleading to new state-of-the-art (SOTA) on two challenging temporal datasets\n(Spiking Speech Command, an audio dataset, and Permuted Sequential MNIST, a\nvision one), and match the SOTA on the now saturated Spiking Heidelberg Digit\ndataset using only vanilla Leaky-Integrate-and-Fire neurons with stateless\n(instantaneous) synapses. Our results demonstrate that recurrent delays are\ncritical for temporal processing in SNNs and can be effectively optimized with\nDelRec, paving the way for efficient deployment on neuromorphic hardware with\nprogrammable delays. Our code is available at\nhttps://github.com/alexmaxad/DelRec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural networks (SNNs) are a bio-inspired alternative to conventional\nreal-valued deep learning models, with the potential for substantially higher\nenergy efficiency. Interest in SNNs has recently exploded due to a major\nbreakthrough: surrogate gradient learning (SGL), which allows training SNNs\nwith backpropagation, strongly outperforming other approaches. In SNNs, each\nsynapse is characterized not only by a weight but also by a transmission delay.\nWhile theoretical works have long suggested that trainable delays significantly\nenhance expressivity, practical methods for learning them have only recently\nemerged. Here, we introduce ``DelRec'', the first SGL-based method to train\naxonal or synaptic delays in recurrent spiking layers, compatible with any\nspiking neuron model. DelRec leverages a differentiable interpolation technique\nto handle non-integer delays with well-defined gradients at training time. We\nshow that SNNs with trainable recurrent delays outperform feedforward ones,\nleading to new state-of-the-art (SOTA) on two challenging temporal datasets\n(Spiking Speech Command, an audio dataset, and Permuted Sequential MNIST, a\nvision one), and match the SOTA on the now saturated Spiking Heidelberg Digit\ndataset using only vanilla Leaky-Integrate-and-Fire neurons with stateless\n(instantaneous) synapses. Our results demonstrate that recurrent delays are\ncritical for temporal processing in SNNs and can be effectively optimized with\nDelRec, paving the way for efficient deployment on neuromorphic hardware with\nprogrammable delays. Our code is available at\nhttps://github.com/alexmaxad/DelRec."
                },
                "authors": [
                    {
                        "name": "Alexandre Queant"
                    },
                    {
                        "name": "Ulysse Ran√ßon"
                    },
                    {
                        "name": "Benoit R Cottereau"
                    },
                    {
                        "name": "Timoth√©e Masquelier"
                    }
                ],
                "author_detail": {
                    "name": "Timoth√©e Masquelier"
                },
                "author": "Timoth√©e Masquelier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24852v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24852v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.12448v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.12448v2",
                "updated": "2025-10-15T13:24:57Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    24,
                    57,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-16T17:40:58Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    17,
                    40,
                    58,
                    2,
                    197,
                    0
                ],
                "title": "Jenga-Krotov algorithm: Efficient compilation of multi-qubit gates for\n  exchange-only qubits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Jenga-Krotov algorithm: Efficient compilation of multi-qubit gates for\n  exchange-only qubits"
                },
                "summary": "Exchange-only (EO) qubits, implemented in triple-quantum-dot systems, offer a\ncompelling platform for scalable semiconductor-based quantum computing by\nenabling universal control through purely exchange interactions. While\nhigh-fidelity single- and two-qubit gates have been demonstrated, the synthesis\nof efficient multi-qubit operations-such as the Toffoli gate-remains a key\nbottleneck. Conventional gate decompositions into elementary operations lead to\nprohibitively long and error-prone pulse sequences, limiting practical\ndeployment. In this work, we introduce a gradient-based optimization algorithm,\nJenga-Krotov (JK), tailored to discover compact, high-fidelity EO gate\nsequences. Applying JK to the Toffoli gate, we reduce the number of required\nexchange unitaries from 216 (in direct decomposition) to 92, and compress the\ntime steps required from 162 to 50, all while maintaining target fidelity.\nUnder realistic noise, the accumulated gate error from our optimized sequence\nis an order of magnitude lower than that of conventional approaches. We have\nalso applied the JK algorithm to other multi-qubit gates and algorithm. For the\nFredkin gate, it reduces the number of time steps from 200 to 104 and the\nnumber of exchange unitaries from 276 to 172. For the quantum Fourier\ntransform, it compresses the sequence from 180 to 80 time steps and from 237 to\n202 exchange unitaries. These results demonstrate that the JK algorithm is a\ngeneral and scalable strategy for multi-qubit gate synthesis in EO\narchitectures, potentially facilitating realization of multi-qubit algorithms\non semiconductor platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exchange-only (EO) qubits, implemented in triple-quantum-dot systems, offer a\ncompelling platform for scalable semiconductor-based quantum computing by\nenabling universal control through purely exchange interactions. While\nhigh-fidelity single- and two-qubit gates have been demonstrated, the synthesis\nof efficient multi-qubit operations-such as the Toffoli gate-remains a key\nbottleneck. Conventional gate decompositions into elementary operations lead to\nprohibitively long and error-prone pulse sequences, limiting practical\ndeployment. In this work, we introduce a gradient-based optimization algorithm,\nJenga-Krotov (JK), tailored to discover compact, high-fidelity EO gate\nsequences. Applying JK to the Toffoli gate, we reduce the number of required\nexchange unitaries from 216 (in direct decomposition) to 92, and compress the\ntime steps required from 162 to 50, all while maintaining target fidelity.\nUnder realistic noise, the accumulated gate error from our optimized sequence\nis an order of magnitude lower than that of conventional approaches. We have\nalso applied the JK algorithm to other multi-qubit gates and algorithm. For the\nFredkin gate, it reduces the number of time steps from 200 to 104 and the\nnumber of exchange unitaries from 276 to 172. For the quantum Fourier\ntransform, it compresses the sequence from 180 to 80 time steps and from 237 to\n202 exchange unitaries. These results demonstrate that the JK algorithm is a\ngeneral and scalable strategy for multi-qubit gate synthesis in EO\narchitectures, potentially facilitating realization of multi-qubit algorithms\non semiconductor platforms."
                },
                "authors": [
                    {
                        "name": "Jiahao Wu"
                    },
                    {
                        "name": "Guanjie He"
                    },
                    {
                        "name": "Wenyuan Zhuo"
                    },
                    {
                        "name": "Quan Fu"
                    },
                    {
                        "name": "Xin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xin Wang"
                },
                "author": "Xin Wang",
                "arxiv_comment": "16+12 pages, 11+2 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.12448v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.12448v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13524v1",
                "updated": "2025-10-15T13:17:16Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    17,
                    16,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:17:16Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    17,
                    16,
                    2,
                    288,
                    0
                ],
                "title": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within\n  the Financial Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodology for Assessing the Risk of Metric Failure in LLMs Within\n  the Financial Domain"
                },
                "summary": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Generative Artificial Intelligence is adopted across the financial\nservices industry, a significant barrier to adoption and usage is measuring\nmodel performance. Historical machine learning metrics can oftentimes fail to\ngeneralize to GenAI workloads and are often supplemented using Subject Matter\nExpert (SME) Evaluation. Even in this combination, many projects fail to\naccount for various unique risks present in choosing specific metrics.\nAdditionally, many widespread benchmarks created by foundational research labs\nand educational institutions fail to generalize to industrial use. This paper\nexplains these challenges and provides a Risk Assessment Framework to allow for\nbetter application of SME and machine learning Metrics"
                },
                "authors": [
                    {
                        "name": "William Flanagan"
                    },
                    {
                        "name": "Mukunda Das"
                    },
                    {
                        "name": "Rajitha Ramanyake"
                    },
                    {
                        "name": "Swaunja Maslekar"
                    },
                    {
                        "name": "Meghana Manipuri"
                    },
                    {
                        "name": "Joong Ho Choi"
                    },
                    {
                        "name": "Shruti Nair"
                    },
                    {
                        "name": "Shambhavi Bhusan"
                    },
                    {
                        "name": "Sanjana Dulam"
                    },
                    {
                        "name": "Mouni Pendharkar"
                    },
                    {
                        "name": "Nidhi Singh"
                    },
                    {
                        "name": "Vashisth Doshi"
                    },
                    {
                        "name": "Sachi Shah Paresh"
                    }
                ],
                "author_detail": {
                    "name": "Sachi Shah Paresh"
                },
                "author": "Sachi Shah Paresh",
                "arxiv_comment": "NeurIPS 2025 GenAI in Finance Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13517v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13517v1",
                "updated": "2025-10-15T13:09:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    9,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:09:31Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    9,
                    31,
                    2,
                    288,
                    0
                ],
                "title": "The Impact of Renewable Energy Communities in the Italian Day-Ahead\n  Electricity Market: A Scenario Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Impact of Renewable Energy Communities in the Italian Day-Ahead\n  Electricity Market: A Scenario Analysis"
                },
                "summary": "This paper evaluates the economic impact of Renewable Energy Communities\n(RECs) on the Italian wholesale power market. Combining a bottom-up engineering\napproach with a short-run economic impact assessment, the study begins by\nmapping existing and emerging RECs in Italy. We identify key characteristics of\nRECs, such as average installed capacity, institutional profiles of members,\ntypes of renewable systems used, and distribution across Italy's electricity\nmarket zones. This mapping yields representative REC configurations, which are\nemployed within a bottom-up engineering model to generate energy injection and\nself-consumption profiles for different REC prosumer and producer categories\n(residential, public, small and medium enterprise, non-profit organization, and\nstandalone installation), considering the different levels of solar irradiance\nin Italy based on latitude. These zonal results, aggregated on an hourly basis,\ninform the implementation of the synthetic counterfactual approach, which\ndevelops alternative scenarios (e.g., 5 GW target for REC-driven capacity set\nby Italian policy for 2027) to assess the impact of REC-driven injection and\nself-consumption on the Italian day-ahead power market. The findings suggest\nthat REC deployment can increase equilibrium quantities during daylight in most\nof the time, while decreasing equilibrium quantities mostly during the cold\nmonths, as electrified heating drives greater self-consumption and offsets\nlower grid injections. Both positive and negative effects on equilibrium\nquantities suggest that REC deployment also has a potential to reduce wholesale\nelectricity prices. Moreover, by reducing grid exchanges through higher\nself-consumption, REC proliferation can alleviate pressure on the distribution\nsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper evaluates the economic impact of Renewable Energy Communities\n(RECs) on the Italian wholesale power market. Combining a bottom-up engineering\napproach with a short-run economic impact assessment, the study begins by\nmapping existing and emerging RECs in Italy. We identify key characteristics of\nRECs, such as average installed capacity, institutional profiles of members,\ntypes of renewable systems used, and distribution across Italy's electricity\nmarket zones. This mapping yields representative REC configurations, which are\nemployed within a bottom-up engineering model to generate energy injection and\nself-consumption profiles for different REC prosumer and producer categories\n(residential, public, small and medium enterprise, non-profit organization, and\nstandalone installation), considering the different levels of solar irradiance\nin Italy based on latitude. These zonal results, aggregated on an hourly basis,\ninform the implementation of the synthetic counterfactual approach, which\ndevelops alternative scenarios (e.g., 5 GW target for REC-driven capacity set\nby Italian policy for 2027) to assess the impact of REC-driven injection and\nself-consumption on the Italian day-ahead power market. The findings suggest\nthat REC deployment can increase equilibrium quantities during daylight in most\nof the time, while decreasing equilibrium quantities mostly during the cold\nmonths, as electrified heating drives greater self-consumption and offsets\nlower grid injections. Both positive and negative effects on equilibrium\nquantities suggest that REC deployment also has a potential to reduce wholesale\nelectricity prices. Moreover, by reducing grid exchanges through higher\nself-consumption, REC proliferation can alleviate pressure on the distribution\nsystem."
                },
                "authors": [
                    {
                        "name": "Maksym Koltunov"
                    },
                    {
                        "name": "Filippo Beltrami"
                    },
                    {
                        "name": "Luigi Grossi"
                    },
                    {
                        "name": "Nicola Blasuttigh"
                    }
                ],
                "author_detail": {
                    "name": "Nicola Blasuttigh"
                },
                "author": "Nicola Blasuttigh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13517v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13517v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13514v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13514v1",
                "updated": "2025-10-15T13:06:56Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    6,
                    56,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T13:06:56Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    6,
                    56,
                    2,
                    288,
                    0
                ],
                "title": "Quantifying the Impact of Missing Risk Markets for Decarbonized Power\n  Systems with Long Duration Energy Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying the Impact of Missing Risk Markets for Decarbonized Power\n  Systems with Long Duration Energy Storage"
                },
                "summary": "The transition to a fully decarbonised electricity system depends on\nintegrating new technologies that ensure reliability alongside sustainability.\nHowever, missing risk markets hinder investment in reliability-enhancing\ntechnologies by exposing investors to revenue uncertainty. This study provides\nthe first quantitative assessment of how missing risk markets affect investment\ndecisions in power systems that depend on long-duration energy storage (LDES)\nfor reliability. We develop a two-stage stochastic equilibrium model with\nrisk-averse market participants, which independently sizes power and energy\ncapacity. We apply the method to a case study of a deeply decarbonised power\nsystem in Great Britain. The results show that incomplete risk markets reduce\nsocial welfare, harm reliability, and discourage investment in LDES and other\ntechnologies with volatile revenue streams. Revenue volatility leads to\nsubstantial risk premiums and higher financing costs for LDES, creating a\nbarrier to its large-scale deployment. These findings demonstrate the\nimportance of policy mechanisms that hedge revenue risk to lower the cost of\ncapital and accelerate investment in reliability-enhancing, zero-carbon\ntechnologies",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition to a fully decarbonised electricity system depends on\nintegrating new technologies that ensure reliability alongside sustainability.\nHowever, missing risk markets hinder investment in reliability-enhancing\ntechnologies by exposing investors to revenue uncertainty. This study provides\nthe first quantitative assessment of how missing risk markets affect investment\ndecisions in power systems that depend on long-duration energy storage (LDES)\nfor reliability. We develop a two-stage stochastic equilibrium model with\nrisk-averse market participants, which independently sizes power and energy\ncapacity. We apply the method to a case study of a deeply decarbonised power\nsystem in Great Britain. The results show that incomplete risk markets reduce\nsocial welfare, harm reliability, and discourage investment in LDES and other\ntechnologies with volatile revenue streams. Revenue volatility leads to\nsubstantial risk premiums and higher financing costs for LDES, creating a\nbarrier to its large-scale deployment. These findings demonstrate the\nimportance of policy mechanisms that hedge revenue risk to lower the cost of\ncapital and accelerate investment in reliability-enhancing, zero-carbon\ntechnologies"
                },
                "authors": [
                    {
                        "name": "Andreas C. Makrides"
                    },
                    {
                        "name": "Adam Suski"
                    },
                    {
                        "name": "Elina Spyrou"
                    }
                ],
                "author_detail": {
                    "name": "Elina Spyrou"
                },
                "author": "Elina Spyrou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13514v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13514v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.08891v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.08891v4",
                "updated": "2025-10-15T13:04:09Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    4,
                    9,
                    2,
                    288,
                    0
                ],
                "published": "2024-10-28T06:41:05Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    6,
                    41,
                    5,
                    0,
                    302,
                    0
                ],
                "title": "Reliable Decision Making via Calibration Oriented Retrieval Augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reliable Decision Making via Calibration Oriented Retrieval Augmented\n  Generation"
                },
                "summary": "Recently, Large Language Models (LLMs) have been increasingly used to support\nvarious decision-making tasks, assisting humans in making informed decisions.\nHowever, when LLMs confidently provide incorrect information, it can lead\nhumans to make suboptimal decisions. To prevent LLMs from generating incorrect\ninformation on topics they are unsure of and to improve the accuracy of\ngenerated content, prior works have proposed Retrieval Augmented Generation\n(RAG), where external documents are referenced to generate responses. However,\nprevious RAG methods focus only on retrieving documents most relevant to the\ninput query, without specifically aiming to ensure that the human user's\ndecisions are well-calibrated. To address this limitation, we propose a novel\nretrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG),\nwhich ensures that decisions informed by RAG are well-calibrated. Then we\nempirically validate that CalibRAG improves calibration performance as well as\naccuracy, compared to other baselines across various datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, Large Language Models (LLMs) have been increasingly used to support\nvarious decision-making tasks, assisting humans in making informed decisions.\nHowever, when LLMs confidently provide incorrect information, it can lead\nhumans to make suboptimal decisions. To prevent LLMs from generating incorrect\ninformation on topics they are unsure of and to improve the accuracy of\ngenerated content, prior works have proposed Retrieval Augmented Generation\n(RAG), where external documents are referenced to generate responses. However,\nprevious RAG methods focus only on retrieving documents most relevant to the\ninput query, without specifically aiming to ensure that the human user's\ndecisions are well-calibrated. To address this limitation, we propose a novel\nretrieval method called Calibrated Retrieval-Augmented Generation (CalibRAG),\nwhich ensures that decisions informed by RAG are well-calibrated. Then we\nempirically validate that CalibRAG improves calibration performance as well as\naccuracy, compared to other baselines across various datasets."
                },
                "authors": [
                    {
                        "name": "Chaeyun Jang"
                    },
                    {
                        "name": "Deukhwan Cho"
                    },
                    {
                        "name": "Seanie Lee"
                    },
                    {
                        "name": "Hyungi Lee"
                    },
                    {
                        "name": "Juho Lee"
                    }
                ],
                "author_detail": {
                    "name": "Juho Lee"
                },
                "author": "Juho Lee",
                "arxiv_comment": "Accepted by NeurIPS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.08891v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.08891v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04364v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04364v4",
                "updated": "2025-10-15T13:02:18Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    13,
                    2,
                    18,
                    2,
                    288,
                    0
                ],
                "published": "2025-05-07T12:32:01Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    12,
                    32,
                    1,
                    2,
                    127,
                    0
                ],
                "title": "Benchmarking LLMs' Swarm intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking LLMs' Swarm intelligence"
                },
                "summary": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict swarm-like constraints-limited local perception and\ncommunication-remains largely unexplored. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination when agents\noperate with incomplete spatio-temporal information. To bridge this gap, we\nintroduce SwarmBench, a novel benchmark designed to systematically evaluate the\nswarm intelligence capabilities of LLMs acting as decentralized agents.\nSwarmBench features five foundational MAS coordination tasks (Pursuit,\nSynchronization, Foraging, Flocking, Transport) within a configurable 2D grid\nenvironment, forcing agents to rely solely on local sensory input ($k\\times k$\nview) and local communication. We propose metrics for coordination\neffectiveness and analyze emergent group dynamics. Zero-shot evaluations of\nleading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent\nperformance variations. While some rudimentary coordination is observed, our\nresults indicate that current LLMs significantly struggle with robust\nlong-range planning and adaptive strategy formation under the uncertainty\ninherent in these decentralized scenarios. Assessing LLMs under such swarm-like\nconstraints is crucial for understanding their utility in future decentralized\nintelligent systems. We release SwarmBench as an open, extensible toolkit-built\non a customizable physical system-providing environments, prompts, evaluation\nscripts, and comprehensive datasets. This aims to foster reproducible research\ninto LLM-based MAS coordination and the theoretical underpinnings of emergent\ncollective behavior under severe informational decentralization. Our code\nrepository is available at https://github.com/x66ccff/swarmbench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show potential for complex reasoning, yet their\ncapacity for emergent coordination in Multi-Agent Systems (MAS) when operating\nunder strict swarm-like constraints-limited local perception and\ncommunication-remains largely unexplored. Existing benchmarks often do not\nfully capture the unique challenges of decentralized coordination when agents\noperate with incomplete spatio-temporal information. To bridge this gap, we\nintroduce SwarmBench, a novel benchmark designed to systematically evaluate the\nswarm intelligence capabilities of LLMs acting as decentralized agents.\nSwarmBench features five foundational MAS coordination tasks (Pursuit,\nSynchronization, Foraging, Flocking, Transport) within a configurable 2D grid\nenvironment, forcing agents to rely solely on local sensory input ($k\\times k$\nview) and local communication. We propose metrics for coordination\neffectiveness and analyze emergent group dynamics. Zero-shot evaluations of\nleading LLMs (e.g., deepseek-v3, o4-mini) reveal significant task-dependent\nperformance variations. While some rudimentary coordination is observed, our\nresults indicate that current LLMs significantly struggle with robust\nlong-range planning and adaptive strategy formation under the uncertainty\ninherent in these decentralized scenarios. Assessing LLMs under such swarm-like\nconstraints is crucial for understanding their utility in future decentralized\nintelligent systems. We release SwarmBench as an open, extensible toolkit-built\non a customizable physical system-providing environments, prompts, evaluation\nscripts, and comprehensive datasets. This aims to foster reproducible research\ninto LLM-based MAS coordination and the theoretical underpinnings of emergent\ncollective behavior under severe informational decentralization. Our code\nrepository is available at https://github.com/x66ccff/swarmbench."
                },
                "authors": [
                    {
                        "name": "Kai Ruan"
                    },
                    {
                        "name": "Mowen Huang"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    },
                    {
                        "name": "Hao Sun"
                    }
                ],
                "author_detail": {
                    "name": "Hao Sun"
                },
                "author": "Hao Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04364v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04364v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14138v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14138v5",
                "updated": "2025-10-15T12:56:42Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    56,
                    42,
                    2,
                    288,
                    0
                ],
                "published": "2024-10-18T03:22:06Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    3,
                    22,
                    6,
                    4,
                    292,
                    0
                ],
                "title": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProReason: Multi-Modal Proactive Reasoning with Decoupled Eyesight and\n  Wisdom"
                },
                "summary": "Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., limited multi-modal reasoning capacities, and insufficient and\nirrelevant visual descriptions). We then decompose visual reasoning process\ninto two stages: proactive visual perception (i.e., eyesight) and textual\nreasoning (i.e., wisdom), and introduce a novel visual reasoning framework\nnamed ProReason. This framework features decoupled vision-reasoning\ncapabilities and multi-run proactive perception. Briefly, given a multi-modal\nquestion, ProReason iterates proactive information collection and reasoning\nuntil the answer can be concluded with necessary and sufficient visual\ndescriptions. Notably, the disassociation of capabilities allows seamless\nintegration of existing large language models (LLMs) to compensate for the\nreasoning deficits of LVLMs. Our extensive experiments demonstrate that\nProReason outperforms existing multi-step reasoning frameworks on various\nbenchmarks for both open-source and closed-source models, with the average\nperformance gain reaching 13.2%. Besides, the integration of LLMs allows\nProReason to produce high-quality visual reasoning data, which empowers\nProReason-distilled models (i.e., ProReason-VL and ProReason-Q3) to achieve\nsuperior performance in downstream tasks. Our insights into existing solutions\nand the decoupled perspective for feasible integration of LLMs illuminate\nfuture research on visual reasoning techniques, especially LLM-assisted ones.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have witnessed significant progress on\nvisual understanding tasks. However, they often prioritize language knowledge\nover image information on visual reasoning tasks, incurring performance\ndegradation. To tackle this issue, we first identify the drawbacks of existing\nsolutions (i.e., limited multi-modal reasoning capacities, and insufficient and\nirrelevant visual descriptions). We then decompose visual reasoning process\ninto two stages: proactive visual perception (i.e., eyesight) and textual\nreasoning (i.e., wisdom), and introduce a novel visual reasoning framework\nnamed ProReason. This framework features decoupled vision-reasoning\ncapabilities and multi-run proactive perception. Briefly, given a multi-modal\nquestion, ProReason iterates proactive information collection and reasoning\nuntil the answer can be concluded with necessary and sufficient visual\ndescriptions. Notably, the disassociation of capabilities allows seamless\nintegration of existing large language models (LLMs) to compensate for the\nreasoning deficits of LVLMs. Our extensive experiments demonstrate that\nProReason outperforms existing multi-step reasoning frameworks on various\nbenchmarks for both open-source and closed-source models, with the average\nperformance gain reaching 13.2%. Besides, the integration of LLMs allows\nProReason to produce high-quality visual reasoning data, which empowers\nProReason-distilled models (i.e., ProReason-VL and ProReason-Q3) to achieve\nsuperior performance in downstream tasks. Our insights into existing solutions\nand the decoupled perspective for feasible integration of LLMs illuminate\nfuture research on visual reasoning techniques, especially LLM-assisted ones."
                },
                "authors": [
                    {
                        "name": "Jingqi Zhou"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Jingwei Dong"
                    },
                    {
                        "name": "Kai Liu"
                    },
                    {
                        "name": "Lei Li"
                    },
                    {
                        "name": "Jiahui Gao"
                    },
                    {
                        "name": "Jiyue Jiang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14138v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14138v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13501v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13501v1",
                "updated": "2025-10-15T12:51:47Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    51,
                    47,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:51:47Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    51,
                    47,
                    2,
                    288,
                    0
                ],
                "title": "Confidence as a Reward: Transforming LLMs into Reward Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Confidence as a Reward: Transforming LLMs into Reward Models"
                },
                "summary": "Reward models can significantly enhance the reasoning capabilities of large\nlanguage models (LLMs), but they typically require extensive curated data and\ncostly training. To mitigate these challenges, training-free approaches such as\nLLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate\nresponses, achieving promising results. Recent works have also indicated that\nmodel confidence can serve effectively as a reward metric, distinguishing\nbetween chain-of-thought (CoT) and non-CoT paths. However, the concept of using\nconfidence as a reward has not been comprehensively studied. In this work, we\nsystematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful\ntraining-free method that utilizes token-level confidence in the model's final\nanswers as a proxy for reward, especially suitable for close-ended tasks.\nThrough extensive experiments on mathematical reasoning tasks, we demonstrate\nthat CRew outperforms existing training-free reward approaches on the MATH500\nand RewardMATH benchmarks, and even surpasses most trained reward models. We\nfurther identify a strong correlation between CRew scores and the actual\nreasoning performance of the model. Additionally, we find that CRew can\neffectively filter high-quality training data. Building upon these insights, we\npropose CRew-DPO, a training strategy that constructs preference data from\nconfidence scores combined with correctness signals. Finetuning with CRew-DPO\nfurther enhances the model's judging capabilities and consistently outperforms\nexisting self-training methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reward models can significantly enhance the reasoning capabilities of large\nlanguage models (LLMs), but they typically require extensive curated data and\ncostly training. To mitigate these challenges, training-free approaches such as\nLLM-as-a-Judge leverage the intrinsic reasoning abilities of LLMs to evaluate\nresponses, achieving promising results. Recent works have also indicated that\nmodel confidence can serve effectively as a reward metric, distinguishing\nbetween chain-of-thought (CoT) and non-CoT paths. However, the concept of using\nconfidence as a reward has not been comprehensively studied. In this work, we\nsystematically investigate Confidence-as-a-Reward (CRew), a simple yet powerful\ntraining-free method that utilizes token-level confidence in the model's final\nanswers as a proxy for reward, especially suitable for close-ended tasks.\nThrough extensive experiments on mathematical reasoning tasks, we demonstrate\nthat CRew outperforms existing training-free reward approaches on the MATH500\nand RewardMATH benchmarks, and even surpasses most trained reward models. We\nfurther identify a strong correlation between CRew scores and the actual\nreasoning performance of the model. Additionally, we find that CRew can\neffectively filter high-quality training data. Building upon these insights, we\npropose CRew-DPO, a training strategy that constructs preference data from\nconfidence scores combined with correctness signals. Finetuning with CRew-DPO\nfurther enhances the model's judging capabilities and consistently outperforms\nexisting self-training methods."
                },
                "authors": [
                    {
                        "name": "He Du"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Chengxing Xie"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Dacheng Tao"
                    }
                ],
                "author_detail": {
                    "name": "Dacheng Tao"
                },
                "author": "Dacheng Tao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13501v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13501v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13500v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13500v1",
                "updated": "2025-10-15T12:50:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    50,
                    33,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:50:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    50,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MedREK: Retrieval-Based Editing for Medical LLMs with Key-Aware Prompts"
                },
                "summary": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, \\hk{an enhanced\nbenchmark with broader coverage of medical subjects, designed to evaluate both\nsingle and batch edits under strict locality constraints}. We then propose\nMedREK, a retrieval-based editing framework that integrates a shared query-key\nmodule for precise matching with an attention-based prompt encoder for\ninformative guidance. Experimental results on various medical benchmarks\ndemonstrate that our MedREK achieves superior performance across different core\nmetrics and provides the first validated solution for batch-editing in medical\nLLMs. Our code and dataset are available at\nhttps://github.com/mylittleriver/MedREK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs hold great promise for healthcare applications, but the rapid evolution\nof medical knowledge and errors in training data often cause them to generate\noutdated or inaccurate information, limiting their applicability in high-stakes\nclinical practice. Model editing has emerged as a potential remedy without full\nretraining. While parameter-based editing often compromises locality and is\nthus ill-suited for the medical domain, retrieval-based editing offers a more\nviable alternative. However, it still faces two critical challenges: (1)\nrepresentation overlap within the medical knowledge space often causes\ninaccurate retrieval and reduces editing accuracy; (2) existing methods are\nrestricted to single-sample edits, while batch-editing remains largely\nunexplored despite its importance for real-world medical applications. To\naddress these challenges, we first construct MedVersa, \\hk{an enhanced\nbenchmark with broader coverage of medical subjects, designed to evaluate both\nsingle and batch edits under strict locality constraints}. We then propose\nMedREK, a retrieval-based editing framework that integrates a shared query-key\nmodule for precise matching with an attention-based prompt encoder for\ninformative guidance. Experimental results on various medical benchmarks\ndemonstrate that our MedREK achieves superior performance across different core\nmetrics and provides the first validated solution for batch-editing in medical\nLLMs. Our code and dataset are available at\nhttps://github.com/mylittleriver/MedREK."
                },
                "authors": [
                    {
                        "name": "Shujun Xia"
                    },
                    {
                        "name": "Haokun Lin"
                    },
                    {
                        "name": "Yichen Wu"
                    },
                    {
                        "name": "Yinan Zhou"
                    },
                    {
                        "name": "Zixuan Li"
                    },
                    {
                        "name": "Zhongwei Wan"
                    },
                    {
                        "name": "Xingrun Xing"
                    },
                    {
                        "name": "Yefeng Zheng"
                    },
                    {
                        "name": "Xiang Li"
                    },
                    {
                        "name": "Caifeng Shan"
                    },
                    {
                        "name": "Zhenan Sun"
                    },
                    {
                        "name": "Quanzheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Quanzheng Li"
                },
                "author": "Quanzheng Li",
                "arxiv_comment": "Preprint, work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13500v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13500v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13499v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13499v1",
                "updated": "2025-10-15T12:49:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    49,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:49:45Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    49,
                    45,
                    2,
                    288,
                    0
                ],
                "title": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConsintBench: Evaluating Language Models on Real-World Consumer Intent\n  Understanding"
                },
                "summary": "Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding human intent is a complex, high-level task for large language\nmodels (LLMs), requiring analytical reasoning, contextual interpretation,\ndynamic information aggregation, and decision-making under uncertainty.\nReal-world public discussions, such as consumer product discussions, are rarely\nlinear or involve a single user. Instead, they are characterized by interwoven\nand often conflicting perspectives, divergent concerns, goals, emotional\ntendencies, as well as implicit assumptions and background knowledge about\nusage scenarios. To accurately understand such explicit public intent, an LLM\nmust go beyond parsing individual sentences; it must integrate multi-source\nsignals, reason over inconsistencies, and adapt to evolving discourse, similar\nto how experts in fields like politics, economics, or finance approach complex,\nuncertain environments. Despite the importance of this capability, no\nlarge-scale benchmark currently exists for evaluating LLMs on real-world human\nintent understanding, primarily due to the challenges of collecting real-world\npublic discussion data and constructing a robust evaluation pipeline. To bridge\nthis gap, we introduce \\bench, the first dynamic, live evaluation benchmark\nspecifically designed for intent understanding, particularly in the consumer\ndomain. \\bench is the largest and most diverse benchmark of its kind,\nsupporting real-time updates while preventing data contamination through an\nautomated curation pipeline."
                },
                "authors": [
                    {
                        "name": "Xiaozhe Li"
                    },
                    {
                        "name": "TianYi Lyu"
                    },
                    {
                        "name": "Siyi Yang"
                    },
                    {
                        "name": "Yuxi Gong"
                    },
                    {
                        "name": "Yizhao Yang"
                    },
                    {
                        "name": "Jinxuan Huang"
                    },
                    {
                        "name": "Ligao Zhang"
                    },
                    {
                        "name": "Zhuoyi Huang"
                    },
                    {
                        "name": "Qingwen Liu"
                    }
                ],
                "author_detail": {
                    "name": "Qingwen Liu"
                },
                "author": "Qingwen Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13499v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13499v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19147v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19147v3",
                "updated": "2025-10-15T12:46:12Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    46,
                    12,
                    2,
                    288,
                    0
                ],
                "published": "2024-11-28T13:46:49Z",
                "published_parsed": [
                    2024,
                    11,
                    28,
                    13,
                    46,
                    49,
                    3,
                    333,
                    0
                ],
                "title": "Spectrum Efficiency and Processing Latency Trade-offs in Panel-Based LIS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spectrum Efficiency and Processing Latency Trade-offs in Panel-Based LIS"
                },
                "summary": "The next generation wireless systems will face stringent new requirements,\nincluding ultra-low latency, high data rates and enhanced reliability. Large\nIntelligent Surfaces, is one proposed solution that has the potential to solve\nthese high demands. The real-life deployment of such systems involves different\ndesign considerations with non-trivial trade-offs. This paper investigates the\ntrade-off between spectral efficiency and processing latency, considering\ndifferent antenna distribution schemes and detection algorithms. A latency\nmodel for the physical layer processing has been developed, using real FPGA and\napplication-specific instruction processor (ASIP) hardware implementation\nresults. Simulation results using an indoor environment show that distributing\nantennas throughout the scenario improves overall reliability, while the impact\nfrom this on latency is limited both when using zero-forcing (ZF) and Minimum\nMean Square Error (MMSE) detection. Changing the detection algorithm to\nmaximum-ratio combining (MRC) from ZF or MMSE, however, reduces the latency\nsignificantly, even if a larger number of antennas are needed to achieve a\nsimilar spectrum efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The next generation wireless systems will face stringent new requirements,\nincluding ultra-low latency, high data rates and enhanced reliability. Large\nIntelligent Surfaces, is one proposed solution that has the potential to solve\nthese high demands. The real-life deployment of such systems involves different\ndesign considerations with non-trivial trade-offs. This paper investigates the\ntrade-off between spectral efficiency and processing latency, considering\ndifferent antenna distribution schemes and detection algorithms. A latency\nmodel for the physical layer processing has been developed, using real FPGA and\napplication-specific instruction processor (ASIP) hardware implementation\nresults. Simulation results using an indoor environment show that distributing\nantennas throughout the scenario improves overall reliability, while the impact\nfrom this on latency is limited both when using zero-forcing (ZF) and Minimum\nMean Square Error (MMSE) detection. Changing the detection algorithm to\nmaximum-ratio combining (MRC) from ZF or MMSE, however, reduces the latency\nsignificantly, even if a larger number of antennas are needed to achieve a\nsimilar spectrum efficiency."
                },
                "authors": [
                    {
                        "name": "Lina Tinnerberg"
                    },
                    {
                        "name": "Dumitra Iancu"
                    },
                    {
                        "name": "Ove Edfors"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Juan Vidal Alegr√≠a"
                    }
                ],
                "author_detail": {
                    "name": "Juan Vidal Alegr√≠a"
                },
                "author": "Juan Vidal Alegr√≠a",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication,\n  copyright information may be affected upon publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19147v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19147v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13495v1",
                "updated": "2025-10-15T12:44:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    44,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:44:34Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    44,
                    34,
                    2,
                    288,
                    0
                ],
                "title": "Radio over Fiber with Cascaded Structure: Algorithm for Uplink\n  Positioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Radio over Fiber with Cascaded Structure: Algorithm for Uplink\n  Positioning"
                },
                "summary": "Recent advancements in polymer microwave fiber (PMF) technology have created\nsignificant opportunities for robust, low-cost, and high-speed sub-terahertz\n(THz) radio-over- fiber communications. Recognizing these potential benefits,\nthis paper explores a novel radio-over-fiber (RoF) structure that interconnects\nmultiple radio units (RUs) in cascade via fiber, envi- sioning its application\nin indoor scenarios. This structure creates a number of research opportunities\nwhen considering cascaded distortion effects introduced by non-linear power\namplifiers (PAs) at the RUs and the propagation channel over the fiber. We\npropose maximum-likelihood and non-linear least-squares algorithms to estimate\nthe propagation distance along the RoF and the time-of-arrival between the RoF\nand the user equipment. For the case of linear PAs, we derive the Cram\\'er-Rao\nlower bound to benchmark the performance of the estimators. Finally, we\ninvestigate the use of the system for uplink positioning. Our simulation\nresults demonstrate that the proposed estimators perform satisfactorily even\nwith the cascaded effects of non- linear PAs, and that the deployment of this\nRoF structure can enable new cost-effective opportunities for high-resolution\npositioning in indoor scenarios. In the numerical evaluation, we also use\nmeasured PMF characteristics for high-density polyethylene fibers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in polymer microwave fiber (PMF) technology have created\nsignificant opportunities for robust, low-cost, and high-speed sub-terahertz\n(THz) radio-over- fiber communications. Recognizing these potential benefits,\nthis paper explores a novel radio-over-fiber (RoF) structure that interconnects\nmultiple radio units (RUs) in cascade via fiber, envi- sioning its application\nin indoor scenarios. This structure creates a number of research opportunities\nwhen considering cascaded distortion effects introduced by non-linear power\namplifiers (PAs) at the RUs and the propagation channel over the fiber. We\npropose maximum-likelihood and non-linear least-squares algorithms to estimate\nthe propagation distance along the RoF and the time-of-arrival between the RoF\nand the user equipment. For the case of linear PAs, we derive the Cram\\'er-Rao\nlower bound to benchmark the performance of the estimators. Finally, we\ninvestigate the use of the system for uplink positioning. Our simulation\nresults demonstrate that the proposed estimators perform satisfactorily even\nwith the cascaded effects of non- linear PAs, and that the deployment of this\nRoF structure can enable new cost-effective opportunities for high-resolution\npositioning in indoor scenarios. In the numerical evaluation, we also use\nmeasured PMF characteristics for high-density polyethylene fibers."
                },
                "authors": [
                    {
                        "name": "Dexin Kong"
                    },
                    {
                        "name": "Diana Pamela Moya Osorio"
                    },
                    {
                        "name": "Erik G. Larsson"
                    }
                ],
                "author_detail": {
                    "name": "Erik G. Larsson"
                },
                "author": "Erik G. Larsson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.16843v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.16843v5",
                "updated": "2025-10-15T12:44:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    44,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2024-03-25T15:04:11Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    15,
                    4,
                    11,
                    0,
                    85,
                    0
                ],
                "title": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLM Agents Have Regret? A Case Study in Online Learning and Games"
                },
                "summary": "Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly employed for\n(interactive) decision-making, via the development of LLM-based autonomous\nagents. Despite their emerging successes, the performance of LLM agents in\ndecision-making has not been fully investigated through quantitative metrics,\nespecially in the multi-agent setting when they interact with each other, a\ntypical scenario in real-world LLM-agent applications. To better understand the\nlimits of LLM agents in these interactive environments, we propose to study\ntheir interactions in benchmark decision-making settings in online learning and\ngame theory, through the performance metric of \\emph{regret}. We first\nempirically study the {no-regret} behaviors of LLMs in canonical\n(non-stationary) online learning problems, as well as the emergence of\nequilibria when LLM agents interact through playing repeated games. We then\nprovide some theoretical insights into the no-regret behaviors of LLM agents,\nunder certain assumptions on the supervised pre-training and the rationality\nmodel of human decision-makers who generate the data. Notably, we also identify\n(simple) cases where advanced LLMs such as GPT-4 fail to be no-regret. To\npromote the no-regret behaviors, we propose a novel \\emph{unsupervised}\ntraining loss of \\emph{regret-loss}, which, in contrast to the supervised\npre-training loss, does not require the labels of (optimal) actions. We then\nestablish the statistical guarantee of generalization bound for regret-loss\nminimization, followed by the optimization guarantee that minimizing such a\nloss may automatically lead to known no-regret learning algorithms. Our further\nexperiments demonstrate the effectiveness of our regret-loss, especially in\naddressing the above ``regrettable'' cases."
                },
                "authors": [
                    {
                        "name": "Chanwoo Park"
                    },
                    {
                        "name": "Xiangyu Liu"
                    },
                    {
                        "name": "Asuman Ozdaglar"
                    },
                    {
                        "name": "Kaiqing Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaiqing Zhang"
                },
                "author": "Kaiqing Zhang",
                "arxiv_comment": "Camera ready version of ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.16843v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.16843v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13494v1",
                "updated": "2025-10-15T12:43:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    43,
                    59,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:43:59Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    43,
                    59,
                    2,
                    288,
                    0
                ],
                "title": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteraryQA: Towards Effective Evaluation of Long-document Narrative QA"
                },
                "summary": "Question Answering (QA) on narrative text poses a unique challenge to current\nsystems, requiring a deep understanding of long, complex documents. However,\nthe reliability of NarrativeQA, the most widely used benchmark in this domain,\nis hindered by noisy documents and flawed QA pairs. In this work, we introduce\nLiteraryQA, a high-quality subset of NarrativeQA focused on literary works.\nUsing a human- and LLM-validated pipeline, we identify and correct low-quality\nQA samples while removing extraneous text from source documents. We then carry\nout a meta-evaluation of automatic metrics to clarify how systems should be\nevaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics\nhave a low system-level correlation to human judgment, while LLM-as-a-Judge\nevaluations, even with small open-weight models, can strongly agree with the\nranking identified by humans. Finally, we benchmark a set of long-context LLMs\non LiteraryQA. We release our code and data at\nhttps://github.com/SapienzaNLP/LiteraryQA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Question Answering (QA) on narrative text poses a unique challenge to current\nsystems, requiring a deep understanding of long, complex documents. However,\nthe reliability of NarrativeQA, the most widely used benchmark in this domain,\nis hindered by noisy documents and flawed QA pairs. In this work, we introduce\nLiteraryQA, a high-quality subset of NarrativeQA focused on literary works.\nUsing a human- and LLM-validated pipeline, we identify and correct low-quality\nQA samples while removing extraneous text from source documents. We then carry\nout a meta-evaluation of automatic metrics to clarify how systems should be\nevaluated on LiteraryQA. This analysis reveals that all n-gram-based metrics\nhave a low system-level correlation to human judgment, while LLM-as-a-Judge\nevaluations, even with small open-weight models, can strongly agree with the\nranking identified by humans. Finally, we benchmark a set of long-context LLMs\non LiteraryQA. We release our code and data at\nhttps://github.com/SapienzaNLP/LiteraryQA."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonomo"
                    },
                    {
                        "name": "Luca Gioffr√©"
                    },
                    {
                        "name": "Roberto Navigli"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Navigli"
                },
                "author": "Roberto Navigli",
                "arxiv_comment": "Accepted to EMNLP 2025 Main Conference. 22 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13481v1",
                "updated": "2025-10-15T12:27:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    27,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:27:34Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    27,
                    34,
                    2,
                    288,
                    0
                ],
                "title": "Tahakom LLM guidelines and receipts: from pre-training data to an Arabic\n  LLM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tahakom LLM guidelines and receipts: from pre-training data to an Arabic\n  LLM"
                },
                "summary": "Large Language Models (LLMs) have significantly advanced the field of natural\nlanguage processing, enhancing capabilities in both language understanding and\ngeneration across diverse domains. However, developing LLMs for Arabic presents\nunique challenges. This paper explores these challenges by focusing on critical\naspects such as data curation, tokenizer design, and evaluation. We detail our\napproach to the collection and filtration of Arabic pre-training datasets,\nassess the impact of various tokenizer designs on model performance, and\nexamine the limitations of existing Arabic evaluation frameworks, for which we\npropose a systematic corrective methodology. To promote transparency and\nfacilitate collaborative development, we share our data and methodologies,\ncontributing to the advancement of language modeling, particularly for the\nArabic language.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have significantly advanced the field of natural\nlanguage processing, enhancing capabilities in both language understanding and\ngeneration across diverse domains. However, developing LLMs for Arabic presents\nunique challenges. This paper explores these challenges by focusing on critical\naspects such as data curation, tokenizer design, and evaluation. We detail our\napproach to the collection and filtration of Arabic pre-training datasets,\nassess the impact of various tokenizer designs on model performance, and\nexamine the limitations of existing Arabic evaluation frameworks, for which we\npropose a systematic corrective methodology. To promote transparency and\nfacilitate collaborative development, we share our data and methodologies,\ncontributing to the advancement of language modeling, particularly for the\nArabic language."
                },
                "authors": [
                    {
                        "name": "Areej AlOtaibi"
                    },
                    {
                        "name": "Lina Alyahya"
                    },
                    {
                        "name": "Raghad Alshabanah"
                    },
                    {
                        "name": "Shahad Alfawzan"
                    },
                    {
                        "name": "Shuruq Alarefei"
                    },
                    {
                        "name": "Reem Alsabti"
                    },
                    {
                        "name": "Nouf Alsubaie"
                    },
                    {
                        "name": "Abdulaziz Alhuzaymi"
                    },
                    {
                        "name": "Lujain Alkhelb"
                    },
                    {
                        "name": "Majd Alsayari"
                    },
                    {
                        "name": "Waad Alahmed"
                    },
                    {
                        "name": "Omar Talabay"
                    },
                    {
                        "name": "Jalal Alowibdi"
                    },
                    {
                        "name": "Salem Alelyani"
                    },
                    {
                        "name": "Adel Bibi"
                    }
                ],
                "author_detail": {
                    "name": "Adel Bibi"
                },
                "author": "Adel Bibi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13467v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13467v1",
                "updated": "2025-10-15T12:13:44Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:13:44Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    13,
                    44,
                    2,
                    288,
                    0
                ],
                "title": "NetMCP: Network-Aware Model Context Protocol Platform for LLM Capability\n  Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NetMCP: Network-Aware Model Context Protocol Platform for LLM Capability\n  Extension"
                },
                "summary": "Large Language Models (LLMs) remain static in functionality after training,\nand extending their capabilities requires integration with external data,\ncomputation, and services. The Model Context Protocol (MCP) has emerged as a\nstandard interface for such extensions, but current implementations rely solely\non semantic matching between users' requests and server function descriptions,\nwhich makes current deployments and simulation testbeds fragile under latency\nfluctuations or server failures. We address this gap by enhancing MCP tool\nrouting algorithms with real-time awareness of network and server status. To\nprovide a controlled test environment for development and evaluation, we\nconstruct a heterogeneous experimental platform, namely Network-aware MCP\n(NetMCP), which offers five representative network states and build a benchmark\nfor latency sequence generation and MCP server datasets. On top of NetMCP\nplatform, we analyze latency sequences and propose a Semantic-Oriented and\nNetwork-Aware Routing (SONAR) algorithm, which jointly optimizes semantic\nsimilarity and network Quality of Service (QoS) metrics for adaptive tool\nrouting. Results show that SONAR consistently improves task success rate and\nreduces completion time and failure number compared with semantic-only,\nLLM-based baselines, demonstrating the value of network-aware design for\nproduction-scale LLM systems. The code for NetMCP is available at\nhttps://github.com/NICE-HKU/NetMCP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) remain static in functionality after training,\nand extending their capabilities requires integration with external data,\ncomputation, and services. The Model Context Protocol (MCP) has emerged as a\nstandard interface for such extensions, but current implementations rely solely\non semantic matching between users' requests and server function descriptions,\nwhich makes current deployments and simulation testbeds fragile under latency\nfluctuations or server failures. We address this gap by enhancing MCP tool\nrouting algorithms with real-time awareness of network and server status. To\nprovide a controlled test environment for development and evaluation, we\nconstruct a heterogeneous experimental platform, namely Network-aware MCP\n(NetMCP), which offers five representative network states and build a benchmark\nfor latency sequence generation and MCP server datasets. On top of NetMCP\nplatform, we analyze latency sequences and propose a Semantic-Oriented and\nNetwork-Aware Routing (SONAR) algorithm, which jointly optimizes semantic\nsimilarity and network Quality of Service (QoS) metrics for adaptive tool\nrouting. Results show that SONAR consistently improves task success rate and\nreduces completion time and failure number compared with semantic-only,\nLLM-based baselines, demonstrating the value of network-aware design for\nproduction-scale LLM systems. The code for NetMCP is available at\nhttps://github.com/NICE-HKU/NetMCP."
                },
                "authors": [
                    {
                        "name": "Enhan Li"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Kaibin Huang"
                    }
                ],
                "author_detail": {
                    "name": "Kaibin Huang"
                },
                "author": "Kaibin Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13467v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13467v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13464v1",
                "updated": "2025-10-15T12:12:55Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    12,
                    55,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:12:55Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    12,
                    55,
                    2,
                    288,
                    0
                ],
                "title": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation\n  for Visual Place Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Through the Lens of Doubt: Robust and Efficient Uncertainty Estimation\n  for Visual Place Recognition"
                },
                "summary": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to\nidentify previously visited locations by matching current observations against\na database of known places. However, VPR systems face significant challenges\nwhen deployed across varying visual environments, lighting conditions, seasonal\nchanges, and viewpoints changes. Failure-critical VPR applications, such as\nloop closure detection in simultaneous localization and mapping (SLAM)\npipelines, require robust estimation of place matching uncertainty. We propose\nthree training-free uncertainty metrics that estimate prediction confidence by\nanalyzing inherent statistical patterns in similarity scores from any existing\nVPR method. Similarity Distribution (SD) quantifies match distinctiveness by\nmeasuring score separation between candidates; Ratio Spread (RS) evaluates\ncompetitive ambiguity among top-scoring locations; and Statistical Uncertainty\n(SU) is a combination of SD and RS that provides a unified metric that\ngeneralizes across datasets and VPR methods without requiring validation data\nto select the optimal metric. All three metrics operate without additional\nmodel training, architectural modifications, or computationally expensive\ngeometric verification. Comprehensive evaluation across nine state-of-the-art\nVPR methods and six benchmark datasets confirms that our metrics excel at\ndiscriminating between correct and incorrect VPR matches, and consistently\noutperform existing approaches while maintaining negligible computational\noverhead, making it deployable for real-time robotic applications across varied\nenvironmental conditions with improved precision-recall performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Place Recognition (VPR) enables robots and autonomous vehicles to\nidentify previously visited locations by matching current observations against\na database of known places. However, VPR systems face significant challenges\nwhen deployed across varying visual environments, lighting conditions, seasonal\nchanges, and viewpoints changes. Failure-critical VPR applications, such as\nloop closure detection in simultaneous localization and mapping (SLAM)\npipelines, require robust estimation of place matching uncertainty. We propose\nthree training-free uncertainty metrics that estimate prediction confidence by\nanalyzing inherent statistical patterns in similarity scores from any existing\nVPR method. Similarity Distribution (SD) quantifies match distinctiveness by\nmeasuring score separation between candidates; Ratio Spread (RS) evaluates\ncompetitive ambiguity among top-scoring locations; and Statistical Uncertainty\n(SU) is a combination of SD and RS that provides a unified metric that\ngeneralizes across datasets and VPR methods without requiring validation data\nto select the optimal metric. All three metrics operate without additional\nmodel training, architectural modifications, or computationally expensive\ngeometric verification. Comprehensive evaluation across nine state-of-the-art\nVPR methods and six benchmark datasets confirms that our metrics excel at\ndiscriminating between correct and incorrect VPR matches, and consistently\noutperform existing approaches while maintaining negligible computational\noverhead, making it deployable for real-time robotic applications across varied\nenvironmental conditions with improved precision-recall performance."
                },
                "authors": [
                    {
                        "name": "Emily Miller"
                    },
                    {
                        "name": "Michael Milford"
                    },
                    {
                        "name": "Muhammad Burhan Hafez"
                    },
                    {
                        "name": "SD Ramchurn"
                    },
                    {
                        "name": "Shoaib Ehsan"
                    }
                ],
                "author_detail": {
                    "name": "Shoaib Ehsan"
                },
                "author": "Shoaib Ehsan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13462v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13462v1",
                "updated": "2025-10-15T12:11:02Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    11,
                    2,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T12:11:02Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    12,
                    11,
                    2,
                    2,
                    288,
                    0
                ],
                "title": "Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored\n  Mixture-of-Experts Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who Speaks for the Trigger? Dynamic Expert Routing in Backdoored\n  Mixture-of-Experts Transformers"
                },
                "summary": "Large language models (LLMs) with Mixture-of-Experts (MoE) architectures\nachieve impressive performance and efficiency by dynamically routing inputs to\nspecialized subnetworks, known as experts. However, this sparse routing\nmechanism inherently exhibits task preferences due to expert specialization,\nintroducing a new and underexplored vulnerability to backdoor attacks. In this\nwork, we investigate the feasibility and effectiveness of injecting backdoors\ninto MoE-based LLMs by exploiting their inherent expert routing preferences. We\nthus propose BadSwitch, a novel backdoor framework that integrates task-coupled\ndynamic trigger optimization with a sensitivity-guided Top-S expert tracing\nmechanism. Our approach jointly optimizes trigger embeddings during pretraining\nwhile identifying S most sensitive experts, subsequently constraining the Top-K\ngating mechanism to these targeted experts. Unlike traditional backdoor attacks\nthat rely on superficial data poisoning or model editing, BadSwitch primarily\nembeds malicious triggers into expert routing paths with strong task affinity,\nenabling precise and stealthy model manipulation. Through comprehensive\nevaluations across three prominent MoE architectures (Switch Transformer,\nQwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack\npre-trained models with up to 100% success rate (ASR) while maintaining the\nhighest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch\nexhibits strong resilience against both text-level and model-level defense\nmechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our\nanalysis of expert activation patterns reveals fundamental insights into MoE\nvulnerabilities. We anticipate this work will expose security risks in MoE\nsystems and contribute to advancing AI safety.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) with Mixture-of-Experts (MoE) architectures\nachieve impressive performance and efficiency by dynamically routing inputs to\nspecialized subnetworks, known as experts. However, this sparse routing\nmechanism inherently exhibits task preferences due to expert specialization,\nintroducing a new and underexplored vulnerability to backdoor attacks. In this\nwork, we investigate the feasibility and effectiveness of injecting backdoors\ninto MoE-based LLMs by exploiting their inherent expert routing preferences. We\nthus propose BadSwitch, a novel backdoor framework that integrates task-coupled\ndynamic trigger optimization with a sensitivity-guided Top-S expert tracing\nmechanism. Our approach jointly optimizes trigger embeddings during pretraining\nwhile identifying S most sensitive experts, subsequently constraining the Top-K\ngating mechanism to these targeted experts. Unlike traditional backdoor attacks\nthat rely on superficial data poisoning or model editing, BadSwitch primarily\nembeds malicious triggers into expert routing paths with strong task affinity,\nenabling precise and stealthy model manipulation. Through comprehensive\nevaluations across three prominent MoE architectures (Switch Transformer,\nQwenMoE, and DeepSeekMoE), we demonstrate that BadSwitch can efficiently hijack\npre-trained models with up to 100% success rate (ASR) while maintaining the\nhighest clean accuracy (ACC) among all baselines. Furthermore, BadSwitch\nexhibits strong resilience against both text-level and model-level defense\nmechanisms, achieving 94.07% ASR and 87.18% ACC on the AGNews dataset. Our\nanalysis of expert activation patterns reveals fundamental insights into MoE\nvulnerabilities. We anticipate this work will expose security risks in MoE\nsystems and contribute to advancing AI safety."
                },
                "authors": [
                    {
                        "name": "Xin Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Bingshan Liu"
                    },
                    {
                        "name": "Haoyu Gao"
                    },
                    {
                        "name": "Zhendong Zhao"
                    },
                    {
                        "name": "Yilong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yilong Chen"
                },
                "author": "Yilong Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13462v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13462v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.00597v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.00597v3",
                "updated": "2025-10-15T11:50:05Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    50,
                    5,
                    2,
                    288,
                    0
                ],
                "published": "2025-04-01T09:55:23Z",
                "published_parsed": [
                    2025,
                    4,
                    1,
                    9,
                    55,
                    23,
                    1,
                    91,
                    0
                ],
                "title": "On the Consistency of Multilingual Context Utilization in\n  Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Consistency of Multilingual Context Utilization in\n  Retrieval-Augmented Generation"
                },
                "summary": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from passages in a different language than the query, but a much\nweaker ability to formulate a full answer in the correct language. Our\nanalysis, based on both accuracy and feature attribution techniques, further\nshows that distracting passages negatively impact answer quality regardless of\ntheir language. However, distractors in the query language exert a slightly\nstronger influence. Taken together, our findings deepen the understanding of\nhow LLMs utilize context in mRAG systems, providing directions for future\nimprovements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) with large language models (LLMs) has\ndemonstrated strong performance in multilingual question-answering (QA) tasks\nby leveraging relevant passages retrieved from corpora. In multilingual RAG\n(mRAG), the retrieved passages can be written in languages other than that of\nthe query entered by the user, making it challenging for LLMs to effectively\nutilize the provided information. Recent research suggests that retrieving\npassages from multilingual corpora can improve RAG performance, particularly\nfor low-resource languages. However, the extent to which LLMs can leverage\ndifferent kinds of multilingual contexts to generate accurate answers,\n*independently from retrieval quality*, remains understudied. In this paper, we\nconduct an extensive assessment of LLMs' ability to (i) make consistent use of\na relevant passage regardless of its language, (ii) respond in the expected\nlanguage, and (iii) focus on the relevant passage even when multiple\n`distracting' passages in different languages are provided in the context. Our\nexperiments with four LLMs across three QA datasets covering a total of 48\nlanguages reveal a surprising ability of LLMs to extract the relevant\ninformation from passages in a different language than the query, but a much\nweaker ability to formulate a full answer in the correct language. Our\nanalysis, based on both accuracy and feature attribution techniques, further\nshows that distracting passages negatively impact answer quality regardless of\ntheir language. However, distractors in the query language exert a slightly\nstronger influence. Taken together, our findings deepen the understanding of\nhow LLMs utilize context in mRAG systems, providing directions for future\nimprovements."
                },
                "authors": [
                    {
                        "name": "Jirui Qi"
                    },
                    {
                        "name": "Raquel Fern√°ndez"
                    },
                    {
                        "name": "Arianna Bisazza"
                    }
                ],
                "author_detail": {
                    "name": "Arianna Bisazza"
                },
                "author": "Arianna Bisazza",
                "arxiv_comment": "MRL Workshop 2025, co-located with EMNLP 2025. All codes and data are\n  released at https://github.com/Betswish/mRAG-Context-Consistency",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.00597v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.00597v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13434v1",
                "updated": "2025-10-15T11:30:49Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    30,
                    49,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:30:49Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    30,
                    49,
                    2,
                    288,
                    0
                ],
                "title": "Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference\n  Optimization for Machine Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Single-Reward: Multi-Pair, Multi-Perspective Preference\n  Optimization for Machine Translation"
                },
                "summary": "Direct Preference Optimization (DPO) is a powerful paradigm for aligning\nLarge Language Models (LLMs) to human preferences in Machine Translation (MT),\nbut current methods are hindered by two fundamental challenges: (1) flawed\nreward signals from Quality Estimation (QE) models that overlook critical\nerrors like translation hallucination, and (2) inefficient data utilization\nthat discards valuable learning signals by selecting only a single win-loss\npair. To address these limitations, we introduce M^2PO: Multi-Pair,\nMulti-Perspective Preference Optimization. Our framework integrates a\nmulti-perspective reward engine that creates a more robust signal by combining\ntwo key viewpoints: a new hallucination penalty for factuality, and an\ninnovative dynamic quality score that adaptively fuses external evaluations\nwith the model's own evolving judgment. This is synergistically paired with a\nmulti-pair construction strategy that systematically creates a comprehensive\nset of preference pairs from the entire pool of translation candidates. This\nsynergistic approach ensures the model learns from a richer spectrum of quality\ntrade-offs, leading to more robust and faithful translations. On challenging\nWMT21-22 benchmarks, M^2PO substantially outperforms existing preference\noptimization methods and demonstrates highly competitive performance against\nleading proprietary LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) is a powerful paradigm for aligning\nLarge Language Models (LLMs) to human preferences in Machine Translation (MT),\nbut current methods are hindered by two fundamental challenges: (1) flawed\nreward signals from Quality Estimation (QE) models that overlook critical\nerrors like translation hallucination, and (2) inefficient data utilization\nthat discards valuable learning signals by selecting only a single win-loss\npair. To address these limitations, we introduce M^2PO: Multi-Pair,\nMulti-Perspective Preference Optimization. Our framework integrates a\nmulti-perspective reward engine that creates a more robust signal by combining\ntwo key viewpoints: a new hallucination penalty for factuality, and an\ninnovative dynamic quality score that adaptively fuses external evaluations\nwith the model's own evolving judgment. This is synergistically paired with a\nmulti-pair construction strategy that systematically creates a comprehensive\nset of preference pairs from the entire pool of translation candidates. This\nsynergistic approach ensures the model learns from a richer spectrum of quality\ntrade-offs, leading to more robust and faithful translations. On challenging\nWMT21-22 benchmarks, M^2PO substantially outperforms existing preference\noptimization methods and demonstrates highly competitive performance against\nleading proprietary LLMs."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Linlong Xu"
                    },
                    {
                        "name": "Heng Liu"
                    },
                    {
                        "name": "Yangyang Liu"
                    },
                    {
                        "name": "Xiaohu Zhao"
                    },
                    {
                        "name": "Bo Zeng"
                    },
                    {
                        "name": "Liangying Shao"
                    },
                    {
                        "name": "Longyue Wang"
                    },
                    {
                        "name": "Weihua Luo"
                    },
                    {
                        "name": "Kaifu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaifu Zhang"
                },
                "author": "Kaifu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.07994v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.07994v2",
                "updated": "2025-10-15T11:26:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    26,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2024-01-15T22:36:31Z",
                "published_parsed": [
                    2024,
                    1,
                    15,
                    22,
                    36,
                    31,
                    0,
                    15,
                    0
                ],
                "title": "Assessing the Latent Automated Program Repair Capabilities of Large\n  Language Models using Round-Trip Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Latent Automated Program Repair Capabilities of Large\n  Language Models using Round-Trip Translation"
                },
                "summary": "Research shows that errors in natural language can be corrected by\ntranslating texts to another language and back using language models. We\nexplore to what extent this latent correction capability extends to Automated\nProgram Repair (APR) by investigating Round-Trip Translation (RTT): translating\ncode from one programming language into another programming or natural language\nand back, using Large Language Models (LLMs). We hypothesize that RTT restores\npatterns most commonly seen in the LLM's training corpora through regression\ntoward the mean, replacing infrequent bugs with more frequent, natural,\nbug-free code. To test this hypothesis, we employ nine LLMs and four common APR\nbenchmarks in Java, and perform a detailed quantitative and qualitative\nanalysis of RTT-generated patches. We find that RTT through English generates\nplausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java\nbenchmark, and 97 are found to be correct in our manual assessment. Moreover,\nRTT uniquely generates plausible patches for 46 bugs that were missed by LLMs\nspecifically fine-tuned for APR. While this demonstrates the viability of RTT\nfor APR, we also observe limitations, such as a lower overall bug fix rate than\nthe state-of-the-art and diluting the original coding style. We analyze the\nimpact of these limitations and discuss the potential of using RTT as a\ncomplementary component in APR frameworks. A replication package is available\nfor download from https://doi.org/10.5281/zenodo.10500593.\n  Keywords: automated program repair, large language model, machine translation",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research shows that errors in natural language can be corrected by\ntranslating texts to another language and back using language models. We\nexplore to what extent this latent correction capability extends to Automated\nProgram Repair (APR) by investigating Round-Trip Translation (RTT): translating\ncode from one programming language into another programming or natural language\nand back, using Large Language Models (LLMs). We hypothesize that RTT restores\npatterns most commonly seen in the LLM's training corpora through regression\ntoward the mean, replacing infrequent bugs with more frequent, natural,\nbug-free code. To test this hypothesis, we employ nine LLMs and four common APR\nbenchmarks in Java, and perform a detailed quantitative and qualitative\nanalysis of RTT-generated patches. We find that RTT through English generates\nplausible patches for 100 of 164 bugs with GPT-4 on the HumanEval-Java\nbenchmark, and 97 are found to be correct in our manual assessment. Moreover,\nRTT uniquely generates plausible patches for 46 bugs that were missed by LLMs\nspecifically fine-tuned for APR. While this demonstrates the viability of RTT\nfor APR, we also observe limitations, such as a lower overall bug fix rate than\nthe state-of-the-art and diluting the original coding style. We analyze the\nimpact of these limitations and discuss the potential of using RTT as a\ncomplementary component in APR frameworks. A replication package is available\nfor download from https://doi.org/10.5281/zenodo.10500593.\n  Keywords: automated program repair, large language model, machine translation"
                },
                "authors": [
                    {
                        "name": "Fernando Vallecillos Ruiz"
                    },
                    {
                        "name": "Anastasiia Grishina"
                    },
                    {
                        "name": "Max Hort"
                    },
                    {
                        "name": "Leon Moonen"
                    }
                ],
                "author_detail": {
                    "name": "Leon Moonen"
                },
                "author": "Leon Moonen",
                "arxiv_doi": "10.1145/3771922",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771922",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.07994v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.07994v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted for publication in ACM Transactions on Software Engineering\n  and Methodology (TOSEM)",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13430v1",
                "updated": "2025-10-15T11:25:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    25,
                    33,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:25:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    25,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "Evaluating Arabic Large Language Models: A Survey of Benchmarks,\n  Methods, and Gaps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evaluating Arabic Large Language Models: A Survey of Benchmarks,\n  Methods, and Gaps"
                },
                "summary": "This survey provides the first systematic review of Arabic LLM benchmarks,\nanalyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,\ncultural understanding, and specialized capabilities. We propose a taxonomy\norganizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and\nDialects, and Target-Specific evaluations. Our analysis reveals significant\nprogress in benchmark diversity while identifying critical gaps: limited\ntemporal evaluation, insufficient multi-turn dialogue assessment, and cultural\nmisalignment in translated datasets. We examine three primary approaches:\nnative collection, translation, and synthetic generation discussing their\ntrade-offs regarding authenticity, scale, and cost. This work serves as a\ncomprehensive reference for Arabic NLP researchers, providing insights into\nbenchmark methodologies, reproducibility standards, and evaluation metrics\nwhile offering recommendations for future development.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This survey provides the first systematic review of Arabic LLM benchmarks,\nanalyzing 40+ evaluation benchmarks across NLP tasks, knowledge domains,\ncultural understanding, and specialized capabilities. We propose a taxonomy\norganizing benchmarks into four categories: Knowledge, NLP Tasks, Culture and\nDialects, and Target-Specific evaluations. Our analysis reveals significant\nprogress in benchmark diversity while identifying critical gaps: limited\ntemporal evaluation, insufficient multi-turn dialogue assessment, and cultural\nmisalignment in translated datasets. We examine three primary approaches:\nnative collection, translation, and synthetic generation discussing their\ntrade-offs regarding authenticity, scale, and cost. This work serves as a\ncomprehensive reference for Arabic NLP researchers, providing insights into\nbenchmark methodologies, reproducibility standards, and evaluation metrics\nwhile offering recommendations for future development."
                },
                "authors": [
                    {
                        "name": "Ahmed Alzubaidi"
                    },
                    {
                        "name": "Shaikha Alsuwaidi"
                    },
                    {
                        "name": "Basma El Amel Boussaha"
                    },
                    {
                        "name": "Leen AlQadi"
                    },
                    {
                        "name": "Omar Alkaabi"
                    },
                    {
                        "name": "Mohammed Alyafeai"
                    },
                    {
                        "name": "Hamza Alobeidli"
                    },
                    {
                        "name": "Hakim Hacid"
                    }
                ],
                "author_detail": {
                    "name": "Hakim Hacid"
                },
                "author": "Hakim Hacid",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13417v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13417v1",
                "updated": "2025-10-15T11:15:00Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    15,
                    0,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:15:00Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    15,
                    0,
                    2,
                    288,
                    0
                ],
                "title": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in\n  Climate Discourse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing LLM Reasoning Through Implicit Causal Chain Discovery in\n  Climate Discourse"
                },
                "summary": "How does a cause lead to an effect, and which intermediate causal steps\nexplain their connection? This work scrutinizes the mechanistic causal\nreasoning capabilities of large language models (LLMs) to answer these\nquestions through the task of implicit causal chain discovery. In a diagnostic\nevaluation framework, we instruct nine LLMs to generate all possible\nintermediate causal steps linking given cause-effect pairs in causal chain\nstructures. These pairs are drawn from recent resources in argumentation\nstudies featuring polarized discussion on climate change. Our analysis reveals\nthat LLMs vary in the number and granularity of causal steps they produce.\nAlthough they are generally self-consistent and confident about the\nintermediate causal connections in the generated chains, their judgments are\nmainly driven by associative pattern matching rather than genuine causal\nreasoning. Nonetheless, human evaluations confirmed the logical coherence and\nintegrity of the generated chains. Our baseline causal chain discovery\napproach, insights from our diagnostic evaluation, and benchmark dataset with\ncausal chains lay a solid foundation for advancing future work in implicit,\nmechanistic causal reasoning in argumentation settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How does a cause lead to an effect, and which intermediate causal steps\nexplain their connection? This work scrutinizes the mechanistic causal\nreasoning capabilities of large language models (LLMs) to answer these\nquestions through the task of implicit causal chain discovery. In a diagnostic\nevaluation framework, we instruct nine LLMs to generate all possible\nintermediate causal steps linking given cause-effect pairs in causal chain\nstructures. These pairs are drawn from recent resources in argumentation\nstudies featuring polarized discussion on climate change. Our analysis reveals\nthat LLMs vary in the number and granularity of causal steps they produce.\nAlthough they are generally self-consistent and confident about the\nintermediate causal connections in the generated chains, their judgments are\nmainly driven by associative pattern matching rather than genuine causal\nreasoning. Nonetheless, human evaluations confirmed the logical coherence and\nintegrity of the generated chains. Our baseline causal chain discovery\napproach, insights from our diagnostic evaluation, and benchmark dataset with\ncausal chains lay a solid foundation for advancing future work in implicit,\nmechanistic causal reasoning in argumentation settings."
                },
                "authors": [
                    {
                        "name": "Liesbeth Allein"
                    },
                    {
                        "name": "Nataly Pineda-Casta√±eda"
                    },
                    {
                        "name": "Andrea Rocci"
                    },
                    {
                        "name": "Marie-Francine Moens"
                    }
                ],
                "author_detail": {
                    "name": "Marie-Francine Moens"
                },
                "author": "Marie-Francine Moens",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13417v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13417v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13405v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13405v1",
                "updated": "2025-10-15T11:03:17Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    3,
                    17,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T11:03:17Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    11,
                    3,
                    17,
                    2,
                    288,
                    0
                ],
                "title": "Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile\n  Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Storage Overhead of User Behavior Log for ML-embedded Mobile\n  Apps"
                },
                "summary": "Machine learning (ML) models are increasingly integrated into modern mobile\napps to enable personalized and intelligent services. These models typically\nrely on rich input features derived from historical user behaviors to capture\nuser intents. However, as ML-driven services become more prevalent, recording\nnecessary user behavior data imposes substantial storage cost on mobile apps,\nleading to lower system responsiveness and more app uninstalls. To address this\nstorage bottleneck, we present AdaLog, a lightweight and adaptive system\ndesigned to improve the storage efficiency of user behavior log in ML-embedded\nmobile apps, without compromising model inference accuracy or latency. We\nidentify two key inefficiencies in current industrial practices of user\nbehavior log: (i) redundant logging of overlapping behavior data across\ndifferent features and models, and (ii) sparse storage caused by storing\nbehaviors with heterogeneous attribute descriptions in a single log file. To\nsolve these issues, AdaLog first formulates the elimination of feature-level\nredundant data as a maximum weighted matching problem in hypergraphs, and\nproposes a hierarchical algorithm for efficient on-device deployment. Then,\nAdaLog employs a virtually hashed attribute design to distribute heterogeneous\nbehaviors into a few log files with physically dense storage. Finally, to\nensure scalability to dynamic user behavior patterns, AdaLog designs an\nincremental update mechanism to minimize the I/O operations needed for adapting\noutdated behavior log. We implement a prototype of AdaLog and deploy it into\npopular mobile apps in collaboration with our industry partner. Evaluations on\nreal-world user data show that AdaLog reduces behavior log size by 19% to 44%\nwith minimal system overhead (only 2 seconds latency and 15 MB memory usage),\nproviding a more efficient data foundation for broader adoption of on-device\nML.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning (ML) models are increasingly integrated into modern mobile\napps to enable personalized and intelligent services. These models typically\nrely on rich input features derived from historical user behaviors to capture\nuser intents. However, as ML-driven services become more prevalent, recording\nnecessary user behavior data imposes substantial storage cost on mobile apps,\nleading to lower system responsiveness and more app uninstalls. To address this\nstorage bottleneck, we present AdaLog, a lightweight and adaptive system\ndesigned to improve the storage efficiency of user behavior log in ML-embedded\nmobile apps, without compromising model inference accuracy or latency. We\nidentify two key inefficiencies in current industrial practices of user\nbehavior log: (i) redundant logging of overlapping behavior data across\ndifferent features and models, and (ii) sparse storage caused by storing\nbehaviors with heterogeneous attribute descriptions in a single log file. To\nsolve these issues, AdaLog first formulates the elimination of feature-level\nredundant data as a maximum weighted matching problem in hypergraphs, and\nproposes a hierarchical algorithm for efficient on-device deployment. Then,\nAdaLog employs a virtually hashed attribute design to distribute heterogeneous\nbehaviors into a few log files with physically dense storage. Finally, to\nensure scalability to dynamic user behavior patterns, AdaLog designs an\nincremental update mechanism to minimize the I/O operations needed for adapting\noutdated behavior log. We implement a prototype of AdaLog and deploy it into\npopular mobile apps in collaboration with our industry partner. Evaluations on\nreal-world user data show that AdaLog reduces behavior log size by 19% to 44%\nwith minimal system overhead (only 2 seconds latency and 15 MB memory usage),\nproviding a more efficient data foundation for broader adoption of on-device\nML."
                },
                "authors": [
                    {
                        "name": "Chen Gong"
                    },
                    {
                        "name": "Yan Zhuang"
                    },
                    {
                        "name": "Zhenzhe Zheng"
                    },
                    {
                        "name": "Yiliu Chen"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_doi": "10.1145/3771575",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3771575",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2510.13405v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13405v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13401v1",
                "updated": "2025-10-15T10:56:37Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:56:37Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    56,
                    37,
                    2,
                    288,
                    0
                ],
                "title": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "F-BFQ: Flexible Block Floating-Point Quantization Accelerator for LLMs"
                },
                "summary": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly prominent for daily\ntasks, from improving sound-totext translation to generating additional frames\nfor the latest video games. With the help of LLM inference frameworks, such as\nllama.cpp, which support optimizations such as KV-caching and quantization, it\nis now easier than ever to deploy LLMs on edge devices. Quantization is\nfundamental to enable LLMs on resource-constrained edge devices, and llama.cpp\nutilizes block floating point (BFP) quantization to drastically reduce the bit\nwidth of weights and input tensors, the memory footprint, and the computational\npower required to run LLMs. LLMs are typically quantized with mixed BFP\nquantization across the model layers to reduce the loss of model accuracy due\nto quantization. Therefore, to efficiently accelerate across the layers of\nBFP-quantized LLMs, specialized accelerators need to support different BFP\nvariants without reconfiguration. To address this issue, we propose a Flexible\nBlock FloatingPoint Quantization (F-BFQ) accelerator, which can dynamically\nswitch between two BFP quantization variants and perform matrix multiplication\n(MatMul) operations. Our initial F-BFQ accelerator design, deployed on the AMD\nKria board, reduces inference time by 1.4x on average over the Arm NEON-based\nCPU execution across three BFP quantized LLMs while achieving 5.2 tokens per\nsecond (~3.9 words per second)."
                },
                "authors": [
                    {
                        "name": "Jude Haris"
                    },
                    {
                        "name": "Jos√© Cano"
                    }
                ],
                "author_detail": {
                    "name": "Jos√© Cano"
                },
                "author": "Jos√© Cano",
                "arxiv_comment": "Accepted to Workshop on New Approaches for Addressing the Computing\n  Requirements of LLMs and GNNs (LG-ARC) @ ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13395v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13395v1",
                "updated": "2025-10-15T10:48:31Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    48,
                    31,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:48:31Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    48,
                    31,
                    2,
                    288,
                    0
                ],
                "title": "Doing Things with Words: Rethinking Theory of Mind Simulation in Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doing Things with Words: Rethinking Theory of Mind Simulation in Large\n  Language Models"
                },
                "summary": "Language is fundamental to human cooperation, facilitating not only the\nexchange of information but also the coordination of actions through shared\ninterpretations of situational contexts. This study explores whether the\nGenerative Agent-Based Model (GABM) Concordia can effectively model Theory of\nMind (ToM) within simulated real-world environments. Specifically, we assess\nwhether this framework successfully simulates ToM abilities and whether GPT-4\ncan perform tasks by making genuine inferences from social context, rather than\nrelying on linguistic memorization. Our findings reveal a critical limitation:\nGPT-4 frequently fails to select actions based on belief attribution,\nsuggesting that apparent ToM-like abilities observed in previous studies may\nstem from shallow statistical associations rather than true reasoning.\nAdditionally, the model struggles to generate coherent causal effects from\nagent actions, exposing difficulties in processing complex social interactions.\nThese results challenge current statements about emergent ToM-like capabilities\nin LLMs and highlight the need for more rigorous, action-based evaluation\nframeworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language is fundamental to human cooperation, facilitating not only the\nexchange of information but also the coordination of actions through shared\ninterpretations of situational contexts. This study explores whether the\nGenerative Agent-Based Model (GABM) Concordia can effectively model Theory of\nMind (ToM) within simulated real-world environments. Specifically, we assess\nwhether this framework successfully simulates ToM abilities and whether GPT-4\ncan perform tasks by making genuine inferences from social context, rather than\nrelying on linguistic memorization. Our findings reveal a critical limitation:\nGPT-4 frequently fails to select actions based on belief attribution,\nsuggesting that apparent ToM-like abilities observed in previous studies may\nstem from shallow statistical associations rather than true reasoning.\nAdditionally, the model struggles to generate coherent causal effects from\nagent actions, exposing difficulties in processing complex social interactions.\nThese results challenge current statements about emergent ToM-like capabilities\nin LLMs and highlight the need for more rigorous, action-based evaluation\nframeworks."
                },
                "authors": [
                    {
                        "name": "Agnese Lombardi"
                    },
                    {
                        "name": "Alessandro Lenci"
                    }
                ],
                "author_detail": {
                    "name": "Alessandro Lenci"
                },
                "author": "Alessandro Lenci",
                "arxiv_journal_ref": "roceedings of the Eleventh Italian Conference on Computational\n  Linguistics (CLiC-it 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13395v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13395v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.01340v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.01340v2",
                "updated": "2025-10-15T10:38:23Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    38,
                    23,
                    2,
                    288,
                    0
                ],
                "published": "2025-06-02T05:45:19Z",
                "published_parsed": [
                    2025,
                    6,
                    2,
                    5,
                    45,
                    19,
                    0,
                    153,
                    0
                ],
                "title": "The Landscape of Arabic Large Language Models (ALLMs): A New Era for\n  Arabic Language Technology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Landscape of Arabic Large Language Models (ALLMs): A New Era for\n  Arabic Language Technology"
                },
                "summary": "The emergence of ChatGPT marked a transformative milestone for Artificial\nIntelligence (AI), showcasing the remarkable potential of Large Language Models\n(LLMs) to generate human-like text. This wave of innovation has revolutionized\nhow we interact with technology, seamlessly integrating LLMs into everyday\ntasks such as vacation planning, email drafting, and content creation. While\nEnglish-speaking users have significantly benefited from these advancements,\nthe Arabic world faces distinct challenges in developing Arabic-specific LLMs.\nArabic, one of the languages spoken most widely around the world, serves more\nthan 422 million native speakers in 27 countries and is deeply rooted in a rich\nlinguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an\nunparalleled opportunity to bridge technological gaps and empower communities.\nThe journey of ALLMs has been both fascinating and complex, evolving from\nrudimentary text processing systems to sophisticated AI-driven models. This\narticle explores the trajectory of ALLMs, from their inception to the present\nday, highlighting the efforts to evaluate these models through benchmarks and\npublic leaderboards. We also discuss the challenges and opportunities that\nALLMs present for the Arab world.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of ChatGPT marked a transformative milestone for Artificial\nIntelligence (AI), showcasing the remarkable potential of Large Language Models\n(LLMs) to generate human-like text. This wave of innovation has revolutionized\nhow we interact with technology, seamlessly integrating LLMs into everyday\ntasks such as vacation planning, email drafting, and content creation. While\nEnglish-speaking users have significantly benefited from these advancements,\nthe Arabic world faces distinct challenges in developing Arabic-specific LLMs.\nArabic, one of the languages spoken most widely around the world, serves more\nthan 422 million native speakers in 27 countries and is deeply rooted in a rich\nlinguistic and cultural heritage. Developing Arabic LLMs (ALLMs) presents an\nunparalleled opportunity to bridge technological gaps and empower communities.\nThe journey of ALLMs has been both fascinating and complex, evolving from\nrudimentary text processing systems to sophisticated AI-driven models. This\narticle explores the trajectory of ALLMs, from their inception to the present\nday, highlighting the efforts to evaluate these models through benchmarks and\npublic leaderboards. We also discuss the challenges and opportunities that\nALLMs present for the Arab world."
                },
                "authors": [
                    {
                        "name": "Shahad Al-Khalifa"
                    },
                    {
                        "name": "Nadir Durrani"
                    },
                    {
                        "name": "Hend Al-Khalifa"
                    },
                    {
                        "name": "Firoj Alam"
                    }
                ],
                "author_detail": {
                    "name": "Firoj Alam"
                },
                "author": "Firoj Alam",
                "arxiv_comment": "Accepted at CACM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.01340v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.01340v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.01052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.01052v2",
                "updated": "2025-10-15T10:33:27Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    33,
                    27,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-01T01:33:16Z",
                "published_parsed": [
                    2025,
                    9,
                    1,
                    1,
                    33,
                    16,
                    0,
                    244,
                    0
                ],
                "title": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashAdventure: A Benchmark for GUI Agents Solving Full Story Arcs in\n  Diverse Adventure Games"
                },
                "summary": "GUI agents powered by LLMs show promise in interacting with diverse digital\nenvironments. Among these, video games offer a valuable testbed due to their\nvaried interfaces, with adventure games posing additional challenges through\ncomplex, narrative-driven interactions. Existing game benchmarks, however, lack\ndiversity and rarely evaluate agents on completing entire storylines. To\naddress this, we introduce FlashAdventure, a benchmark of 34 Flash-based\nadventure games designed to test full story arc completion and tackle the\nobservation-behavior gap: the challenge of remembering and acting on earlier\ngameplay information. We also propose CUA-as-a-Judge, an automated gameplay\nevaluator, and COAST, an agentic framework leveraging long-term clue memory to\nbetter plan and solve sequential tasks. Experiments show current GUI agents\nstruggle with full story arcs, while COAST improves milestone completion by\nbridging the observation-behavior gap. Nonetheless, a marked discrepancy\nbetween humans and best-performing agents warrants continued research efforts\nto narrow this divide.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUI agents powered by LLMs show promise in interacting with diverse digital\nenvironments. Among these, video games offer a valuable testbed due to their\nvaried interfaces, with adventure games posing additional challenges through\ncomplex, narrative-driven interactions. Existing game benchmarks, however, lack\ndiversity and rarely evaluate agents on completing entire storylines. To\naddress this, we introduce FlashAdventure, a benchmark of 34 Flash-based\nadventure games designed to test full story arc completion and tackle the\nobservation-behavior gap: the challenge of remembering and acting on earlier\ngameplay information. We also propose CUA-as-a-Judge, an automated gameplay\nevaluator, and COAST, an agentic framework leveraging long-term clue memory to\nbetter plan and solve sequential tasks. Experiments show current GUI agents\nstruggle with full story arcs, while COAST improves milestone completion by\nbridging the observation-behavior gap. Nonetheless, a marked discrepancy\nbetween humans and best-performing agents warrants continued research efforts\nto narrow this divide."
                },
                "authors": [
                    {
                        "name": "Jaewoo Ahn"
                    },
                    {
                        "name": "Junseo Kim"
                    },
                    {
                        "name": "Heeseung Yun"
                    },
                    {
                        "name": "Jaehyeon Son"
                    },
                    {
                        "name": "Dongmin Park"
                    },
                    {
                        "name": "Jaewoong Cho"
                    },
                    {
                        "name": "Gunhee Kim"
                    }
                ],
                "author_detail": {
                    "name": "Gunhee Kim"
                },
                "author": "Gunhee Kim",
                "arxiv_comment": "EMNLP 2025 Main. Project page:\n  https://ahnjaewoo.github.io/flashadventure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.01052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.01052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13390v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13390v1",
                "updated": "2025-10-15T10:28:50Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    28,
                    50,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:28:50Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    28,
                    50,
                    2,
                    288,
                    0
                ],
                "title": "Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic\n  Distillation and Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generalizing WiFi Gesture Recognition via Large-Model-Aware Semantic\n  Distillation and Alignment"
                },
                "summary": "WiFi-based gesture recognition has emerged as a promising RF sensing paradigm\nfor enabling non-contact and privacy-preserving human-computer interaction in\nAIoT environments. However, existing methods often suffer from limited\ngeneralization and semantic expressiveness due to the domain-sensitive nature\nof Channel State Information and the lack of high-level gesture abstraction. To\naddress these challenges, we propose a novel generalization framework, termed\nLarge-Model-Aware Semantic Distillation and Alignment (GLSDA), which leverages\nthe semantic prior of pre-trained large foundation models to enhance gesture\nrepresentation learning in both in-domain and cross-domain scenarios.\nSpecifically, we first design a dual-path CSI encoding pipeline that captures\ngeometric and dynamic gesture patterns via CSI-Ratio phase sequences and\nDoppler spectrograms. These representations are then fed into a Multiscale\nSemantic Encoder, which learns robust temporal embeddings and aligns them with\ngesture semantics through cross-modal attention mechanisms. To further enhance\ncategory discrimination, we introduce a Semantic-Aware Soft Supervision scheme\nthat encodes inter-class correlations and reduces label ambiguity, especially\nfor semantically similar gestures. Finally, we develop a Robust\nDual-Distillation strategy to compress the aligned model into a lightweight\nstudent network, jointly distilling intermediate features and semantic-informed\nsoft labels from the teacher model. Extensive experiments on the Widar3.0\nbenchmark show that GLSDA consistently outperforms state-of-the-art methods in\nboth in-domain and cross-domain gesture recognition tasks, while significantly\nreducing model size and inference latency. Our method offers a scalable and\ndeployable solution for generalized RF-based gesture interfaces in real-world\nAIoT applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WiFi-based gesture recognition has emerged as a promising RF sensing paradigm\nfor enabling non-contact and privacy-preserving human-computer interaction in\nAIoT environments. However, existing methods often suffer from limited\ngeneralization and semantic expressiveness due to the domain-sensitive nature\nof Channel State Information and the lack of high-level gesture abstraction. To\naddress these challenges, we propose a novel generalization framework, termed\nLarge-Model-Aware Semantic Distillation and Alignment (GLSDA), which leverages\nthe semantic prior of pre-trained large foundation models to enhance gesture\nrepresentation learning in both in-domain and cross-domain scenarios.\nSpecifically, we first design a dual-path CSI encoding pipeline that captures\ngeometric and dynamic gesture patterns via CSI-Ratio phase sequences and\nDoppler spectrograms. These representations are then fed into a Multiscale\nSemantic Encoder, which learns robust temporal embeddings and aligns them with\ngesture semantics through cross-modal attention mechanisms. To further enhance\ncategory discrimination, we introduce a Semantic-Aware Soft Supervision scheme\nthat encodes inter-class correlations and reduces label ambiguity, especially\nfor semantically similar gestures. Finally, we develop a Robust\nDual-Distillation strategy to compress the aligned model into a lightweight\nstudent network, jointly distilling intermediate features and semantic-informed\nsoft labels from the teacher model. Extensive experiments on the Widar3.0\nbenchmark show that GLSDA consistently outperforms state-of-the-art methods in\nboth in-domain and cross-domain gesture recognition tasks, while significantly\nreducing model size and inference latency. Our method offers a scalable and\ndeployable solution for generalized RF-based gesture interfaces in real-world\nAIoT applications."
                },
                "authors": [
                    {
                        "name": "Feng-Qi Cui"
                    },
                    {
                        "name": "Yu-Tong Guo"
                    },
                    {
                        "name": "Tianyue Zheng"
                    },
                    {
                        "name": "Jinyang Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jinyang Huang"
                },
                "author": "Jinyang Huang",
                "arxiv_comment": "Accepted by IEEE ICPADS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13390v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13390v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13387v2",
                "updated": "2025-10-16T03:08:13Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    3,
                    8,
                    13,
                    3,
                    289,
                    0
                ],
                "published": "2025-10-15T10:26:02Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    26,
                    2,
                    2,
                    288,
                    0
                ],
                "title": "Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in\n  Real-World Dialogues without Pre-Commitment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make an Offer They Can't Refuse: Grounding Bayesian Persuasion in\n  Real-World Dialogues without Pre-Commitment"
                },
                "summary": "Persuasion, a fundamental social capability for humans, remains a challenge\nfor AI systems such as large language models (LLMs). Current studies often\noverlook the strategic use of information asymmetry in message design or rely\non strong assumptions regarding pre-commitment. In this work, we explore the\napplication of Bayesian Persuasion (BP) in natural language within single-turn\ndialogue settings, to enhance the strategic persuasion capabilities of LLMs.\nOur framework incorporates a commitment-communication mechanism, where the\npersuader explicitly outlines an information schema by narrating their\npotential types (e.g., honest or dishonest), thereby guiding the persuadee in\nperforming the intended Bayesian belief update. We evaluate two variants of our\napproach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language\n(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)\nbaselines within a comprehensive evaluation framework. This framework covers a\ndiverse set of persuadees -- including LLM instances with varying prompts and\nfine-tuning and human participants -- across tasks ranging from specially\ndesigned persuasion scenarios to general everyday situations. Experimental\nresults on LLM-based agents reveal three main findings: (1) LLMs guided by BP\nstrategies consistently achieve higher persuasion success rates than NBP\nbaselines; (2) SFNL exhibits greater credibility and logical coherence, while\nFNL shows stronger emotional resonance and robustness in naturalistic\nconversations; (3) with supervised fine-tuning, smaller models can attain BP\nperformance comparable to that of larger models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persuasion, a fundamental social capability for humans, remains a challenge\nfor AI systems such as large language models (LLMs). Current studies often\noverlook the strategic use of information asymmetry in message design or rely\non strong assumptions regarding pre-commitment. In this work, we explore the\napplication of Bayesian Persuasion (BP) in natural language within single-turn\ndialogue settings, to enhance the strategic persuasion capabilities of LLMs.\nOur framework incorporates a commitment-communication mechanism, where the\npersuader explicitly outlines an information schema by narrating their\npotential types (e.g., honest or dishonest), thereby guiding the persuadee in\nperforming the intended Bayesian belief update. We evaluate two variants of our\napproach: Semi-Formal-Natural-Language (SFNL) BP and Fully-Natural-Language\n(FNL) BP, benchmarking them against both naive and strong non-BP (NBP)\nbaselines within a comprehensive evaluation framework. This framework covers a\ndiverse set of persuadees -- including LLM instances with varying prompts and\nfine-tuning and human participants -- across tasks ranging from specially\ndesigned persuasion scenarios to general everyday situations. Experimental\nresults on LLM-based agents reveal three main findings: (1) LLMs guided by BP\nstrategies consistently achieve higher persuasion success rates than NBP\nbaselines; (2) SFNL exhibits greater credibility and logical coherence, while\nFNL shows stronger emotional resonance and robustness in naturalistic\nconversations; (3) with supervised fine-tuning, smaller models can attain BP\nperformance comparable to that of larger models."
                },
                "authors": [
                    {
                        "name": "Buwei He"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Zhaowei Zhang"
                    },
                    {
                        "name": "Zixia Jia"
                    },
                    {
                        "name": "Huijia Wu"
                    },
                    {
                        "name": "Zhaofeng He"
                    },
                    {
                        "name": "Zilong Zheng"
                    },
                    {
                        "name": "Yipeng Kang"
                    }
                ],
                "author_detail": {
                    "name": "Yipeng Kang"
                },
                "author": "Yipeng Kang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.11409v6",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.11409v6",
                "updated": "2025-10-15T10:14:34Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    14,
                    34,
                    2,
                    288,
                    0
                ],
                "published": "2023-10-17T17:15:41Z",
                "published_parsed": [
                    2023,
                    10,
                    17,
                    17,
                    15,
                    41,
                    1,
                    290,
                    0
                ],
                "title": "LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs as Hackers: Autonomous Linux Privilege Escalation Attacks"
                },
                "summary": "Penetration-testing is crucial for identifying system vulnerabilities, with\nprivilege-escalation being a critical subtask to gain elevated access to\nprotected resources. Language Models (LLMs) presents new avenues for automating\nthese security practices by emulating human behavior. However, a comprehensive\nunderstanding of LLMs' efficacy and limitations in performing autonomous Linux\nprivilege-escalation attacks remains under-explored. To address this gap, we\nintroduce hackingBuddyGPT, a fully automated LLM-driven prototype designed for\nautonomous Linux privilege-escalation. We curated a novel, publicly available\nLinux privilege-escalation benchmark, enabling controlled and reproducible\nevaluation.\n  Our empirical analysis assesses the quantitative success rates and\nqualitative operational behaviors of various LLMs -- GPT-3.5-Turbo,\nGPT-4-Turbo, and Llama3 -- against baselines of human professional pen-testers\nand traditional automated tools. We investigate the impact of context\nmanagement strategies, different context sizes, and various high-level guidance\nmechanisms on LLM performance.\n  Results show that GPT-4-Turbo demonstrates high efficacy, successfully\nexploiting 33-83% of vulnerabilities, a performance comparable to human\npen-testers (75%). In contrast, local models like Llama3 exhibited limited\nsuccess (0-33%), and GPT-3.5-Turbo achieved moderate rates (16-50%). We show\nthat both high-level guidance and state-management through LLM-driven\nreflection significantly boost LLM success rates.\n  Qualitative analysis reveals both LLMs' strengths and weaknesses in\ngenerating valid commands and highlights challenges in common-sense reasoning,\nerror handling, and multi-step exploitation, particularly with temporal\ndependencies. Cost analysis indicates that GPT-4-Turbo can achieve\nhuman-comparable performance at competitive costs, especially with optimized\ncontext management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Penetration-testing is crucial for identifying system vulnerabilities, with\nprivilege-escalation being a critical subtask to gain elevated access to\nprotected resources. Language Models (LLMs) presents new avenues for automating\nthese security practices by emulating human behavior. However, a comprehensive\nunderstanding of LLMs' efficacy and limitations in performing autonomous Linux\nprivilege-escalation attacks remains under-explored. To address this gap, we\nintroduce hackingBuddyGPT, a fully automated LLM-driven prototype designed for\nautonomous Linux privilege-escalation. We curated a novel, publicly available\nLinux privilege-escalation benchmark, enabling controlled and reproducible\nevaluation.\n  Our empirical analysis assesses the quantitative success rates and\nqualitative operational behaviors of various LLMs -- GPT-3.5-Turbo,\nGPT-4-Turbo, and Llama3 -- against baselines of human professional pen-testers\nand traditional automated tools. We investigate the impact of context\nmanagement strategies, different context sizes, and various high-level guidance\nmechanisms on LLM performance.\n  Results show that GPT-4-Turbo demonstrates high efficacy, successfully\nexploiting 33-83% of vulnerabilities, a performance comparable to human\npen-testers (75%). In contrast, local models like Llama3 exhibited limited\nsuccess (0-33%), and GPT-3.5-Turbo achieved moderate rates (16-50%). We show\nthat both high-level guidance and state-management through LLM-driven\nreflection significantly boost LLM success rates.\n  Qualitative analysis reveals both LLMs' strengths and weaknesses in\ngenerating valid commands and highlights challenges in common-sense reasoning,\nerror handling, and multi-step exploitation, particularly with temporal\ndependencies. Cost analysis indicates that GPT-4-Turbo can achieve\nhuman-comparable performance at competitive costs, especially with optimized\ncontext management."
                },
                "authors": [
                    {
                        "name": "Andreas Happe"
                    },
                    {
                        "name": "Aaron Kaplan"
                    },
                    {
                        "name": "Juergen Cito"
                    }
                ],
                "author_detail": {
                    "name": "Juergen Cito"
                },
                "author": "Juergen Cito",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.11409v6",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.11409v6",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.15568v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.15568v4",
                "updated": "2025-10-16T08:27:41Z",
                "updated_parsed": [
                    2025,
                    10,
                    16,
                    8,
                    27,
                    41,
                    3,
                    289,
                    0
                ],
                "published": "2025-08-21T13:42:49Z",
                "published_parsed": [
                    2025,
                    8,
                    21,
                    13,
                    42,
                    49,
                    3,
                    233,
                    0
                ],
                "title": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Backpropagation-Free Test-Time Adaptation via Probabilistic Gaussian\n  Alignment"
                },
                "summary": "Test-time adaptation (TTA) enhances the zero-shot robustness under\ndistribution shifts by leveraging unlabeled test data during inference. Despite\nnotable advances, several challenges still limit its broader applicability.\nFirst, most methods rely on backpropagation or iterative optimization, which\nlimits scalability and hinders real-time deployment. Second, they lack explicit\nmodeling of class-conditional feature distributions. This modeling is crucial\nfor producing reliable decision boundaries and calibrated predictions, but it\nremains underexplored due to the lack of both source data and supervision at\ntest time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and\nbackPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian\nprobabilistic inference task by modeling class-conditional likelihoods using\ngradually updated class means and a shared covariance matrix. This enables\nclosed-form, training-free inference. To correct potential likelihood bias, we\nintroduce lightweight regularization guided by CLIP priors and a historical\nknowledge bank. ADAPT requires no source data, no gradient updates, and no full\naccess to target data, supporting both online and transductive settings.\nExtensive experiments across diverse benchmarks demonstrate that our method\nachieves state-of-the-art performance under a wide range of distribution shifts\nwith superior scalability and robustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time adaptation (TTA) enhances the zero-shot robustness under\ndistribution shifts by leveraging unlabeled test data during inference. Despite\nnotable advances, several challenges still limit its broader applicability.\nFirst, most methods rely on backpropagation or iterative optimization, which\nlimits scalability and hinders real-time deployment. Second, they lack explicit\nmodeling of class-conditional feature distributions. This modeling is crucial\nfor producing reliable decision boundaries and calibrated predictions, but it\nremains underexplored due to the lack of both source data and supervision at\ntest time. In this paper, we propose ADAPT, an Advanced Distribution-Aware and\nbackPropagation-free Test-time adaptation method. We reframe TTA as a Gaussian\nprobabilistic inference task by modeling class-conditional likelihoods using\ngradually updated class means and a shared covariance matrix. This enables\nclosed-form, training-free inference. To correct potential likelihood bias, we\nintroduce lightweight regularization guided by CLIP priors and a historical\nknowledge bank. ADAPT requires no source data, no gradient updates, and no full\naccess to target data, supporting both online and transductive settings.\nExtensive experiments across diverse benchmarks demonstrate that our method\nachieves state-of-the-art performance under a wide range of distribution shifts\nwith superior scalability and robustness."
                },
                "authors": [
                    {
                        "name": "Youjia Zhang"
                    },
                    {
                        "name": "Youngeun Kim"
                    },
                    {
                        "name": "Young-Geun Choi"
                    },
                    {
                        "name": "Hongyeob Kim"
                    },
                    {
                        "name": "Huiling Liu"
                    },
                    {
                        "name": "Sungeun Hong"
                    }
                ],
                "author_detail": {
                    "name": "Sungeun Hong"
                },
                "author": "Sungeun Hong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.15568v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.15568v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13371v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13371v1",
                "updated": "2025-10-15T10:03:29Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    3,
                    29,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:03:29Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    3,
                    29,
                    2,
                    288,
                    0
                ],
                "title": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive\n  Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MADREC: A Multi-Aspect Driven LLM Agent for Explainable and Adaptive\n  Recommendation"
                },
                "summary": "Recent attempts to integrate large language models (LLMs) into recommender\nsystems have gained momentum, but most remain limited to simple text generation\nor static prompt-based inference, failing to capture the complexity of user\npreferences and real-world interactions. This study proposes the Multi-Aspect\nDriven LLM Agent MADRec, an autonomous LLM-based recommender that constructs\nuser and item profiles by unsupervised extraction of multi-aspect information\nfrom reviews and performs direct recommendation, sequential recommendation, and\nexplanation generation. MADRec generates structured profiles via\naspect-category-based summarization and applies Re-Ranking to construct\nhigh-density inputs. When the ground-truth item is missing from the output, the\nSelf-Feedback mechanism dynamically adjusts the inference criteria. Experiments\nacross multiple domains show that MADRec outperforms traditional and LLM-based\nbaselines in both precision and explainability, with human evaluation further\nconfirming the persuasiveness of the generated explanations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent attempts to integrate large language models (LLMs) into recommender\nsystems have gained momentum, but most remain limited to simple text generation\nor static prompt-based inference, failing to capture the complexity of user\npreferences and real-world interactions. This study proposes the Multi-Aspect\nDriven LLM Agent MADRec, an autonomous LLM-based recommender that constructs\nuser and item profiles by unsupervised extraction of multi-aspect information\nfrom reviews and performs direct recommendation, sequential recommendation, and\nexplanation generation. MADRec generates structured profiles via\naspect-category-based summarization and applies Re-Ranking to construct\nhigh-density inputs. When the ground-truth item is missing from the output, the\nSelf-Feedback mechanism dynamically adjusts the inference criteria. Experiments\nacross multiple domains show that MADRec outperforms traditional and LLM-based\nbaselines in both precision and explainability, with human evaluation further\nconfirming the persuasiveness of the generated explanations."
                },
                "authors": [
                    {
                        "name": "Jiin Park"
                    },
                    {
                        "name": "Misuk Kim"
                    }
                ],
                "author_detail": {
                    "name": "Misuk Kim"
                },
                "author": "Misuk Kim",
                "arxiv_comment": "18 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13371v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13371v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13369v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13369v1",
                "updated": "2025-10-15T10:00:33Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    0,
                    33,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T10:00:33Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    10,
                    0,
                    33,
                    2,
                    288,
                    0
                ],
                "title": "A theory-based AI automation exposure index: Applying Moravec's Paradox\n  to the US labor market",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A theory-based AI automation exposure index: Applying Moravec's Paradox\n  to the US labor market"
                },
                "summary": "This paper develops a theory-driven automation exposure index based on\nMoravec's Paradox. Scoring 19,000 O*NET tasks on performance variance, tacit\nknowledge, data abundance, and algorithmic gaps reveals that management, STEM,\nand sciences occupations show the highest exposure. In contrast, maintenance,\nagriculture, and construction show the lowest. The positive relationship\nbetween wages and exposure challenges the notion of skill-biased technological\nchange if AI substitutes for workers. At the same time, tacit knowledge\nexhibits a positive relationship with wages consistent with seniority-biased\ntechnological change. This index identifies fundamental automatability rather\nthan current capabilities, while also validating the AI annotation method\npioneered by Eloundou et al. (2024) with a correlation of 0.72. The\nnon-positive relationship with pre-LLM indices suggests a paradigm shift in\nautomation patterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper develops a theory-driven automation exposure index based on\nMoravec's Paradox. Scoring 19,000 O*NET tasks on performance variance, tacit\nknowledge, data abundance, and algorithmic gaps reveals that management, STEM,\nand sciences occupations show the highest exposure. In contrast, maintenance,\nagriculture, and construction show the lowest. The positive relationship\nbetween wages and exposure challenges the notion of skill-biased technological\nchange if AI substitutes for workers. At the same time, tacit knowledge\nexhibits a positive relationship with wages consistent with seniority-biased\ntechnological change. This index identifies fundamental automatability rather\nthan current capabilities, while also validating the AI annotation method\npioneered by Eloundou et al. (2024) with a correlation of 0.72. The\nnon-positive relationship with pre-LLM indices suggests a paradigm shift in\nautomation patterns."
                },
                "authors": [
                    {
                        "name": "Jacob Schaal"
                    }
                ],
                "author_detail": {
                    "name": "Jacob Schaal"
                },
                "author": "Jacob Schaal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13369v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13369v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.GN",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.GN",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.EC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00605v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00605v2",
                "updated": "2025-10-15T09:58:45Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    58,
                    45,
                    2,
                    288,
                    0
                ],
                "published": "2025-07-01T09:38:15Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    9,
                    38,
                    15,
                    1,
                    182,
                    0
                ],
                "title": "Quantize-Sample-and-Verify: LLM Acceleration via Adaptive Edge-Cloud\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantize-Sample-and-Verify: LLM Acceleration via Adaptive Edge-Cloud\n  Speculative Decoding"
                },
                "summary": "In edge-cloud speculative decoding (SD), edge devices equipped with small\nlanguage models (SLMs) generate draft tokens that are verified by large\nlanguage models (LLMs) in the cloud. A key bottleneck in such systems is the\nlimited communication bandwidth between edge and cloud, which necessitates\nquantization of the information transmitted about generated tokens. In this\nwork, we introduce a novel quantize-sample (Q-S) strategy that provably\npreserves the output distribution of the cloud-based model, ensuring that the\nverified tokens match the distribution of those that would have been generated\ndirectly by the LLM. We develop a throughput model for edge-cloud SD that\nexplicitly accounts for communication latency. Leveraging this model, we\npropose an adaptive mechanism that optimizes token throughput by dynamically\nadjusting the draft length and quantization precision in response to both\nsemantic uncertainty and channel conditions. Simulations demonstrate that the\nproposed Q-S approach significantly improves decoding efficiency in realistic\nedge-cloud deployment scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In edge-cloud speculative decoding (SD), edge devices equipped with small\nlanguage models (SLMs) generate draft tokens that are verified by large\nlanguage models (LLMs) in the cloud. A key bottleneck in such systems is the\nlimited communication bandwidth between edge and cloud, which necessitates\nquantization of the information transmitted about generated tokens. In this\nwork, we introduce a novel quantize-sample (Q-S) strategy that provably\npreserves the output distribution of the cloud-based model, ensuring that the\nverified tokens match the distribution of those that would have been generated\ndirectly by the LLM. We develop a throughput model for edge-cloud SD that\nexplicitly accounts for communication latency. Leveraging this model, we\npropose an adaptive mechanism that optimizes token throughput by dynamically\nadjusting the draft length and quantization precision in response to both\nsemantic uncertainty and channel conditions. Simulations demonstrate that the\nproposed Q-S approach significantly improves decoding efficiency in realistic\nedge-cloud deployment scenarios."
                },
                "authors": [
                    {
                        "name": "Guangyi Zhang"
                    },
                    {
                        "name": "Yunlong Cai"
                    },
                    {
                        "name": "Guanding Yu"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "arxiv_comment": "Submit for review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00605v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00605v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13366v1",
                "updated": "2025-10-15T09:57:03Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    57,
                    3,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T09:57:03Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    57,
                    3,
                    2,
                    288,
                    0
                ],
                "title": "Document Intelligence in the Era of Large Language Models: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document Intelligence in the Era of Large Language Models: A Survey"
                },
                "summary": "Document AI (DAI) has emerged as a vital application area, and is\nsignificantly transformed by the advent of large language models (LLMs). While\nearlier approaches relied on encoder-decoder architectures, decoder-only LLMs\nhave revolutionized DAI, bringing remarkable advancements in understanding and\ngeneration. This survey provides a comprehensive overview of DAI's evolution,\nhighlighting current research attempts and future prospects of LLMs in this\nfield. We explore key advancements and challenges in multimodal, multilingual,\nand retrieval-augmented DAI, while also suggesting future research directions,\nincluding agent-based approaches and document-specific foundation models. This\npaper aims to provide a structured analysis of the state-of-the-art in DAI and\nits implications for both academic and practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Document AI (DAI) has emerged as a vital application area, and is\nsignificantly transformed by the advent of large language models (LLMs). While\nearlier approaches relied on encoder-decoder architectures, decoder-only LLMs\nhave revolutionized DAI, bringing remarkable advancements in understanding and\ngeneration. This survey provides a comprehensive overview of DAI's evolution,\nhighlighting current research attempts and future prospects of LLMs in this\nfield. We explore key advancements and challenges in multimodal, multilingual,\nand retrieval-augmented DAI, while also suggesting future research directions,\nincluding agent-based approaches and document-specific foundation models. This\npaper aims to provide a structured analysis of the state-of-the-art in DAI and\nits implications for both academic and practical applications."
                },
                "authors": [
                    {
                        "name": "Weishi Wang"
                    },
                    {
                        "name": "Hengchang Hu"
                    },
                    {
                        "name": "Zhijie Zhang"
                    },
                    {
                        "name": "Zhaochen Li"
                    },
                    {
                        "name": "Hongxin Shao"
                    },
                    {
                        "name": "Daniel Dahlmeier"
                    }
                ],
                "author_detail": {
                    "name": "Daniel Dahlmeier"
                },
                "author": "Daniel Dahlmeier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.13363v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.13363v1",
                "updated": "2025-10-15T09:53:11Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    53,
                    11,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-15T09:53:11Z",
                "published_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    53,
                    11,
                    2,
                    288,
                    0
                ],
                "title": "D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured\n  Memory And Reasoning Tree",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D-SMART: Enhancing LLM Dialogue Consistency via Dynamic Structured\n  Memory And Reasoning Tree"
                },
                "summary": "Large Language Models (LLMs) often exhibit factual inconsistencies and\nlogical decay in extended, multi-turn dialogues, a challenge stemming from\ntheir reliance on static, pre-trained knowledge and an inability to reason\nadaptively over the dialogue history. Prevailing mitigation strategies, such as\nRetrieval-Augmented Generation (RAG) and agentic working memories, improve\ninformation recall but still engage with fundamentally static knowledge sources\nand follow pre-defined single reasoning path. This hinders their ability to\npreserve factual and logical consistency of their responses in multi-turn\ndialogues while the context evolves over time. To address this issue, we\npropose D-SMART, a model-agnostic framework designed to maintain multi-turn\ndialogue consistency by enabling LLMs to build and reason over a dynamic,\nstructured representation of the conversational context. This is achieved via\ntwo synergistic components: (1) a Dynamic Structured Memory (DSM), which\nincrementally constructs and maintains an authoritative, OWL-compliant\nknowledge graph of the conversation; and (2) a Reasoning Tree (RT), which\nexecutes inferences as an explicit and traceable multi-step search over the\ngraph. As the popular-used quality score (judged by GPT-4) can overlook logical\nflaws, we introduce new NLI-based metrics to better measure multi-turn dialogue\nconsistency. Comprehensive experiments on the MT-Bench-101 benchmark show that\nD-SMART significantly outperforms state-of-the-art baselines, elevating the\ndialogue consistency score by over 48\\% for both proprietary and open-source\nmodels, and notably improves the quality score of the latter by up to 10.1\\%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) often exhibit factual inconsistencies and\nlogical decay in extended, multi-turn dialogues, a challenge stemming from\ntheir reliance on static, pre-trained knowledge and an inability to reason\nadaptively over the dialogue history. Prevailing mitigation strategies, such as\nRetrieval-Augmented Generation (RAG) and agentic working memories, improve\ninformation recall but still engage with fundamentally static knowledge sources\nand follow pre-defined single reasoning path. This hinders their ability to\npreserve factual and logical consistency of their responses in multi-turn\ndialogues while the context evolves over time. To address this issue, we\npropose D-SMART, a model-agnostic framework designed to maintain multi-turn\ndialogue consistency by enabling LLMs to build and reason over a dynamic,\nstructured representation of the conversational context. This is achieved via\ntwo synergistic components: (1) a Dynamic Structured Memory (DSM), which\nincrementally constructs and maintains an authoritative, OWL-compliant\nknowledge graph of the conversation; and (2) a Reasoning Tree (RT), which\nexecutes inferences as an explicit and traceable multi-step search over the\ngraph. As the popular-used quality score (judged by GPT-4) can overlook logical\nflaws, we introduce new NLI-based metrics to better measure multi-turn dialogue\nconsistency. Comprehensive experiments on the MT-Bench-101 benchmark show that\nD-SMART significantly outperforms state-of-the-art baselines, elevating the\ndialogue consistency score by over 48\\% for both proprietary and open-source\nmodels, and notably improves the quality score of the latter by up to 10.1\\%."
                },
                "authors": [
                    {
                        "name": "Xiang Lei"
                    },
                    {
                        "name": "Qin Li"
                    },
                    {
                        "name": "Min Zhang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "arxiv_comment": "8 pages, 6 figures (main content); 25 pages, 18 figures (total)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.13363v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.13363v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2510.11891v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2510.11891v2",
                "updated": "2025-10-15T09:52:28Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    52,
                    28,
                    2,
                    288,
                    0
                ],
                "published": "2025-10-13T19:50:43Z",
                "published_parsed": [
                    2025,
                    10,
                    13,
                    19,
                    50,
                    43,
                    0,
                    286,
                    0
                ],
                "title": "Based on Deep Neural Networks: A Machine Learning-Assisted Channel\n  Estimation Method for MIMO Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Based on Deep Neural Networks: A Machine Learning-Assisted Channel\n  Estimation Method for MIMO Systems"
                },
                "summary": "This paper proposes a machine learning-assisted channel estimation approach\nfor massive MIMO systems, leveraging DNNs to outperform traditional LS and MMSE\nmethods. In 5G and beyond, accurate channel estimation mitigates pilot\ncontamination and high mobility issues that harm system reliability. The\nproposed DNN architecture includes multi-layer perceptrons with ReLU\nactivation, 3 hidden layers (256, 128, 64 neurons respectively), uses Adam\noptimizer (learning rate 1e-4) and MSE loss function. It learns from pilot\nsignals to predict channel matrices, achieving lower NMSE and BER across\ndifferent SNR levels. Simulations use the COST 2100 public standard dataset (a\nwell-recognized MIMO channel dataset for 5G, not synthetic datasets) with\n10,000 samples of 4x4 MIMO channels under urban macro scenarios. Results show\nthe DNN outperforms LS and MMSE by 3-5 dB in NMSE at medium SNR, with robust\nperformance in high-mobility scenarios. The study evaluates metrics like NMSE\nvs. SNR, BER vs. SNR, and sensitivity to pilot length, antenna configurations,\nand computational complexity. The DNN has 2.3 GFlOPs computational complexity,\n15.6k parameters, and 1.8 ms inference time on Raspberry Pi 4, verifying\ndeployment feasibility. This work advances ML integration in wireless\ncommunications, facilitating efficient resource allocation and improved\nspectral efficiency in next-generation networks. Future work may use more\nreal-world datasets and hybrid architectures for better generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a machine learning-assisted channel estimation approach\nfor massive MIMO systems, leveraging DNNs to outperform traditional LS and MMSE\nmethods. In 5G and beyond, accurate channel estimation mitigates pilot\ncontamination and high mobility issues that harm system reliability. The\nproposed DNN architecture includes multi-layer perceptrons with ReLU\nactivation, 3 hidden layers (256, 128, 64 neurons respectively), uses Adam\noptimizer (learning rate 1e-4) and MSE loss function. It learns from pilot\nsignals to predict channel matrices, achieving lower NMSE and BER across\ndifferent SNR levels. Simulations use the COST 2100 public standard dataset (a\nwell-recognized MIMO channel dataset for 5G, not synthetic datasets) with\n10,000 samples of 4x4 MIMO channels under urban macro scenarios. Results show\nthe DNN outperforms LS and MMSE by 3-5 dB in NMSE at medium SNR, with robust\nperformance in high-mobility scenarios. The study evaluates metrics like NMSE\nvs. SNR, BER vs. SNR, and sensitivity to pilot length, antenna configurations,\nand computational complexity. The DNN has 2.3 GFlOPs computational complexity,\n15.6k parameters, and 1.8 ms inference time on Raspberry Pi 4, verifying\ndeployment feasibility. This work advances ML integration in wireless\ncommunications, facilitating efficient resource allocation and improved\nspectral efficiency in next-generation networks. Future work may use more\nreal-world datasets and hybrid architectures for better generalization."
                },
                "authors": [
                    {
                        "name": "Haoran He"
                    }
                ],
                "author_detail": {
                    "name": "Haoran He"
                },
                "author": "Haoran He",
                "arxiv_comment": "4 pages, 8 figures, ISCIPT 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2510.11891v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2510.11891v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2509.24101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2509.24101v2",
                "updated": "2025-10-15T09:50:59Z",
                "updated_parsed": [
                    2025,
                    10,
                    15,
                    9,
                    50,
                    59,
                    2,
                    288,
                    0
                ],
                "published": "2025-09-28T22:39:40Z",
                "published_parsed": [
                    2025,
                    9,
                    28,
                    22,
                    39,
                    40,
                    6,
                    271,
                    0
                ],
                "title": "BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment\n  Analysis Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BTC-SAM: Leveraging LLMs for Generation of Bias Test Cases for Sentiment\n  Analysis Models"
                },
                "summary": "Sentiment Analysis (SA) models harbor inherent social biases that can be\nharmful in real-world applications. These biases are identified by examining\nthe output of SA models for sentences that only vary in the identity groups of\nthe subjects. Constructing natural, linguistically rich, relevant, and diverse\nsets of sentences that provide sufficient coverage over the domain is\nexpensive, especially when addressing a wide range of biases: it requires\ndomain experts and/or crowd-sourcing. In this paper, we present a novel bias\ntesting framework, BTC-SAM, which generates high-quality test cases for bias\ntesting in SA models with minimal specification using Large Language Models\n(LLMs) for the controllable generation of test sentences. Our experiments show\nthat relying on LLMs can provide high linguistic variation and diversity in the\ntest sentences, thereby offering better test coverage compared to base\nprompting methods even for previously unseen biases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sentiment Analysis (SA) models harbor inherent social biases that can be\nharmful in real-world applications. These biases are identified by examining\nthe output of SA models for sentences that only vary in the identity groups of\nthe subjects. Constructing natural, linguistically rich, relevant, and diverse\nsets of sentences that provide sufficient coverage over the domain is\nexpensive, especially when addressing a wide range of biases: it requires\ndomain experts and/or crowd-sourcing. In this paper, we present a novel bias\ntesting framework, BTC-SAM, which generates high-quality test cases for bias\ntesting in SA models with minimal specification using Large Language Models\n(LLMs) for the controllable generation of test sentences. Our experiments show\nthat relying on LLMs can provide high linguistic variation and diversity in the\ntest sentences, thereby offering better test coverage compared to base\nprompting methods even for previously unseen biases."
                },
                "authors": [
                    {
                        "name": "Zsolt T. Kardkovacs"
                    },
                    {
                        "name": "Lynda Djennane"
                    },
                    {
                        "name": "Anna Field"
                    },
                    {
                        "name": "Boualem Benatallah"
                    },
                    {
                        "name": "Yacine Gaci"
                    },
                    {
                        "name": "Fabio Casati"
                    },
                    {
                        "name": "Walid Gaaloul"
                    }
                ],
                "author_detail": {
                    "name": "Walid Gaaloul"
                },
                "author": "Walid Gaaloul",
                "arxiv_comment": "Accepted at EMNLP 2025 main conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2509.24101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2509.24101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]