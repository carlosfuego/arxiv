[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08784v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08784v2",
                "updated": "2025-02-02T14:38:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    38,
                    15,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-12T07:35:30Z",
                "published_parsed": [
                    2023,
                    10,
                    12,
                    7,
                    35,
                    30,
                    3,
                    285,
                    0
                ],
                "title": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implicit Shape and Appearance Priors for Few-Shot Full Head\n  Reconstruction"
                },
                "summary": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in learning techniques that employ coordinate-based\nneural representations have yielded remarkable results in multi-view 3D\nreconstruction tasks. However, these approaches often require a substantial\nnumber of input views (typically several tens) and computationally intensive\noptimization procedures to achieve their effectiveness. In this paper, we\naddress these limitations specifically for the problem of few-shot full 3D head\nreconstruction. We accomplish this by incorporating a probabilistic shape and\nappearance prior into coordinate-based representations, enabling faster\nconvergence and improved generalization when working with only a few input\nimages (even as low as a single image). During testing, we leverage this prior\nto guide the fitting process of a signed distance function using a\ndifferentiable renderer. By incorporating the statistical prior alongside\nparallelizable ray tracing and dynamic caching strategies, we achieve an\nefficient and accurate approach to few-shot full 3D head reconstruction.\nMoreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D\nfull head scans and their corresponding posed images and masks, which we use\nfor evaluation purposes. By leveraging this dataset, we demonstrate the\nremarkable capabilities of our approach in achieving state-of-the-art results\nin geometry reconstruction while being an order of magnitude faster than\nprevious approaches."
                },
                "authors": [
                    {
                        "name": "Pol Caselles"
                    },
                    {
                        "name": "Eduard Ramon"
                    },
                    {
                        "name": "Jaime Garcia"
                    },
                    {
                        "name": "Gil Triginer"
                    },
                    {
                        "name": "Francesc Moreno-Noguer"
                    }
                ],
                "author_detail": {
                    "name": "Francesc Moreno-Noguer"
                },
                "author": "Francesc Moreno-Noguer",
                "arxiv_comment": "Accepted at IEEE Transactions on Pattern Analysis and Machine\n  Intelligence (TPAMI) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08784v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08784v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05262v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05262v3",
                "updated": "2025-02-01T16:00:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    0,
                    50,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-09T14:16:43Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    14,
                    16,
                    43,
                    3,
                    9,
                    0
                ],
                "title": "QMDB: Quick Merkle Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QMDB: Quick Merkle Database"
                },
                "summary": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quick Merkle Database (QMDB) addresses longstanding bottlenecks in blockchain\nstate management by integrating key-value (KV) and Merkle tree storage into a\nsingle unified architecture. QMDB delivers a significant throughput improvement\nover existing architectures, achieving up to 6X over the widely used RocksDB\nand 8X over NOMT, a leading verifiable database. Its novel append-only\ntwig-based design enables one SSD read per state access, O(1) IOs for updates,\nand in-memory Merkleization on a memory footprint as small as 2.3 bytes per\nentry, enabling it to run on even modest consumer-grade PCs. QMDB scales\nseamlessly across both commodity and enterprise hardware, achieving up to 2.28\nmillion state updates per second. This performance enables support for 1\nmillion token transfers per second (TPS), marking QMDB as the first solution\nachieving such a milestone. QMDB has been benchmarked with workloads exceeding\n15 billion entries (10X Ethereum's 2024 state) and has proven the capacity to\nscale to 280 billion entries on a single server. Furthermore, QMDB introduces\nhistorical proofs, unlocking the ability to query its blockchain's historical\nstate at the latest block. QMDB not only meets the demands of current\nblockchains but also provides a robust foundation for building scalable,\nefficient, and verifiable decentralized applications across diverse use cases."
                },
                "authors": [
                    {
                        "name": "Isaac Zhang"
                    },
                    {
                        "name": "Ryan Zarick"
                    },
                    {
                        "name": "Daniel Wong"
                    },
                    {
                        "name": "Thomas Kim"
                    },
                    {
                        "name": "Bryan Pellegrino"
                    },
                    {
                        "name": "Mignon Li"
                    },
                    {
                        "name": "Kelvin Wong"
                    }
                ],
                "author_detail": {
                    "name": "Kelvin Wong"
                },
                "author": "Kelvin Wong",
                "arxiv_comment": "11 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05262v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05262v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11305v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11305v2",
                "updated": "2025-02-01T04:24:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    4,
                    24,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-15T05:57:51Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    5,
                    57,
                    51,
                    1,
                    289,
                    0
                ],
                "title": "QSpec: Speculative Decoding with Complementary Quantization Schemes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QSpec: Speculative Decoding with Complementary Quantization Schemes"
                },
                "summary": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization has been substantially adopted to accelerate inference and\nreduce memory consumption of large language models (LLMs). While\nactivation-weight joint quantization speeds up the inference process through\nlow-precision kernels, we demonstrate that it suffers severe performance\ndegradation on multi-step reasoning tasks, rendering it ineffective. We propose\na novel quantization paradigm called QSPEC, which seamlessly integrates two\ncomplementary quantization schemes for speculative decoding. Leveraging nearly\ncost-free execution switching, QSPEC drafts tokens with low-precision, fast\nactivation-weight quantization, and verifies them with high-precision\nweight-only quantization, effectively combining the strengths of both\nquantization schemes. Compared to high-precision quantization methods, QSPEC\nempirically boosts token generation throughput by up to 1.64x without any\nquality compromise, distinguishing it from other low-precision quantization\napproaches. This enhancement is also consistent across various serving tasks,\nmodel sizes, quantization methods, and batch sizes. Compared to state-of-art\nspeculative decoding methods, our approach reuses weights and the KV cache,\navoiding extra memory overhead while achieving up to 1.55x speedup in batched\nserving with a high acceptance rate. Furthermore, QSPEC offers a plug-and-play\nadvantage without requiring any training. We believe that QSPEC demonstrates\nunique strengths for future deployment of high-fidelity quantization schemes,\nparticularly in memory-constrained scenarios (e.g., edge devices)."
                },
                "authors": [
                    {
                        "name": "Juntao Zhao"
                    },
                    {
                        "name": "Wenhao Lu"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Lingpeng Kong"
                    },
                    {
                        "name": "Chuan Wu"
                    }
                ],
                "author_detail": {
                    "name": "Chuan Wu"
                },
                "author": "Chuan Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11305v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11305v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.07331v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.07331v2",
                "updated": "2025-02-01T03:40:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    3,
                    40,
                    37,
                    5,
                    32,
                    0
                ],
                "published": "2024-09-11T15:11:39Z",
                "published_parsed": [
                    2024,
                    9,
                    11,
                    15,
                    11,
                    39,
                    2,
                    255,
                    0
                ],
                "title": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Compress Contexts for Efficient Knowledge-based Visual\n  Question Answering"
                },
                "summary": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal large language models (MLLMs) have demonstrated great performance\non visual question answering (VQA). When it comes to knowledge-based Visual\nQuestion Answering (KB-VQA), MLLMs may lack the specialized domain knowledge\nneeded to answer questions, necessitating the retrieval of necessary\ninformation from external knowledge sources. Previous works like\nRetrival-Augmented VQA-v2 (RAVQA-v2) focus on utilizing as much input\ninformation, such as image-based textual descriptions and retrieved knowledge,\nas possible to improve performance, but they all overlook the issue that with\nthe number of input tokens increasing, inference efficiency significantly\ndecreases, which contradicts the demands of practical applications. To address\nthis issue, we propose \\textbf{R}etrieval-\\textbf{A}ugmented MLLMs with\nCompressed Contexts (RACC). RACC learns to compress and aggregate retrieved\nknowledge for a given image-question pair, generating a compact modulation in\nthe form of Key-Value (KV) cache to adapt the downstream frozen MLLM, thereby\nachieving effective and efficient inference. RACC achieves a state-of-the-art\n(SOTA) performance of 63.92\\% on OK-VQA. Moreover, it significantly reduces\ninference latency by 22.0\\%-59.7\\% compared to the prominent RAVQA-v2. Abundant\nexperiments show RACC's broad applicability. It is compatible with various\noff-the-shelf MLLMs and can also handle different knowledge sources including\ntextual and multimodal documents."
                },
                "authors": [
                    {
                        "name": "Weixi Weng"
                    },
                    {
                        "name": "Jieming Zhu"
                    },
                    {
                        "name": "Xiaojun Meng"
                    },
                    {
                        "name": "Hao Zhang"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Chun Yuan"
                },
                "author": "Chun Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.07331v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.07331v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12178v2",
                "updated": "2025-01-31T19:09:19Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    19,
                    9,
                    19,
                    4,
                    31,
                    0
                ],
                "published": "2024-12-13T02:26:54Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    2,
                    26,
                    54,
                    4,
                    348,
                    0
                ],
                "title": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activation Sparsity Opportunities for Compressing General Large Language\n  Models"
                },
                "summary": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying local AI models, such as Large Language Models (LLMs), to edge\ndevices can substantially enhance devices' independent capabilities, alleviate\nthe server's burden, and lower the response time. Owing to these tremendous\npotentials, many big tech companies have released several lightweight Small\nLanguage Models (SLMs) to bridge this gap. However, we still have huge\nmotivations to deploy more powerful (LLMs) AI models on edge devices and\nenhance their smartness level. Unlike the conventional approaches for AI model\ncompression, we investigate activation sparsity. The activation sparsity method\nis orthogonal and combinable with existing techniques to maximize the\ncompression rate while maintaining great accuracy. LLMs' Feed-Forward Network\n(FFN) components, which typically comprise a large proportion of parameters\n(around 2/3), ensure that our FFN optimizations would have a better chance of\nachieving effective compression. Moreover, our findings are beneficial to\ngeneral LLMs and are not restricted to ReLU-based models. This work\nsystematically investigates the tradeoff between enforcing activation sparsity\nand perplexity (accuracy) on state-of-the-art LLMs. Our empirical analysis\ndemonstrates that we can obtain around 50% of main memory and computing\nreductions for critical FFN components with negligible accuracy degradation.\nThis extra 50% sparsity does not naturally exist in the current LLMs, which\nrequire tuning LLMs' activation outputs by injecting zero-enforcing thresholds.\nTo obtain the benefits of activation sparsity, we provide a guideline for the\nsystem architect for LLM prediction and prefetching. The success prediction\nallows the system to prefetch the necessary weights while omitting the inactive\nones and their successors, therefore lowering cache and memory pollution and\nreducing LLM execution time on resource-constrained edge devices."
                },
                "authors": [
                    {
                        "name": "Nobel Dhar"
                    },
                    {
                        "name": "Bobin Deng"
                    },
                    {
                        "name": "Md Romyull Islam"
                    },
                    {
                        "name": "Kazi Fahim Ahmad Nasif"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Kun Suo"
                    }
                ],
                "author_detail": {
                    "name": "Kun Suo"
                },
                "author": "Kun Suo",
                "arxiv_doi": "10.1109/IPCCC59868.2024.10850382",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/IPCCC59868.2024.10850382",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2412.12178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "pp. 1-9, doi: 10.1109/IPCCC59868.2024.10850382. keywords:\n  {Accuracy;Prefetching;Large language models;Computational\n  modeling;Companies;Transformers;User experience;Time\n  factors;Tuning;Guidelines;Large Language Models (LLMs);AI\n  Compression;Activation Sparsity;Edge LLM},",
                "arxiv_journal_ref": "2024 IEEE International Performance, Computing, and Communications\n  Conference (IPCCC), Orlando, FL, USA, 2024",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19392v1",
                "updated": "2025-01-31T18:47:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T18:47:42Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    47,
                    42,
                    4,
                    31,
                    0
                ],
                "title": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Me If You Must: Adaptive Key-Value Quantization for Large Language\n  Models"
                },
                "summary": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient real-world deployments of large language models (LLMs) rely on\nKey-Value (KV) caching for processing and generating long outputs, reducing the\nneed for repetitive computation. For large contexts, Key-Value caches can take\nup tens of gigabytes of device memory, as they store vector representations for\neach token and layer. Recent work has shown that the cached vectors can be\ncompressed through quantization, pruning or merging, but these techniques often\ncompromise quality towards higher compression rates. In this work, we aim to\nimprove Key & Value compression by exploiting two observations: 1) the inherent\ndependencies between keys and values across different layers, and 2)\nhigh-compression mechanisms for internal network states. We propose AQUA-KV, an\nadaptive quantization for Key-Value caches that relies on compact adapters to\nexploit existing dependencies between Keys and Values, and aims to \"optimally\"\ncompress the information that cannot be predicted. AQUA-KV significantly\nimproves compression rates, while maintaining high accuracy on state-of-the-art\nLLM families. On Llama 3.2 LLMs, we achieve near-lossless inference at 2-2.5\nbits per value with under $1\\%$ relative error in perplexity and LongBench\nscores. AQUA-KV is one-shot, simple, and efficient: it can be calibrated on a\nsingle GPU within 1-6 hours, even for 70B models."
                },
                "authors": [
                    {
                        "name": "Alina Shutova"
                    },
                    {
                        "name": "Vladimir Malinovskii"
                    },
                    {
                        "name": "Vage Egiazarian"
                    },
                    {
                        "name": "Denis Kuznedelev"
                    },
                    {
                        "name": "Denis Mazur"
                    },
                    {
                        "name": "Nikita Surkov"
                    },
                    {
                        "name": "Ivan Ermakov"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "Preprint, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19300v1",
                "updated": "2025-01-31T16:56:18Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T16:56:18Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    16,
                    56,
                    18,
                    4,
                    31,
                    0
                ],
                "title": "Offline Learning for Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Offline Learning for Combinatorial Multi-armed Bandits"
                },
                "summary": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The combinatorial multi-armed bandit (CMAB) is a fundamental sequential\ndecision-making framework, extensively studied over the past decade. However,\nexisting work primarily focuses on the online setting, overlooking the\nsubstantial costs of online interactions and the readily available offline\ndatasets. To overcome these limitations, we introduce Off-CMAB, the first\noffline learning framework for CMAB. Central to our framework is the\ncombinatorial lower confidence bound (CLCB) algorithm, which combines\npessimistic reward estimations with combinatorial solvers. To characterize the\nquality of offline datasets, we propose two novel data coverage conditions and\nprove that, under these conditions, CLCB achieves a near-optimal suboptimality\ngap, matching the theoretical lower bound up to a logarithmic factor. We\nvalidate Off-CMAB through practical applications, including learning to rank,\nlarge language model (LLM) caching, and social influence maximization, showing\nits ability to handle nonlinear reward functions, general feedback models, and\nout-of-distribution action samples that excludes optimal or even feasible\nactions. Extensive experiments on synthetic and real-world datasets further\nhighlight the superior performance of CLCB."
                },
                "authors": [
                    {
                        "name": "Xutong Liu"
                    },
                    {
                        "name": "Xiangxiang Dai"
                    },
                    {
                        "name": "Jinhang Zuo"
                    },
                    {
                        "name": "Siwei Wang"
                    },
                    {
                        "name": "Carlee-Joe Wong"
                    },
                    {
                        "name": "John C. S. Lui"
                    },
                    {
                        "name": "Wei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Wei Chen"
                },
                "author": "Wei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v1",
                "updated": "2025-01-31T15:58:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching (especially\nover-caching). On the ImageNet dataset, without significantly increasing the\ncomputational burden, this method improves the quality of the generated images\nunder the over-caching, rule-based, and training-based methods. Specifically,\nthe Fr\\'echet Inception Distance (FID) values are improved as follows: from\n6.857 to 5.821, from 3.870 to 3.692 and form 3.539 to 3.451 respectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v3",
                "updated": "2025-01-31T14:26:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    26,
                    5,
                    4,
                    31,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Our code will be released upon acceptance. The Change Logs on Page 9\n  reveal our significant changes compared with v1 and v2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.17426v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.17426v3",
                "updated": "2025-01-31T14:13:49Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    14,
                    13,
                    49,
                    4,
                    31,
                    0
                ],
                "published": "2024-11-26T13:34:02Z",
                "published_parsed": [
                    2024,
                    11,
                    26,
                    13,
                    34,
                    2,
                    1,
                    331,
                    0
                ],
                "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning"
                },
                "summary": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B."
                },
                "authors": [
                    {
                        "name": "Fanxu Meng"
                    },
                    {
                        "name": "Pingzhi Tang"
                    },
                    {
                        "name": "Fan jiang"
                    },
                    {
                        "name": "Muhan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Muhan Zhang"
                },
                "author": "Muhan Zhang",
                "arxiv_comment": "https://github.com/GraphPKU/PiSSA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.17426v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.17426v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19051v1",
                "updated": "2025-01-31T11:25:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T11:25:40Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    25,
                    40,
                    4,
                    31,
                    0
                ],
                "title": "Swift: Rethinking RDMA Control Plane for Elastic Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Swift: Rethinking RDMA Control Plane for Elastic Computing"
                },
                "summary": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Elastic computing enables dynamic scaling to meet workload demands, and\nRemote Direct Memory Access (RDMA) enhances this by providing high-throughput,\nlow-latency network communication. However, integrating RDMA into elastic\ncomputing remains a challenge, particularly in control plane operations for\nRDMA connection setup.\n  This paper revisits the assumptions of prior work on high-performance RDMA\nfor elastic computing, and reveals that extreme microsecond-level control plane\noptimizations are often unnecessary. By challenging the conventional beliefs on\nthe slowness of user-space RDMA control plane and the difficulty of user-space\nRDMA resource sharing, we uncover new design opportunities. Our key insight is\nthat user-space RDMA connection setup can be significantly improved with\ncaching, while RDMA resources can be efficiently shared among processes using\nfork. In light of this, we propose Swift, a simple yet effective solution that\nco-designs RDMA with a serverless framework to optimize performance for elastic\ncomputing. At its very core, Swift handles cold and warm serverless requests by\nswiftly initializing the RDMA control plane with cache-optimized libibverbs,\nand manages fork requests by leveraging the RDMA's fork capability. Implemented\nwith OpenWhisk, Swift delivers 30.56-46.50% higher average throughput and\n18.55-37.21% lower latency, at a cost of 6.5% control plane overhead, compared\nto prior solutions."
                },
                "authors": [
                    {
                        "name": "Junxue Zhang"
                    },
                    {
                        "name": "Han Tian"
                    },
                    {
                        "name": "Xinyang Huang"
                    },
                    {
                        "name": "Wenxue Li"
                    },
                    {
                        "name": "Kaiqiang Xu"
                    },
                    {
                        "name": "Dian Shen"
                    },
                    {
                        "name": "Yong Wang"
                    },
                    {
                        "name": "Kai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Kai Chen"
                },
                "author": "Kai Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19021v1",
                "updated": "2025-01-31T10:43:00Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T10:43:00Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    10,
                    43,
                    0,
                    4,
                    31,
                    0
                ],
                "title": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The development of IBIC microscopy at the 100 kV ion implanter of the\n  University of Torino (LIUTo) and the application for the assessment of the\n  radiation hardness of a silicon photodiode"
                },
                "summary": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Ion Beam Induced Charge (IBIC) technique is widely used to characterize\nthe electronic properties of semiconductor materials and devices. Its main\nadvantage over other charge collection microscopies stems in the use of MeV ion\nprobes, which provide both measurable induced charge signals from single ions,\nand high spatial resolution, which is maintained along the ion range. It is a\nfact, however, that the use of low-energy ions in the keV range can provide the\nIBIC technique with complementary analytical capabilities, that are not\navailable with MeV ions, for example the higher sensitivity to the status,\ncontamination and morphology of the surface and the fact that the induced\nsignal depends on the transport of only one type of charge carrier. This paper\noutlines the upgrade that was made at the 100 kV ion implanter of the\nUniversity of Torino, originally installed for material and surface\nmodification, to explore the rather unexplored keV-IBIC field and to assess its\npotential to characterize semiconductor devices. Finally, we report the first\nIBIC application of our apparatus, which regards the assessment of the\nradiation damage of a commercially available silicon photodiode, adopting the\nIAEA experimental protocol and the relevant interpretative model."
                },
                "authors": [
                    {
                        "name": "Emilio Corte"
                    },
                    {
                        "name": "Alberto Bortone"
                    },
                    {
                        "name": "Elena Nieto Hernández"
                    },
                    {
                        "name": "Carlo Ceresa"
                    },
                    {
                        "name": "Georgios Provatas"
                    },
                    {
                        "name": "Karla Ivanković Nizić"
                    },
                    {
                        "name": "Milko Jaksić"
                    },
                    {
                        "name": "Ettore Vittone"
                    },
                    {
                        "name": "Sviatoslav Ditalia Tchernij"
                    }
                ],
                "author_detail": {
                    "name": "Sviatoslav Ditalia Tchernij"
                },
                "author": "Sviatoslav Ditalia Tchernij",
                "arxiv_comment": "15 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18824v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18824v1",
                "updated": "2025-01-31T00:43:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "published": "2025-01-31T00:43:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    0,
                    43,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "Memory-Efficient Fine-Tuning of Transformers via Token Selection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-Efficient Fine-Tuning of Transformers via Token Selection"
                },
                "summary": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning provides an effective means to specialize pre-trained models for\nvarious downstream tasks. However, fine-tuning often incurs high memory\noverhead, especially for large transformer-based models, such as LLMs. While\nexisting methods may reduce certain parts of the memory required for\nfine-tuning, they still require caching all intermediate activations computed\nin the forward pass to update weights during the backward pass. In this work,\nwe develop TokenTune, a method to reduce memory usage, specifically the memory\nto store intermediate activations, in the fine-tuning of transformer-based\nmodels. During the backward pass, TokenTune approximates the gradient\ncomputation by backpropagating through just a subset of input tokens. Thus,\nwith TokenTune, only a subset of intermediate activations are cached during the\nforward pass. Also, TokenTune can be easily combined with existing methods like\nLoRA, further reducing the memory cost. We evaluate our approach on pre-trained\ntransformer models with up to billions of parameters, considering the\nperformance on multiple downstream tasks such as text classification and\nquestion answering in a few-shot learning setup. Overall, TokenTune achieves\nperformance on par with full fine-tuning or representative memory-efficient\nfine-tuning methods, while greatly reducing the memory footprint, especially\nwhen combined with other methods with complementary memory reduction\nmechanisms. We hope that our approach will facilitate the fine-tuning of large\ntransformers, in specializing them for specific domains or co-training them\nwith other neural components from a larger system. Our code is available at\nhttps://github.com/facebookresearch/tokentune."
                },
                "authors": [
                    {
                        "name": "Antoine Simoulin"
                    },
                    {
                        "name": "Namyong Park"
                    },
                    {
                        "name": "Xiaoyi Liu"
                    },
                    {
                        "name": "Grey Yang"
                    }
                ],
                "author_detail": {
                    "name": "Grey Yang"
                },
                "author": "Grey Yang",
                "arxiv_comment": "EMNLP 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18824v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18824v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21625v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21625v3",
                "updated": "2025-01-30T18:23:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    18,
                    23,
                    46,
                    3,
                    30,
                    0
                ],
                "published": "2024-07-31T14:17:49Z",
                "published_parsed": [
                    2024,
                    7,
                    31,
                    14,
                    17,
                    49,
                    2,
                    213,
                    0
                ],
                "title": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REPS: Recycled Entropy Packet Spraying for Adaptive Load Balancing and\n  Failure Mitigation"
                },
                "summary": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next-generation datacenters require highly efficient network load balancing\nto manage the growing scale of artificial intelligence (AI) training and\ngeneral datacenter traffic. Existing solutions designed for Ethernet, such as\nEqual Cost Multi-Path (ECMP) and oblivious packet spraying (OPS), struggle to\nmaintain high network utilizations as datacenter topologies (and network\nfailures as a consequence) continue to grow. To address these limitations, we\npropose REPS, a lightweight decentralized per-packet adaptive load balancing\nalgorithm designed to optimize network utilization while ensuring rapid\nrecovery from link failures. REPS adapts to network conditions by caching\ngood-performing paths. In case of a network failure, REPS re-routes traffic\naway from it in less than 100 microseconds. REPS is designed to be deployed\nwith next-generation out-of-order transports, such as Ultra Ethernet, and\nintroduces less than 25 bytes of per-connection state. We extensively evaluate\nREPS in large-scale simulations and FPGA-based NICs."
                },
                "authors": [
                    {
                        "name": "Tommaso Bonato"
                    },
                    {
                        "name": "Abdul Kabbani"
                    },
                    {
                        "name": "Ahmad Ghalayini"
                    },
                    {
                        "name": "Michael Papamichael"
                    },
                    {
                        "name": "Mohammad Dohadwala"
                    },
                    {
                        "name": "Lukas Gianinazzi"
                    },
                    {
                        "name": "Mikhail Khalilov"
                    },
                    {
                        "name": "Elias Achermann"
                    },
                    {
                        "name": "Daniele De Sensi"
                    },
                    {
                        "name": "Torsten Hoefler"
                    }
                ],
                "author_detail": {
                    "name": "Torsten Hoefler"
                },
                "author": "Torsten Hoefler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21625v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21625v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18356v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18356v1",
                "updated": "2025-01-30T14:03:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "published": "2025-01-30T14:03:36Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    14,
                    3,
                    36,
                    3,
                    30,
                    0
                ],
                "title": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "State Stream Transformer (SST) : Emergent Metacognitive Behaviours\n  Through Latent State Persistence"
                },
                "summary": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the State Stream Transformer (SST), a novel LLM architecture\nthat reveals emergent reasoning behaviours and capabilities latent in\npretrained weights through addressing a fundamental limitation in traditional\ntransformer models: the lack of latent computational continuity across\nautoregressive generations in the state space. SST introduces a sliding window\nlatent state (FFN) cache with weighted decay that maintains and evolves\npersistent latent processes throughout autoregressive generations. Through\ncontrolled experiments comparing base and SST architectures using the same\nfrozen weights, we demonstrate that this architectural modification alone\nenables enhanced reasoning capabilities which appear best explained by some\nform of potential higher-order processing, as evidenced by emergent\nmetacognitive behaviours. These behaviours persist under controlled conditions\ndesigned to eliminate confounding factors such as stochastic variation or\nlearned response patterns. Analysis of latent state distributions and\nprocessing dynamics provides evidence that it is solely the 'state stream' that\nis responsible for these phenomena. In quantitative evaluations, the SST\nachieves substantial performance improvements over the base model on two\nreasoning benchmarks, reaching 89.01\\% accuracy on GSM-8K (0-shot) and 91.04\\%\non ARC Challenge (0-shot CoT). These findings indicate that persistent\ncomputation in the latent state space enables fundamentally different\ninformation processing and internal reasoning strategies, with implications for\nour understanding of artificial intelligence systems."
                },
                "authors": [
                    {
                        "name": "Thea Aviss"
                    }
                ],
                "author_detail": {
                    "name": "Thea Aviss"
                },
                "author": "Thea Aviss",
                "arxiv_comment": "25 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18356v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18356v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01805v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01805v2",
                "updated": "2025-01-30T13:07:37Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    13,
                    7,
                    37,
                    3,
                    30,
                    0
                ],
                "published": "2024-10-02T17:59:52Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    59,
                    52,
                    2,
                    276,
                    0
                ],
                "title": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locret: Enhancing Eviction in Long-Context LLM Inference with Trained\n  Retaining Heads on Consumer-Grade Devices"
                },
                "summary": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling the input context length of a large language model (LLM) incurs a\nsignificant increase in computation cost and memory footprint to maintain the\nattention key-value (KV) cache. Existing KV cache compression methods suffer\nfrom inefficient compression strategies and limited memory reduction effects,\nmaking it difficult for LLMs to conduct long-context inference on\nconsumer-grade devices, especially when inferring long-context stream input.\nSuch obstacles prevent consumer-grade devices from supporting more complex\napplications, creating challenges for the democratization of LLMs. To overcome\nthis, we propose Locret, the first framework to create an eviction policy\ncompatible with chunked prefill. By evaluating the causal importance of KV\ncache units by learnable retaining heads, Locret enables precise eviction of\ncache units, facilitating efficient long-context inference. In our extensive\nempirical studies, Locret outperforms the recent popular and competitive\napproaches in terms of memory efficiency and generation quality -- Locret\nachieves up to 20x of KV cache compression ratio within less than 10%\nperformance loss. Furthermore, Locret achieves 128K+ long-context inference on\na single NVIDIA 4090 GPU without compromising generation quality and only costs\n<1 GPU hour of additional training."
                },
                "authors": [
                    {
                        "name": "Yuxiang Huang"
                    },
                    {
                        "name": "Binhang Yuan"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Chaojun Xiao"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhiyuan Liu"
                },
                "author": "Zhiyuan Liu",
                "arxiv_comment": "Preprints",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01805v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01805v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05172v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05172v2",
                "updated": "2025-01-30T06:02:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    30,
                    6,
                    2,
                    11,
                    3,
                    30,
                    0
                ],
                "published": "2023-10-08T14:06:06Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    14,
                    6,
                    6,
                    6,
                    281,
                    0
                ],
                "title": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Systematic Evaluation of Randomized Cache Designs against Cache\n  Occupancy"
                },
                "summary": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Randomizing the address-to-set mapping and partitioning of the cache has been\nshown to be an effective mechanism in designing secured caches. Several designs\nhave been proposed on a variety of rationales: (1) randomized design, (2)\nrandomized-and-partitioned design, and (3) psuedo-fully associative design.\nThis work fills in a crucial gap in current literature on randomized caches:\ncurrently most randomized cache designs defend only contention-based attacks,\nand leave out considerations of cache occupancy. We perform a systematic\nevaluation of 5 randomized cache designs- CEASER, CEASER-S, MIRAGE,\nScatter-Cache, and Sass-cache against cache occupancy wrt. both performance as\nwell as security.\n  With respect to performance, we first establish that benchmarking strategies\nused by contemporary designs are unsuitable for a fair evaluation (because of\ndiffering cache configurations, choice of benchmarking suites, additional\nimplementation-specific assumptions). We thus propose a uniform benchmarking\nstrategy, which allows us to perform a fair and comparative analysis across all\ndesigns under various replacement policies. Likewise, with respect to security\nagainst cache occupancy attacks, we evaluate the cache designs against various\nthreat assumptions: (1) covert channels, (2) process fingerprinting, and (3)\nAES key recovery (to the best of our knowledge, this work is the first to\ndemonstrate full AES key recovery on a randomized cache design using cache\noccupancy attack). Our results establish the need to also consider cache\noccupancy side-channel in randomized cache design considerations."
                },
                "authors": [
                    {
                        "name": "Anirban Chakraborty"
                    },
                    {
                        "name": "Nimish Mishra"
                    },
                    {
                        "name": "Sayandeep Saha"
                    },
                    {
                        "name": "Sarani Bhattacharya"
                    },
                    {
                        "name": "Debdeep Mukhopadhyay"
                    }
                ],
                "author_detail": {
                    "name": "Debdeep Mukhopadhyay"
                },
                "author": "Debdeep Mukhopadhyay",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05172v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05172v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v5",
                "updated": "2025-01-29T16:44:27Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    16,
                    44,
                    27,
                    2,
                    29,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04437v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04437v3",
                "updated": "2025-01-29T04:10:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    29,
                    4,
                    10,
                    41,
                    2,
                    29,
                    0
                ],
                "published": "2024-05-07T16:00:32Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    16,
                    0,
                    32,
                    1,
                    128,
                    0
                ],
                "title": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "vAttention: Dynamic Memory Management for Serving LLMs without\n  PagedAttention"
                },
                "summary": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PagedAttention is a popular approach for dynamic memory allocation in LLM\nserving systems. It enables on-demand allocation of GPU memory to mitigate KV\ncache fragmentation -- a phenomenon that crippled the batch size (and\nconsequently throughput) in prior systems. However, in trying to allocate\nphysical memory at runtime, PagedAttention ends up changing the virtual memory\nlayout of the KV cache from contiguous to non-contiguous. Such a design leads\nto non-trivial programming and performance overheads.\n  We present vAttention -- an approach that mitigates fragmentation in physical\nmemory while retaining the contiguity of KV cache in virtual memory. We achieve\nthis by decoupling the allocation of virtual and physical memory using CUDA\nvirtual memory management APIs. We also introduce various LLM-specific\noptimizations to address the limitations of CUDA virtual memory support.\nOverall, vAttention is a simpler, portable, and performant alternative to\nPagedAttention: it supports various attention kernels out-of-the-box and\nimproves LLM serving throughput by up to 1.23x compared to the use of\nPagedAttention-based kernels of FlashAttention and FlashInfer."
                },
                "authors": [
                    {
                        "name": "Ramya Prabhu"
                    },
                    {
                        "name": "Ajay Nayak"
                    },
                    {
                        "name": "Jayashree Mohan"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Ashish Panwar"
                    }
                ],
                "author_detail": {
                    "name": "Ashish Panwar"
                },
                "author": "Ashish Panwar",
                "arxiv_comment": "To appear in ASPLOS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04437v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04437v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14770v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14770v2",
                "updated": "2025-01-28T20:35:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    35,
                    23,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:37:18Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    37,
                    18,
                    6,
                    364,
                    0
                ],
                "title": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing SSD Caches for Cloud Block Storage Systems Using Machine\n  Learning Approaches"
                },
                "summary": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing demand for efficient cloud storage solutions has led to the\nwidespread adoption of Solid-State Drives (SSDs) for caching in cloud block\nstorage systems. The management of data writes to SSD caches plays a crucial\nrole in improving overall system performance, reducing latency, and extending\nthe lifespan of storage devices. A critical challenge arises from the large\nvolume of write-only data, which significantly impacts the performance of SSD\ncaches when handled inefficiently. Specifically, writes that have not been read\nfor a certain period may introduce unnecessary write traffic to the SSD cache\nwithout offering substantial benefits for cache performance. This paper\nproposes a novel approach to mitigate this issue by leveraging machine learning\ntechniques to dynamically optimize the write policy in cloud-based storage\nsystems. The proposed method identifies write-only data and selectively filters\nit out in real-time, thereby minimizing the number of unnecessary write\noperations and improving the overall performance of the cache system.\nExperimental results demonstrate that the proposed machine learning-based\npolicy significantly outperforms traditional approaches by reducing the number\nof harmful writes and optimizing cache utilization. This solution is\nparticularly suitable for cloud environments with varying and unpredictable\nworkloads, where traditional cache management strategies often fall short."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing citations in Sections 2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14770v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14770v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14771v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14771v2",
                "updated": "2025-01-28T20:33:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    20,
                    33,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2024-12-29T17:39:37Z",
                "published_parsed": [
                    2024,
                    12,
                    29,
                    17,
                    39,
                    37,
                    6,
                    364,
                    0
                ],
                "title": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Adaptation in Data Storage: Real-Time Machine Learning for\n  Enhanced Prefetching"
                },
                "summary": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The exponential growth of data storage demands has necessitated the evolution\nof hierarchical storage management strategies [1]. This study explores the\napplication of streaming machine learning [3] to revolutionize data prefetching\nwithin multi-tiered storage systems. Unlike traditional batch-trained models,\nstreaming machine learning [5] offers adaptability, real-time insights, and\ncomputational efficiency, responding dynamically to workload variations. This\nwork designs and validates an innovative framework that integrates streaming\nclassification models for predicting file access patterns, specifically the\nnext file offset. Leveraging comprehensive feature engineering and real-time\nevaluation over extensive production traces, the proposed methodology achieves\nsubstantial improvements in prediction accuracy, memory efficiency, and system\nadaptability. The results underscore the potential of streaming models in\nreal-time storage management, setting a precedent for advanced caching and\ntiering strategies."
                },
                "authors": [
                    {
                        "name": "Chiyu Cheng"
                    },
                    {
                        "name": "Chang Zhou"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Jin Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Cao"
                },
                "author": "Jin Cao",
                "arxiv_comment": "I uploaded the paper without obtaining consent from all the authors.\n  One of the authors now refuses to publish this paper, as it has been\n  demonstrated to be unreliable, contains significant flaws in prior research,\n  and is missing proper citations in Sections 2 and 3",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14771v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14771v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17123v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17123v1",
                "updated": "2025-01-28T18:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T18:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    18,
                    14,
                    43,
                    1,
                    28,
                    0
                ],
                "title": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hybrid Deep Learning Model for Multiple Cache Side Channel Attacks\n  Detection: A Comparative Analysis"
                },
                "summary": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache side channel attacks are a sophisticated and persistent threat that\nexploit vulnerabilities in modern processors to extract sensitive information.\nThese attacks leverage weaknesses in shared computational resources,\nparticularly the last level cache, to infer patterns in data access and\nexecution flows, often bypassing traditional security defenses. Such attacks\nare especially dangerous as they can be executed remotely without requiring\nphysical access to the victim's device. This study focuses on a specific class\nof these threats: fingerprinting attacks, where an adversary monitors and\nanalyzes the behavior of co-located processes via cache side channels. This can\npotentially reveal confidential information, such as encryption keys or user\nactivity patterns. A comprehensive threat model illustrates how attackers\nsharing computational resources with target systems exploit these side channels\nto compromise sensitive data. To mitigate such risks, a hybrid deep learning\nmodel is proposed for detecting cache side channel attacks. Its performance is\ncompared with five widely used deep learning models: Multi-Layer Perceptron,\nConvolutional Neural Network, Simple Recurrent Neural Network, Long Short-Term\nMemory, and Gated Recurrent Unit. The experimental results demonstrate that the\nhybrid model achieves a detection rate of up to 99.96%. These findings\nhighlight the limitations of existing models, the need for enhanced defensive\nmechanisms, and directions for future research to secure sensitive data against\nevolving side channel threats."
                },
                "authors": [
                    {
                        "name": "Tejal Joshi"
                    },
                    {
                        "name": "Aarya Kawalay"
                    },
                    {
                        "name": "Anvi Jamkhande"
                    },
                    {
                        "name": "Amit Joshi"
                    }
                ],
                "author_detail": {
                    "name": "Amit Joshi"
                },
                "author": "Amit Joshi",
                "arxiv_comment": "8 pages, 4 figures. Accepted in IEEE's 2nd International Conference\n  on Computational Intelligence and Network Systems",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17123v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17123v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10854v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10854v2",
                "updated": "2025-01-28T16:19:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    16,
                    19,
                    24,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-18T19:10:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    19,
                    10,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Achievable DoF Bounds for Cache-Aided Asymmetric MIMO Communications"
                },
                "summary": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating coded caching (CC) into multiple-input multiple-output (MIMO)\ncommunications can significantly enhance the achievable degrees of freedom\n(DoF) in wireless networks. This paper investigates a practical cache-aided\nasymmetric MIMO configuration with cache ratio $\\gamma$, where a server\nequipped with $L$ transmit antennas communicates with $K$ users, each having\n$G_k$ receive antennas. We propose three content-aware MIMO-CC strategies: the\n\\emph{min-G} scheme, which treats the system as symmetric by assuming all users\nhave the same number of antennas, equal to the smallest among them; the\n\\emph{Grouping} scheme, which maximizes spatial multiplexing gain separately\nwithin each user subset at the cost of some global caching gain; and the\n\\emph{Phantom} scheme, which dynamically redistributes spatial resources using\nvirtual or ``phantom'' antennas at the users, bridging the performance gains of\nthe min-$G$ and Grouping schemes. These strategies jointly optimize the number\nof users, $\\Omega$, and the parallel streams decoded by each user, $\\beta_k$,\nensuring linear decodability for all target users. Analytical and numerical\nresults confirm that the proposed schemes achieve significant DoF improvements\nacross various system configurations."
                },
                "authors": [
                    {
                        "name": "Mohammad NaseriTehrani"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10854v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10854v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16909v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16909v1",
                "updated": "2025-01-28T12:57:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T12:57:53Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    12,
                    57,
                    53,
                    1,
                    28,
                    0
                ],
                "title": "Measuring GPU utilization one level deeper",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring GPU utilization one level deeper"
                },
                "summary": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU hardware is vastly underutilized. Even resource-intensive AI applications\nhave diverse resource profiles that often leave parts of GPUs idle. While\ncolocating applications can improve utilization, current spatial sharing\nsystems lack performance guarantees. Providing predictable performance\nguarantees requires a deep understanding of how applications contend for shared\nGPU resources such as block schedulers, compute units, L1/L2 caches, and memory\nbandwidth. We propose a methodology to profile resource interference of GPU\nkernels across these dimensions and discuss how to build GPU schedulers that\nprovide strict performance guarantees while colocating applications to minimize\ncost."
                },
                "authors": [
                    {
                        "name": "Paul Elvinger"
                    },
                    {
                        "name": "Foteini Strati"
                    },
                    {
                        "name": "Natalie Enright Jerger"
                    },
                    {
                        "name": "Ana Klimovic"
                    }
                ],
                "author_detail": {
                    "name": "Ana Klimovic"
                },
                "author": "Ana Klimovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16909v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16909v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16597v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16597v1",
                "updated": "2025-01-28T00:22:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "published": "2025-01-28T00:22:34Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    22,
                    34,
                    1,
                    28,
                    0
                ],
                "title": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing Smart Helper Placement for Enhanced Cache Efficiency in\n  F-RANs"
                },
                "summary": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart helpers (SHs) have been proposed to improve content delivery delays and\nalleviate high fronthaul loads in fog radio access networks (F-RANs). They\noffer an alternative to deploying additional enhanced remote radio heads\n(RRHs), which are often infeasible due to site constraints.} The optimal\nplacement of SHs can significantly increase the number of users they serve\nwhich leads to enhanced cache efficiency and improved content delivery delay.\nIn this letter, we optimize SH placement within an F-RAN to maximize the cache\nhit rate and further reduce the content delivery latency. We model the SH cache\nhit rate as a function of outage probability and user density distribution. We\ndevelop a function to estimate user density distribution leveraging the radial\nbasis functions (RBFs) method and optimize SH placement utilizing the particle\nswarm optimization (PSO) algorithm. \\an{Our} numerical results confirm the\neffectiveness of the proposed approach in maximizing the \\an{SH cache hit\nrate}, thereby improving delivery delays and fronthaul loads of the network."
                },
                "authors": [
                    {
                        "name": "Hesameddin Mokhtarzadeh"
                    },
                    {
                        "name": "Mohammed Saif"
                    },
                    {
                        "name": "Md. Jahangir Hossain"
                    },
                    {
                        "name": "Julian Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Julian Cheng"
                },
                "author": "Julian Cheng",
                "arxiv_comment": "5 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16597v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16597v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16535v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16535v1",
                "updated": "2025-01-27T22:14:43Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T22:14:43Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    22,
                    14,
                    43,
                    0,
                    27,
                    0
                ],
                "title": "Latency Guarantees for Caching with Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency Guarantees for Caching with Delayed Hits"
                },
                "summary": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the classical caching problem, when a requested page is not present in the\ncache (i.e., a \"miss\"), it is assumed to travel from the backing store into the\ncache \"before\" the next request arrives. However, in many real-life\napplications, such as content delivery networks, this assumption is\nunrealistic.\n  The \"delayed-hits\" model for caching, introduced by Atre, Sherry, Wang, and\nBerger, accounts for the latency between a missed cache request and the\ncorresponding arrival from the backing store. This theoretical model has two\nparameters: the \"delay\" $Z$, representing the ratio between the retrieval delay\nand the inter-request delay in an application, and the \"cache size\" $k$, as in\nclassical caching. Classical caching corresponds to $Z=1$, whereas larger\nvalues of $Z$ model applications where retrieving missed requests is expensive.\nDespite the practical relevance of the delayed-hits model, its theoretical\nunderpinnings are still poorly understood.\n  We present the first tight theoretical guarantee for optimizing delayed-hits\ncaching: The \"Least Recently Used\" algorithm, a natural, deterministic, online\nalgorithm widely used in practice, is $O(Zk)$-competitive, meaning it incurs at\nmost $O(Zk)$ times more latency than the (offline) optimal schedule. Our result\nextends to any so-called \"marking\" algorithm."
                },
                "authors": [
                    {
                        "name": "Keerthana Gurushankar"
                    },
                    {
                        "name": "Noah G. Singer"
                    },
                    {
                        "name": "Bernardo Subercaseaux"
                    }
                ],
                "author_detail": {
                    "name": "Bernardo Subercaseaux"
                },
                "author": "Bernardo Subercaseaux",
                "arxiv_comment": "Accepted at INFOCOM2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16535v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16535v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16245v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16245v1",
                "updated": "2025-01-27T17:42:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T17:42:20Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    17,
                    42,
                    20,
                    0,
                    27,
                    0
                ],
                "title": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SP-IMPact: A Framework for Static Partitioning Interference Mitigation\n  and Performance Analysis"
                },
                "summary": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedded systems are evolving toward complex, heterogeneous\narchitectures to accommodate increasingly demanding applications. Driven by\nSWAP-C constraints, this shift has led to consolidating multiple systems onto\nsingle hardware platforms. Static Partitioning Hypervisors offer a promising\nsolution to partition hardware resources and provide spatial isolation between\ncritical workloads. However, shared resources like the Last-Level Cache and\nsystem bus can introduce temporal interference between virtual machines (VMs),\nnegatively impacting performance and predictability. Over the past decade,\nacademia and industry have developed interference mitigation techniques, such\nas cache partitioning and memory bandwidth reservation. However, configuring\nthese techniques is complex and time-consuming. Cache partitioning requires\nbalancing cache sections across VMs, while memory bandwidth reservation needs\ntuning bandwidth budgets and periods. Testing all configurations is impractical\nand often leads to suboptimal results. Moreover, understanding how these\ntechniques interact is limited, as their combined use can produce compounded or\nconflicting effects on performance. Static analysis tools estimating worst-case\nexecution times offer guidance for configuring mitigation techniques but often\nfail to capture the complexity of modern multi-core systems. They typically\nfocus on limited shared resources while neglecting others, such as IOMMUs and\ninterrupt controllers. To address these challenges, we present SP-IMPact, an\nopen-source framework for analyzing and guiding interference mitigation\nconfigurations. SP-IMPact supports (i) cache coloring and (ii) memory bandwidth\nreservation, while evaluating their interactions and cumulative impact. By\nproviding insights on real hardware, SP-IMPact helps optimize configurations\nfor mixed-criticality systems, ensuring performance and predictability."
                },
                "authors": [
                    {
                        "name": "Diogo Costa"
                    },
                    {
                        "name": "Gonçalo Moreira"
                    },
                    {
                        "name": "Afonso Oliveira"
                    },
                    {
                        "name": "José Martins"
                    },
                    {
                        "name": "Sandro Pinto"
                    }
                ],
                "author_detail": {
                    "name": "Sandro Pinto"
                },
                "author": "Sandro Pinto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16245v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16245v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.00080v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.00080v4",
                "updated": "2025-01-27T14:55:40Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    55,
                    40,
                    0,
                    27,
                    0
                ],
                "published": "2024-04-30T16:35:08Z",
                "published_parsed": [
                    2024,
                    4,
                    30,
                    16,
                    35,
                    8,
                    1,
                    121,
                    0
                ],
                "title": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recommenadation aided Caching using Combinatorial Multi-armed Bandits"
                },
                "summary": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study content caching with recommendations in a wireless network where the\nusers are connected through a base station equipped with a finite-capacity\ncache. We assume a fixed set of contents with unknown user preferences and\ncontent popularities. The base station can cache a subset of the contents and\ncan also recommend subsets of the contents to different users in order to\nencourage them to request the recommended contents. Recommendations, depending\non their acceptability, can thus be used to increase cache hits. We first\nassume that the users' recommendation acceptabilities are known and formulate\nthe cache hit optimization problem as a combinatorial multi-armed bandit\n(CMAB). We propose a UCB-based algorithm to decide which contents to cache and\nrecommend and provide an upper bound on the regret of this algorithm.\nSubsequently, we consider a more general scenario where the users'\nrecommendation acceptabilities are also unknown and propose another UCB-based\nalgorithm that learns these as well. We numerically demonstrate the performance\nof our algorithms and compare these to state-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Pavamana K J"
                    },
                    {
                        "name": "Chandramani Kishore Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Kishore Singh"
                },
                "author": "Chandramani Kishore Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.00080v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.00080v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11126v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11126v2",
                "updated": "2025-01-27T14:37:24Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    14,
                    37,
                    24,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-19T17:33:28Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    17,
                    33,
                    28,
                    6,
                    19,
                    0
                ],
                "title": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SIC-free Multicast Scheduling for Multi-antenna Coded Caching"
                },
                "summary": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-antenna coded caching (CC) with multicast beamforming typically relies\non a complex successive interference cancellation (SIC) structure to decode a\nsuperposition of multiple streams received by each user. Signal-level CC\nschemes require the regeneration and cancellation of interfering signals at the\nphysical layer of each receiver, which complicates practical implementations.\nTo address this, we propose a bit-level multicast scheduling scheme enabling\nlinear, SIC-free decoding of parallel streams by repeatedly transmitting data\nterms with linearly independent coefficients. Two reference strategies and a\nnovel sparse strategy are considered for constructing the coefficient matrix.\nThe reference cases include the random strategy, which lacks control over\nmatrix construction, and the equal-distant strategy, which balances users'\ninterference and data terms equally. In contrast, the sparse strategy minimizes\nthe number of multicast streams transmitted in parallel during each interval.\nThis approach simplifies both the decoding process and the beamforming design\nby decoupling the desired data terms for each user and reducing the number of\nSINR constraints, respectively. To further enhance the symmetric rate, a\nsuccessive projection algorithm is applied to exploit channel properties and\noptimize user ordering. With the coefficient matrix and optimized user ordering\nin place, multicast beamformers are devised to aggregate desired data from\nrelevant multicast streams. Numerical simulations validate the effectiveness of\nthe sparse strategy and user scheduling, demonstrating significant gains in\nsymmetric rate."
                },
                "authors": [
                    {
                        "name": "MohammadJavad Sojdeh"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Antti Tölli"
                    }
                ],
                "author_detail": {
                    "name": "Antti Tölli"
                },
                "author": "Antti Tölli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11126v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11126v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16055v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16055v1",
                "updated": "2025-01-27T13:53:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T13:53:12Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    53,
                    12,
                    0,
                    27,
                    0
                ],
                "title": "Random Reshuffling for Stochastic Gradient Langevin Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Random Reshuffling for Stochastic Gradient Langevin Dynamics"
                },
                "summary": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine the use of different randomisation policies for stochastic\ngradient algorithms used in sampling, based on first-order (or overdamped)\nLangevin dynamics, the most popular of which is known as Stochastic Gradient\nLangevin Dynamics. Conventionally, this algorithm is combined with a specific\nstochastic gradient strategy, called Robbins-Monro. In this work, we study an\nalternative strategy, Random Reshuffling, and show convincingly that it leads\nto improved performance via: a) a proof of reduced bias in the Wasserstein\nmetric for strongly convex, gradient Lipschitz potentials; b) an analytical\ndemonstration of reduced bias for a Gaussian model problem; and c) an empirical\ndemonstration of reduced bias in numerical experiments for some logistic\nregression problems. This is especially important since Random Reshuffling is\ntypically more efficient due to memory access and cache reasons. Such\nacceleration for the Random Reshuffling policy is familiar from the\noptimisation literature on stochastic gradient descent."
                },
                "authors": [
                    {
                        "name": "Luke Shaw"
                    },
                    {
                        "name": "Peter A. Whalley"
                    }
                ],
                "author_detail": {
                    "name": "Peter A. Whalley"
                },
                "author": "Peter A. Whalley",
                "arxiv_comment": "23 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16055v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16055v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65C05, 82C31, 62F15",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05265v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05265v2",
                "updated": "2025-01-27T13:39:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    13,
                    39,
                    25,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-07T17:59:35Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    17,
                    59,
                    35,
                    0,
                    281,
                    0
                ],
                "title": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefixQuant: Eliminating Outliers by Prefixed Tokens for Large Language\n  Models Quantization"
                },
                "summary": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing weight-activation quantization methods for Large Language Models\n(LLMs) primarily address channel-wise outliers but often neglect token-wise\noutliers, which limits the accuracy of quantized models. In this work, we\npropose PrefixQuant, a novel quantization method that achieves state-of-the-art\nperformance across various precision levels (W4A4KV4 and W4A8KV4) and\ngranularities (dynamic and static quantization) by effectively isolating\ntoken-wise outliers. First, PrefixQuant eliminates token-wise outliers by\nprefixing outlier tokens in the KV cache, a process that is training-free and\nhighly efficient (e.g., 1 minutes for Llama-3-70B). Second, PrefixQuant\nintroduces new trainable parameters for block-wise training to compensate for\nquantization error. Our experiments show that PrefixQuant significantly\noutperforms existing dynamic quantization methods, even under coarser static\nquantization settings. For instance, PrefixQuant achieves an average accuracy\nimprovement of +3.08 and +2.85 points over SpinQuant (dynamic quantization) on\nfive zero-shot reasoning tasks under dynamic and static quantization settings,\nrespectively, on W4A4KV4 Llama-3-8B. Additionally, we demonstrate up to 2.74x\nprefilling speedup and 2.16x decoding speedup for LLMs using W4A4 PrefixQuant.\nOur code is available at https://github.com/ChenMnZ/PrefixQuant."
                },
                "authors": [
                    {
                        "name": "Mengzhao Chen"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Yi Bin"
                    },
                    {
                        "name": "Wenqi Shao"
                    },
                    {
                        "name": "Ping Luo"
                    }
                ],
                "author_detail": {
                    "name": "Ping Luo"
                },
                "author": "Ping Luo",
                "arxiv_comment": "PrefixQuant improves quantization accuracy across various precision\n  and quantization settings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05265v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05265v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18627v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18627v3",
                "updated": "2025-01-27T06:47:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    6,
                    47,
                    20,
                    0,
                    27,
                    0
                ],
                "published": "2024-10-24T10:36:16Z",
                "published_parsed": [
                    2024,
                    10,
                    24,
                    10,
                    36,
                    16,
                    3,
                    298,
                    0
                ],
                "title": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Content Caching with Waiting Costs via Restless Multi-Armed\n  Bandits"
                },
                "summary": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a system with a local cache connected to a backend server and an\nend user population. A set of contents are stored at the the server where they\ncontinuously get updated. The local cache keeps copies, potentially stale, of a\nsubset of the contents. The users make content requests to the local cache\nwhich either can serve the local version if available or can fetch a fresh\nversion or can wait for additional requests before fetching and serving a fresh\nversion. Serving a stale version of a content incurs an age-of-version(AoV)\ndependent ageing cost, fetching it from the server incurs a fetching cost, and\nmaking a request wait incurs a per unit time waiting cost. We focus on the\noptimal actions subject to the cache capacity constraint at each decision\nepoch, aiming at minimizing the long term average cost. We pose the problem as\na Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based\npolicy which is known to be asymptotically optimal. We explicitly characterize\nthe Whittle indices. We numerically evaluate the proposed policy and also\ncompare it to a greedy policy. We show that it is close to the optimal policy\nand substantially outperforms the exising policies."
                },
                "authors": [
                    {
                        "name": "Ankita Koley"
                    },
                    {
                        "name": "Chandramani Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandramani Singh"
                },
                "author": "Chandramani Singh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18627v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18627v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15782v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15782v1",
                "updated": "2025-01-27T05:02:05Z",
                "updated_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "published": "2025-01-27T05:02:05Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    5,
                    2,
                    5,
                    0,
                    27,
                    0
                ],
                "title": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online Allocation with Multi-Class Arrivals: Group Fairness vs\n  Individual Welfare"
                },
                "summary": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce and study a multi-class online resource allocation problem with\ngroup fairness guarantees. The problem involves allocating a fixed amount of\nresources to a sequence of agents, each belonging to a specific group. The\nprimary objective is to ensure fairness across different groups in an online\nsetting. We focus on three fairness notions: one based on quantity and two\nbased on utility. To achieve fair allocations, we develop two threshold-based\nonline algorithms, proving their optimality under two fairness notions and\nnear-optimality for the more challenging one. Additionally, we demonstrate a\nfundamental trade-off between group fairness and individual welfare using a\nnovel representative function-based approach. To address this trade-off, we\npropose a set-aside multi-threshold algorithm that reserves a portion of the\nresource to ensure fairness across groups while utilizing the remaining\nresource to optimize efficiency under utility-based fairness notions. This\nalgorithm is proven to achieve the Pareto-optimal trade-off. We also\ndemonstrate that our problem can model a wide range of real-world applications,\nincluding network caching and cloud computing, and empirically evaluate our\nproposed algorithms in the network caching problem using real datasets."
                },
                "authors": [
                    {
                        "name": "Faraz Zargari"
                    },
                    {
                        "name": "Hossein Nekouyan Jazi"
                    },
                    {
                        "name": "Bo Sun"
                    },
                    {
                        "name": "Xiaoqi Tan"
                    }
                ],
                "author_detail": {
                    "name": "Xiaoqi Tan"
                },
                "author": "Xiaoqi Tan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15782v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15782v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15570v1",
                "updated": "2025-01-26T15:56:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T15:56:56Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    15,
                    56,
                    56,
                    6,
                    26,
                    0
                ],
                "title": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARWKV: Pretrain is not what we need, an RNN-Attention-Based Language\n  Model Born from Transformer"
                },
                "summary": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As is known, hybrid quadratic and subquadratic attention models in multi-head\narchitectures have surpassed both Transformer and Linear RNN models , with\nthese works primarily focusing on reducing KV complexity and improving\nefficiency. For further research on expressiveness, we introduce our series of\nmodels distilled from Qwen 2.5, based on pure native RWKV-7 attention, which\naims to make RNN more expressive and demonstrates state tracking ability beyond\ntransformers. We work with QRWK 32B based on RWKV-6 architecture, another\napproach that reduces the entire knowledge processing time to just 8 hours\nusing 16 AMD MI300X GPUs while maintaining Qwen 2.5's performance. In fact, the\ndistillation process can utilize any LLM, not just Qwen, and enables knowledge\ntransfer from larger LLMs to smaller ones with more fewer tokens. We will\nexplain the detailed process and share our insights on building more powerful\nfoundation models. Please note that this is an ongoing work that will be\nupdated continuously. The model checkpoints and source code are available at\n\\href{https://github.com/yynil/RWKVInside}{https://github.com/yynil/RWKVInside},\n\\href{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}{https://huggingface.co/RWKV-Red-Team/ARWKV-7B-Preview-0.1}."
                },
                "authors": [
                    {
                        "name": "Lin Yueyu"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Peter Yue"
                    },
                    {
                        "name": "Liu Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Liu Xiao"
                },
                "author": "Liu Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15481v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15481v1",
                "updated": "2025-01-26T11:01:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-26T11:01:10Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    11,
                    1,
                    10,
                    6,
                    26,
                    0
                ],
                "title": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Query-based versus resource-based cache strategies in tag-based browsing\n  systems"
                },
                "summary": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tag-based browsing is a popular interaction model for navigating digital\nlibraries. According to this model, users select descriptive tags to filter\nresources in the collections. Typical implementations of the model are based on\ninverted indexes. However, these implementations can require a considerable\namount of set operations to update the browsing state. To palliate this\ninconven-ience, it is possible to adopt suitable cache strategies. In this\npaper we describe and compare two of these strategies: (i) a query-based\nstrategy, according to which previously computed browsing states are indexed by\nsets of selected tags; and (ii) a resource-based strategy, according to which\nbrowsing states are in-dexed by sets of filtered resources. Our comparison\nfocused on runtime perfor-mance, and was carried out empirically, using a\nreal-world web-based collec-tion in the field of digital humanities. The\nresults obtained show that the re-source-based strategy clearly outperforms the\nquery-based one."
                },
                "authors": [
                    {
                        "name": "Joaquín Gayoso-Cabada"
                    },
                    {
                        "name": "Mercedes Gómez-Albarrán"
                    },
                    {
                        "name": "José-Luis Sierra"
                    }
                ],
                "author_detail": {
                    "name": "José-Luis Sierra"
                },
                "author": "José-Luis Sierra",
                "arxiv_doi": "10.1007/978-3-030-04257-8_4",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/978-3-030-04257-8_4",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.15481v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15481v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "camera-ready",
                "arxiv_journal_ref": "MATURITY AND INNOVATION IN DIGITAL LIBRARIES, ICADL 2018",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11550v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11550v4",
                "updated": "2025-01-26T07:29:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    7,
                    29,
                    6,
                    6,
                    26,
                    0
                ],
                "published": "2024-07-16T09:53:32Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    9,
                    53,
                    32,
                    1,
                    198,
                    0
                ],
                "title": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ada-KV: Optimizing KV Cache Eviction by Adaptive Budget Allocation for\n  Efficient LLM Inference"
                },
                "summary": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models have excelled in various domains but face efficiency\nchallenges due to the growing Key-Value (KV) cache required for long-sequence\ninference. Recent efforts aim to reduce KV cache size by evicting vast\nnon-critical cache elements during runtime while preserving generation quality.\nHowever, these methods typically allocate compression budgets uniformly across\nall attention heads, ignoring the unique attention patterns of each head. In\nthis paper, we establish a theoretical loss upper bound between pre- and\npost-eviction attention output, explaining the optimization target of prior\ncache eviction methods, while guiding the optimization of adaptive budget\nallocation. Base on this, we propose {\\it Ada-KV}, the first head-wise adaptive\nbudget allocation strategy. It offers plug-and-play benefits, enabling seamless\nintegration with prior cache eviction methods. Extensive evaluations on 13\ndatasets from Ruler and 16 datasets from LongBench, all conducted under both\nquestion-aware and question-agnostic scenarios, demonstrate substantial quality\nimprovements over existing methods."
                },
                "authors": [
                    {
                        "name": "Yuan Feng"
                    },
                    {
                        "name": "Junlin Lv"
                    },
                    {
                        "name": "Yukun Cao"
                    },
                    {
                        "name": "Xike Xie"
                    },
                    {
                        "name": "S. Kevin Zhou"
                    }
                ],
                "author_detail": {
                    "name": "S. Kevin Zhou"
                },
                "author": "S. Kevin Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11550v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11550v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v2",
                "updated": "2025-01-26T01:43:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    26,
                    1,
                    43,
                    46,
                    6,
                    26,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed MIMO Gaussian\nbroadcast channel. We propose a novel delivery scheme consisting of two phases:\npartitioning and transmission. In the partitioning phase, users with identical\ncache profiles are partitioned into the minimum number of sets, such that users\nwithin each set can successfully decode their desired message from a joint\ntransmission enabled by MIMO precoding. To optimally partition the users, we\nemploy the branch and bound method. In the transmission phase, each partition\nis treated as a single entity, and codewords are multicast to partitions with\ndistinct cache profiles. The proposed delivery scheme is applicable to any\npartially connected network, and while the partitioning is optimal, the overall\ndelivery scheme, including transmission, is heuristic. Interestingly,\nsimulation results show that its performance closely approximates that of the\nfully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15348v1",
                "updated": "2025-01-25T23:16:03Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T23:16:03Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    23,
                    16,
                    3,
                    5,
                    25,
                    0
                ],
                "title": "ReInc: Scaling Training of Dynamic Graph Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ReInc: Scaling Training of Dynamic Graph Neural Networks"
                },
                "summary": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Graph Neural Networks (DGNNs) have gained widespread attention due to\ntheir applicability in diverse domains such as traffic network prediction,\nepidemiological forecasting, and social network analysis. In this paper, we\npresent ReInc, a system designed to enable efficient and scalable training of\nDGNNs on large-scale graphs. ReInc introduces key innovations that capitalize\non the unique combination of Graph Neural Networks (GNNs) and Recurrent Neural\nNetworks (RNNs) inherent in DGNNs. By reusing intermediate results and\nincrementally computing aggregations across consecutive graph snapshots, ReInc\nsignificantly enhances computational efficiency. To support these\noptimizations, ReInc incorporates a novel two-level caching mechanism with a\nspecialized caching policy aligned to the DGNN execution workflow.\nAdditionally, ReInc addresses the challenges of managing structural and\ntemporal dependencies in dynamic graphs through a new distributed training\nstrategy. This approach eliminates communication overheads associated with\naccessing remote features and redistributing intermediate results. Experimental\nresults demonstrate that ReInc achieves up to an order of magnitude speedup\ncompared to state-of-the-art frameworks, tested across various dynamic GNN\narchitectures and real-world graph datasets."
                },
                "authors": [
                    {
                        "name": "Mingyu Guan"
                    },
                    {
                        "name": "Saumia Singhal"
                    },
                    {
                        "name": "Taesoo Kim"
                    },
                    {
                        "name": "Anand Padmanabha Iyer"
                    }
                ],
                "author_detail": {
                    "name": "Anand Padmanabha Iyer"
                },
                "author": "Anand Padmanabha Iyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09479v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09479v2",
                "updated": "2025-01-25T12:17:41Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    17,
                    41,
                    5,
                    25,
                    0
                ],
                "published": "2024-10-12T10:38:39Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    10,
                    38,
                    39,
                    5,
                    286,
                    0
                ],
                "title": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Viscoelastic Effects on the Hydrodynamics of an Active Compound Particle"
                },
                "summary": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the hydrodynamics of microswimmers in viscoelastic fluids and\nconfined environments is crucial for interpreting their behaviour in natural\nsettings and designing synthetic microswimmers for practical applications like\ncargo transport. In this study, we explore the hydrodynamics of a concentric\nactive compound particle - a model microswimmer (a squirmer) positioned at the\ncentre of a viscoelastic fluid droplet (a model cargo) suspended in another\nviscoelastic medium. We consider the Oldroyd-B constitutive model to\ncharacterize the fluids and employ a perturbative approach in the Deborah\nnumber to analyze viscoelastic effects analytically, assuming a small Capillary\nnumber so that the droplet remains spherical and does not deform. We examine\nthree cases: (i) a squirmer confined within a viscoelastic fluid droplet\nsuspended in a Newtonian fluid, (ii) a squirmer confined within a Newtonian\nfluid droplet suspended in a viscoelastic fluid, and (iii) a squirmer confined\nwithin a viscoelastic fluid droplet suspended in another viscoelastic fluid.\nOur findings reveal that the swimming speeds of the squirmer and the droplet\nare determined by the complex interplay of viscoelasticity, the size ratio of\nthe droplet to the squirmer (confinement strength), and the viscosity ratio of\nthe surrounding fluid to the droplet fluid. A critical aspect of this\ninteraction is the positioning of stagnation points within the fluid flow,\nwhich governs the distribution of polymeric stress. This distribution, in turn,\nplays a crucial role in determining the influence of viscoelasticity on the\nsquirmer's dynamics. Our analysis suggests that viscoelastic effects can either\nenhance or hinder the swimming speed of the squirmer when confined in a\ndroplet, depending on the specific configuration of the system."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Sumesh P. Thampi"
                    }
                ],
                "author_detail": {
                    "name": "Sumesh P. Thampi"
                },
                "author": "Sumesh P. Thampi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09479v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09479v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.flu-dyn",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.11828v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.11828v2",
                "updated": "2025-01-25T10:38:11Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    10,
                    38,
                    11,
                    5,
                    25,
                    0
                ],
                "published": "2024-12-16T14:49:32Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    14,
                    49,
                    32,
                    0,
                    351,
                    0
                ],
                "title": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Selection Problem in Multi-Query Optimization: a Comprehensive\n  Survey"
                },
                "summary": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "View materialization, index selection, and plan caching are well-known\ntechniques for optimization of query processing in database systems. The\nessence of these tasks is to select and save a subset of the most useful\ncandidates (views/indexes/plans) for reuse within given space/time budget\nconstraints. In this paper, we propose a unified view on these selection\nproblems. We make a detailed analysis of the root causes of their complexity\nand summarize techniques to address them. Our survey provides a modern\nclassification of selection algorithms known in the literature, including the\nlatest ones based on Machine Learning. We provide a ground for reuse of the\nselection techniques between different optimization scenarios and highlight\nchallenges and promising directions in the field. Based on our analysis we\nderive a method to exponentially accelerate some of the state-of-the-art\nselection algorithms."
                },
                "authors": [
                    {
                        "name": "Sergey Zinchenko"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    }
                ],
                "author_detail": {
                    "name": "Denis Ponomaryov"
                },
                "author": "Denis Ponomaryov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.11828v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.11828v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15126v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15126v1",
                "updated": "2025-01-25T08:27:26Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T08:27:26Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    8,
                    27,
                    26,
                    5,
                    25,
                    0
                ],
                "title": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fully-Automated Code Generation for Efficient Computation of Sparse\n  Matrix Permanents on GPUs"
                },
                "summary": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Registers are the fastest memory components within the GPU's complex memory\nhierarchy, accessed by names rather than addresses. They are managed entirely\nby the compiler through a process called register allocation, during which the\ncompiler attempts to cache predictable data from thread-local memory into\nthread-private registers. Computing the permanent of a sparse matrix poses a\nchallenge for compilers, as optimizing this process is hindered by the\nunpredictable distribution of nonzero elements, which only become known at\nruntime. In this work, we employ fully-automated code generation to address\nthis, producing highly optimized kernels tailored to the matrix's sparsity\npattern. State-of-the-art permanent computation algorithms require each thread\nto store a private array, denoted x, of size n. We first propose a technique\nthat fully stores these arrays in registers, with inclusion and exclusion\nkernels generated for each column. To minimize control divergence and reduce\nthe number of unique kernels within a warp, we exploit the internal structure\nof Gray codes, which are also used in the state-of-the-art algorithm. Our\nsecond technique reduces register pressure by utilizing both registers and\nglobal memory and introduces a matrix ordering and partitioning strategy for\ngreater efficiency. On synthetic matrices, this approach achieves a 31x speedup\nover state-of-the-art CPU implementations on 112 cores, and an 8x speedup\ncompared to our traditional GPU implementation. For real-world matrices, these\nspeedups are 24.9x and 4.9x."
                },
                "authors": [
                    {
                        "name": "Deniz Elbek"
                    },
                    {
                        "name": "Kamer Kaya"
                    }
                ],
                "author_detail": {
                    "name": "Kamer Kaya"
                },
                "author": "Kamer Kaya",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15126v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15126v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15113v1",
                "updated": "2025-01-25T07:28:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T07:28:13Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    7,
                    28,
                    13,
                    5,
                    25,
                    0
                ],
                "title": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Task-KV: Task-aware KV Cache Optimization via Semantic Differentiation\n  of Attention Heads"
                },
                "summary": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache is a widely used acceleration technique for large language models\n(LLMs) inference. However, its memory requirement grows rapidly with input\nlength. Previous studies have reduced the size of KV cache by either removing\nthe same number of unimportant tokens for all attention heads or by allocating\ndifferentiated KV cache budgets for pre-identified attention heads. However,\ndue to the importance of attention heads varies across different tasks, the\npre-identified attention heads fail to adapt effectively to various downstream\ntasks. To address this issue, we propose Task-KV, a method that leverages the\nsemantic differentiation of attention heads to allocate differentiated KV cache\nbudgets across various tasks. We demonstrate that attention heads far from the\nsemantic center (called heterogeneous heads) make an significant contribution\nto task outputs and semantic understanding. In contrast, other attention heads\nplay the role of aggregating important information and focusing reasoning.\nTask-KV allocates full KV cache budget to heterogeneous heads to preserve\ncomprehensive semantic information, while reserving a small number of recent\ntokens and attention sinks for non-heterogeneous heads. Furthermore, we\ninnovatively introduce middle activations to preserve key contextual\ninformation aggregated from non-heterogeneous heads. To dynamically perceive\nsemantic differences among attention heads, we design a semantic separator to\ndistinguish heterogeneous heads from non-heterogeneous ones based on their\ndistances from the semantic center. Experimental results on multiple benchmarks\nand different model architectures demonstrate that Task-KV significantly\noutperforms existing baseline methods."
                },
                "authors": [
                    {
                        "name": "Xingyang He"
                    },
                    {
                        "name": "Jie Liu"
                    },
                    {
                        "name": "Shaowei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Shaowei Chen"
                },
                "author": "Shaowei Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11855v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11855v2",
                "updated": "2025-01-25T04:21:57Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    4,
                    21,
                    57,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-21T03:13:21Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    3,
                    13,
                    21,
                    1,
                    21,
                    0
                ],
                "title": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A New Construction Structure on Coded Caching with Linear\n  Subpacketization: Non-Half-Sum Disjoint Packing"
                },
                "summary": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching is a promising technique to effectively reduce peak traffic by\nusing local caches and the multicast gains generated by these local caches. We\nprefer to design a coded caching scheme with the subpacketization $F$ and\ntransmission load $R$ as small as possible since these are the key metrics for\nevaluating the implementation complexity and transmission efficiency of the\nscheme, respectively. However, most of the existing coded caching schemes have\nlarge subpacketizations which grow exponentially with the number of users $K$,\nand there are a few schemes with linear subpacketizations which have large\ntransmission loads. In this paper, we focus on studying the linear\nsubpacketization, i.e., $K=F$, coded caching scheme with low transmission load.\nSpecifically, we first introduce a new combinatorial structure called\nnon-half-sum disjoint packing (NHSDP) which can be used to generate a coded\ncaching scheme with $K=F$. Then a class of new schemes is obtained by\nconstructing NHSDP. Theoretical and numerical comparisons show that (i)\ncompared to the existing schemes with linear subpacketization (to the number of\nusers), the proposed scheme achieves a lower load; (ii) compared to some\nexisting schemes with polynomial subpacketization, the proposed scheme can also\nachieve a lower load in some cases; (iii) compared to some existing schemes\nwith exponential subpacketization, the proposed scheme has loads close to those\nof these schemes in some cases. Moreover, the new concept of NHSDP is closely\nrelated to the classical combinatorial structures such as cyclic difference\npacking (CDP), non-three-term arithmetic progressions (NTAP), and perfect hash\nfamily (PHF). These connections indicate that NHSDP is an important\ncombinatorial structure in the field of combinatorial design."
                },
                "authors": [
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Huimei Wei"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11855v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11855v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15021v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15021v1",
                "updated": "2025-01-25T02:01:56Z",
                "updated_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "published": "2025-01-25T02:01:56Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    2,
                    1,
                    56,
                    5,
                    25,
                    0
                ],
                "title": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AKVQ-VL: Attention-Aware KV Cache Adaptive 2-Bit Quantization for\n  Vision-Language Models"
                },
                "summary": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs) show remarkable performance in multimodal\ntasks. However, excessively long multimodal inputs lead to oversized Key-Value\n(KV) caches, resulting in significant memory consumption and I/O bottlenecks.\nPrevious KV quantization methods for Large Language Models (LLMs) may alleviate\nthese issues but overlook the attention saliency differences of multimodal\ntokens, resulting in suboptimal performance. In this paper, we investigate the\nattention-aware token saliency patterns in VLM and propose AKVQ-VL. AKVQ-VL\nleverages the proposed Text-Salient Attention (TSA) and Pivot-Token-Salient\nAttention (PSA) patterns to adaptively allocate bit budgets. Moreover,\nachieving extremely low-bit quantization requires effectively addressing\noutliers in KV tensors. AKVQ-VL utilizes the Walsh-Hadamard transform (WHT) to\nconstruct outlier-free KV caches, thereby reducing quantization difficulty.\nEvaluations of 2-bit quantization on 12 long-context and multimodal tasks\ndemonstrate that AKVQ-VL maintains or even improves accuracy, outperforming\nLLM-oriented methods. AKVQ-VL can reduce peak memory usage by 2.13x, support up\nto 3.25x larger batch sizes and 2.46x throughput."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15021v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15021v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12689v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12689v2",
                "updated": "2025-01-24T19:13:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    19,
                    13,
                    12,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-22T07:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    7,
                    52,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EchoLM: Accelerating LLM Serving with Real-time Knowledge Distillation"
                },
                "summary": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have excelled in various applications, yet\nserving them at scale is challenging due to their substantial resource demands\nand high latency. Our real-world studies reveal that over 60% of user requests\nto LLMs have semantically similar counterparts, suggesting the potential for\nknowledge sharing among requests. However, naively caching and reusing past\nresponses leads to large quality degradation. In this paper, we introduce\nEchoLM, an in-context caching system that leverages historical requests as\nexamples to guide response generation, enabling selective offloading of\nrequests to more efficient LLMs. However, enabling this real-time knowledge\ntransfer leads to intricate tradeoffs between response quality, latency, and\nsystem throughput at scale. For a new request, EchoLM identifies similar,\nhigh-utility examples and efficiently prepends them to the input for better\nresponse. At scale, EchoLM adaptively routes requests to LLMs of varying\ncapabilities, accounting for response quality and serving loads. EchoLM employs\na cost-aware cache replay mechanism to improve example quality and coverage\noffline, maximizing cache utility and runtime efficiency. Evaluations on\nmillions of open-source requests demonstrate that EchoLM has a throughput\nimprovement of 1.4-5.9x while reducing latency by 28-71% without hurting\nresponse quality on average."
                },
                "authors": [
                    {
                        "name": "Yifan Yu"
                    },
                    {
                        "name": "Yu Gan"
                    },
                    {
                        "name": "Lillian Tsai"
                    },
                    {
                        "name": "Nikhil Sarda"
                    },
                    {
                        "name": "Jiaming Shen"
                    },
                    {
                        "name": "Yanqi Zhou"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    },
                    {
                        "name": "Fan Lai"
                    },
                    {
                        "name": "Henry M. Levy"
                    },
                    {
                        "name": "David Culler"
                    }
                ],
                "author_detail": {
                    "name": "David Culler"
                },
                "author": "David Culler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12689v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12689v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09398v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09398v2",
                "updated": "2025-01-24T15:16:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    16,
                    48,
                    4,
                    24,
                    0
                ],
                "published": "2024-09-14T10:15:37Z",
                "published_parsed": [
                    2024,
                    9,
                    14,
                    10,
                    15,
                    37,
                    5,
                    258,
                    0
                ],
                "title": "Language-Queried Target Sound Extraction Without Parallel Training Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-Queried Target Sound Extraction Without Parallel Training Data"
                },
                "summary": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language-queried target sound extraction (TSE) aims to extract specific\nsounds from mixtures based on language queries. Traditional fully-supervised\ntraining schemes require extensively annotated parallel audio-text data, which\nare labor-intensive. We introduce a parallel-data-free training scheme,\nrequiring only unlabelled audio clips for TSE model training by utilizing the\ncontrastive language-audio pre-trained model (CLAP). In a vanilla\nparallel-data-free training stage, target audio is encoded using the\npre-trained CLAP audio encoder to form a condition embedding, while during\ntesting, user language queries are encoded by CLAP text encoder as the\ncondition embedding. This vanilla approach assumes perfect alignment between\ntext and audio embeddings, which is unrealistic. Two major challenges arise\nfrom training-testing mismatch: the persistent modality gap between text and\naudio and the risk of overfitting due to the exposure of rich acoustic details\nin target audio embedding during training. To address this, we propose a\nretrieval-augmented strategy. Specifically, we create an embedding cache using\naudio captions generated by a large language model (LLM). During training,\ntarget audio embeddings retrieve text embeddings from this cache to use as\ncondition embeddings, ensuring consistent modalities between training and\ntesting and eliminating information leakage. Extensive experiment results show\nthat our retrieval-augmented approach achieves consistent and notable\nperformance improvements over existing state-of-the-art with better\ngeneralizability."
                },
                "authors": [
                    {
                        "name": "Hao Ma"
                    },
                    {
                        "name": "Zhiyuan Peng"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Yukai Li"
                    },
                    {
                        "name": "Mingjie Shao"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Ju Liu"
                    }
                ],
                "author_detail": {
                    "name": "Ju Liu"
                },
                "author": "Ju Liu",
                "arxiv_comment": "Accepted by ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09398v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09398v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.16300v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.16300v2",
                "updated": "2025-01-24T14:32:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    14,
                    32,
                    34,
                    4,
                    24,
                    0
                ],
                "published": "2024-07-23T08:55:10Z",
                "published_parsed": [
                    2024,
                    7,
                    23,
                    8,
                    55,
                    10,
                    1,
                    205,
                    0
                ],
                "title": "A Programming Model for Disaggregated Memory over CXL",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Programming Model for Disaggregated Memory over CXL"
                },
                "summary": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CXL (Compute Express Link) is an emerging open industry-standard interconnect\nbetween processing and memory devices that is expected to revolutionize the way\nsystems are designed in the near future. It enables cache-coherent shared\nmemory pools in a disaggregated fashion at unprecedented scales, allowing\nalgorithms to interact with a variety of storage devices using simple loads and\nstores. Alongside unleashing unique opportunities for a wide range of\napplications, CXL introduces new challenges of data management and crash\nconsistency. Alas, CXL lacks an adequate programming model, which makes\nreasoning about the correctness and expected behaviors of algorithms and\nsystems on top of it nearly impossible.\n  In this work, we present CXL0, the first programming model for concurrent\nprograms running on top of CXL. We propose a high-level abstraction for CXL\nmemory accesses and formally define operational semantics on top of that\nabstraction. We perform initial measurements that provide practical insight\ninto CXL0. We provide a set of general transformations that adapt concurrent\nalgorithms to the new disruptive technology. These transformations enhance\nlinearizable algorithms with durability under a general partial-failure model.\nWe provide an additional transformation for algorithms designed for persistent\nmain memory and full-system crashes. We believe that this work will serve as a\nstepping stone for systems design and modeling on top of CXL, and support the\ndevelopment of future models as software and hardware evolve."
                },
                "authors": [
                    {
                        "name": "Gal Assa"
                    },
                    {
                        "name": "Lucas Bürgi"
                    },
                    {
                        "name": "Michal Friedman"
                    },
                    {
                        "name": "Ori Lahav"
                    }
                ],
                "author_detail": {
                    "name": "Ori Lahav"
                },
                "author": "Ori Lahav",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.16300v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.16300v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14387v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14387v1",
                "updated": "2025-01-24T10:39:45Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:39:45Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    39,
                    45,
                    4,
                    24,
                    0
                ],
                "title": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Application-Aware Resource Allocation and Data Management for\n  MEC-assisted IoT Service Providers"
                },
                "summary": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To support the growing demand for data-intensive and low-latency IoT\napplications, Multi-Access Edge Computing (MEC) is emerging as an effective\nedge-computing approach enabling the execution of delay-sensitive processing\ntasks close to end-users. However, most of the existing works on resource\nallocation and service placement in MEC systems overlook the unique\ncharacteristics of new IoT use cases. For instance, many IoT applications\nrequire the periodic execution of computing tasks on real-time data streams\nthat originate from devices dispersed over a wide area. Thus, users requesting\nIoT services are typically distant from the data producers. To fill this gap,\nthe contribution of this work is two-fold. Firstly, we propose a MEC-compliant\narchitectural solution to support the operation of multiple IoT service\nproviders over a common MEC platform deployment, which enables the steering and\nshaping of IoT data transport within the platform. Secondly, we model the\nproblem of service placement and data management in the proposed MEC-based\nsolution taking into account the dependencies at the data level between IoT\nservices and sensing resources. Our model also considers that caches can be\ndeployed on MEC hosts, to allow the sharing of the same data between different\nIoT services with overlapping geographical scope, and provides support for IoT\nservices with heterogeneous QoS requirements, such as different frequencies of\nperiodic task execution. Due to the complexity of the optimisation problem, a\nheuristic algorithm is proposed using linear relaxation and rounding\ntechniques. Extensive simulation results demonstrate the efficiency of the\nproposed approach, especially when traffic demands generated by the service\nrequests are not uniform."
                },
                "authors": [
                    {
                        "name": "Simone Bolettieri"
                    },
                    {
                        "name": "Raffaele Bruno"
                    },
                    {
                        "name": "Enzo Mingozzi"
                    }
                ],
                "author_detail": {
                    "name": "Enzo Mingozzi"
                },
                "author": "Enzo Mingozzi",
                "arxiv_doi": "10.1016/j.jnca.2021.103020",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.jnca.2021.103020",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.14387v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14387v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Journal of Network and Computer Applications, Volume 181, 1 May\n  2021, 103020",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14367v1",
                "updated": "2025-01-24T10:00:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T10:00:21Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    10,
                    0,
                    21,
                    4,
                    24,
                    0
                ],
                "title": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint System Latency and Data Freshness Optimization for Cache-enabled\n  Mobile Crowdsensing Networks"
                },
                "summary": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile crowdsensing (MCS) networks enable large-scale data collection by\nleveraging the ubiquity of mobile devices. However, frequent sensing and data\ntransmission can lead to significant resource consumption. To mitigate this\nissue, edge caching has been proposed as a solution for storing recently\ncollected data. Nonetheless, this approach may compromise data freshness. In\nthis paper, we investigate the trade-off between re-using cached task results\nand re-sensing tasks in cache-enabled MCS networks, aiming to minimize system\nlatency while maintaining information freshness. To this end, we formulate a\nweighted delay and age of information (AoI) minimization problem, jointly\noptimizing sensing decisions, user selection, channel selection, task\nallocation, and caching strategies. The problem is a mixed-integer non-convex\nprogramming problem which is intractable. Therefore, we decompose the long-term\nproblem into sequential one-shot sub-problems and design a framework that\noptimizes system latency, task sensing decision, and caching strategy\nsubproblems. When one task is re-sensing, the one-shot problem simplifies to\nthe system latency minimization problem, which can be solved optimally. The\ntask sensing decision is then made by comparing the system latency and AoI.\nAdditionally, a Bayesian update strategy is developed to manage the cached task\nresults. Building upon this framework, we propose a lightweight and\ntime-efficient algorithm that makes real-time decisions for the long-term\noptimization problem. Extensive simulation results validate the effectiveness\nof our approach."
                },
                "authors": [
                    {
                        "name": "Kexin Shi"
                    },
                    {
                        "name": "Yaru Fu"
                    },
                    {
                        "name": "Yongna Guo"
                    },
                    {
                        "name": "Fu Lee Wang"
                    },
                    {
                        "name": "Yan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yan Zhang"
                },
                "author": "Yan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14312v1",
                "updated": "2025-01-24T08:12:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T08:12:47Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    8,
                    12,
                    47,
                    4,
                    24,
                    0
                ],
                "title": "Locality-aware Fair Scheduling in LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locality-aware Fair Scheduling in LLM Serving"
                },
                "summary": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language model (LLM) inference workload dominates a wide variety of\nmodern AI applications, ranging from multi-turn conversation to document\nanalysis. Balancing fairness and efficiency is critical for managing diverse\nclient workloads with varying prefix patterns. Unfortunately, existing fair\nscheduling algorithms for LLM serving, such as Virtual Token Counter (VTC),\nfail to take prefix locality into consideration and thus suffer from poor\nperformance. On the other hand, locality-aware scheduling algorithms in\nexisting LLM serving frameworks tend to maximize the prefix cache hit rate\nwithout considering fair sharing among clients.\n  This paper introduces the first locality-aware fair scheduling algorithm,\nDeficit Longest Prefix Match (DLPM), which can maintain a high degree of prefix\nlocality with a fairness guarantee. We also introduce a novel algorithm, Double\nDeficit LPM (D$^2$LPM), extending DLPM for the distributed setup that can find\na balance point among fairness, locality, and load-balancing. Our extensive\nevaluation demonstrates the superior performance of DLPM and D$^2$LPM in\nensuring fairness while maintaining high throughput (up to 2.87$\\times$ higher\nthan VTC) and low per-client (up to 7.18$\\times$ lower than state-of-the-art\ndistributed LLM serving system) latency."
                },
                "authors": [
                    {
                        "name": "Shiyi Cao"
                    },
                    {
                        "name": "Yichuan Wang"
                    },
                    {
                        "name": "Ziming Mao"
                    },
                    {
                        "name": "Pin-Lun Hsu"
                    },
                    {
                        "name": "Liangsheng Yin"
                    },
                    {
                        "name": "Tian Xia"
                    },
                    {
                        "name": "Dacheng Li"
                    },
                    {
                        "name": "Shu Liu"
                    },
                    {
                        "name": "Yineng Zhang"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Ying Sheng"
                    },
                    {
                        "name": "Joseph Gonzalez"
                    },
                    {
                        "name": "Ion Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Ion Stoica"
                },
                "author": "Ion Stoica",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14205v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14205v1",
                "updated": "2025-01-24T03:21:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "published": "2025-01-24T03:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    3,
                    21,
                    20,
                    4,
                    24,
                    0
                ],
                "title": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving Long-Context LLMs at the Mobile Edge: Test-Time Reinforcement\n  Learning-based Model Caching and Inference Offloading"
                },
                "summary": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can perform zero-shot learning on unseen tasks\nand few-shot learning on complex reasoning tasks. However, resource-limited\nmobile edge networks struggle to support long-context LLM serving for LLM\nagents during multi-round interactions with users. Unlike stateless computation\noffloading and static service offloading in edge computing, optimizing LLM\nserving at edge servers is challenging because LLMs continuously learn from\ncontext which raises accuracy, latency, and resource consumption dynamics. In\nthis paper, we propose a joint model caching and inference offloading framework\nthat utilizes test-time deep reinforcement learning (T2DRL) to optimize\ndeployment and execution strategies for long-context LLM serving. In this\nframework, we analyze the performance convergence and design an optimization\nproblem considering the utilization of context windows in LLMs. Furthermore,\nthe T2DRL algorithm can learn in both the training phase and the testing phase\nto proactively manage cached models and service requests and adapt to context\nchanges and usage patterns during execution. To further enhance resource\nallocation efficiency, we propose a double Dutch auction (DDA) mechanism, which\ndynamically matches supply and demand while maximizing social welfare. Finally,\nexperimental results demonstrate that the T2DRL algorithm can reduce system\ncosts by at least 30% compared to baselines while guaranteeing the performance\nof LLM agents in real-world perception and reasoning tasks."
                },
                "authors": [
                    {
                        "name": "Minrui Xu"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Christopher G. Brinton"
                    }
                ],
                "author_detail": {
                    "name": "Christopher G. Brinton"
                },
                "author": "Christopher G. Brinton",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14205v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14205v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13629v1",
                "updated": "2025-01-23T12:58:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T12:58:14Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    12,
                    58,
                    14,
                    3,
                    23,
                    0
                ],
                "title": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sigma: Differential Rescaling of Query, Key and Value for Efficient\n  Language Models"
                },
                "summary": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce Sigma, an efficient large language model specialized for the\nsystem domain, empowered by a novel architecture including DiffQKV attention,\nand pre-trained on our meticulously collected system domain data. DiffQKV\nattention significantly enhances the inference efficiency of Sigma by\noptimizing the Query (Q), Key (K), and Value (V) components in the attention\nmechanism differentially, based on their varying impacts on the model\nperformance and efficiency indicators. Specifically, we (1) conduct extensive\nexperiments that demonstrate the model's varying sensitivity to the compression\nof K and V components, leading to the development of differentially compressed\nKV, and (2) propose augmented Q to expand the Q head dimension, which enhances\nthe model's representation capacity with minimal impacts on the inference\nspeed. Rigorous theoretical and empirical analyses reveal that DiffQKV\nattention significantly enhances efficiency, achieving up to a 33.36%\nimprovement in inference speed over the conventional grouped-query attention\n(GQA) in long-context scenarios. We pre-train Sigma on 6T tokens from various\nsources, including 19.5B system domain data that we carefully collect and 1T\ntokens of synthesized and rewritten data. In general domains, Sigma achieves\ncomparable performance to other state-of-arts models. In the system domain, we\nintroduce the first comprehensive benchmark AIMicius, where Sigma demonstrates\nremarkable performance across all tasks, significantly outperforming GPT-4 with\nan absolute improvement up to 52.5%."
                },
                "authors": [
                    {
                        "name": "Zhenghao Lin"
                    },
                    {
                        "name": "Zihao Tang"
                    },
                    {
                        "name": "Xiao Liu"
                    },
                    {
                        "name": "Yeyun Gong"
                    },
                    {
                        "name": "Yi Cheng"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Hang Li"
                    },
                    {
                        "name": "Ying Xin"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Xiao Liang"
                    },
                    {
                        "name": "Shuai Lu"
                    },
                    {
                        "name": "Yiming Huang"
                    },
                    {
                        "name": "Zheheng Luo"
                    },
                    {
                        "name": "Lei Qu"
                    },
                    {
                        "name": "Xuan Feng"
                    },
                    {
                        "name": "Yaoxiang Wang"
                    },
                    {
                        "name": "Yuqing Xia"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Yuting Jiang"
                    },
                    {
                        "name": "Yasen Hu"
                    },
                    {
                        "name": "Hao Ni"
                    },
                    {
                        "name": "Binyang Li"
                    },
                    {
                        "name": "Guoshuai Zhao"
                    },
                    {
                        "name": "Jui-Hao Chiang"
                    },
                    {
                        "name": "Zhongxin Guo"
                    },
                    {
                        "name": "Chen Lin"
                    },
                    {
                        "name": "Kun Kuang"
                    },
                    {
                        "name": "Wenjie Li"
                    },
                    {
                        "name": "Yelong Shen"
                    },
                    {
                        "name": "Jian Jiao"
                    },
                    {
                        "name": "Peng Cheng"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13998v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13998v1",
                "updated": "2025-01-23T11:18:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T11:18:42Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    11,
                    18,
                    42,
                    3,
                    23,
                    0
                ],
                "title": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterisation of the plutonium isotopic composition of a sediment\n  core from Palomares, Spain, by low-energy AMS and alpha-spectrometry"
                },
                "summary": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The measurement of plutonium isotopes, 239Pu and 240Pu, at 670 kV on the\ncompact accelerator mass spectrometry (AMS) system at the Centro Nacional de\nAceleradores (CNA) in Seville, Spain, is now a reality. In this work, we\npresent first Pu AMS results for environmental samples: a sediment core\ncollected in a submarine canyon in the Mediterranean coast of the Spanish\nregion of Palomares, affected by a nuclear accident in 1966. From the study of\nthe 240Pu/239Pu atomic ratio profile, showing on average levels lower than 11%,\nwe confirm that the weapon-grade plutonium released on land during the\naccident, with a characteristic 240Pu/239Pu atomic ratio of 5.8%, has found its\nway into the marine environment. A two-plutonium sources mixture model\n(Palomares and fallout) is used to elucidate the percentage of the plutonium\ncoming from the accident. As a validation exercise of the Pu AMS measuring\ntechnique and in order to obtain the 238Pu/(239+240)Pu activity ratios, samples\nwere also studied by alpha-spectrometry (AS). The obtained AS 239+240Pu\nactivity concentration results fit in with the AMS ones in a wide dynamic\nrange, thus validating the AMS technique."
                },
                "authors": [
                    {
                        "name": "E. Chamizo"
                    },
                    {
                        "name": "M. C. Jiménez-Ramos"
                    },
                    {
                        "name": "S. M. Enamorado"
                    },
                    {
                        "name": "M. García-León"
                    },
                    {
                        "name": "R. García-Tenorio"
                    },
                    {
                        "name": "J. L. Mas"
                    },
                    {
                        "name": "P. Masqué"
                    },
                    {
                        "name": "J. Merino"
                    },
                    {
                        "name": "J. A. Sanchez-Cabeza"
                    }
                ],
                "author_detail": {
                    "name": "J. A. Sanchez-Cabeza"
                },
                "author": "J. A. Sanchez-Cabeza",
                "arxiv_doi": "10.1016/j.nimb.2009.10.151",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.nimb.2009.10.151",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2501.13998v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13998v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "12 pages, 1 table, 3 figures",
                "arxiv_journal_ref": "Nuclear Instruments and Methods in Physics Research Section B:\n  Beam Interactions with Materials and Atoms, Volume 268, Issues 7-8, April\n  2010, Pages 1273-1276",
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.ao-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13540v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13540v1",
                "updated": "2025-01-23T10:40:09Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T10:40:09Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    10,
                    40,
                    9,
                    3,
                    23,
                    0
                ],
                "title": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "POPS: From History to Mitigation of DNS Cache Poisoning Attacks"
                },
                "summary": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel yet simple and comprehensive DNS cache POisoning\nPrevention System (POPS), designed to integrate as a module in Intrusion\nPrevention Systems (IPS). POPS addresses statistical DNS poisoning attacks,\nincluding those documented from 2002 to the present, and offers robust\nprotection against similar future threats. It consists of two main components:\na detection module that employs three simple rules, and a mitigation module\nthat leverages the TC flag in the DNS header to enhance security. Once\nactivated, the mitigation module has zero false positives or negatives,\ncorrecting any such errors on the side of the detection module.\n  We first analyze POPS against historical DNS services and attacks, showing\nthat it would have mitigated all network-based statistical poisoning attacks,\nyielding a success rate of only 0.0076% for the adversary. We then simulate\nPOPS on traffic benchmarks (PCAPs) incorporating current potential\nnetwork-based statistical poisoning attacks, and benign PCAPs; the simulated\nattacks still succeed with a probability of 0.0076%. This occurs because five\nmalicious packets go through before POPS detects the attack and activates the\nmitigation module. In addition, POPS completes its task using only 20%-50% of\nthe time required by other tools (e.g., Suricata or Snort), and after examining\njust 5%-10% as many packets. Furthermore, it successfully identifies DNS cache\npoisoning attacks-such as fragmentation attacks-that both Suricata and Snort\nfail to detect, underscoring its superiority in providing comprehensive DNS\nprotection."
                },
                "authors": [
                    {
                        "name": "Yehuda Afek"
                    },
                    {
                        "name": "Harel Berger"
                    },
                    {
                        "name": "Anat Bremler-Barr"
                    }
                ],
                "author_detail": {
                    "name": "Anat Bremler-Barr"
                },
                "author": "Anat Bremler-Barr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13540v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13540v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.09827v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.09827v3",
                "updated": "2025-01-23T07:25:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    7,
                    25,
                    28,
                    3,
                    23,
                    0
                ],
                "published": "2024-06-14T08:32:45Z",
                "published_parsed": [
                    2024,
                    6,
                    14,
                    8,
                    32,
                    45,
                    4,
                    166,
                    0
                ],
                "title": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Training-free Sub-quadratic Cost Transformer Model Serving Framework\n  With Hierarchically Pruned Attention"
                },
                "summary": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In modern large language models (LLMs), increasing the context length is\ncrucial for improving comprehension and coherence in long-context, multi-modal,\nand retrieval-augmented language generation. While many recent transformer\nmodels attempt to extend their context length over a million tokens, they\nremain impractical due to the quadratic time and space complexities. Although\nrecent works on linear and sparse attention mechanisms can achieve this goal,\ntheir real-world applicability is often limited by the need to re-train from\nscratch and significantly worse performance. In response, we propose a novel\napproach, Hierarchically Pruned Attention (HiP), which reduces the time\ncomplexity of the attention mechanism to $O(T \\log T)$ and the space complexity\nto $O(T)$, where $T$ is the sequence length. We notice a pattern in the\nattention scores of pretrained LLMs where tokens close together tend to have\nsimilar scores, which we call ``attention locality''. Based on this\nobservation, we utilize a novel tree-search-like algorithm that estimates the\ntop-$k$ key tokens for a given query on the fly, which is mathematically\nguaranteed to have better performance than random attention pruning. In\naddition to improving the time complexity of the attention mechanism, we\nfurther optimize GPU memory usage by implementing KV cache offloading, which\nstores only $O(\\log T)$ tokens on the GPU while maintaining similar decoding\nthroughput. Experiments on benchmarks show that HiP, with its training-free\nnature, significantly reduces both prefill and decoding latencies, as well as\nmemory usage, while maintaining high-quality generation with minimal\ndegradation. HiP enables pretrained LLMs to scale up to millions of tokens on\ncommodity GPUs, potentially unlocking long-context LLM applications previously\ndeemed infeasible."
                },
                "authors": [
                    {
                        "name": "Heejun Lee"
                    },
                    {
                        "name": "Geon Park"
                    },
                    {
                        "name": "Youngwan Lee"
                    },
                    {
                        "name": "Jaduk Suh"
                    },
                    {
                        "name": "Jina Kim"
                    },
                    {
                        "name": "Wonyoung Jeong"
                    },
                    {
                        "name": "Bumsik Kim"
                    },
                    {
                        "name": "Hyemin Lee"
                    },
                    {
                        "name": "Myeongjae Jeon"
                    },
                    {
                        "name": "Sung Ju Hwang"
                    }
                ],
                "author_detail": {
                    "name": "Sung Ju Hwang"
                },
                "author": "Sung Ju Hwang",
                "arxiv_comment": "44 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.09827v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.09827v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07523v2",
                "updated": "2025-01-23T06:48:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    6,
                    48,
                    22,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-13T17:50:30Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    50,
                    30,
                    0,
                    13,
                    0
                ],
                "title": "Parallel Key-Value Cache Fusion for Position Invariant RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Key-Value Cache Fusion for Position Invariant RAG"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) underscore the necessity\nof Retrieval Augmented Generation (RAG) to leverage external information.\nHowever, LLMs are sensitive to the position of relevant information within\ncontexts and tend to generate incorrect responses when such information is\nplaced in the middle, known as `Lost in the Middle' phenomenon. In this paper,\nwe introduce a framework that generates consistent outputs for decoder-only\nmodels, irrespective of the input context order. Experimental results for three\nopen domain question answering tasks demonstrate position invariance, where the\nmodel is not sensitive to input context order, and superior robustness to\nirrelevent passages compared to prevailing approaches for RAG pipelines."
                },
                "authors": [
                    {
                        "name": "Philhoon Oh"
                    },
                    {
                        "name": "Jinwoo Shin"
                    },
                    {
                        "name": "James Thorne"
                    }
                ],
                "author_detail": {
                    "name": "James Thorne"
                },
                "author": "James Thorne",
                "arxiv_comment": "5 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13331v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13331v1",
                "updated": "2025-01-23T02:20:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "published": "2025-01-23T02:20:08Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    2,
                    20,
                    8,
                    3,
                    23,
                    0
                ],
                "title": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Qrazor: Reliable and effortless 4-bit llm quantization by significant\n  data razoring"
                },
                "summary": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large-scale language models (LLMs) have demonstrated outstanding performance\nin language processing tasks, yet their deployment is often hindered by high\nmemory demands and computational complexity. Although low-bit quantization\ntechniques, such as 4-bit quantization, present a potential solution, they\nfrequently lead to significant accuracy degradation or require substantial\neffort for such aggressive quantization approaches. To overcome these\nchallenges, we introduce QRazor, a reliable and effortless quantization scheme\ndesigned to enable 4-bit quantization for weights, activations, and KV cache in\ntransformer-based LLMs. The scheme involves two main stages: quantization and\ncompression. During the quantization stage, weights, activations, and KV cache\nvalues are quantized with wider 8 or 16-bit integers as a basis to achieve\nnearly identical accuracy to the original full-precision LLM models, using the\nabsolute max scaling. Subsequently, all data are compressed to 4-bit using our\nproposed significant data razoring (SDR) technique, which retains only the four\nmost salient bits while discarding the others. Furthermore, we present an\ninteger-based arithmetic unit dedicated to QRazor, enabling direct\nlow-precision arithmetic operations without decompressing the SDR data. Despite\nthe reduced quantization effort, QRazor achieves LLM accuracies better or\ncomparable to state-of-the-art 4-bit methods. By also validating the hardware\nefficiency, our decompression-free arithmetic unit achieves 61.2% and 57.8%\nreduction in area and power consumption, respectively."
                },
                "authors": [
                    {
                        "name": "Dongyoung Lee"
                    },
                    {
                        "name": "Seungkyu Choi"
                    },
                    {
                        "name": "Ik Joon Chang"
                    }
                ],
                "author_detail": {
                    "name": "Ik Joon Chang"
                },
                "author": "Ik Joon Chang",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13331v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13331v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11745v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11745v2",
                "updated": "2025-01-22T16:25:47Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    16,
                    25,
                    47,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-20T21:07:44Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    21,
                    7,
                    44,
                    0,
                    20,
                    0
                ],
                "title": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Personalized Federated Learning for Cellular VR: Online Learning and\n  Dynamic Caching"
                },
                "summary": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering an immersive experience to virtual reality (VR) users through\nwireless connectivity offers the freedom to engage from anywhere at any time.\nNevertheless, it is challenging to ensure seamless wireless connectivity that\ndelivers real-time and high-quality videos to the VR users. This paper proposes\na field of view (FoV) aware caching for mobile edge computing (MEC)-enabled\nwireless VR network. In particular, the FoV of each VR user is\ncached/prefetched at the base stations (BSs) based on the caching strategies\ntailored to each BS. Specifically, decentralized and personalized federated\nlearning (DP-FL) based caching strategies with guarantees are presented.\nConsidering VR systems composed of multiple VR devices and BSs, a DP-FL caching\nalgorithm is implemented at each BS to personalize content delivery for VR\nusers. The utilized DP-FL algorithm guarantees a probably approximately correct\n(PAC) bound on the conditional average cache hit. Further, to reduce the cost\nof communicating gradients, one-bit quantization of the stochastic gradient\ndescent (OBSGD) is proposed, and a convergence guarantee of\n$\\mathcal{O}(1/\\sqrt{T})$ is obtained for the proposed algorithm, where $T$ is\nthe number of iterations. Additionally, to better account for the wireless\nchannel dynamics, the FoVs are grouped into multicast or unicast groups based\non the number of requesting VR users. The performance of the proposed DP-FL\nalgorithm is validated through realistic VR head-tracking dataset, and the\nproposed algorithm is shown to have better performance in terms of average\ndelay and cache hit as compared to baseline algorithms."
                },
                "authors": [
                    {
                        "name": "Krishnendu S. Tharakan"
                    },
                    {
                        "name": "Hayssam Dahrouj"
                    },
                    {
                        "name": "Nour Kouzayha"
                    },
                    {
                        "name": "Hesham ElSawy"
                    },
                    {
                        "name": "Tareq Y. Al-Naffouri"
                    }
                ],
                "author_detail": {
                    "name": "Tareq Y. Al-Naffouri"
                },
                "author": "Tareq Y. Al-Naffouri",
                "arxiv_comment": "accepted for publication in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11745v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11745v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12959v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12959v1",
                "updated": "2025-01-22T15:33:17Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T15:33:17Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    33,
                    17,
                    2,
                    22,
                    0
                ],
                "title": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Prompt Compression with Evaluator Heads for Long-Context\n  Transformer Inference"
                },
                "summary": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although applications involving long-context inputs are crucial for the\neffective utilization of large language models (LLMs), they also result in\nincreased computational costs and reduced performance. To address this\nchallenge, we propose an efficient, training-free prompt compression method\nthat retains key information within compressed prompts. We identify specific\nattention heads in transformer-based LLMs, which we designate as evaluator\nheads, that are capable of selecting tokens in long inputs that are most\nsignificant for inference. Building on this discovery, we develop EHPC, an\nEvaluator Head-based Prompt Compression method, which enables LLMs to rapidly\n\"skim through\" input prompts by leveraging only the first few layers with\nevaluator heads during the pre-filling stage, subsequently passing only the\nimportant tokens to the model for inference. EHPC achieves state-of-the-art\nresults across two mainstream benchmarks: prompt compression and long-context\ninference acceleration. Consequently, it effectively reduces the complexity and\ncosts associated with commercial API calls. We further demonstrate that EHPC\nattains competitive results compared to key-value cache-based acceleration\nmethods, thereby highlighting its potential to enhance the efficiency of LLMs\nfor long-context tasks."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Xueyan Niu"
                    },
                    {
                        "name": "Guoqing Xie"
                    },
                    {
                        "name": "Yingqing Liu"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    }
                ],
                "author_detail": {
                    "name": "Wei Han"
                },
                "author": "Wei Han",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12959v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12959v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01253v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01253v5",
                "updated": "2025-01-22T15:09:58Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    9,
                    58,
                    2,
                    22,
                    0
                ],
                "published": "2024-12-02T08:22:56Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    8,
                    22,
                    56,
                    0,
                    337,
                    0
                ],
                "title": "Yi-Lightning Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yi-Lightning Technical Report"
                },
                "summary": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This technical report presents Yi-Lightning, our latest flagship large\nlanguage model (LLM). It achieves exceptional performance, ranking 6th overall\non Chatbot Arena, with particularly strong results (2nd to 4th place) in\nspecialized categories including Chinese, Math, Coding, and Hard Prompts.\nYi-Lightning leverages an enhanced Mixture-of-Experts (MoE) architecture,\nfeaturing advanced expert segmentation and routing mechanisms coupled with\noptimized KV-caching techniques. Our development process encompasses\ncomprehensive pre-training, supervised fine-tuning (SFT), and reinforcement\nlearning from human feedback (RLHF), where we devise deliberate strategies for\nmulti-stage training, synthetic data construction, and reward modeling.\nFurthermore, we implement RAISE (Responsible AI Safety Engine), a\nfour-component framework to address safety issues across pre-training,\npost-training, and serving phases. Empowered by our scalable super-computing\ninfrastructure, all these innovations substantially reduce training, deployment\nand inference costs while maintaining high-performance standards. With further\nevaluations on public academic benchmarks, Yi-Lightning demonstrates\ncompetitive performance against top-tier LLMs, while we observe a notable\ndisparity between traditional, static benchmark results and real-world, dynamic\nhuman preferences. This observation prompts a critical reassessment of\nconventional benchmarks' utility in guiding the development of more intelligent\nand powerful AI systems for practical applications. Yi-Lightning is now\navailable through our developer platform at https://platform.lingyiwanwu.com."
                },
                "authors": [
                    {
                        "name": "Alan Wake"
                    },
                    {
                        "name": "Bei Chen"
                    },
                    {
                        "name": "C. X. Lv"
                    },
                    {
                        "name": "Chao Li"
                    },
                    {
                        "name": "Chengen Huang"
                    },
                    {
                        "name": "Chenglin Cai"
                    },
                    {
                        "name": "Chujie Zheng"
                    },
                    {
                        "name": "Daniel Cooper"
                    },
                    {
                        "name": "Fan Zhou"
                    },
                    {
                        "name": "Feng Hu"
                    },
                    {
                        "name": "Ge Zhang"
                    },
                    {
                        "name": "Guoyin Wang"
                    },
                    {
                        "name": "Heng Ji"
                    },
                    {
                        "name": "Howard Qiu"
                    },
                    {
                        "name": "Jiangcheng Zhu"
                    },
                    {
                        "name": "Jun Tian"
                    },
                    {
                        "name": "Katherine Su"
                    },
                    {
                        "name": "Lihuan Zhang"
                    },
                    {
                        "name": "Liying Li"
                    },
                    {
                        "name": "Ming Song"
                    },
                    {
                        "name": "Mou Li"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Qicheng Hu"
                    },
                    {
                        "name": "Shawn Wang"
                    },
                    {
                        "name": "Shijun Zhou"
                    },
                    {
                        "name": "Shiming Yang"
                    },
                    {
                        "name": "Shiyong Li"
                    },
                    {
                        "name": "Tianhang Zhu"
                    },
                    {
                        "name": "Wen Xie"
                    },
                    {
                        "name": "Wenhao Huang"
                    },
                    {
                        "name": "Xiang He"
                    },
                    {
                        "name": "Xiaobo Chen"
                    },
                    {
                        "name": "Xiaohui Hu"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Xinyao Niu"
                    },
                    {
                        "name": "Yanpeng Li"
                    },
                    {
                        "name": "Yongke Zhao"
                    },
                    {
                        "name": "Yongzhen Luo"
                    },
                    {
                        "name": "Yuchi Xu"
                    },
                    {
                        "name": "Yuxuan Sha"
                    },
                    {
                        "name": "Zhaodong Yan"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Zirui Zhang"
                    },
                    {
                        "name": "Zonghong Dai"
                    }
                ],
                "author_detail": {
                    "name": "Zonghong Dai"
                },
                "author": "Zonghong Dai",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01253v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01253v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.08894v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.08894v3",
                "updated": "2025-01-22T15:05:08Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    15,
                    5,
                    8,
                    2,
                    22,
                    0
                ],
                "published": "2023-10-13T06:58:07Z",
                "published_parsed": [
                    2023,
                    10,
                    13,
                    6,
                    58,
                    7,
                    4,
                    286,
                    0
                ],
                "title": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Antenna Coded Caching for Multi-Access Networks with Cyclic\n  Wrap-Around"
                },
                "summary": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This work explores a multiple transmit antenna setting in a multi-access\ncoded caching (MACC) network where each user accesses more than one cache. A\nMACC network has $K$ users and $K$ caches, and each user has access to $r < K$\nconsecutive caches in a cyclic wrap-around manner. There are $L$ antennas at\nthe server, and each cache has a normalized size of $M/N \\leq 1$. The cyclic\nwrap-around MACC network with a single antenna at the server has been a\nwell-investigated topic, and several coded caching schemes and improved lower\nbounds on the performance are known for the same. However, this MACC network\nhas not yet been studied under multi-antenna settings in the coded caching\nliterature. We study the multi-antenna MACC problem and propose a solution for\nthe same by constructing a pair of arrays called caching and delivery arrays.\nWe present three constructions of caching and delivery arrays for different\nscenarios and obtain corresponding multi-antenna MACC schemes for the same. Two\nschemes resulting from the above constructions achieve optimal performance\nunder uncoded placement and one-shot delivery. The optimality is shown by\nmatching the performance of the multi-antenna MACC scheme to that of an optimal\nmulti-antenna scheme for a dedicated cache network having an identical number\nof users, and each user has a normalized cache size of $rM/N$. Further, as a\nspecial case, one of the proposed schemes subsumes an existing optimal MACC\nscheme for the single-antenna setting."
                },
                "authors": [
                    {
                        "name": "Elizabath Peter"
                    },
                    {
                        "name": "K. K. Krishnan Namboodiri"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "To appear in IEEE Transactions on Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.08894v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.08894v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.03940v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.03940v2",
                "updated": "2025-01-22T10:39:50Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    10,
                    39,
                    50,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-07T17:00:49Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    17,
                    0,
                    49,
                    1,
                    7,
                    0
                ],
                "title": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Not all tokens are created equal: Perplexity Attention Weighted Networks\n  for AI generated text detection"
                },
                "summary": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement in large language models (LLMs) has significantly\nenhanced their ability to generate coherent and contextually relevant text,\nraising concerns about the misuse of AI-generated content and making it\ncritical to detect it. However, the task remains challenging, particularly in\nunseen domains or with unfamiliar LLMs. Leveraging LLM next-token distribution\noutputs offers a theoretically appealing approach for detection, as they\nencapsulate insights from the models' extensive pre-training on diverse\ncorpora. Despite its promise, zero-shot methods that attempt to operationalize\nthese outputs have met with limited success. We hypothesize that one of the\nproblems is that they use the mean to aggregate next-token distribution metrics\nacross tokens, when some tokens are naturally easier or harder to predict and\nshould be weighted differently. Based on this idea, we propose the Perplexity\nAttention Weighted Network (PAWN), which uses the last hidden states of the LLM\nand positions to weight the sum of a series of features based on metrics from\nthe next-token distribution across the sequence length. Although not zero-shot,\nour method allows us to cache the last hidden states and next-token\ndistribution metrics on disk, greatly reducing the training resource\nrequirements. PAWN shows competitive and even better performance\nin-distribution than the strongest baselines (fine-tuned LMs) with a fraction\nof their trainable parameters. Our model also generalizes better to unseen\ndomains and source models, with smaller variability in the decision boundary\nacross distribution shifts. It is also more robust to adversarial attacks, and\nif the backbone has multilingual capabilities, it presents decent\ngeneralization to languages not seen during supervised training, with LLaMA3-1B\nreaching a mean macro-averaged F1 score of 81.46% in cross-validation with nine\nlanguages."
                },
                "authors": [
                    {
                        "name": "Pablo Miralles-González"
                    },
                    {
                        "name": "Javier Huertas-Tato"
                    },
                    {
                        "name": "Alejandro Martín"
                    },
                    {
                        "name": "David Camacho"
                    }
                ],
                "author_detail": {
                    "name": "David Camacho"
                },
                "author": "David Camacho",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.03940v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.03940v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12744v1",
                "updated": "2025-01-22T09:25:29Z",
                "updated_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "published": "2025-01-22T09:25:29Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    9,
                    25,
                    29,
                    2,
                    22,
                    0
                ],
                "title": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bright single-photon source in a silicon chip by nanoscale positioning\n  of a color center in a microcavity"
                },
                "summary": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an all-silicon source of near-infrared linearly-polarized single\nphotons, fabricated by nanoscale positioning of a color center in a\nsilicon-on-insulator microcavity. The color center consists of a single W\ncenter, created at a well-defined position by Si$^{+}$ ion implantation through\na 150 nm-diameter nanohole in a mask. A circular Bragg grating cavity resonant\nwith the W's zero-phonon line at 1217 nm is fabricated at the same location as\nthe nanohole. Under above-gap continuous-wave excitation, a very clean photon\nantibunching behavior ($g{^2} \\leq 0.06$) is observed over the entire power\nrange, which highlights the absence of parasitic emitters. Purcell-enhancement\nof W's zero-phonon emission provides both a record-high photoluminescence count\nrate among Si color centers (ca $1.2 \\times 10^{6}$ counts/s) and apparent\nDebye-Waller factor around 99%. We also demonstrate the triggered emission of\nsingle photons with 93% purity under weak pulsed laser excitation. At high\npulsed laser power, we reveal a detrimental effect of repumping processes, that\ncould be mitigated using selective pumping schemes in the future. These results\nrepresent a major step towards on-demand sources of indistinguishable\nnear-infrared single photons within silicon photonics chips."
                },
                "authors": [
                    {
                        "name": "Baptiste Lefaucher"
                    },
                    {
                        "name": "Yoann Baron"
                    },
                    {
                        "name": "Jean-Baptiste Jager"
                    },
                    {
                        "name": "Vincent Calvo"
                    },
                    {
                        "name": "Christian Elsässer"
                    },
                    {
                        "name": "Giuliano Coppola"
                    },
                    {
                        "name": "Frédéric Mazen"
                    },
                    {
                        "name": "Sébastien Kerdilès"
                    },
                    {
                        "name": "Félix Cache"
                    },
                    {
                        "name": "Anaïs Dréau"
                    },
                    {
                        "name": "Jean-Michel Gérard"
                    }
                ],
                "author_detail": {
                    "name": "Jean-Michel Gérard"
                },
                "arxiv_affiliation": "Univ. Grenoble Alpes, CEA, Grenoble INP, IRIG, PHELIQS",
                "author": "Jean-Michel Gérard",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.optics",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.optics",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12528v1",
                "updated": "2025-01-21T22:33:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T22:33:15Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    22,
                    33,
                    15,
                    1,
                    21,
                    0
                ],
                "title": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improved Coded Caching Scheme for Multi-User Information Retrieval\n  System"
                },
                "summary": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we study the coded caching scheme for the $(L, K, M, N)$\nmulti-user information retrieval (MIR) system, which consists of a content\nlibrary containing $N$ files, a base station (BS) with $L$ antennas that cannot\naccess the library, and $K$ single-antenna users, each of which can cache at\nmost $M$ files from the library. The users communicate with the others assisted\nby the BS to decode their required files. In this paper, we focus on designing\na coded caching scheme with low communication latency measured by normalized\ndelivery time (NDT), computational complexity, and subpacketizations. When\n$\\frac{KM}{N}\\geq L$ we first simply the precoding matrix in the downlink step\nto an identity matrix and use the multiple-antenna placement delivery array\n(MAPDA), which was originally proposed for the multiple-input single-output\nnetworks, to generate several new schemes for MIR system. Compared to the\nexisting schemes, both the theoretical and numerical analyses show that our new\nschemes achieve much lower computational complexity and smaller\nsubpacketizations with the same NDT."
                },
                "authors": [
                    {
                        "name": "Junyi Wang"
                    },
                    {
                        "name": "Quan Zang"
                    },
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Minquan Cheng"
                },
                "author": "Minquan Cheng",
                "arxiv_comment": "14",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12084v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12084v1",
                "updated": "2025-01-21T12:19:02Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T12:19:02Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    12,
                    19,
                    2,
                    1,
                    21,
                    0
                ],
                "title": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Hopper Architecture through Microbenchmarking and\n  Multiple Level Analysis"
                },
                "summary": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern GPUs, with their specialized hardware like tensor cores, are essential\nfor demanding AI and deep learning applications. This study presents a\ncomprehensive, multi-level microbenchmarking analysis of the NVIDIA Hopper GPU\narchitecture, delving into its performance characteristics and novel features.\nWe benchmark Hopper's memory subsystem latency and throughput, comparing its L2\npartitioned cache behavior and global memory access patterns against recent GPU\ngenerations, Ampere and Ada Lovelace. Our analysis reveals significant\nperformance differences and architectural improvements in Hopper. A core\ncontribution of this work is a detailed evaluation of Hopper's\nfourth-generation tensor cores, including their FP8 precision support and the\nnovel asynchronous wgmma instructions, assessing their impact on matrix\nmultiply-accumulate operations. We further investigate the performance\nimplications of other key Hopper innovations: DPX instructions for accelerating\ndynamic programming algorithms, distributed shared memory (DSM) for inter-SM\ncommunication, and the Tensor Memory Accelerator (TMA) for asynchronous data\nmovement. This multi-level approach encompasses instruction-level\nmicrobenchmarks, library-level analysis of the Transformer Engine, and\napplication-level benchmarks of tensor core performance within large language\nmodels. Our findings provide valuable, in-depth insights for software\ndevelopers seeking to optimize performance and develop accurate performance\nmodels for the Hopper architecture, ultimately contributing to a deeper\nunderstanding of its potential for accelerating AI and other computationally\nintensive workloads."
                },
                "authors": [
                    {
                        "name": "Weile Luo"
                    },
                    {
                        "name": "Ruibo Fan"
                    },
                    {
                        "name": "Zeyu Li"
                    },
                    {
                        "name": "Dayou Du"
                    },
                    {
                        "name": "Hongyuan Liu"
                    },
                    {
                        "name": "Qiang Wang"
                    },
                    {
                        "name": "Xiaowen Chu"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowen Chu"
                },
                "author": "Xiaowen Chu",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2402.13499",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12084v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12084v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11940v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11940v1",
                "updated": "2025-01-21T07:32:06Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T07:32:06Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    7,
                    32,
                    6,
                    1,
                    21,
                    0
                ],
                "title": "Build Optimization: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Build Optimization: A Systematic Literature Review"
                },
                "summary": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continuous Integration (CI) consists of an automated build process involving\ncontinuous compilation, testing, and packaging of the software system. While CI\ncomes up with several advantages related to quality and time to delivery, CI\nalso presents several challenges addressed by a large body of research. To\nbetter understand the literature so as to help practitioners find solutions for\ntheir problems and guide future research, we conduct a systematic review of 97\nstudies on build optimization published between 2006 and 2024, which we\nsummarized according to their goals, methodologies, used datasets, and\nleveraged metrics. The identified build optimization studies focus on two main\nchallenges: (1) long build durations, and (2) build failures. To meet the first\nchallenge, existing studies have developed a range of techniques, including\npredicting build outcome and duration, selective build execution, and build\nacceleration using caching or repairing performance smells. The causes of build\nfailures have been the subject of several studies, leading to the development\nof techniques for predicting build script maintenance and automating repair.\nRecent studies have also focused on predicting flaky build failures caused by\nenvironmental issues. The majority of these techniques use machine learning\nalgorithms and leverage build metrics, which we classify into five categories.\nAdditionally, we identify eight publicly available build datasets for build\noptimization research."
                },
                "authors": [
                    {
                        "name": "Henri Aïdasso"
                    },
                    {
                        "name": "Mohammed Sayagh"
                    },
                    {
                        "name": "Francis Bordeleau"
                    }
                ],
                "author_detail": {
                    "name": "Francis Bordeleau"
                },
                "author": "Francis Bordeleau",
                "arxiv_comment": "An earlier version of this work was submitted to ACM CSUR in November\n  2023",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11940v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11940v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11834v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11834v1",
                "updated": "2025-01-21T02:35:31Z",
                "updated_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "published": "2025-01-21T02:35:31Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    2,
                    35,
                    31,
                    1,
                    21,
                    0
                ],
                "title": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PDA Construction via Union of Cartesian Product Cache Configurations for\n  Coded Caching"
                },
                "summary": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching is an efficient technique to reduce peak traffic by storing popular\ncontent in local caches. Placement delivery array (PDA) proposed by Yan et al.\nis a combinatorial structure to design coded caching schemes with uncoded\nplacement and one-shot linear delivery. By taking the $m$-fold Cartesian\nproduct of a small base PDA, Wang et al. constructed a big PDA while\nmaintaining the memory ratio and transmission load unchanged, which achieves\nlinear growth in both the number of users and coded caching gain. In order to\nachieve exponential growth in both the number of users and coded caching gain,\nin this paper we propose a PDA construction by taking the union operation of\nthe cache configurations from the $m$-fold Cartesian product of a base PDA. The\nresulting PDA leads to a coded caching scheme with subpacketization increasing\nsub-exponentially with the number of users while keeping the load constant for\nfixed memory ratio. By applying the proposed construction to existing base\nPDAs, three new coded caching schemes are obtained, which cover some existing\nschemes as special cases and can achieve lower load with simultaneously lower\nsubpacketization for some memory ratios."
                },
                "authors": [
                    {
                        "name": "Jinyu Wang"
                    },
                    {
                        "name": "Minquan Cheng"
                    },
                    {
                        "name": "Kai Wan"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "arxiv_comment": "35 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11834v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11834v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11779v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11779v1",
                "updated": "2025-01-20T23:10:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T23:10:13Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    23,
                    10,
                    13,
                    0,
                    20,
                    0
                ],
                "title": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Glinthawk: A Two-Tiered Architecture for High-Throughput LLM Inference"
                },
                "summary": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLM) have revolutionized natural language processing,\nbut their inference demands substantial resources, while under-utilizing\nhigh-end accelerators like GPUs. A major bottleneck arises from the attention\nmechanism, which requires storing large key-value caches, limiting the maximum\nachievable throughput way below the available computing resources. Current\napproaches attempt to mitigate this issue through memory-efficient attention\nand paging mechanisms, but remained constrained by the assumption that all\noperations must be performed on high-end accelerators.\n  In this work, we propose Glinthawk, a two-tiered architecture that decouples\nthe attention mechanism from the rest of the Transformer model. This approach\nallows the memory requirements for attention to scale independently, enabling\nlarger batch sizes and more efficient use of the high-end accelerators. We\nprototype Glinthawk with NVIDIA T4 GPUs as one tier and standard CPU VMs as the\nother. Compared to a traditional single-tier setup, it improves throughput by\n$5.9\\times$ and reduces cost of generation by $2.8\\times$. For longer sequence\nlengths, it achieves $16.3\\times$ throughput improvement at $2.4\\times$ less\ncost. Our evaluation shows that this architecture can tolerate moderate network\nlatency with minimal performance degradation, making it highly effective for\nlatency-tolerant, throughput-oriented applications such as batch processing. We\nshared our prototype publicly at \\url{https://github.com/microsoft/glinthawk}."
                },
                "authors": [
                    {
                        "name": "Pouya Hamadanian"
                    },
                    {
                        "name": "Sadjad Fouladi"
                    }
                ],
                "author_detail": {
                    "name": "Sadjad Fouladi"
                },
                "author": "Sadjad Fouladi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11779v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11779v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11502v1",
                "updated": "2025-01-20T14:19:48Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "published": "2025-01-20T14:19:48Z",
                "published_parsed": [
                    2025,
                    1,
                    20,
                    14,
                    19,
                    48,
                    0,
                    20,
                    0
                ],
                "title": "Hierarchical Coded Caching in High Memory Regime with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching in High Memory Regime with Coded Placement"
                },
                "summary": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider a two-layer hierarchical coded caching network where a server\nwith a library of $N$ files is connected to $K_1$ mirrors, each having a cache\nmemory of size $M_1$. Each mirror is further connected to $K_2$ users, each\nequipped with a dedicated cache of size $M_2$. In this paper, we propose two\ndistinct coded caching schemes based on coded placement, corresponding to two\ndistinct memory pairs, \\( (M_1, M_2) \\). We show that the proposed schemes\noutperform the existing schemes at these memory points given by the proposed\nschemes for smaller values of $K_2$. In setups where mirrors are positioned\nnear each other, avoiding signal interference is crucial. This can be ensured\nby having all mirrors transmit using orthogonal carrier frequencies. To compare\nour schemes with existing ones, we used the composite rate metric, which\naccurately represents the total bandwidth utilized in such setups. The\ncomposite rate is given by $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to the mirrors, and $R_2$ is the rate from the mirrors to\nthe users, with respect to $M_1$ and $M_2$."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "7 pages, 3 figures and 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v3",
                "updated": "2025-01-20T08:44:01Z",
                "updated_parsed": [
                    2025,
                    1,
                    20,
                    8,
                    44,
                    1,
                    0,
                    20,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty; only the dirty\nelements need be processed to draw the next frame, dramatically reducing\nlatency. However, the standard incremental layout algorithm must search the\npage for dirty elements, accessing a number of auxiliary elements in the\nprocess. These auxiliary elements add cache misses and stalled cycles, and are\nresponsible for a sizable fraction of all layout latency. We introduce a new,\nfaster incremental layout algorithm called Spineless Traversal. Spineless\nTraversal uses a more computationally demanding priority queue algorithm to\navoid the need to access auxiliary nodes and thus reduces cache traffic and\nstalls. This leads to dramatic speedups on the most latency-critical\ninteractions such as hovering, typing, or animations. Moreover, thanks to\nnumerous low-level optimizations, we are able to make Spineless Traversal\ncompetitive across the whole spectrum of incremental layout workloads. As a\nresult, across 2216 benchmarks, Spineless Traversal is faster on 78.2% of the\nbenchmark, with a mean speedup of 3.23x concentrated in the most\nlatency-critical interactions such as hovering, typing, and animations."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.11175v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.11175v1",
                "updated": "2025-01-19T21:25:53Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "published": "2025-01-19T21:25:53Z",
                "published_parsed": [
                    2025,
                    1,
                    19,
                    21,
                    25,
                    53,
                    6,
                    19,
                    0
                ],
                "title": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ProKeR: A Kernel Perspective on Few-Shot Adaptation of Large\n  Vision-Language Models"
                },
                "summary": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing popularity of Contrastive Language-Image Pretraining (CLIP) has\nled to its widespread application in various visual downstream tasks. To\nenhance CLIP's effectiveness and versatility, efficient few-shot adaptation\ntechniques have been widely adopted. Among these approaches, training-free\nmethods, particularly caching methods exemplified by Tip-Adapter, have gained\nattention for their lightweight adaptation without the need for additional\nfine-tuning. In this paper, we revisit Tip-Adapter from a kernel perspective,\nshowing that caching methods function as local adapters and are connected to a\nwell-established kernel literature. Drawing on this insight, we offer a\ntheoretical understanding of how these methods operate and suggest multiple\navenues for enhancing the Tip-Adapter baseline. Notably, our analysis shows the\nimportance of incorporating global information in local adapters. Therefore, we\nsubsequently propose a global method that learns a proximal regularizer in a\nreproducing kernel Hilbert space (RKHS) using CLIP as a base learner. Our\nmethod, which we call ProKeR (Proximal Kernel ridge Regression), has a closed\nform solution and achieves state-of-the-art performances across 11 datasets in\nthe standard few-shot adaptation benchmark."
                },
                "authors": [
                    {
                        "name": "Yassir Bendou"
                    },
                    {
                        "name": "Amine Ouasfi"
                    },
                    {
                        "name": "Vincent Gripon"
                    },
                    {
                        "name": "Adnane Boukhayma"
                    }
                ],
                "author_detail": {
                    "name": "Adnane Boukhayma"
                },
                "author": "Adnane Boukhayma",
                "arxiv_comment": "Code available at https://ybendou.github.io/ProKeR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.11175v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.11175v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.02088v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.02088v3",
                "updated": "2025-01-19T19:46:21Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    19,
                    46,
                    21,
                    6,
                    19,
                    0
                ],
                "published": "2024-09-03T17:40:24Z",
                "published_parsed": [
                    2024,
                    9,
                    3,
                    17,
                    40,
                    24,
                    1,
                    247,
                    0
                ],
                "title": "Cache Coherence Over Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache Coherence Over Disaggregated Memory"
                },
                "summary": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disaggregating memory from compute offers the opportunity to better utilize\nstranded memory in cloud data centers. It is important to cache data in the\ncompute nodes and maintain cache coherence across multiple compute nodes.\nHowever, the limited computing power on disaggregated memory servers makes\ntraditional cache coherence protocols suboptimal, particularly in the case of\nstranded memory. This paper introduces SELCC; a Shared-Exclusive Latch Cache\nCoherence protocol that maintains cache coherence without imposing any\ncomputational burden on the remote memory side. It aligns the state machine of\nthe shared-exclusive latch protocol with the MSI protocol by introducing lazy\nlatch-release and invalidation messages, thereby ensuring both atomicity of\ndata access and cache coherence. SELCC embeds cache-ownership metadata directly\ninto the RDMA latch word, enabling efficient cache ownership management via\nRDMA atomic operations. SELCC can serve as an abstraction layer over\ndisaggregated memory with APIs that resemble main-memory accesses. A concurrent\nB-tree and three transaction concurrency control algorithms are realized using\nSELCC's abstraction layer. Experimental results show that SELCC significantly\noutperforms Remote-Procedure-Call-based protocols for cache coherence under\nlimited remote computing power. Applications on SELCC achieve comparable or\nsuperior performance over disaggregated memory compared to competitors."
                },
                "authors": [
                    {
                        "name": "Ruihong Wang"
                    },
                    {
                        "name": "Jianguo Wang"
                    },
                    {
                        "name": "Walid G. Aref"
                    }
                ],
                "author_detail": {
                    "name": "Walid G. Aref"
                },
                "author": "Walid G. Aref",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.02088v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.02088v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.15024v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.15024v2",
                "updated": "2025-01-19T15:47:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    19,
                    15,
                    47,
                    14,
                    6,
                    19,
                    0
                ],
                "published": "2023-12-22T19:15:23Z",
                "published_parsed": [
                    2023,
                    12,
                    22,
                    19,
                    15,
                    23,
                    4,
                    356,
                    0
                ],
                "title": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded Caching for Hierarchical Two-Layer Networks with Coded Placement"
                },
                "summary": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We examine a two-layered hierarchical coded caching problem, a configuration\naddressed in existing research. This involves a server connected to $K_1$\nmirrors, each of which serves $K_2$ users. The mirrors and the users are\nequipped with caches of size $M_1$ and $M_2$, respectively. We propose a\nhierarchical coded caching scheme with coded placements that outperforms\nexisting schemes. To ensure a fair comparison, we introduce the notion of\ncomposite rate, defined as $\\overline{R} = R_1 + K_1 R_2$, where $R_1$ is the\nrate from the server to mirrors and $R_2$ is the rate from mirrors to users.\nThe composite rate has not been discussed before in the literature and is\npertinent when mirrors transmit with different carrier frequencies. For the\nproposed scheme, we show a trade-off between the global memory\n$\\overline{M}=K_1M_1+K_1K_2M_2$ of the system and the composite rate and\ncompare with the existing schemes. Additionally, we conduct this comparative\nanalysis by plotting $R_1$ + $R_2$ against global memory, which is particularly\nbeneficial for systems wherein each mirror can utilize the same carrier\nfrequency, given their significant spatial separation. Additionally, we propose\nan optimized scheme for the specific case of a single mirror, showing improved\nperformance in this scenario."
                },
                "authors": [
                    {
                        "name": "Rajlaxmi Pandey"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "47 pages, 16 figures and 2 tables. More figures, explanations and\n  comparisons included",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.15024v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.15024v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10756v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10756v1",
                "updated": "2025-01-18T13:04:23Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T13:04:23Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    13,
                    4,
                    23,
                    5,
                    18,
                    0
                ],
                "title": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "D2D Coded Caching Schemes for Multiaccess Networks with Combinatorial\n  Access Topology"
                },
                "summary": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper considers wireless device-to-device (D2D) coded caching in a\nmultiaccess network, where the users communicate with each other and each user\ncan access multiple cache nodes. Access topologies derived from two\ncombinatorial designs known as the $t$-design and $t$-group divisible design\n($t$-GDD), referred to as the $t$-design and $t$-GDD topologies respectively,\nwhich subsume a few other known topologies, have been studied for the\nmultiaccess coded caching (MACC) network by Cheng \\textit{et al.} in\n\\cite{MACC_des}. These access topologies are extended to a multiaccess D2D\ncoded caching (MADCC) network and novel MADCC schemes are proposed. MADCC\nnetwork has been studied so far only for the cyclic wrap-around topology. Apart\nfrom the proposed novel MADCC schemes, MADCC schemes are also derived from the\nexisting MACC schemes in \\cite{MACC_des}. To compare the performance of\ndifferent MADCC schemes, the metrics of load per user and subpacketization\nlevel are used while keeping the number of caches and cache memory size same.\nThe proposed MADCC scheme with $t$-design topology performs better in terms of\nsubpacketization level while achieving the same load per user compared to the\nMADCC scheme derived from the MACC scheme with $t$-design topology in\n\\cite{MACC_des}. The proposed MADCC scheme with $t$-GDD topology performs\nbetter in terms of load per user while achieving the same subpacketization\nlevel compared to the MADCC scheme derived from the MACC scheme with $t$-GDD\ntopology in \\cite{MACC_des} in some cases. Compared to the existing MADCC\nscheme with cyclic wrap-around topology, the proposed MADCC scheme with\n$t$-design topology performs better in terms of load per user, and the proposed\nMADCC scheme with $t$-GDD topology performs better in terms of subpacketization\nlevel at the expense of an increase in load per user."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "21 pages, 12 figures and 4 tables. Some overlap with 2409.14350v1\n  [cs.IT] 22 Sept. 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10756v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10756v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10682v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10682v1",
                "updated": "2025-01-18T07:29:20Z",
                "updated_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "published": "2025-01-18T07:29:20Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    7,
                    29,
                    20,
                    5,
                    18,
                    0
                ],
                "title": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SkyByte: Architecting an Efficient Memory-Semantic CXL-based SSD with OS\n  and Hardware Co-design"
                },
                "summary": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CXL-based solid-state drive (CXL-SSD) provides a promising approach\ntowards scaling the main memory capacity at low cost. However, the CXL-SSD\nfaces performance challenges due to the long flash access latency and\nunpredictable events such as garbage collection in the SSD device, stalling the\nhost processor and wasting compute cycles. Although the CXL interface enables\nthe byte-granular data access to the SSD, accessing flash chips is still at\npage granularity due to physical limitations. The mismatch of access\ngranularity causes significant unnecessary I/O traffic to flash chips,\nworsening the suboptimal end-to-end data access performance. In this paper, we\npresent SkyByte, an efficient CXL-based SSD that employs a holistic approach to\naddress the aforementioned challenges by co-designing the host operating system\n(OS) and SSD controller. To alleviate the long memory stall when accessing the\nCXL-SSD, SkyByte revisits the OS context switch mechanism and enables\nopportunistic context switches upon the detection of long access delays. To\naccommodate byte-granular data accesses, SkyByte architects the internal DRAM\nof the SSD controller into a cacheline-level write log and a page-level data\ncache, and enables data coalescing upon log cleaning to reduce the I/O traffic\nto flash chips. SkyByte also employs optimization techniques that include\nadaptive page migration for exploring the performance benefits of fast host\nmemory by promoting hot pages in CXL-SSD to the host. We implement SkyByte with\na CXL-SSD simulator and evaluate its efficiency with various data-intensive\napplications. Our experiments show that SkyByte outperforms current CXL-based\nSSD by 6.11X, and reduces the I/O traffic to flash chips by 23.08X on average.\nSkyByte also reaches 75% of the performance of the ideal case that assumes\nunlimited DRAM capacity in the host, which offers an attractive cost-effective\nsolution."
                },
                "authors": [
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Yuqi Xue"
                    },
                    {
                        "name": "Yirui Eric Zhou"
                    },
                    {
                        "name": "Shaobo Li"
                    },
                    {
                        "name": "Jian Huang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Huang"
                },
                "author": "Jian Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10682v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10682v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.05221v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.05221v3",
                "updated": "2025-01-17T16:16:54Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    16,
                    16,
                    54,
                    4,
                    17,
                    0
                ],
                "published": "2024-09-08T20:47:44Z",
                "published_parsed": [
                    2024,
                    9,
                    8,
                    20,
                    47,
                    44,
                    6,
                    252,
                    0
                ],
                "title": "Geometric rigidity of simple modules for algebraic groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Geometric rigidity of simple modules for algebraic groups"
                },
                "summary": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Let k be a field, let G be an affine algebraic k-group and V a\nfinite-dimensional G-module. We say V is rigid if the socle series and radical\nseries coincide for the action of G on each indecomposable summand of V; say V\nis geometrically rigid (resp. absolutely rigid) if V is rigid after base change\nof G and V to k (resp. any field extension of k). We show that all simple\nG-modules are geometrically rigid, though not in general absolutely rigid. More\nprecisely, we show that if V is a simple G-module, then there is a finite\npurely inseparable extension kV /k naturally attached to V such that V is\nabsolutely rigid as a G-module after base change to kV. The proof turns on an\ninvestigation of algebras of the form K otimes E where K and E are field\nextensions of k; we give an example of such an algebra which is not rigid as a\nmodule over itself. We establish the existence of the purely inseparable field\nextension kV /k through an analogous version for artinian algebras.\n  In the second half of the paper we apply recent results on the structure and\nrepresentation theory of pseudo-reductive groups to give a concrete description\nof kV when G is smooth and connected. Namely, we combine the main structure\ntheorem of the Conrad-Prasad classification of pseudo-reductive G together with\nour previous high weight theory. For V a simple G-module, we calculate the\nminimal field of definition of the geometric Jacobson radical of EndG(V) in\nterms of the high weight of V and the Conrad-Prasad classification data; this\ngives a concrete construction of the field kV as a subextension of the minimal\nfield of definition of the geometric unipotent radical of G. We also observe\nthat the Conrad-Prasad classification can be used to hone the dimension formula\nfor V we had previously established; we also use it to give a description of\nEndG(V) which includes a dimension formula."
                },
                "authors": [
                    {
                        "name": "Michael Bate"
                    },
                    {
                        "name": "David I. Stewart"
                    }
                ],
                "author_detail": {
                    "name": "David I. Stewart"
                },
                "author": "David I. Stewart",
                "arxiv_comment": "v3; 30 pages; Theorem 1 now holds for arbitrary affine algebraic\n  groups over fields",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.05221v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.05221v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.RT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.RT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.RA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "20G05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10138v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10138v1",
                "updated": "2025-01-17T12:01:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T12:01:28Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    12,
                    1,
                    28,
                    4,
                    17,
                    0
                ],
                "title": "The NIC should be part of the OS",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The NIC should be part of the OS"
                },
                "summary": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The network interface adapter (NIC) is a critical component of a modern cloud\nserver which occupies a unique position. Not only is network performance vital\nto the efficient operation of the machine, but unlike application-oriented\ncompute accelerators like GPUs, the network subsystem must react to\nunpredictable events like the arrival of a network packet and communicate with\nthe appropriate application end point with minimal latency. Current approaches\nto server stacks navigate a trade-off between flexibility, efficiency, and\nperformance: the fastest kernel-bypass approaches dedicate cores to\napplications, busy-wait on receive queues, etc. while more flexible approaches\nappropriate to more dynamic workload mixes incur much greater software overhead\non the data path. However, we reject this trade-off, which we ascribe to an\narbitrary (and sub-optimal) split in system state between the OS and the NIC.\nInstead, by exploiting the properties of cache-coherent interconnects and\nintegrating the NIC closely with the OS kernel, we can achieve something\nsurprising: performance for RPC workloads better than the fastest kernel-bypass\napproaches without sacrificing the robustness and dynamic adaptation of\nkernel-based network subsystems."
                },
                "authors": [
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10138v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10138v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.03594v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.03594v2",
                "updated": "2025-01-17T09:37:36Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    9,
                    37,
                    36,
                    4,
                    17,
                    0
                ],
                "published": "2024-11-29T05:57:37Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    57,
                    37,
                    4,
                    334,
                    0
                ],
                "title": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BatchLLM: Optimizing Large Batched LLM Inference with Global Prefix\n  Sharing and Throughput-oriented Token Batching"
                },
                "summary": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) increasingly play an important role in a wide\nrange of information processing and management tasks. Many of these tasks are\nperformed in large batches or even offline, and the performance indictor for\nwhich is throughput. These tasks usually show the characteristic of prefix\nsharing, where different prompt input can partially show the common prefix.\nHowever, the existing LLM inference engines tend to optimize the streaming\nrequests and show limitations of supporting the large batched tasks with the\nprefix sharing characteristic. The existing solutions use the LRU-based cache\nto reuse the KV context of common prefix between requests. The KV context that\nare about to be reused may prematurely evicted with the implicit cache\nmanagement. Besides, the streaming oriented systems do not leverage the\nrequest-batch information and can not mix the decoding tokens with the prefill\nchunks to the best for the batched scenarios, and thus fails to saturate the\nGPU. We propose BatchLLM to address the above problems. BatchLLM explicitly\nidentifies the common prefixes globally. The requests sharing the same prefix\nwill be scheduled together to reuse the KV context the best. BatchLLM reorders\nthe requests and schedules the requests with larger ratio of decoding first to\nbetter mix the decoding tokens with the latter prefill chunks, and applies\nmemory-centric token batching to enlarge the token-batch sizes, which helps to\nincrease the GPU utilization. Finally, BatchLLM optimizes the prefix-shared\nAttention kernel with horizontal fusion to reduce tail effect and kernel launch\noverhead. Extensive evaluation shows that BatchLLM outperforms vLLM and SGLang\nby 1.3$\\times$ to 10.8$\\times$ on a set of microbenchmarks and a typical\nindustry workload under different hardware environments."
                },
                "authors": [
                    {
                        "name": "Zhen Zheng"
                    },
                    {
                        "name": "Xin Ji"
                    },
                    {
                        "name": "Taosong Fang"
                    },
                    {
                        "name": "Fanghao Zhou"
                    },
                    {
                        "name": "Chuanjie Liu"
                    },
                    {
                        "name": "Gang Peng"
                    }
                ],
                "author_detail": {
                    "name": "Gang Peng"
                },
                "author": "Gang Peng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.03594v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.03594v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09902v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09902v1",
                "updated": "2025-01-17T01:24:12Z",
                "updated_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "published": "2025-01-17T01:24:12Z",
                "published_parsed": [
                    2025,
                    1,
                    17,
                    1,
                    24,
                    12,
                    4,
                    17,
                    0
                ],
                "title": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Dimensional Vector ISA Extension for Mobile In-Cache Computing"
                },
                "summary": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In-cache computing technology transforms existing caches into long-vector\ncompute units and offers low-cost alternatives to building expensive vector\nengines for mobile CPUs. Unfortunately, existing long-vector Instruction Set\nArchitecture (ISA) extensions, such as RISC-V Vector Extension (RVV) and Arm\nScalable Vector Extension (SVE), provide only one-dimensional strided and\nrandom memory accesses. While this is sufficient for typical vector engines, it\nfails to effectively utilize the large Single Instruction, Multiple Data (SIMD)\nwidths of in-cache vector engines. This is because mobile data-parallel kernels\nexpose limited parallelism across a single dimension.\n  Based on our analysis of mobile vector kernels, we introduce a long-vector\nMulti-dimensional Vector ISA Extension (MVE) for mobile in-cache computing. MVE\nachieves high SIMD resource utilization and enables flexible programming by\nabstracting cache geometry and data layout. The proposed ISA features\nmulti-dimensional strided and random memory accesses and efficient\ndimension-level masked execution to encode parallelism across multiple\ndimensions. Using a wide range of data-parallel mobile workloads, we\ndemonstrate that MVE offers significant performance and energy reduction\nbenefits of 2.9x and 8.8x, on average, compared to the SIMD units of a\ncommercial mobile processor, at an area overhead of 3.6%."
                },
                "authors": [
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Daichi Fujiki"
                    },
                    {
                        "name": "Hilbert Chen"
                    },
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Nishil Talati"
                    },
                    {
                        "name": "Scott Mahlke"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_comment": "2025 IEEE International Symposium on High-Performance Computer\n  Architecture (HPCA)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09902v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09902v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04501v2",
                "updated": "2025-01-16T15:11:42Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    15,
                    11,
                    42,
                    3,
                    16,
                    0
                ],
                "published": "2024-07-05T13:42:30Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    13,
                    42,
                    30,
                    4,
                    187,
                    0
                ],
                "title": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryogenic Behavior of High-Permittivity Gate Dielectrics: The Impact of\n  the Atomic Layer Deposition Temperature and the Lithographic Patterning\n  Method"
                },
                "summary": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dielectrics featuring a high relative permittivity, i.e., high-k dielectrics,\nhave become the standard insulators in gate architectures, enhancing the\nelectrical performance of both room temperature and cryogenic electronics. This\nstudy delves into the cryogenic (3 K) performance of high-k dielectrics\ncommonly used as gate insulators. We fabricated Al2O3 and HfO2 layers via\nAtomic Layer Deposition (ALD) and we extrapolated relative permittivity (k) and\ndielectric strength (E_BD) from AC (100 Hz to 100 kHz) and DC measurements on\nmetal-insulator-metal capacitors. Our findings reveal a strong dependence of\nHfO2 cryogenic performance on the ALD growth temperature, while the latter\nshows a negligible impact on Al2O3. We estimated a ~9 % and ~14 % reduction of\nthe relative permittivity of HfO2 and Al2O3, respectively, from 300 K to 3 K.\nAdditionally, we designed and fabricated Al2O3/HfO2 bilayers and we checked\ntheir properties at cryogenic temperatures. The study also investigates the\nimpact of the patterning method, namely, UV or electron-beam lithography\n(acceleration voltage of 10, 20, or 30 kV), on the high-k dielectric\nproperties."
                },
                "authors": [
                    {
                        "name": "Alessandro Paghi"
                    },
                    {
                        "name": "Sebastiano Battisti"
                    },
                    {
                        "name": "Simone Tortorella"
                    },
                    {
                        "name": "Giorgio De Simoni"
                    },
                    {
                        "name": "Francesco Giazotto"
                    }
                ],
                "author_detail": {
                    "name": "Francesco Giazotto"
                },
                "author": "Francesco Giazotto",
                "arxiv_doi": "10.1063/5.0250428",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1063/5.0250428",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2407.04501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "17 pages, 4 figures, supporting information at the end of the paper",
                "arxiv_journal_ref": "J. Appl. Phys. 137, 044103 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.11501v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.11501v2",
                "updated": "2025-01-16T10:35:59Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    10,
                    35,
                    59,
                    3,
                    16,
                    0
                ],
                "published": "2023-12-08T15:11:26Z",
                "published_parsed": [
                    2023,
                    12,
                    8,
                    15,
                    11,
                    26,
                    4,
                    342,
                    0
                ],
                "title": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Write+Sync: Software Cache Write Covert Channels Exploiting Memory-disk\n  Synchronization"
                },
                "summary": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory-disk synchronization is a critical technology for ensuring data\ncorrectness, integrity, and security, especially in systems that handle\nsensitive information like financial transactions and medical records. We\npropose SYNC+SYNC, a group of attacks that exploit the memory-disk\nsynchronization primitives. SYNC+SYNC works by subtly varying the timing of\nsynchronization on the write buffer, offering several advantages: 1)\nimplemented purely in software, enabling deployment on any hardware devices; 2)\nresilient against existing cache partitioning and randomization techniques; 3)\nunaffected by prefetching techniques and cache replacement strategies. We\npresent the principles of SYNC+SYNC through the implementation of two write\ncovert channel protocols, using either a single file or page, and introduce\nthree enhanced strategies that utilize multiple files and pages. The\nfeasibility of these channels is demonstrated in both cross-process and\ncross-sandbox scenarios across diverse operating systems (OSes). Experimental\nresults show that, the average rate can reach 2.036 Kb/s (with a peak rate of\n14.762 Kb/s) and the error rate is 0% on Linux; when running on macOS, the\naverage rate achieves 10.211 Kb/s (with a peak rate of 253.022 Kb/s) and the\nerror rate is 0.004%. To the best of our knowledge, SYNC+SYNC is the first\nhigh-speed write covert channel for software cache."
                },
                "authors": [
                    {
                        "name": "Congcong Chen"
                    },
                    {
                        "name": "Jinhua Cui"
                    },
                    {
                        "name": "Gang Qu"
                    },
                    {
                        "name": "Jiliang Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jiliang Zhang"
                },
                "author": "Jiliang Zhang",
                "arxiv_comment": "This manuscript was published in IEEE Transactions on Information\n  Forensics and Security, 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.11501v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.11501v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09383v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09383v1",
                "updated": "2025-01-16T08:52:38Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T08:52:38Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    8,
                    52,
                    38,
                    3,
                    16,
                    0
                ],
                "title": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Contextual Caching for Mobile Edge Large Language Model Service"
                },
                "summary": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile edge Large Language Model (LLM) deployments face inherent constraints,\nsuch as limited computational resources and network bandwidth. Although\nRetrieval-Augmented Generation (RAG) mitigates some challenges by integrating\nexternal knowledge bases, inefficient cache management can still result in high\nretrieval latency and frequent cache updates. To address these issues, we\npropose an Adaptive Contextual Caching (ACC) framework that anticipates user\nneeds by proactively caching semantically relevant data for mobile-edge LLMs.\nACC utilizes a deep reinforcement learning (DRL) module to refine cache\nreplacement policies, balancing user context, document similarity, and the\noverhead associated with cache misses. Experimental results demonstrate that\nACC increases cache hit rates to over 80\\% after only 11 training episodes,\noutperforming FIFO, LRU, and semantic-only caching while reducing retrieval\nlatency by up to 40\\%. In particular, ACC also reduces local caching overhead\n(i.e., the cost of updating the cache when a miss occurs) by as much as 55\\%,\nenabling scalable, low-latency LLM services in resource-constrained edge\nenvironments."
                },
                "authors": [
                    {
                        "name": "Guangyuan Liu"
                    },
                    {
                        "name": "Yinqiu Liu"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Dusit Niyato"
                    },
                    {
                        "name": "Jiawen Kang"
                    },
                    {
                        "name": "Zehui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Zehui Xiong"
                },
                "author": "Zehui Xiong",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09383v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09383v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09290v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09290v1",
                "updated": "2025-01-16T04:50:15Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T04:50:15Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    4,
                    50,
                    15,
                    3,
                    16,
                    0
                ],
                "title": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interoceptive Robots for Convergent Shared Control in Collaborative\n  Construction Work"
                },
                "summary": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Building autonomous mobile robots (AMRs) with optimized efficiency and\nadaptive capabilities-able to respond to changing task demands and dynamic\nenvironments-is a strongly desired goal for advancing construction robotics.\nSuch robots can play a critical role in enabling automation, reducing\noperational carbon footprints, and supporting modular construction processes.\nInspired by the adaptive autonomy of living organisms, we introduce\ninteroception, which centers on the robot's internal state representation, as a\nfoundation for developing self-reflection and conscious learning to enable\ncontinual learning and adaptability in robotic agents. In this paper, we\nfactorize internal state variables and mathematical properties as \"cognitive\ndissonance\" in shared control paradigms, where human interventions occasionally\noccur. We offer a new perspective on how interoception can help build adaptive\nmotion planning in AMRs by integrating the legacy of heuristic costs from\ngrid/graph-based algorithms with recent advances in neuroscience and\nreinforcement learning. Declarative and procedural knowledge extracted from\nhuman semantic inputs is encoded into a hypergraph model that overlaps with the\nspatial configuration of onsite layout for path planning. In addition, we\ndesign a velocity-replay module using an encoder-decoder architecture with\nfew-shot learning to enable robots to replicate velocity profiles in\ncontextualized scenarios for multi-robot synchronization and handover\ncollaboration. These \"cached\" knowledge representations are demonstrated in\nsimulated environments for multi-robot motion planning and stacking tasks. The\ninsights from this study pave the way toward artificial general intelligence in\nAMRs, fostering their progression from complexity to competence in construction\nautomation."
                },
                "authors": [
                    {
                        "name": "Xiaoshan Zhou"
                    },
                    {
                        "name": "Carol C. Menassa"
                    },
                    {
                        "name": "Vineet R. Kamat"
                    }
                ],
                "author_detail": {
                    "name": "Vineet R. Kamat"
                },
                "author": "Vineet R. Kamat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09290v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09290v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09253v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09253v1",
                "updated": "2025-01-16T02:40:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "published": "2025-01-16T02:40:07Z",
                "published_parsed": [
                    2025,
                    1,
                    16,
                    2,
                    40,
                    7,
                    3,
                    16,
                    0
                ],
                "title": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PATCHEDSERVE: A Patch Management Framework for SLO-Optimized Hybrid\n  Resolution Diffusion Serving"
                },
                "summary": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Text-to-Image (T2I) diffusion model is one of the most popular models in\nthe world. However, serving diffusion models at the entire image level faces\nseveral problems, especially when there are multiple candidate resolutions.\nFirst, image based serving system prevents requests with different resolutions\nfrom batching together. On the other hand, requests with hybrid resolutions\nalso indicate diverse locality features, which makes it hard to apply the same\ncache policy to all of them. To this end, we propose PATCHEDSERVE, A Patch\nManagement Framework for SLO-Optimized Hybrid Resolution Diffusion Serving that\nprovides a patch-level management strategy to gather hybrid resolution requests\ninto batches. Specifically, PATCHEDSERVE incorporates a novel patch-based\nprocessing workflow, significantly enhancing throughput for hybrid resolution\ninputs. Furthermore, PATCHEDSERVE designs a patch-level cache reuse policy to\nfully exploit the redundancy in diffusion. In addition, PATCHEDSERVE features\nan SLO-aware scheduling algorithm with lightweight online latency prediction,\nachieving higher SLO satisfaction rates. We show that PATCHEDSERVE can achieve\n30.1 % higher SLO satisfaction compared to SOTA diffusion serving system while\nnot hurt the image quality."
                },
                "authors": [
                    {
                        "name": "Desen Sun"
                    },
                    {
                        "name": "Zepeng Zhao"
                    },
                    {
                        "name": "Yuke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yuke Wang"
                },
                "author": "Yuke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09253v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09253v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.10845v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.10845v2",
                "updated": "2025-01-15T21:09:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    21,
                    9,
                    22,
                    2,
                    15,
                    0
                ],
                "published": "2024-04-16T18:47:07Z",
                "published_parsed": [
                    2024,
                    4,
                    16,
                    18,
                    47,
                    7,
                    1,
                    107,
                    0
                ],
                "title": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Top-k Multi-Armed Bandit Learning for Content Dissemination in Swarms of\n  Micro-UAVs"
                },
                "summary": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a Micro-Unmanned Aerial Vehicle (UAV)-enhanced content\nmanagement system for disaster scenarios where communication infrastructure is\ngenerally compromised. Utilizing a hybrid network of stationary and mobile\nMicro-UAVs, this system aims to provide crucial content access to isolated\ncommunities. In the developed architecture, stationary anchor UAVs, equipped\nwith vertical and lateral links, serve users in individual disaster-affected\ncommunities. and mobile micro-ferrying UAVs, with enhanced mobility, extend\ncoverage across multiple such communities. The primary goal is to devise a\ncontent dissemination system that dynamically learns caching policies to\nmaximize content accessibility to users left without communication\ninfrastructure. The core contribution is an adaptive content dissemination\nframework that employs a decentralized Top-k Multi-Armed Bandit learning\napproach for efficient UAV caching decisions. This approach accounts for\ngeo-temporal variations in content popularity and diverse user demands.\nAdditionally, a Selective Caching Algorithm is proposed to minimize redundant\ncontent copies by leveraging inter-UAV information sharing. Through functional\nverification and performance evaluation, the proposed framework demonstrates\nimproved system performance and adaptability across varying network sizes,\nmicro-UAV swarms, and content popularity distributions."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "16 pages, 8 figures, 2 algorithms, 2 tables, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.10845v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.10845v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09146v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09146v1",
                "updated": "2025-01-15T20:55:13Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "published": "2025-01-15T20:55:13Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    55,
                    13,
                    2,
                    15,
                    0
                ],
                "title": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Federated Multi-Armed Bandit Learning for Content Dissemination\n  using Swarm of UAVs"
                },
                "summary": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces an Unmanned Aerial Vehicle - enabled content management\narchitecture that is suitable for critical content access in communities of\nusers that are communication-isolated during diverse types of disaster\nscenarios. The proposed architecture leverages a hybrid network of stationary\nanchor UAVs and mobile Micro-UAVs for ubiquitous content dissemination. The\nanchor UAVs are equipped with both vertical and lateral communication links,\nand they serve local users, while the mobile micro-ferrying UAVs extend\ncoverage across communities with increased mobility. The focus is on developing\na content dissemination system that dynamically learns optimal caching policies\nto maximize content availability. The core innovation is an adaptive content\ndissemination framework based on distributed Federated Multi-Armed Bandit\nlearning. The goal is to optimize UAV content caching decisions based on\ngeo-temporal content popularity and user demand variations. A Selective Caching\nAlgorithm is also introduced to reduce redundant content replication by\nincorporating inter-UAV information sharing. This method strategically\npreserves the uniqueness in user preferences while amalgamating the\nintelligence across a distributed learning system. This approach improves the\nlearning algorithm's ability to adapt to diverse user preferences. Functional\nverification and performance evaluation confirm the proposed architecture's\nutility across different network sizes, UAV swarms, and content popularity\npatterns."
                },
                "authors": [
                    {
                        "name": "Amit Kumar Bhuyan"
                    },
                    {
                        "name": "Hrishikesh Dutta"
                    },
                    {
                        "name": "Subir Biswas"
                    }
                ],
                "author_detail": {
                    "name": "Subir Biswas"
                },
                "author": "Subir Biswas",
                "arxiv_comment": "25 pages, 11 figures, 1 table, 4 algorithms, journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09146v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09146v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.11",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20166v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20166v2",
                "updated": "2025-01-15T01:34:46Z",
                "updated_parsed": [
                    2025,
                    1,
                    15,
                    1,
                    34,
                    46,
                    2,
                    15,
                    0
                ],
                "published": "2024-12-28T14:38:16Z",
                "published_parsed": [
                    2024,
                    12,
                    28,
                    14,
                    38,
                    16,
                    5,
                    363,
                    0
                ],
                "title": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoL-PIM: Long-Context LLM Decoding with Scalable DRAM-PIM System"
                },
                "summary": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The expansion of large language models (LLMs) with hundreds of billions of\nparameters presents significant challenges to computational resources,\nparticularly data movement and memory bandwidth. Long-context LLMs, which\nprocess sequences of tens of thousands of tokens, further increase the demand\non the memory system as the complexity in attention layers and key-value cache\nsizes is proportional to the context length. Processing-in-Memory (PIM)\nmaximizes memory bandwidth by moving compute to the data and can address the\nmemory bandwidth challenges; however, PIM is not necessarily scalable to\naccelerate long-context LLM because of limited per-module memory capacity and\nthe inflexibility of fixed-functional unit PIM architecture and static memory\nmanagement. In this work, we propose LoL-PIM which is a multi-node PIM\narchitecture that accelerates long context LLM through hardware-software\nco-design. In particular, we propose how pipeline parallelism can be exploited\nacross a multi-PIM module while a direct PIM access (DPA) controller (or DMA\nfor PIM) is proposed that enables dynamic PIM memory management and results in\nefficient PIM utilization across a diverse range of context length. We\ndeveloped an MLIR-based compiler for LoL-PIM extending a commercial PIM-based\ncompiler where the software modifications were implemented and evaluated, while\nthe hardware changes were modeled in the simulator. Our evaluations demonstrate\nthat LoL-PIM significantly improves throughput and reduces latency for\nlong-context LLM inference, outperforming both multi-GPU and GPU-PIM systems\n(up to 8.54x and 16.0x speedup, respectively), thereby enabling more efficient\ndeployment of LLMs in real-world applications."
                },
                "authors": [
                    {
                        "name": "Hyucksung Kwon"
                    },
                    {
                        "name": "Kyungmo Koo"
                    },
                    {
                        "name": "Janghyeon Kim"
                    },
                    {
                        "name": "Woongkyu Lee"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyungdeok Lee"
                    },
                    {
                        "name": "Yousub Jung"
                    },
                    {
                        "name": "Jaehan Park"
                    },
                    {
                        "name": "Yosub Song"
                    },
                    {
                        "name": "Byeongsu Yang"
                    },
                    {
                        "name": "Haerang Choi"
                    },
                    {
                        "name": "Guhyun Kim"
                    },
                    {
                        "name": "Jongsoon Won"
                    },
                    {
                        "name": "Woojae Shin"
                    },
                    {
                        "name": "Changhyun Kim"
                    },
                    {
                        "name": "Gyeongcheol Shin"
                    },
                    {
                        "name": "Yongkee Kwon"
                    },
                    {
                        "name": "Ilkon Kim"
                    },
                    {
                        "name": "Euicheol Lim"
                    },
                    {
                        "name": "John Kim"
                    },
                    {
                        "name": "Jungwook Choi"
                    }
                ],
                "author_detail": {
                    "name": "Jungwook Choi"
                },
                "author": "Jungwook Choi",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20166v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20166v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08484v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08484v1",
                "updated": "2025-01-14T23:13:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T23:13:14Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    23,
                    13,
                    14,
                    1,
                    14,
                    0
                ],
                "title": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CORD: Co-design of Resource Allocation and Deadline Decomposition with\n  Generative Profiling"
                },
                "summary": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As multicore hardware is becoming increasingly common in real-time systems,\ntraditional scheduling techniques that assume a single worst-case execution\ntime for a task are no longer adequate, since they ignore the impact of shared\nresources on execution time. When tasks execute concurrently on different\ncores, their execution times often vary substantially with their allocated\nbudgets of shared resources, such as cache and memory bandwidth. Even under a\nspecific resource allocation, the resource use pattern of a task also changes\nwith time during a job execution. It is therefore important to consider the\nrelationship between multicore resources and execution time in task modeling\nand scheduling algorithm design.\n  In this paper, we propose a much more precise execution model for DAG-based\nreal-time tasks that captures the time-varying resource use characteristics of\na task under different budgets of shared resources. We present a generative\nresource profiling algorithm that efficiently predicts, from limited\nmeasurement data, the resource profile of a task at any time during its\nexecution under a given resource budget. The generative profiles can then be\nused to construct the execution models for tasks, using which one can make\ninformed resource allocation decisions. We further introduce a multicore\nresource allocation and deadline decomposition co-design technique for\nDAG-based tasks that leverages the generated execution models to jointly\nallocate resources and deadlines to subtasks, to maximize resource efficiency\nand schedulability. Our evaluation results show that our generative profiling\nalgorithm achieves high accuracy while being efficient, and that our\nco-allocation technique substantially improves schedulability compared to a\nstate-of-the-art deadline decomposition method."
                },
                "authors": [
                    {
                        "name": "Robert Gifford"
                    },
                    {
                        "name": "Abby Eisenklam"
                    },
                    {
                        "name": "Georgiy A. Bondar"
                    },
                    {
                        "name": "Yifan Cai"
                    },
                    {
                        "name": "Tushar Sial"
                    },
                    {
                        "name": "Linh Thi Xuan Phan"
                    },
                    {
                        "name": "Abhishek Halder"
                    }
                ],
                "author_detail": {
                    "name": "Abhishek Halder"
                },
                "author": "Abhishek Halder",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08484v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08484v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.08192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.08192v1",
                "updated": "2025-01-14T15:14:10Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-14T15:14:10Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    15,
                    14,
                    10,
                    1,
                    14,
                    0
                ],
                "title": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PRESERVE: Prefetching Model Weights and KV-Cache in Distributed LLM\n  Serving"
                },
                "summary": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely used across various applications, but\ntheir substantial computational requirements pose significant challenges,\nparticularly in terms of HBM bandwidth bottlenecks and inter-device\ncommunication overhead. In this paper, we present PRESERVE, a novel prefetching\nframework designed to optimize LLM inference by overlapping memory reads for\nmodel weights and KV-cache with collective communication operations. Through\nextensive experiments conducted on commercial AI accelerators, we demonstrate\nup to 1.6x end-to-end speedup on state-of-the-art, open-source LLMs.\nAdditionally, we perform a design space exploration that identifies the optimal\nhardware configuration for the proposed method, showing a further 1.25x\nimprovement in performance per cost by selecting the optimal L2 cache size. Our\nresults show that PRESERVE has the potential to mitigate the memory bottlenecks\nand communication overheads, offering a solution to improve the performance and\nscalability of the LLM inference systems."
                },
                "authors": [
                    {
                        "name": "Ahmet Caner Yüzügüler"
                    },
                    {
                        "name": "Jiawei Zhuang"
                    },
                    {
                        "name": "Lukas Cavigelli"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Cavigelli"
                },
                "author": "Lukas Cavigelli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.08192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.08192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15102v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15102v2",
                "updated": "2025-01-14T14:07:55Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    14,
                    7,
                    55,
                    1,
                    14,
                    0
                ],
                "published": "2024-11-22T18:06:14Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    18,
                    6,
                    14,
                    4,
                    327,
                    0
                ],
                "title": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AttriBoT: A Bag of Tricks for Efficiently Approximating Leave-One-Out\n  Context Attribution"
                },
                "summary": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The influence of contextual input on the behavior of large language models\n(LLMs) has prompted the development of context attribution methods that aim to\nquantify each context span's effect on an LLM's generations. The leave-one-out\n(LOO) error, which measures the change in the likelihood of the LLM's response\nwhen a given span of the context is removed, provides a principled way to\nperform context attribution, but can be prohibitively expensive to compute for\nlarge models. In this work, we introduce AttriBoT, a series of novel techniques\nfor efficiently computing an approximation of the LOO error for context\nattribution. Specifically, AttriBoT uses cached activations to avoid redundant\noperations, performs hierarchical attribution to reduce computation, and\nemulates the behavior of large target models with smaller proxy models. Taken\ntogether, AttriBoT can provide a >300x speedup while remaining more faithful to\na target model's LOO error than prior context attribution methods. This stark\nincrease in performance makes computing context attributions for a given\nresponse 30x faster than generating the response itself, empowering real-world\napplications that require computing attributions at scale. We release a\nuser-friendly and efficient implementation of AttriBoT to enable efficient LLM\ninterpretability as well as encourage future development of efficient context\nattribution methods."
                },
                "authors": [
                    {
                        "name": "Fengyuan Liu"
                    },
                    {
                        "name": "Nikhil Kandpal"
                    },
                    {
                        "name": "Colin Raffel"
                    }
                ],
                "author_detail": {
                    "name": "Colin Raffel"
                },
                "author": "Colin Raffel",
                "arxiv_comment": "29 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15102v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15102v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04987v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04987v2",
                "updated": "2025-01-14T12:06:33Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    12,
                    6,
                    33,
                    1,
                    14,
                    0
                ],
                "published": "2025-01-09T06:00:27Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    6,
                    0,
                    27,
                    3,
                    9,
                    0
                ],
                "title": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TreeKV: Smooth Key-Value Cache Compression with Tree Structures"
                },
                "summary": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient key-value (KV) cache compression is critical for scaling\ntransformer-based Large Language Models (LLMs) in long sequences and\nresource-limited settings. Existing methods evict tokens based on their\npositions or importance scores, but position-based strategies can miss crucial\ninformation outside predefined regions, while those relying on global\nimportance scores resulting in strong regional biases, limiting the KV cache's\noverall context retention and potentially impairing the performance of LLMs on\ncomplex tasks. Our wavelet analysis reveals that as tokens approach the end of\nsequence, their contributions to generation gradually increase and tends to\ndiverge more from neighboring tokens, indicating a smooth transition with\nincreasing complexity and variability from distant to nearby context. Motivated\nby this observation, we propose TreeKV, an intuitive, training-free method that\nemploys a tree structure for smooth cache compression. TreeKV maintains a fixed\ncache size, allowing LLMs to deliver high-quality output even in long text\nscenarios. Unlike most compression methods, TreeKV is applicable to both the\ngeneration and prefilling stages. TreeKV consistently surpasses all baseline\nmodels in language modeling tasks on PG19 and OpenWebText2, allowing LLMs\ntrained with short context window to generalize to longer window with a 16x\ncache reduction. On the Longbench benchmark, TreeKV achieves the best\nperformance with only 6\\% of the budget at optimal efficiency."
                },
                "authors": [
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Jian Yuan"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Jingwen Leng"
                    },
                    {
                        "name": "Bo Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Bo Jiang"
                },
                "author": "Bo Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04987v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04987v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.15896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.15896v2",
                "updated": "2025-01-14T11:41:14Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    11,
                    41,
                    14,
                    1,
                    14,
                    0
                ],
                "published": "2024-03-23T17:38:57Z",
                "published_parsed": [
                    2024,
                    3,
                    23,
                    17,
                    38,
                    57,
                    5,
                    83,
                    0
                ],
                "title": "Cell-level modelling of homeostasis in confined epithelial monolayers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cell-level modelling of homeostasis in confined epithelial monolayers"
                },
                "summary": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tissue homeostasis, the biological process of maintaining a steady state in\ntissue via control of cell proliferation, death, and metabolic function, is\nessential for the development, growth, maintenance, and proper function of\nliving organisms. Disruptions to this process can lead to serious diseases and\neven death. In this study, we use the vertex model for the cell-level\ndescription of tissue mechanics to investigate the impact of the tissue\nmicroenvironment and local mechanical properties of cells on homeostasis in\nconfined epithelial tissues. We find a dynamic steady state, where the balance\nbetween cell divisions and removals sustains homeostasis. By characterising\nhomeostasis in terms of cell count, tissue area, and the cells' neighbour count\ndistribution, we identify the factors that govern regulated and ordered tissue\ngrowth. This work, therefore, sheds light on the mechanisms underlying tissue\nhomeostasis and highlights the importance of mechanics in the control of\nbiological processes such as tissue development and disease pathology."
                },
                "authors": [
                    {
                        "name": "KVS Chaithanya"
                    },
                    {
                        "name": "Jan Rozman"
                    },
                    {
                        "name": "Andrej Košmrlj"
                    },
                    {
                        "name": "Rastko Sknepnek"
                    }
                ],
                "author_detail": {
                    "name": "Rastko Sknepnek"
                },
                "author": "Rastko Sknepnek",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.15896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.15896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19255v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19255v2",
                "updated": "2025-01-14T05:48:07Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    48,
                    7,
                    1,
                    14,
                    0
                ],
                "published": "2024-12-26T15:45:45Z",
                "published_parsed": [
                    2024,
                    12,
                    26,
                    15,
                    45,
                    45,
                    3,
                    361,
                    0
                ],
                "title": "Multi-matrix Factorization Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-matrix Factorization Attention"
                },
                "summary": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose novel attention architectures, Multi-matrix Factorization\nAttention (MFA) and MFA-Key-Reuse (MFA-KR). Existing variants for standard\nMulti-Head Attention (MHA), including SOTA methods like MLA, fail to maintain\nas strong performance under stringent Key-Value cache (KV cache) constraints.\nMFA enhances model capacity by efficiently scaling up both the number and\ndimension of attention heads through low-rank matrix factorization in the\nQuery-Key (QK) circuit. Extending MFA, MFA-KR further reduces memory\nrequirements by repurposing the key cache as value through value projection\nre-parameterization. MFA's design enables strong model capacity when working\nunder tight KV cache budget, while MFA-KR is suitable for even harsher KV cache\nlimits with minor performance trade-off. Notably, in our extensive and\nlarge-scale experiments, the proposed architecture outperforms MLA and performs\ncomparably to MHA, while reducing KV cache usage by up to 56% and 93.7%,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Shuigeng Zhou"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Daxin Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Daxin Jiang"
                },
                "author": "Daxin Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19255v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19255v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10480v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10480v2",
                "updated": "2025-01-14T05:00:34Z",
                "updated_parsed": [
                    2025,
                    1,
                    14,
                    5,
                    0,
                    34,
                    1,
                    14,
                    0
                ],
                "published": "2024-05-17T00:52:39Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    0,
                    52,
                    39,
                    4,
                    138,
                    0
                ],
                "title": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lean Attention: Hardware-Aware Scalable Attention Mechanism for the\n  Decode-Phase of Transformers"
                },
                "summary": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models have emerged as one of the most widely used\narchitectures for natural language processing, natural language generation, and\nimage generation. The size of the state-of-the-art models has increased\nsteadily reaching billions of parameters. These huge models are memory hungry\nand incur significant inference latency even on cutting edge AI-accelerators,\nsuch as GPUs. Specifically, the time and memory complexity of the attention\noperation is quadratic in terms of the total context length, i.e., prompt and\noutput tokens. Thus, several optimizations such as key-value tensor caching and\nFlashAttention computation have been proposed to deliver the low latency\ndemands of applications relying on such large models. However, these techniques\ndo not cater to the computationally distinct nature of different phases during\ninference.\n  To that end, we propose LeanAttention, a scalable technique of computing\nself-attention for the token-generation phase (decode-phase) of decoder-only\ntransformer models. LeanAttention enables scaling the attention mechanism\nimplementation for the challenging case of long context lengths by re-designing\nthe execution flow for the decode-phase. We identify that the associative\nproperty of online softmax can be treated as a reduction operation thus\nallowing us to parallelize the attention computation over these large context\nlengths. We extend the \"stream-K\" style reduction of tiled calculation to\nself-attention to enable parallel computation resulting in an average of 2.6x\nattention execution speedup over FlashAttention-2 and up to 8.33x speedup for\n512k context lengths."
                },
                "authors": [
                    {
                        "name": "Rya Sanovar"
                    },
                    {
                        "name": "Srikant Bharadwaj"
                    },
                    {
                        "name": "Renee St. Amant"
                    },
                    {
                        "name": "Victor Rühle"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    }
                ],
                "author_detail": {
                    "name": "Saravan Rajmohan"
                },
                "author": "Saravan Rajmohan",
                "arxiv_comment": "13 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10480v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10480v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.07752v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.07752v2",
                "updated": "2025-01-13T17:34:22Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    17,
                    34,
                    22,
                    0,
                    13,
                    0
                ],
                "published": "2024-12-10T18:50:37Z",
                "published_parsed": [
                    2024,
                    12,
                    10,
                    18,
                    50,
                    37,
                    1,
                    345,
                    0
                ],
                "title": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlashRNN: Optimizing Traditional RNNs on Modern Hardware"
                },
                "summary": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While Transformers and other sequence-parallelizable neural network\narchitectures seem like the current state of the art in sequence modeling, they\nspecifically lack state-tracking capabilities. These are important for\ntime-series tasks and logical reasoning. Traditional RNNs like LSTMs and GRUs,\nas well as modern variants like sLSTM do have these capabilities at the cost of\nstrictly sequential processing. While this is often seen as a strong\nlimitation, we show how fast these networks can get with our\nhardware-optimization FlashRNN in Triton and CUDA, optimizing kernels to the\nregister level on modern GPUs. We extend traditional RNNs with a\nparallelization variant that processes multiple RNNs of smaller hidden state in\nparallel, similar to the head-wise processing in Transformers. To enable\nflexibility on different GPU variants, we introduce a new optimization\nframework for hardware-internal cache sizes, memory and compute handling. It\nmodels the hardware in a setting using polyhedral-like constraints, including\nthe notion of divisibility. This speeds up the solution process in our\nConstrINT library for general integer constraint satisfaction problems (integer\nCSPs). We show that our kernels can achieve 50x speed-ups over a vanilla\nPyTorch implementation and allow 40x larger hidden sizes compared to our Triton\nimplementation. Our open-source kernels and the optimization library are\nreleased here to boost research in the direction of state-tracking enabled RNNs\nand sequence modeling: \\url{https://github.com/NX-AI/flashrnn}"
                },
                "authors": [
                    {
                        "name": "Korbinian Pöppel"
                    },
                    {
                        "name": "Maximilian Beck"
                    },
                    {
                        "name": "Sepp Hochreiter"
                    }
                ],
                "author_detail": {
                    "name": "Sepp Hochreiter"
                },
                "author": "Sepp Hochreiter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.07752v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.07752v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07533v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07533v4",
                "updated": "2025-01-13T09:33:25Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    9,
                    33,
                    25,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-13T08:03:32Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    8,
                    3,
                    32,
                    0,
                    134,
                    0
                ],
                "title": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DID Link: Authentication in TLS with Decentralized Identifiers and\n  Verifiable Credentials"
                },
                "summary": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Authentication in TLS is predominately carried out with X.509 digital\ncertificates issued by certificate authorities (CA). The centralized nature of\ncurrent public key infrastructures, however, comes along with severe risks,\nsuch as single points of failure and susceptibility to cyber-attacks,\npotentially undermining the security and trustworthiness of the entire system.\nWith Decentralized Identifiers (DID) alongside distributed ledger technology,\nit becomes technically feasible to prove ownership of a unique identifier\nwithout requiring an attestation of the proof's public key by a centralized and\ntherefore vulnerable CA. This article presents DID Link, a novel authentication\nscheme for TLS 1.3 that empowers entities to authenticate in a TLS-compliant\nway with self-issued X.509 certificates that are equipped with ledger-anchored\nDIDs instead of CA-issued identifiers. It facilitates the exchange of\ntamper-proof and 3rd-party attested claims in the form of DID-bound Verifiable\nCredentials after the TLS handshake to complete the authentication with a full\nidentification of the communication partner. A prototypical implementation\nshows comparable TLS handshake durations of DID Link if verification material\nis cached and reasonable prolongations if it is obtained from a ledger. The\nsignificant speed improvement of the resulting TLS channel over a widely used,\nDID-based alternative transport protocol on the application layer demonstrates\nthe potential of DID Link to become a viable solution for the establishment of\nsecure and trustful end-to-end communication links with decentrally managed\ndigital identities."
                },
                "authors": [
                    {
                        "name": "Sandro Rodriguez Garzon"
                    },
                    {
                        "name": "Dennis Natusch"
                    },
                    {
                        "name": "Artur Philipp"
                    },
                    {
                        "name": "Axel Küpper"
                    },
                    {
                        "name": "Hans Joachim Einsiedler"
                    },
                    {
                        "name": "Daniela Schneider"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Schneider"
                },
                "author": "Daniela Schneider",
                "arxiv_doi": "10.1109/PST62714.2024.10788053",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/PST62714.2024.10788053",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07533v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07533v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted by and presented at 21st Annual International Conference on\n  Privacy, Security, and Trust (PST2024)",
                "arxiv_journal_ref": "2024 21st Annual International Conference on Privacy, Security and\n  Trust (PST), 2024, pp. 1-11",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07056v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07056v1",
                "updated": "2025-01-13T04:31:04Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "published": "2025-01-13T04:31:04Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    4,
                    31,
                    4,
                    0,
                    13,
                    0
                ],
                "title": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generating Data Locality to Accelerate Sparse Matrix-Matrix\n  Multiplication on CPUs"
                },
                "summary": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse GEneral Matrix-matrix Multiplication (SpGEMM) is a critical operation\nin many applications. Current multithreaded implementations are based on\nGustavson's algorithm and often perform poorly on large matrices due to limited\ncache reuse by the accumulators. We present MAGNUS (Matrix Algebra for Gigantic\nNUmerical Systems), a novel algorithm to maximize data locality in SpGEMM. To\ngenerate locality, MAGNUS reorders the intermediate product into discrete\ncache-friendly chunks using a two-level hierarchical approach. The accumulator\nis applied to each chunk, where the chunk size is chosen such that the\naccumulator is cache-efficient. MAGNUS is input- and system-aware: based on the\nmatrix characteristics and target system specifications, the optimal number of\nchunks is computed by minimizing the storage cost of the necessary data\nstructures. MAGNUS allows for a hybrid accumulation strategy in which each\nchunk uses a different accumulator based on an input threshold. We consider two\naccumulators: an AVX-512 vectorized bitonic sorting algorithm and classical\ndense accumulation. An OpenMP implementation of MAGNUS is compared with several\nbaselines for a variety of different matrices on three Intel x86 architectures.\nFor matrices from the SuiteSparse collection, MAGNUS is faster than all the\nbaselines in most cases and is orders of magnitude faster than Intel MKL for\nseveral matrices. For massive random matrices that model social network graphs,\nMAGNUS scales to the largest matrix sizes, while the baselines fail to do so.\nFurthermore, MAGNUS is close to the optimal bound for these matrices,\nregardless of the matrix size, structure, and density."
                },
                "authors": [
                    {
                        "name": "Jordi Wolfson-Pou"
                    },
                    {
                        "name": "Jan Laukemann"
                    },
                    {
                        "name": "Fabrizio Petrini"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Petrini"
                },
                "author": "Fabrizio Petrini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07056v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07056v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.03058v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.03058v5",
                "updated": "2025-01-13T03:11:28Z",
                "updated_parsed": [
                    2025,
                    1,
                    13,
                    3,
                    11,
                    28,
                    0,
                    13,
                    0
                ],
                "published": "2024-05-05T21:41:43Z",
                "published_parsed": [
                    2024,
                    5,
                    5,
                    21,
                    41,
                    43,
                    6,
                    126,
                    0
                ],
                "title": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified Framework for Automated Code Transformation and Pragma\n  Insertion"
                },
                "summary": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-level synthesis, source-to-source compilers, and various Design Space\nExploration techniques for pragma insertion have significantly improved the\nQuality of Results of generated designs. These tools offer benefits such as\nreduced development time and enhanced performance. However, achieving\nhigh-quality results often requires additional manual code transformations and\ntiling selections, which are typically performed separately or as\npre-processing steps. Although DSE techniques enable code transformation\nupfront, the vastness of the search space often limits the exploration of all\npossible code transformations, making it challenging to determine which\ntransformations are necessary. Additionally, ensuring correctness remains\nchallenging, especially for complex transformations and optimizations.\n  To tackle this obstacle, we first propose a comprehensive framework\nleveraging HLS compilers. Our system streamlines code transformation, pragma\ninsertion, and tiles size selection for on-chip data caching through a unified\noptimization problem, aiming to enhance parallelization, particularly\nbeneficial for computation-bound kernels. Them employing a novel Non-Linear\nProgramming (NLP) approach, we simultaneously ascertain transformations,\npragmas, and tile sizes, focusing on regular loop-based kernels. Our evaluation\ndemonstrates that our framework adeptly identifies the appropriate\ntransformations, including scenarios where no transformation is necessary, and\ninserts pragmas to achieve a favorable Quality of Results."
                },
                "authors": [
                    {
                        "name": "Stéphane Pouget"
                    },
                    {
                        "name": "Louis-Noël Pouchet"
                    },
                    {
                        "name": "Jason Cong"
                    }
                ],
                "author_detail": {
                    "name": "Jason Cong"
                },
                "author": "Jason Cong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.03058v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.03058v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.14723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14723v2",
                "updated": "2025-02-03T18:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    57,
                    5,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-24T18:58:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    58,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering"
                },
                "summary": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys."
                },
                "authors": [
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Bradley Brown"
                    },
                    {
                        "name": "Jordan Juravsky"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18626v2",
                "updated": "2025-02-03T18:19:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    19,
                    4,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-27T12:48:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    48,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt\n  Adversarial Attacks on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt\n  Adversarial Attacks on LLMs"
                },
                "summary": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11004v2",
                "updated": "2025-02-03T18:17:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    17,
                    53,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-25T17:58:26Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    58,
                    26,
                    1,
                    177,
                    0
                ],
                "title": "The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators"
                },
                "summary": "Large pretrained models can be used as annotators, helping replace or augment\ncrowdworkers and enabling distilling generalist models into smaller specialist\nmodels. Unfortunately, this comes at a cost: employing top-of-the-line models\noften requires paying thousands of dollars for API calls, while the resulting\ndatasets are static and challenging to audit. To address these challenges, we\npropose a simple alternative: rather than directly querying labels from\npretrained models, we task models to generate programs that can produce labels.\nThese programs can be stored and applied locally, re-used and extended, and\ncost orders of magnitude less. Our system, Alchemist, obtains comparable to or\nbetter performance than large language model-based annotation in a range of\ntasks for a fraction of the cost: on average, improvements amount to a 12.9%\nenhancement while the total labeling costs across all datasets are reduced by a\nfactor of approximately 500x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pretrained models can be used as annotators, helping replace or augment\ncrowdworkers and enabling distilling generalist models into smaller specialist\nmodels. Unfortunately, this comes at a cost: employing top-of-the-line models\noften requires paying thousands of dollars for API calls, while the resulting\ndatasets are static and challenging to audit. To address these challenges, we\npropose a simple alternative: rather than directly querying labels from\npretrained models, we task models to generate programs that can produce labels.\nThese programs can be stored and applied locally, re-used and extended, and\ncost orders of magnitude less. Our system, Alchemist, obtains comparable to or\nbetter performance than large language model-based annotation in a range of\ntasks for a fraction of the cost: on average, improvements amount to a 12.9%\nenhancement while the total labeling costs across all datasets are reduced by a\nfactor of approximately 500x."
                },
                "authors": [
                    {
                        "name": "Tzu-Heng Huang"
                    },
                    {
                        "name": "Catherine Cao"
                    },
                    {
                        "name": "Vaishnavi Bhargava"
                    },
                    {
                        "name": "Frederic Sala"
                    }
                ],
                "author_detail": {
                    "name": "Frederic Sala"
                },
                "author": "Frederic Sala",
                "arxiv_comment": "NeurIPS 2024 Spotlight Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14515v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14515v2",
                "updated": "2025-02-03T18:10:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    10,
                    18,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-18T14:54:40Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    14,
                    54,
                    40,
                    4,
                    292,
                    0
                ],
                "title": "Efficient Annotator Reliability Assessment and Sample Weighting for\n  Knowledge-Based Misinformation Detection on Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Annotator Reliability Assessment and Sample Weighting for\n  Knowledge-Based Misinformation Detection on Social Media"
                },
                "summary": "Misinformation spreads rapidly on social media, confusing the truth and\ntargeting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Misinformation spreads rapidly on social media, confusing the truth and\ntargeting potentially vulnerable people. To effectively mitigate the negative\nimpact of misinformation, it must first be accurately detected before applying\na mitigation strategy, such as X's community notes, which is currently a manual\nprocess. This study takes a knowledge-based approach to misinformation\ndetection, modelling the problem similarly to one of natural language\ninference. The EffiARA annotation framework is introduced, aiming to utilise\ninter- and intra-annotator agreement to understand the reliability of each\nannotator and influence the training of large language models for\nclassification based on annotator reliability. In assessing the EffiARA\nannotation framework, the Russo-Ukrainian Conflict Knowledge-Based\nMisinformation Classification Dataset (RUC-MCD) was developed and made publicly\navailable. This study finds that sample weighting using annotator reliability\nperforms the best, utilising both inter- and intra-annotator agreement and\nsoft-label training. The highest classification performance achieved using\nLlama-3.2-1B was a macro-F1 of 0.757 and 0.740 using TwHIN-BERT-large."
                },
                "authors": [
                    {
                        "name": "Owen Cook"
                    },
                    {
                        "name": "Charlie Grimshaw"
                    },
                    {
                        "name": "Ben Wu"
                    },
                    {
                        "name": "Sophie Dillon"
                    },
                    {
                        "name": "Jack Hicks"
                    },
                    {
                        "name": "Luke Jones"
                    },
                    {
                        "name": "Thomas Smith"
                    },
                    {
                        "name": "Matyas Szert"
                    },
                    {
                        "name": "Xingyi Song"
                    }
                ],
                "author_detail": {
                    "name": "Xingyi Song"
                },
                "author": "Xingyi Song",
                "arxiv_comment": "8 pages, 3 figures, 3 tables. Code available here:\n  https://github.com/MiniEggz/ruc-misinfo; annotation framework available here:\n  https://github.com/MiniEggz/EffiARA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14515v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14515v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13843v2",
                "updated": "2025-02-03T18:06:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    6,
                    34,
                    0,
                    34,
                    0
                ],
                "published": "2024-09-20T18:34:38Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    18,
                    34,
                    38,
                    4,
                    264,
                    0
                ],
                "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on\n  Offensive Progressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STOP! Benchmarking Large Language Models with Sensitivity Testing on\n  Offensive Progressions"
                },
                "summary": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has\nbecome a critical focus in the field of natural language processing. However,\nmany current methodologies evaluate scenarios in isolation, without considering\nthe broader context or the spectrum of potential biases within each situation.\nTo address this, we introduce the Sensitivity Testing on Offensive Progressions\n(STOP) dataset, which includes 450 offensive progressions containing 2,700\nunique sentences of varying severity that progressively escalate from less to\nmore explicitly offensive. Covering a broad spectrum of 9 demographics and 46\nsub-demographics, STOP ensures inclusivity and comprehensive coverage. We\nevaluate several leading closed- and open-source models, including GPT-4,\nMixtral, and Llama 3. Our findings reveal that even the best-performing models\ndetect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We\nalso demonstrate how aligning models with human judgments on STOP can improve\nmodel answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs\nby up to 191%, while maintaining or even improving performance. STOP presents a\nnovel framework for assessing the complex nature of biases in LLMs, which will\nenable more effective bias mitigation strategies and facilitates the creation\nof fairer language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has\nbecome a critical focus in the field of natural language processing. However,\nmany current methodologies evaluate scenarios in isolation, without considering\nthe broader context or the spectrum of potential biases within each situation.\nTo address this, we introduce the Sensitivity Testing on Offensive Progressions\n(STOP) dataset, which includes 450 offensive progressions containing 2,700\nunique sentences of varying severity that progressively escalate from less to\nmore explicitly offensive. Covering a broad spectrum of 9 demographics and 46\nsub-demographics, STOP ensures inclusivity and comprehensive coverage. We\nevaluate several leading closed- and open-source models, including GPT-4,\nMixtral, and Llama 3. Our findings reveal that even the best-performing models\ndetect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We\nalso demonstrate how aligning models with human judgments on STOP can improve\nmodel answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs\nby up to 191%, while maintaining or even improving performance. STOP presents a\nnovel framework for assessing the complex nature of biases in LLMs, which will\nenable more effective bias mitigation strategies and facilitates the creation\nof fairer language models."
                },
                "authors": [
                    {
                        "name": "Robert Morabito"
                    },
                    {
                        "name": "Sangmitra Madhusudan"
                    },
                    {
                        "name": "Tyler McDonald"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_doi": "10.18653/v1/2024.emnlp-main.243",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.emnlp-main.243",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.13843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages (excluding references), accepted to EMNLP 2024 Main\n  Conference",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19377v2",
                "updated": "2025-02-03T17:35:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    17,
                    35,
                    35,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T18:30:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    30,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions"
                },
                "summary": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline."
                },
                "authors": [
                    {
                        "name": "Dominik Wagner"
                    },
                    {
                        "name": "Alexander Churchill"
                    },
                    {
                        "name": "Siddharth Sigtia"
                    },
                    {
                        "name": "Erik Marchi"
                    }
                ],
                "author_detail": {
                    "name": "Erik Marchi"
                },
                "author": "Erik Marchi",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10988v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10988v2",
                "updated": "2025-02-03T17:27:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    17,
                    27,
                    27,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-14T18:15:54Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    18,
                    15,
                    54,
                    0,
                    288,
                    0
                ],
                "title": "The evolution of [OIII]$+\\rm{H}β$ equivalent width from\n  $\\mathbf{z\\simeq3-8}$: implications for the production and escape of ionizing\n  photons during reionization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The evolution of [OIII]$+\\rm{H}β$ equivalent width from\n  $\\mathbf{z\\simeq3-8}$: implications for the production and escape of ionizing\n  photons during reionization"
                },
                "summary": "Accurately quantifying the ionizing photon production efficiency\n($\\xi_\\rm{ion}$) of $z>6$ star-forming galaxies (SFGs) is necessary to\nunderstand their contribution to reionization. We investigate the ionizing\nproperties of N=279 SFGs selected at $z=6.9-7.6$ from the JWST Cycle-1 imaging\nprogrammes; PRIMER and JADES. We use BAGPIPES to consistently infer the\nequivalent widths of their [OIII]+$\\rm{H\\beta}$ emission lines ($W_\\lambda$)\nand their physical properties. To supplement our high-redshift galaxies, we\nmeasure $W_\\lambda$ photometrically for a sample of N=253 $z=3.2-3.6$ SFGs\nselected from the VANDELS spectroscopic survey. Comparing these samples, we\nfind a strong apparent redshift evolution in their median $W_\\lambda$,\nincreasing from $W_\\lambda =310\\pm25\\,\\r{A}$ in VANDELS to $W_\\lambda\n=540\\pm25\\,\\r{A}$ in our JWST-based sample. In the JWST sample at $z>7$, we\nfind that $W_\\lambda$ correlates with both stellar mass and UV luminosity, with\nhigh-mass, $M_{ UV}-$faint galaxies producing systematically weaker emission\nlines. Moreover, we discover a departure from the standard log-normal shape of\nthe $W_\\lambda$ distribution, characterised by a more pronounced tail at lower\n$W_\\lambda$, consistent with increasingly bursty star formation. Using\n$W_\\lambda$ as a proxy for $\\xi_\\rm{ion}$, and UV spectral slope as a proxy for\nLyC escape fraction ($f_\\rm{esc}$), we find a minority of galaxies with high\n$\\xi_\\rm{ion}$ and $f_\\rm{esc}$ (e.g., $\\rm{log(\\xi_{ion}/erg^{-1}Hz})\\sim25.6$\nand $f_\\rm{esc}\\sim0.15$). However, we find that the LyC photon budget at $z>7$\nis dominated by galaxies with more moderate output, close to the sample median\nof $\\rm{log(\\xi_{ion}/erg^{-1}Hz})\\sim25.3$ and $f_\\rm{esc}\\sim0.05$. This is\nconsistent with estimates for the number of LyC photons required to power\nreionization at $z>7$, with no evidence for over or under-production.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately quantifying the ionizing photon production efficiency\n($\\xi_\\rm{ion}$) of $z>6$ star-forming galaxies (SFGs) is necessary to\nunderstand their contribution to reionization. We investigate the ionizing\nproperties of N=279 SFGs selected at $z=6.9-7.6$ from the JWST Cycle-1 imaging\nprogrammes; PRIMER and JADES. We use BAGPIPES to consistently infer the\nequivalent widths of their [OIII]+$\\rm{H\\beta}$ emission lines ($W_\\lambda$)\nand their physical properties. To supplement our high-redshift galaxies, we\nmeasure $W_\\lambda$ photometrically for a sample of N=253 $z=3.2-3.6$ SFGs\nselected from the VANDELS spectroscopic survey. Comparing these samples, we\nfind a strong apparent redshift evolution in their median $W_\\lambda$,\nincreasing from $W_\\lambda =310\\pm25\\,\\r{A}$ in VANDELS to $W_\\lambda\n=540\\pm25\\,\\r{A}$ in our JWST-based sample. In the JWST sample at $z>7$, we\nfind that $W_\\lambda$ correlates with both stellar mass and UV luminosity, with\nhigh-mass, $M_{ UV}-$faint galaxies producing systematically weaker emission\nlines. Moreover, we discover a departure from the standard log-normal shape of\nthe $W_\\lambda$ distribution, characterised by a more pronounced tail at lower\n$W_\\lambda$, consistent with increasingly bursty star formation. Using\n$W_\\lambda$ as a proxy for $\\xi_\\rm{ion}$, and UV spectral slope as a proxy for\nLyC escape fraction ($f_\\rm{esc}$), we find a minority of galaxies with high\n$\\xi_\\rm{ion}$ and $f_\\rm{esc}$ (e.g., $\\rm{log(\\xi_{ion}/erg^{-1}Hz})\\sim25.6$\nand $f_\\rm{esc}\\sim0.15$). However, we find that the LyC photon budget at $z>7$\nis dominated by galaxies with more moderate output, close to the sample median\nof $\\rm{log(\\xi_{ion}/erg^{-1}Hz})\\sim25.3$ and $f_\\rm{esc}\\sim0.05$. This is\nconsistent with estimates for the number of LyC photons required to power\nreionization at $z>7$, with no evidence for over or under-production."
                },
                "authors": [
                    {
                        "name": "R. Begley"
                    },
                    {
                        "name": "R. J. McLure"
                    },
                    {
                        "name": "F. Cullen"
                    },
                    {
                        "name": "D. J. McLeod"
                    },
                    {
                        "name": "J. S. Dunlop"
                    },
                    {
                        "name": "A. C. Carnall"
                    },
                    {
                        "name": "T. M. Stanton"
                    },
                    {
                        "name": "A. E. Shapley"
                    },
                    {
                        "name": "R. Cochrane"
                    },
                    {
                        "name": "C. T. Donnan"
                    },
                    {
                        "name": "R. S. Ellis"
                    },
                    {
                        "name": "A. Fontana"
                    },
                    {
                        "name": "N. A. Grogin"
                    },
                    {
                        "name": "A. M. Koekemoer"
                    }
                ],
                "author_detail": {
                    "name": "A. M. Koekemoer"
                },
                "author": "A. M. Koekemoer",
                "arxiv_comment": "18 pages, 9 figures + 1 appendix. Accepted to MNRAS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10988v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10988v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.07175v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.07175v3",
                "updated": "2025-02-03T16:49:31Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    49,
                    31,
                    0,
                    34,
                    0
                ],
                "published": "2024-08-13T19:37:46Z",
                "published_parsed": [
                    2024,
                    8,
                    13,
                    19,
                    37,
                    46,
                    1,
                    226,
                    0
                ],
                "title": "Evolving Dark Energy or Supernovae Systematics?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Evolving Dark Energy or Supernovae Systematics?"
                },
                "summary": "Recent results from the Dark Energy Spectroscopic Instrument (DESI)\ncollaboration have been interpreted as evidence for evolving dark energy.\nHowever, this interpretation is strongly dependent on which Type Ia supernova\n(SN) sample is combined with DESI measurements of baryon acoustic oscillations\n(BAO) and observations of the cosmic microwave background (CMB) radiation. The\nstrength of the evidence for evolving dark energy ranges from ~3.9 sigma for\nthe Dark Energy 5 year (DES5Y) SN sample to ~2.5 sigma for the Pantheon+\nsample. The cosmology inferred from Pantheon+ sample alone is consistent with\nthe Planck LCDM model and shows no preference for evolving dark energy. In\ncontrast, the the DES5Y SN sample favours evolving dark energy and is\ndiscrepant with the Planck LCDM model at about the 3 sigma level. Given these\ndifference, it is important to question whether they are caused by systematics\nin the SN compilations. A comparison of SN common to both the DES5Y and\nPantheon+ compilations shows evidence for an offset of ~0.04 mag. between low\nand high redshifts. Systematics of this order can bring the DES5Y sample into\ngood agreement with the Planck LCDM cosmology and Pantheon+. I comment on a\nrecent paper by the DES collaboration that rejects this possibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent results from the Dark Energy Spectroscopic Instrument (DESI)\ncollaboration have been interpreted as evidence for evolving dark energy.\nHowever, this interpretation is strongly dependent on which Type Ia supernova\n(SN) sample is combined with DESI measurements of baryon acoustic oscillations\n(BAO) and observations of the cosmic microwave background (CMB) radiation. The\nstrength of the evidence for evolving dark energy ranges from ~3.9 sigma for\nthe Dark Energy 5 year (DES5Y) SN sample to ~2.5 sigma for the Pantheon+\nsample. The cosmology inferred from Pantheon+ sample alone is consistent with\nthe Planck LCDM model and shows no preference for evolving dark energy. In\ncontrast, the the DES5Y SN sample favours evolving dark energy and is\ndiscrepant with the Planck LCDM model at about the 3 sigma level. Given these\ndifference, it is important to question whether they are caused by systematics\nin the SN compilations. A comparison of SN common to both the DES5Y and\nPantheon+ compilations shows evidence for an offset of ~0.04 mag. between low\nand high redshifts. Systematics of this order can bring the DES5Y sample into\ngood agreement with the Planck LCDM cosmology and Pantheon+. I comment on a\nrecent paper by the DES collaboration that rejects this possibility."
                },
                "authors": [
                    {
                        "name": "George Efstathiou"
                    }
                ],
                "author_detail": {
                    "name": "George Efstathiou"
                },
                "author": "George Efstathiou",
                "arxiv_comment": "7pages 7 figures. Expanded discussion of bias corrections",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.07175v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.07175v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00397v2",
                "updated": "2025-02-03T16:40:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    40,
                    43,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-29T10:50:23Z",
                "published_parsed": [
                    2024,
                    6,
                    29,
                    10,
                    50,
                    23,
                    5,
                    181,
                    0
                ],
                "title": "Learning Time-Varying Multi-Region Communications via Scalable Markovian\n  Gaussian Processes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Time-Varying Multi-Region Communications via Scalable Markovian\n  Gaussian Processes"
                },
                "summary": "Understanding and constructing brain communications that capture dynamic\ncommunications across multiple regions is fundamental to modern system\nneuroscience, yet current methods struggle to find time-varying region-level\ncommunications or scale to large neural datasets with long recording durations.\nWe present a novel framework using Markovian Gaussian Processes to learn brain\ncommunications with time-varying temporal delays from multi-region neural\nrecordings, named Adaptive Delay Model (ADM). Our method combines Gaussian\nProcesses with State Space Models and employs parallel scan inference\nalgorithms, enabling efficient scaling to large datasets while identifying\nconcurrent communication patterns that evolve over time. This time-varying\napproach captures how brain region interactions shift dynamically during\ncognitive processes. Validated on synthetic and multi-region neural recordings\ndatasets, our approach discovers both the directionality and temporal dynamics\nof neural communication. This work advances our understanding of distributed\nneural computation and provides a scalable tool for analyzing dynamic brain\nnetworks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding and constructing brain communications that capture dynamic\ncommunications across multiple regions is fundamental to modern system\nneuroscience, yet current methods struggle to find time-varying region-level\ncommunications or scale to large neural datasets with long recording durations.\nWe present a novel framework using Markovian Gaussian Processes to learn brain\ncommunications with time-varying temporal delays from multi-region neural\nrecordings, named Adaptive Delay Model (ADM). Our method combines Gaussian\nProcesses with State Space Models and employs parallel scan inference\nalgorithms, enabling efficient scaling to large datasets while identifying\nconcurrent communication patterns that evolve over time. This time-varying\napproach captures how brain region interactions shift dynamically during\ncognitive processes. Validated on synthetic and multi-region neural recordings\ndatasets, our approach discovers both the directionality and temporal dynamics\nof neural communication. This work advances our understanding of distributed\nneural computation and provides a scalable tool for analyzing dynamic brain\nnetworks."
                },
                "authors": [
                    {
                        "name": "Weihan Li"
                    },
                    {
                        "name": "Yule Wang"
                    },
                    {
                        "name": "Chengrui Li"
                    },
                    {
                        "name": "Anqi Wu"
                    }
                ],
                "author_detail": {
                    "name": "Anqi Wu"
                },
                "author": "Anqi Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07773v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07773v2",
                "updated": "2025-02-03T16:33:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    33,
                    20,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-14T01:08:15Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    1,
                    8,
                    15,
                    1,
                    14,
                    0
                ],
                "title": "Symmetry-Aware Generative Modeling through Learned Canonicalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Symmetry-Aware Generative Modeling through Learned Canonicalization"
                },
                "summary": "Generative modeling of symmetric densities has a range of applications in AI\nfor science, from drug discovery to physics simulations. The existing\ngenerative modeling paradigm for invariant densities combines an invariant\nprior with an equivariant generative process. However, we observe that this\ntechnique is not necessary and has several drawbacks resulting from the\nlimitations of equivariant networks. Instead, we propose to model a learned\nslice of the density so that only one representative element per orbit is\nlearned. To accomplish this, we learn a group-equivariant canonicalization\nnetwork that maps training samples to a canonical pose and train a\nnon-equivariant generative model over these canonicalized samples. We implement\nthis idea in the context of diffusion models. Our preliminary experimental\nresults on molecular modeling are promising, demonstrating improved sample\nquality and faster inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative modeling of symmetric densities has a range of applications in AI\nfor science, from drug discovery to physics simulations. The existing\ngenerative modeling paradigm for invariant densities combines an invariant\nprior with an equivariant generative process. However, we observe that this\ntechnique is not necessary and has several drawbacks resulting from the\nlimitations of equivariant networks. Instead, we propose to model a learned\nslice of the density so that only one representative element per orbit is\nlearned. To accomplish this, we learn a group-equivariant canonicalization\nnetwork that maps training samples to a canonical pose and train a\nnon-equivariant generative model over these canonicalized samples. We implement\nthis idea in the context of diffusion models. Our preliminary experimental\nresults on molecular modeling are promising, demonstrating improved sample\nquality and faster inference time."
                },
                "authors": [
                    {
                        "name": "Kusha Sareen"
                    },
                    {
                        "name": "Daniel Levy"
                    },
                    {
                        "name": "Arnab Kumar Mondal"
                    },
                    {
                        "name": "Sékou-Oumar Kaba"
                    },
                    {
                        "name": "Tara Akhound-Sadegh"
                    },
                    {
                        "name": "Siamak Ravanbakhsh"
                    }
                ],
                "author_detail": {
                    "name": "Siamak Ravanbakhsh"
                },
                "author": "Siamak Ravanbakhsh",
                "arxiv_comment": "NeurReps 2024 Workshop Version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07773v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07773v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14528v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14528v2",
                "updated": "2025-02-03T16:26:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    26,
                    20,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-20T17:40:18Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    17,
                    40,
                    18,
                    3,
                    172,
                    0
                ],
                "title": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeciMamba: Exploring the Length Extrapolation Potential of Mamba"
                },
                "summary": "Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In\nthis paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this\nconstraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context\nlengths that are significantly longer than the ones seen during training, while\nenjoying faster inference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-range sequence processing poses a significant challenge for Transformers\ndue to their quadratic complexity in input length. A promising alternative is\nMamba, which demonstrates high performance and achieves Transformer-level\ncapabilities while requiring substantially fewer computational resources. In\nthis paper we explore the length-generalization capabilities of Mamba, which we\nfind to be relatively limited. Through a series of visualizations and analyses\nwe identify that the limitations arise from a restricted effective receptive\nfield, dictated by the sequence length used during training. To address this\nconstraint, we introduce DeciMamba, a context-extension method specifically\ndesigned for Mamba. This mechanism, built on top of a hidden filtering\nmechanism embedded within the S6 layer, enables the trained model to\nextrapolate well even without additional training. Empirical experiments over\nreal-world long-range NLP tasks show that DeciMamba can extrapolate to context\nlengths that are significantly longer than the ones seen during training, while\nenjoying faster inference."
                },
                "authors": [
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Itamar Zimerman"
                    },
                    {
                        "name": "Shady Abu-Hussein"
                    },
                    {
                        "name": "Nadav Cohen"
                    },
                    {
                        "name": "Amir Globerson"
                    },
                    {
                        "name": "Lior Wolf"
                    },
                    {
                        "name": "Raja Giryes"
                    }
                ],
                "author_detail": {
                    "name": "Raja Giryes"
                },
                "author": "Raja Giryes",
                "arxiv_comment": "Official Implementation: https://github.com/assafbk/DeciMamba",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14528v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14528v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08959v2",
                "updated": "2025-02-03T16:03:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    3,
                    18,
                    0,
                    34,
                    0
                ],
                "published": "2024-08-16T18:07:48Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    18,
                    7,
                    48,
                    4,
                    229,
                    0
                ],
                "title": "Trust-Oriented Adaptive Guardrails for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust-Oriented Adaptive Guardrails for Large Language Models"
                },
                "summary": "Guardrail, an emerging mechanism designed to ensure that large language\nmodels (LLMs) align with human values by moderating harmful or toxic responses,\nrequires a sociotechnical approach in their design. This paper addresses a\ncritical issue: existing guardrails lack a well-founded methodology to\naccommodate the diverse needs of different user groups, particularly concerning\naccess rights. Supported by trust modeling (primarily on `social' aspect) and\nenhanced with online in-context learning via retrieval-augmented generation (on\n`technical' aspect), we introduce an adaptive guardrail mechanism, to\ndynamically moderate access to sensitive content based on user trust metrics.\nUser trust metrics, defined as a novel combination of direct interaction trust\nand authority-verified trust, enable the system to precisely tailor the\nstrictness of content moderation by aligning with the user's credibility and\nthe specific context of their inquiries. Our empirical evaluation demonstrates\nthe effectiveness of the adaptive guardrail in meeting diverse user needs,\noutperforming existing guardrails while securing sensitive information and\nprecisely managing potentially hazardous content through a context-aware\nknowledge base. To the best of our knowledge, this work is the first to\nintroduce trust-oriented concept into a guardrail system, offering a scalable\nsolution that enriches the discourse on ethical deployment for next-generation\nLLM service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guardrail, an emerging mechanism designed to ensure that large language\nmodels (LLMs) align with human values by moderating harmful or toxic responses,\nrequires a sociotechnical approach in their design. This paper addresses a\ncritical issue: existing guardrails lack a well-founded methodology to\naccommodate the diverse needs of different user groups, particularly concerning\naccess rights. Supported by trust modeling (primarily on `social' aspect) and\nenhanced with online in-context learning via retrieval-augmented generation (on\n`technical' aspect), we introduce an adaptive guardrail mechanism, to\ndynamically moderate access to sensitive content based on user trust metrics.\nUser trust metrics, defined as a novel combination of direct interaction trust\nand authority-verified trust, enable the system to precisely tailor the\nstrictness of content moderation by aligning with the user's credibility and\nthe specific context of their inquiries. Our empirical evaluation demonstrates\nthe effectiveness of the adaptive guardrail in meeting diverse user needs,\noutperforming existing guardrails while securing sensitive information and\nprecisely managing potentially hazardous content through a context-aware\nknowledge base. To the best of our knowledge, this work is the first to\nintroduce trust-oriented concept into a guardrail system, offering a scalable\nsolution that enriches the discourse on ethical deployment for next-generation\nLLM service."
                },
                "authors": [
                    {
                        "name": "Jinwei Hu"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13846v2",
                "updated": "2025-02-03T16:03:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    3,
                    13,
                    0,
                    34,
                    0
                ],
                "published": "2024-02-21T14:44:00Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    14,
                    44,
                    0,
                    2,
                    52,
                    0
                ],
                "title": "Large Language Models are Advanced Anonymizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Advanced Anonymizers"
                },
                "summary": "Recent privacy research on large language models (LLMs) has shown that they\nachieve near-human-level performance at inferring personal data from online\ntexts. With ever-increasing model capabilities, existing text anonymization\nmethods are currently lacking behind regulatory requirements and adversarial\nthreats. In this work, we take two steps to bridge this gap: First, we present\na new setting for evaluating anonymization in the face of adversarial LLM\ninferences, allowing for a natural measurement of anonymization performance\nwhile remedying some of the shortcomings of previous metrics. Then, within this\nsetting, we develop a novel LLM-based adversarial anonymization framework\nleveraging the strong inferential capabilities of LLMs to inform our\nanonymization procedure. We conduct a comprehensive experimental evaluation of\nadversarial anonymization across 13 LLMs on real-world and synthetic online\ntexts, comparing it against multiple baselines and industry-grade anonymizers.\nOur evaluation shows that adversarial anonymization outperforms current\ncommercial anonymizers both in terms of the resulting utility and privacy. We\nsupport our findings with a human study (n=50) highlighting a strong and\nconsistent human preference for LLM-anonymized texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent privacy research on large language models (LLMs) has shown that they\nachieve near-human-level performance at inferring personal data from online\ntexts. With ever-increasing model capabilities, existing text anonymization\nmethods are currently lacking behind regulatory requirements and adversarial\nthreats. In this work, we take two steps to bridge this gap: First, we present\na new setting for evaluating anonymization in the face of adversarial LLM\ninferences, allowing for a natural measurement of anonymization performance\nwhile remedying some of the shortcomings of previous metrics. Then, within this\nsetting, we develop a novel LLM-based adversarial anonymization framework\nleveraging the strong inferential capabilities of LLMs to inform our\nanonymization procedure. We conduct a comprehensive experimental evaluation of\nadversarial anonymization across 13 LLMs on real-world and synthetic online\ntexts, comparing it against multiple baselines and industry-grade anonymizers.\nOur evaluation shows that adversarial anonymization outperforms current\ncommercial anonymizers both in terms of the resulting utility and privacy. We\nsupport our findings with a human study (n=50) highlighting a strong and\nconsistent human preference for LLM-anonymized texts."
                },
                "authors": [
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "International Conference on Learning Representations (ICLR 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.04728v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.04728v2",
                "updated": "2025-02-03T15:51:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    51,
                    59,
                    0,
                    34,
                    0
                ],
                "published": "2024-11-07T14:08:35Z",
                "published_parsed": [
                    2024,
                    11,
                    7,
                    14,
                    8,
                    35,
                    3,
                    312,
                    0
                ],
                "title": "Neuromorphic Wireless Split Computing with Multi-Level Spikes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neuromorphic Wireless Split Computing with Multi-Level Spikes"
                },
                "summary": "Inspired by biological processes, neuromorphic computing leverages spiking\nneural networks (SNNs) to perform inference tasks, offering significant\nefficiency gains for workloads involving sequential data. Recent advances in\nhardware and software have shown that embedding a small payload within each\nspike exchanged between spiking neurons can enhance inference accuracy without\nincreasing energy consumption. To scale neuromorphic computing to larger\nworkloads, split computing - where an SNN is partitioned across two devices -\nis a promising solution. In such architectures, the device hosting the initial\nlayers must transmit information about the spikes generated by its output\nneurons to the second device. This establishes a trade-off between the benefits\nof multi-level spikes, which carry additional payload information, and the\ncommunication resources required for transmitting extra bits between devices.\nThis paper presents the first comprehensive study of a neuromorphic wireless\nsplit computing architecture that employs multi-level SNNs. We propose digital\nand analog modulation schemes for an orthogonal frequency division multiplexing\n(OFDM) radio interface to enable efficient communication. Simulation and\nexperimental results using software-defined radios reveal performance\nimprovements achieved by multi-level SNN models and provide insights into the\noptimal payload size as a function of the connection quality between the\ntransmitter and receiver.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inspired by biological processes, neuromorphic computing leverages spiking\nneural networks (SNNs) to perform inference tasks, offering significant\nefficiency gains for workloads involving sequential data. Recent advances in\nhardware and software have shown that embedding a small payload within each\nspike exchanged between spiking neurons can enhance inference accuracy without\nincreasing energy consumption. To scale neuromorphic computing to larger\nworkloads, split computing - where an SNN is partitioned across two devices -\nis a promising solution. In such architectures, the device hosting the initial\nlayers must transmit information about the spikes generated by its output\nneurons to the second device. This establishes a trade-off between the benefits\nof multi-level spikes, which carry additional payload information, and the\ncommunication resources required for transmitting extra bits between devices.\nThis paper presents the first comprehensive study of a neuromorphic wireless\nsplit computing architecture that employs multi-level SNNs. We propose digital\nand analog modulation schemes for an orthogonal frequency division multiplexing\n(OFDM) radio interface to enable efficient communication. Simulation and\nexperimental results using software-defined radios reveal performance\nimprovements achieved by multi-level SNN models and provide insights into the\noptimal payload size as a function of the connection quality between the\ntransmitter and receiver."
                },
                "authors": [
                    {
                        "name": "Dengyu Wu"
                    },
                    {
                        "name": "Jiechen Chen"
                    },
                    {
                        "name": "Bipin Rajendran"
                    },
                    {
                        "name": "H. Vincent Poor"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.04728v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.04728v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18463v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18463v2",
                "updated": "2025-02-03T15:51:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    51,
                    16,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-30T16:30:20Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    16,
                    30,
                    20,
                    3,
                    30,
                    0
                ],
                "title": "A Benchmark and Evaluation for Real-World Out-of-Distribution Detection\n  Using Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Benchmark and Evaluation for Real-World Out-of-Distribution Detection\n  Using Vision-Language Models"
                },
                "summary": "Out-of-distribution (OOD) detection is a task that detects OOD samples during\ninference to ensure the safety of deployed models. However, conventional\nbenchmarks have reached performance saturation, making it difficult to compare\nrecent OOD detection methods. To address this challenge, we introduce three\nnovel OOD detection benchmarks that enable a deeper understanding of method\ncharacteristics and reflect real-world conditions. First, we present\nImageNet-X, designed to evaluate performance under challenging semantic shifts.\nSecond, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing\nrobustness to covariate shifts (feature distribution shifts). Finally, we\npropose Wilds-FS-X, which extends these evaluations to real-world datasets,\noffering a more comprehensive testbed. Our experiments reveal that recent\nCLIP-based OOD detection methods struggle to varying degrees across the three\nproposed benchmarks, and none of them consistently outperforms the others. We\nhope the community goes beyond specific benchmarks and includes more\nchallenging conditions reflecting real-world scenarios. The code is\nhttps://github.com/hoshi23/OOD-X-Benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is a task that detects OOD samples during\ninference to ensure the safety of deployed models. However, conventional\nbenchmarks have reached performance saturation, making it difficult to compare\nrecent OOD detection methods. To address this challenge, we introduce three\nnovel OOD detection benchmarks that enable a deeper understanding of method\ncharacteristics and reflect real-world conditions. First, we present\nImageNet-X, designed to evaluate performance under challenging semantic shifts.\nSecond, we propose ImageNet-FS-X for full-spectrum OOD detection, assessing\nrobustness to covariate shifts (feature distribution shifts). Finally, we\npropose Wilds-FS-X, which extends these evaluations to real-world datasets,\noffering a more comprehensive testbed. Our experiments reveal that recent\nCLIP-based OOD detection methods struggle to varying degrees across the three\nproposed benchmarks, and none of them consistently outperforms the others. We\nhope the community goes beyond specific benchmarks and includes more\nchallenging conditions reflecting real-world scenarios. The code is\nhttps://github.com/hoshi23/OOD-X-Benchmarks."
                },
                "authors": [
                    {
                        "name": "Shiho Noda"
                    },
                    {
                        "name": "Atsuyuki Miyai"
                    },
                    {
                        "name": "Qing Yu"
                    },
                    {
                        "name": "Go Irie"
                    },
                    {
                        "name": "Kiyoharu Aizawa"
                    }
                ],
                "author_detail": {
                    "name": "Kiyoharu Aizawa"
                },
                "author": "Kiyoharu Aizawa",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18463v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18463v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00154v2",
                "updated": "2025-02-03T15:33:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    33,
                    59,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-31T18:59:46Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    59,
                    46,
                    3,
                    305,
                    0
                ],
                "title": "Scaling Up Membership Inference: When and How Attacks Succeed on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Membership Inference: When and How Attacks Succeed on Large\n  Language Models"
                },
                "summary": "Membership inference attacks (MIA) attempt to verify the membership of a\ngiven data sample in the training set for a model. MIA has become relevant in\nrecent years, following the rapid development of large language models (LLM).\nMany are concerned about the usage of copyrighted materials for training them\nand call for methods for detecting such usage. However, recent research has\nlargely concluded that current MIA methods do not work on LLMs. Even when they\nseem to work, it is usually because of the ill-designed experimental setup\nwhere other shortcut features enable \"cheating.\" In this work, we argue that\nMIA still works on LLMs, but only when multiple documents are presented for\ntesting. We construct new benchmarks that measure the MIA performances at a\ncontinuous scale of data samples, from sentences (n-grams) to a collection of\ndocuments (multiple chunks of tokens). To validate the efficacy of current MIA\napproaches at greater scales, we adapt a recent work on Dataset Inference (DI)\nfor the task of binary membership detection that aggregates paragraph-level MIA\nfeatures to enable MIA at document and collection of documents level. This\nbaseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIA) attempt to verify the membership of a\ngiven data sample in the training set for a model. MIA has become relevant in\nrecent years, following the rapid development of large language models (LLM).\nMany are concerned about the usage of copyrighted materials for training them\nand call for methods for detecting such usage. However, recent research has\nlargely concluded that current MIA methods do not work on LLMs. Even when they\nseem to work, it is usually because of the ill-designed experimental setup\nwhere other shortcut features enable \"cheating.\" In this work, we argue that\nMIA still works on LLMs, but only when multiple documents are presented for\ntesting. We construct new benchmarks that measure the MIA performances at a\ncontinuous scale of data samples, from sentences (n-grams) to a collection of\ndocuments (multiple chunks of tokens). To validate the efficacy of current MIA\napproaches at greater scales, we adapt a recent work on Dataset Inference (DI)\nfor the task of binary membership detection that aggregates paragraph-level MIA\nfeatures to enable MIA at document and collection of documents level. This\nbaseline achieves the first successful MIA on pre-trained and fine-tuned LLMs."
                },
                "authors": [
                    {
                        "name": "Haritz Puerto"
                    },
                    {
                        "name": "Martin Gubri"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Seong Joon Oh"
                    }
                ],
                "author_detail": {
                    "name": "Seong Joon Oh"
                },
                "author": "Seong Joon Oh",
                "arxiv_comment": "Findings of NAACL 2025. Our code is available at\n  https://github.com/parameterlab/mia-scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10342v2",
                "updated": "2025-02-03T15:23:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    23,
                    2,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-13T18:40:10Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    10,
                    4,
                    348,
                    0
                ],
                "title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining"
                },
                "summary": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks."
                },
                "authors": [
                    {
                        "name": "Zhiqi Ge"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Xinglei Pang"
                    },
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Wang Lin"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17115v2",
                "updated": "2025-02-03T15:18:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    18,
                    5,
                    0,
                    34,
                    0
                ],
                "published": "2024-07-24T09:24:49Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    9,
                    24,
                    49,
                    2,
                    206,
                    0
                ],
                "title": "Reinforced Prompt Personalization for Recommendation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Prompt Personalization for Recommendation with Large Language\n  Models"
                },
                "summary": "Designing effective prompts can empower LLMs to understand user preferences\nand provide recommendations with intent comprehension and knowledge utilization\ncapabilities. Nevertheless, recent studies predominantly concentrate on\ntask-wise prompting, developing fixed prompt templates shared across all users\nin a given recommendation task (e.g., rating or ranking). Although convenient,\ntask-wise prompting overlooks individual user differences, leading to\ninaccurate analysis of user interests. In this work, we introduce the concept\nof instance-wise prompting, aiming at personalizing discrete prompts for\nindividual users. Toward this end, we propose Reinforced Prompt Personalization\n(RPP) to realize it automatically. To improve efficiency and quality, RPP\npersonalizes prompts at the sentence level rather than searching in the vast\nvocabulary word-by-word. Specifically, RPP breaks down the prompt into four\npatterns, tailoring patterns based on multi-agent and combining them. Then the\npersonalized prompts interact with LLMs (environment) iteratively, to boost\nLLMs' recommending performance (reward). In addition to RPP, to improve the\nscalability of action space, our proposal of RPP+ dynamically refines the\nselected actions with LLMs throughout the iterative process. Extensive\nexperiments on various datasets demonstrate the superiority of RPP/RPP+ over\ntraditional recommender models, few-shot methods, and other prompt-based\nmethods, underscoring the significance of instance-wise prompting in LLMs for\nrecommendation. Our code is available at https://github.com/maowenyu-11/RPP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing effective prompts can empower LLMs to understand user preferences\nand provide recommendations with intent comprehension and knowledge utilization\ncapabilities. Nevertheless, recent studies predominantly concentrate on\ntask-wise prompting, developing fixed prompt templates shared across all users\nin a given recommendation task (e.g., rating or ranking). Although convenient,\ntask-wise prompting overlooks individual user differences, leading to\ninaccurate analysis of user interests. In this work, we introduce the concept\nof instance-wise prompting, aiming at personalizing discrete prompts for\nindividual users. Toward this end, we propose Reinforced Prompt Personalization\n(RPP) to realize it automatically. To improve efficiency and quality, RPP\npersonalizes prompts at the sentence level rather than searching in the vast\nvocabulary word-by-word. Specifically, RPP breaks down the prompt into four\npatterns, tailoring patterns based on multi-agent and combining them. Then the\npersonalized prompts interact with LLMs (environment) iteratively, to boost\nLLMs' recommending performance (reward). In addition to RPP, to improve the\nscalability of action space, our proposal of RPP+ dynamically refines the\nselected actions with LLMs throughout the iterative process. Extensive\nexperiments on various datasets demonstrate the superiority of RPP/RPP+ over\ntraditional recommender models, few-shot methods, and other prompt-based\nmethods, underscoring the significance of instance-wise prompting in LLMs for\nrecommendation. Our code is available at https://github.com/maowenyu-11/RPP."
                },
                "authors": [
                    {
                        "name": "Wenyu Mao"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Weijian Chen"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.19717v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.19717v2",
                "updated": "2025-02-03T15:11:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    11,
                    48,
                    0,
                    34,
                    0
                ],
                "published": "2024-09-29T14:20:21Z",
                "published_parsed": [
                    2024,
                    9,
                    29,
                    14,
                    20,
                    21,
                    6,
                    273,
                    0
                ],
                "title": "Covariance Regression for High Dimensional Neural Data via Graph",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariance Regression for High Dimensional Neural Data via Graph"
                },
                "summary": "Modern recording techniques enable neuroscientists to simultaneously study\nneural activity across large populations of neurons, with capturing\npredictor-dependent correlations being a fundamental challenge in neuroscience.\nMoreover, the fact that input covariates often lie in restricted subdomains,\naccording to experimental settings, makes inference even more challenging. To\naddress these challenges, we propose a set of nonparametric mean-covariance\nregression models for high-dimensional neural activity with restricted inputs.\nThese models reduce the dimensionality of neural responses by employing a\nlower-dimensional latent factor model, where both factor loadings and latent\nfactors are predictor-dependent, to jointly model mean and covariance across\ncovariates. The smoothness of neural activity across experimental conditions is\nmodeled nonparametrically using two Gaussian processes (GPs), applied to both\nloading basis and latent factors. Additionally, to account for the covariates\nlying in restricted subspace, we incorporate graph information into the\ncovariance structure. To flexibly infer the model, we use an MCMC algorithm to\nsample from posterior distributions. After validating and studying the\nproperties of proposed methods by simulations, we apply them to two neural\ndatasets (local field potential and neural spiking data) to demonstrate the\nusage of models for continuous and counting observations. Overall, the proposed\nmethods provide a framework to jointly model covariate-dependent mean and\ncovariance in high dimensional neural data, especially when the covariates lie\nin restricted domains. The framework is general and can be easily adapted to\nvarious applications beyond neuroscience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recording techniques enable neuroscientists to simultaneously study\nneural activity across large populations of neurons, with capturing\npredictor-dependent correlations being a fundamental challenge in neuroscience.\nMoreover, the fact that input covariates often lie in restricted subdomains,\naccording to experimental settings, makes inference even more challenging. To\naddress these challenges, we propose a set of nonparametric mean-covariance\nregression models for high-dimensional neural activity with restricted inputs.\nThese models reduce the dimensionality of neural responses by employing a\nlower-dimensional latent factor model, where both factor loadings and latent\nfactors are predictor-dependent, to jointly model mean and covariance across\ncovariates. The smoothness of neural activity across experimental conditions is\nmodeled nonparametrically using two Gaussian processes (GPs), applied to both\nloading basis and latent factors. Additionally, to account for the covariates\nlying in restricted subspace, we incorporate graph information into the\ncovariance structure. To flexibly infer the model, we use an MCMC algorithm to\nsample from posterior distributions. After validating and studying the\nproperties of proposed methods by simulations, we apply them to two neural\ndatasets (local field potential and neural spiking data) to demonstrate the\nusage of models for continuous and counting observations. Overall, the proposed\nmethods provide a framework to jointly model covariate-dependent mean and\ncovariance in high dimensional neural data, especially when the covariates lie\nin restricted domains. The framework is general and can be easily adapted to\nvarious applications beyond neuroscience."
                },
                "authors": [
                    {
                        "name": "Ganchao Wei"
                    }
                ],
                "author_detail": {
                    "name": "Ganchao Wei"
                },
                "author": "Ganchao Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.19717v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.19717v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.AP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02980v2",
                "updated": "2025-02-03T15:04:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    4,
                    47,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-03T20:43:59Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    20,
                    43,
                    59,
                    3,
                    277,
                    0
                ],
                "title": "DecTrain: Deciding When to Train a Monocular Depth DNN Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecTrain: Deciding When to Train a Monocular Depth DNN Online"
                },
                "summary": "Deep neural networks (DNNs) can deteriorate in accuracy when deployment data\ndiffers from training data. While performing online training at all timesteps\ncan improve accuracy, it is computationally expensive. We propose DecTrain, a\nnew algorithm that decides when to train a monocular depth DNN online using\nself-supervision with low overhead. To make the decision at each timestep,\nDecTrain compares the cost of training with the predicted accuracy gain. We\nevaluate DecTrain on out-of-distribution data, and find DecTrain maintains\naccuracy compared to online training at all timesteps, while training only 44%\nof the time on average. We also compare the recovery of a low inference cost\nDNN using DecTrain and a more generalizable high inference cost DNN on various\nsequences. DecTrain recovers the majority (97%) of the accuracy gain of online\ntraining at all timesteps while reducing computation compared to the high\ninference cost DNN which recovers only 66%. With an even smaller DNN, we\nachieve 89% recovery while reducing computation by 56%. DecTrain enables\nlow-cost online training for a smaller DNN to have competitive accuracy with a\nlarger, more generalizable DNN at a lower overall computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) can deteriorate in accuracy when deployment data\ndiffers from training data. While performing online training at all timesteps\ncan improve accuracy, it is computationally expensive. We propose DecTrain, a\nnew algorithm that decides when to train a monocular depth DNN online using\nself-supervision with low overhead. To make the decision at each timestep,\nDecTrain compares the cost of training with the predicted accuracy gain. We\nevaluate DecTrain on out-of-distribution data, and find DecTrain maintains\naccuracy compared to online training at all timesteps, while training only 44%\nof the time on average. We also compare the recovery of a low inference cost\nDNN using DecTrain and a more generalizable high inference cost DNN on various\nsequences. DecTrain recovers the majority (97%) of the accuracy gain of online\ntraining at all timesteps while reducing computation compared to the high\ninference cost DNN which recovers only 66%. With an even smaller DNN, we\nachieve 89% recovery while reducing computation by 56%. DecTrain enables\nlow-cost online training for a smaller DNN to have competitive accuracy with a\nlarger, more generalizable DNN at a lower overall computational cost."
                },
                "authors": [
                    {
                        "name": "Zih-Sing Fu"
                    },
                    {
                        "name": "Soumya Sudhakar"
                    },
                    {
                        "name": "Sertac Karaman"
                    },
                    {
                        "name": "Vivienne Sze"
                    }
                ],
                "author_detail": {
                    "name": "Vivienne Sze"
                },
                "author": "Vivienne Sze",
                "arxiv_doi": "10.1109/LRA.2025.3536206",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3536206",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.02980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07959v2",
                "updated": "2025-02-03T14:51:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    51,
                    44,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-10T14:23:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    23,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act"
                },
                "summary": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice."
                },
                "authors": [
                    {
                        "name": "Philipp Guldimann"
                    },
                    {
                        "name": "Alexander Spiridonov"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Velko Vechev"
                    },
                    {
                        "name": "Anna-Maria Gueorguieva"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Nikola Konstantinov"
                    },
                    {
                        "name": "Pavol Bielik"
                    },
                    {
                        "name": "Petar Tsankov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.18556v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.18556v2",
                "updated": "2025-02-03T14:16:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    16,
                    36,
                    0,
                    34,
                    0
                ],
                "published": "2024-04-29T09:56:32Z",
                "published_parsed": [
                    2024,
                    4,
                    29,
                    9,
                    56,
                    32,
                    0,
                    120,
                    0
                ],
                "title": "Doubly Adaptive Importance Sampling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Doubly Adaptive Importance Sampling"
                },
                "summary": "We propose an adaptive importance sampling scheme for Gaussian approximations\nof intractable posteriors. Optimization-based approximations like variational\ninference can be too inaccurate while existing Monte Carlo methods can be too\nslow. Therefore, we propose a hybrid where, at each iteration, the Monte Carlo\neffective sample size can be guaranteed at a fixed computational cost by\ninterpolating between natural-gradient variational inference and importance\nsampling. The amount of damping in the updates adapts to the posterior and\nguarantees the effective sample size. Gaussianity enables the use of Stein's\nlemma to obtain gradient-based optimization in the highly damped variational\ninference regime and a reduction of Monte Carlo error for undamped adaptive\nimportance sampling. The result is a generic, embarrassingly parallel and\nadaptive posterior approximation method. Numerical studies on simulated and\nreal data show its competitiveness with other, less general methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose an adaptive importance sampling scheme for Gaussian approximations\nof intractable posteriors. Optimization-based approximations like variational\ninference can be too inaccurate while existing Monte Carlo methods can be too\nslow. Therefore, we propose a hybrid where, at each iteration, the Monte Carlo\neffective sample size can be guaranteed at a fixed computational cost by\ninterpolating between natural-gradient variational inference and importance\nsampling. The amount of damping in the updates adapts to the posterior and\nguarantees the effective sample size. Gaussianity enables the use of Stein's\nlemma to obtain gradient-based optimization in the highly damped variational\ninference regime and a reduction of Monte Carlo error for undamped adaptive\nimportance sampling. The result is a generic, embarrassingly parallel and\nadaptive posterior approximation method. Numerical studies on simulated and\nreal data show its competitiveness with other, less general methods."
                },
                "authors": [
                    {
                        "name": "Willem van den Boom"
                    },
                    {
                        "name": "Andrea Cremaschi"
                    },
                    {
                        "name": "Alexandre H. Thiery"
                    }
                ],
                "author_detail": {
                    "name": "Alexandre H. Thiery"
                },
                "author": "Alexandre H. Thiery",
                "arxiv_comment": "36 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.18556v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.18556v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19202v2",
                "updated": "2025-02-03T14:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    5,
                    36,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T15:12:20Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    12,
                    20,
                    4,
                    31,
                    0
                ],
                "title": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning"
                },
                "summary": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances."
                },
                "authors": [
                    {
                        "name": "Dang Huu-Tien"
                    },
                    {
                        "name": "Hoang Thanh-Tung"
                    },
                    {
                        "name": "Le-Minh Nguyen"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "12 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.19008v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.19008v2",
                "updated": "2025-02-03T14:05:32Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    5,
                    32,
                    0,
                    34,
                    0
                ],
                "published": "2024-07-26T18:00:02Z",
                "published_parsed": [
                    2024,
                    7,
                    26,
                    18,
                    0,
                    2,
                    4,
                    208,
                    0
                ],
                "title": "GA-NIFS: Multi-phase analysis of a star-forming galaxy at $z \\sim 5.5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GA-NIFS: Multi-phase analysis of a star-forming galaxy at $z \\sim 5.5$"
                },
                "summary": "In this study, we present a detailed multiphase analysis of HZ4, a\nmain-sequence star-forming galaxy at z ~ 5.5, known for being a turbulent\nrotating disk and having a detection of a [CII] outflow in the ALMA\nobservations. We exploit JWST/NIRSpec observations in the integral field\nspectroscopy mode with low- and high-spectral resolution that allow us for the\nfirst time to spatially resolve the rest-frame UV and optical emission of the\ngalaxy to investigate the galaxy properties. In particular, the high-resolution\ndataset allows us to study the kinematics of the ionized gas phase, and the\nconditions of the interstellar medium, such as the excitation mechanism, dust\nattenuation, and metallicity. The lower-spectral resolution observations allow\nus to study the continuum emission and infer the stellar populations' ages and\nproperties. Our findings suggest that HZ4 is a galaxy merger rather than a\nrotating disk as previously inferred from lower resolution [CII] data. The\nmerger is associated with an extended broad, blueshifted emission, potentially\nindicative of an outflow originating from a region of intense star formation\nand extending up to 4 kpc. In light of these new observations we reanalyzed the\nALMA data to compare the multiphase gas properties. If we interpret the broad\ncomponents seen in [CII] and [OIII]$\\lambda$5007\\.A as outflows, the neutral\nand ionized components are co-spatial, the mass loading factor of the ionized\nphase is significantly lower than that of the neutral phase, aligning with\ntrends observed in multi-phase systems at lower redshifts. Nonetheless,\nadditional observations and larger statistical samples are essential to\ndetermine the role of mergers and outflows in the early Universe and to clarify\nthe origin of the broad emission components observed in this system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we present a detailed multiphase analysis of HZ4, a\nmain-sequence star-forming galaxy at z ~ 5.5, known for being a turbulent\nrotating disk and having a detection of a [CII] outflow in the ALMA\nobservations. We exploit JWST/NIRSpec observations in the integral field\nspectroscopy mode with low- and high-spectral resolution that allow us for the\nfirst time to spatially resolve the rest-frame UV and optical emission of the\ngalaxy to investigate the galaxy properties. In particular, the high-resolution\ndataset allows us to study the kinematics of the ionized gas phase, and the\nconditions of the interstellar medium, such as the excitation mechanism, dust\nattenuation, and metallicity. The lower-spectral resolution observations allow\nus to study the continuum emission and infer the stellar populations' ages and\nproperties. Our findings suggest that HZ4 is a galaxy merger rather than a\nrotating disk as previously inferred from lower resolution [CII] data. The\nmerger is associated with an extended broad, blueshifted emission, potentially\nindicative of an outflow originating from a region of intense star formation\nand extending up to 4 kpc. In light of these new observations we reanalyzed the\nALMA data to compare the multiphase gas properties. If we interpret the broad\ncomponents seen in [CII] and [OIII]$\\lambda$5007\\.A as outflows, the neutral\nand ionized components are co-spatial, the mass loading factor of the ionized\nphase is significantly lower than that of the neutral phase, aligning with\ntrends observed in multi-phase systems at lower redshifts. Nonetheless,\nadditional observations and larger statistical samples are essential to\ndetermine the role of mergers and outflows in the early Universe and to clarify\nthe origin of the broad emission components observed in this system."
                },
                "authors": [
                    {
                        "name": "Eleonora Parlanti"
                    },
                    {
                        "name": "Stefano Carniani"
                    },
                    {
                        "name": "Giacomo Venturi"
                    },
                    {
                        "name": "Rodrigo Herrera-Camus"
                    },
                    {
                        "name": "Santiago Arribas"
                    },
                    {
                        "name": "Andrew J. Bunker"
                    },
                    {
                        "name": "Stephane Charlot"
                    },
                    {
                        "name": "Francesco D'Eugenio"
                    },
                    {
                        "name": "Roberto Maiolino"
                    },
                    {
                        "name": "Michele Perna"
                    },
                    {
                        "name": "Hannah Übler"
                    },
                    {
                        "name": "Torsten Böker"
                    },
                    {
                        "name": "Giovanni Cresci"
                    },
                    {
                        "name": "Mirko Curti"
                    },
                    {
                        "name": "Gareth C. Jones"
                    },
                    {
                        "name": "Isabella Lamperti"
                    },
                    {
                        "name": "Pablo G. Pérez-González"
                    },
                    {
                        "name": "Bruno Rodríguez Del Pino"
                    },
                    {
                        "name": "Sandra Zamora"
                    }
                ],
                "author_detail": {
                    "name": "Sandra Zamora"
                },
                "author": "Sandra Zamora",
                "arxiv_comment": "23 pages, 20 figures, accepted in A&A",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.19008v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.19008v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14615v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14615v2",
                "updated": "2025-02-03T13:46:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    46,
                    34,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-24T16:33:52Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    16,
                    33,
                    52,
                    4,
                    24,
                    0
                ],
                "title": "Single-neuron deep generative model uncovers underlying physics of\n  neuronal activity in Ca imaging data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Single-neuron deep generative model uncovers underlying physics of\n  neuronal activity in Ca imaging data"
                },
                "summary": "Calcium imaging has become a powerful alternative to electrophysiology for\nstudying neuronal activity, offering spatial resolution and the ability to\nmeasure large populations of neurons in a minimally invasive manner. This\ntechnique has broad applications in neuroscience, neuroengineering, and\nmedicine, enabling researchers to explore the relationship between neuron\nlocation and activity. Recent advancements in deep generative models (DGMs)\nhave facilitated the modeling of neuronal population dynamics, uncovering\nlatent representations that provide insights into behavior prediction and\nneuronal variance. However, these models often rely on spike inference\nalgorithms and primarily focus on population-level dynamics, limiting their\napplicability for single-neuron analyses. To address this gap, we propose a\nnovel framework for single-neuron representation learning using autoregressive\nvariational autoencoders (AVAEs). Our approach embeds individual neurons'\nspatiotemporal signals into a reduced-dimensional space without the need for\nspike inference algorithms. The AVAE excels over traditional linear methods by\ngenerating more informative and discriminative latent representations,\nimproving tasks such as visualization, clustering, and the understanding of\nneuronal activity. Additionally, the reconstruction performance of the AVAE\noutperforms the state of the art, demonstrating its ability to accurately\nrecover the original fluorescence signal from the learned representation. Using\nrealistic simulations, we show that our model captures underlying physical\nproperties and connectivity patterns, enabling it to distinguish between\ndifferent firing and connectivity types. These findings position the AVAE as a\nversatile and powerful tool for advancing single-neuron analysis and lays the\ngroundwork for future integration of multimodal single-cell datasets in\nneuroscience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calcium imaging has become a powerful alternative to electrophysiology for\nstudying neuronal activity, offering spatial resolution and the ability to\nmeasure large populations of neurons in a minimally invasive manner. This\ntechnique has broad applications in neuroscience, neuroengineering, and\nmedicine, enabling researchers to explore the relationship between neuron\nlocation and activity. Recent advancements in deep generative models (DGMs)\nhave facilitated the modeling of neuronal population dynamics, uncovering\nlatent representations that provide insights into behavior prediction and\nneuronal variance. However, these models often rely on spike inference\nalgorithms and primarily focus on population-level dynamics, limiting their\napplicability for single-neuron analyses. To address this gap, we propose a\nnovel framework for single-neuron representation learning using autoregressive\nvariational autoencoders (AVAEs). Our approach embeds individual neurons'\nspatiotemporal signals into a reduced-dimensional space without the need for\nspike inference algorithms. The AVAE excels over traditional linear methods by\ngenerating more informative and discriminative latent representations,\nimproving tasks such as visualization, clustering, and the understanding of\nneuronal activity. Additionally, the reconstruction performance of the AVAE\noutperforms the state of the art, demonstrating its ability to accurately\nrecover the original fluorescence signal from the learned representation. Using\nrealistic simulations, we show that our model captures underlying physical\nproperties and connectivity patterns, enabling it to distinguish between\ndifferent firing and connectivity types. These findings position the AVAE as a\nversatile and powerful tool for advancing single-neuron analysis and lays the\ngroundwork for future integration of multimodal single-cell datasets in\nneuroscience."
                },
                "authors": [
                    {
                        "name": "Jordi Abante"
                    },
                    {
                        "name": "Angelo Piga"
                    },
                    {
                        "name": "Berta Ros"
                    },
                    {
                        "name": "Clara F López-León"
                    },
                    {
                        "name": "Josep M Canals"
                    },
                    {
                        "name": "Jordi Soriano"
                    }
                ],
                "author_detail": {
                    "name": "Jordi Soriano"
                },
                "author": "Jordi Soriano",
                "arxiv_comment": "12 pages, 5 figures, ECCB 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14615v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14615v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v4",
                "updated": "2025-02-03T13:13:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    13,
                    44,
                    0,
                    34,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "arxiv_comment": "Accepted to NAACL 2025 main, long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06159v3",
                "updated": "2025-02-03T13:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    11,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2024-11-09T12:06:40Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    12,
                    6,
                    40,
                    5,
                    314,
                    0
                ],
                "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Knowledge Minigraph Agents for Literature Review Generation"
                },
                "summary": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research."
                },
                "authors": [
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Sheng-hua Zhong"
                    },
                    {
                        "name": "Gong Chen"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15606v2",
                "updated": "2025-02-03T12:56:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    56,
                    55,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-20T07:00:46Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    0,
                    46,
                    4,
                    355,
                    0
                ],
                "title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage"
                },
                "summary": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Zhi Gao"
                    },
                    {
                        "name": "Bofei Zhang"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Yunde Jia"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "ICLR 2025, https://mat-agent.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21009v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21009v4",
                "updated": "2025-02-03T12:53:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    53,
                    41,
                    0,
                    34,
                    0
                ],
                "published": "2024-07-30T17:55:36Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    55,
                    36,
                    1,
                    212,
                    0
                ],
                "title": "AI-Assisted Generation of Difficult Math Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Assisted Generation of Difficult Math Questions"
                },
                "summary": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills."
                },
                "authors": [
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Dingli Yu"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Jiatong Yu"
                    },
                    {
                        "name": "Yinghui He"
                    },
                    {
                        "name": "Nan Rosemary Ke"
                    },
                    {
                        "name": "Michael Mozer"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Anirudh Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Goyal"
                },
                "author": "Anirudh Goyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21009v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21009v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04844v2",
                "updated": "2025-02-03T12:33:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    33,
                    5,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-07T09:04:50Z",
                "published_parsed": [
                    2024,
                    10,
                    7,
                    9,
                    4,
                    50,
                    0,
                    281,
                    0
                ],
                "title": "PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PostEdit: Posterior Sampling for Efficient Zero-Shot Image Editing"
                },
                "summary": "In the field of image editing, three core challenges persist:\ncontrollability, background preservation, and efficiency. Inversion-based\nmethods rely on time-consuming optimization to preserve the features of the\ninitial images, which results in low efficiency due to the requirement for\nextensive network inference. Conversely, inversion-free methods lack\ntheoretical support for background similarity, as they circumvent the issue of\nmaintaining initial features to achieve efficiency. As a consequence, none of\nthese methods can achieve both high efficiency and background consistency. To\ntackle the challenges and the aforementioned disadvantages, we introduce\nPostEdit, a method that incorporates a posterior scheme to govern the diffusion\nsampling process. Specifically, a corresponding measurement term related to\nboth the initial features and Langevin dynamics is introduced to optimize the\nestimated image generated by the given target prompt. Extensive experimental\nresults indicate that the proposed PostEdit achieves state-of-the-art editing\nperformance while accurately preserving unedited regions. Furthermore, the\nmethod is both inversion- and training-free, necessitating approximately 1.5\nseconds and 18 GB of GPU memory to generate high-quality results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the field of image editing, three core challenges persist:\ncontrollability, background preservation, and efficiency. Inversion-based\nmethods rely on time-consuming optimization to preserve the features of the\ninitial images, which results in low efficiency due to the requirement for\nextensive network inference. Conversely, inversion-free methods lack\ntheoretical support for background similarity, as they circumvent the issue of\nmaintaining initial features to achieve efficiency. As a consequence, none of\nthese methods can achieve both high efficiency and background consistency. To\ntackle the challenges and the aforementioned disadvantages, we introduce\nPostEdit, a method that incorporates a posterior scheme to govern the diffusion\nsampling process. Specifically, a corresponding measurement term related to\nboth the initial features and Langevin dynamics is introduced to optimize the\nestimated image generated by the given target prompt. Extensive experimental\nresults indicate that the proposed PostEdit achieves state-of-the-art editing\nperformance while accurately preserving unedited regions. Furthermore, the\nmethod is both inversion- and training-free, necessitating approximately 1.5\nseconds and 18 GB of GPU memory to generate high-quality results."
                },
                "authors": [
                    {
                        "name": "Feng Tian"
                    },
                    {
                        "name": "Yixuan Li"
                    },
                    {
                        "name": "Yichao Yan"
                    },
                    {
                        "name": "Shanyan Guan"
                    },
                    {
                        "name": "Yanhao Ge"
                    },
                    {
                        "name": "Xiaokang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokang Yang"
                },
                "author": "Xiaokang Yang",
                "arxiv_comment": "30 pages",
                "arxiv_journal_ref": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19060v2",
                "updated": "2025-02-03T12:12:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    12,
                    30,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T11:47:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    47,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment"
                },
                "summary": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed."
                },
                "authors": [
                    {
                        "name": "Song-Lin Lv"
                    },
                    {
                        "name": "Yu-Yang Chen"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    }
                ],
                "author_detail": {
                    "name": "Lan-Zhe Guo"
                },
                "author": "Lan-Zhe Guo",
                "arxiv_comment": "We are withdrawing the paper due to comments indicating overlap with\n  parts of another paper. We will revise the appendix and submit a new version\n  after addressing the issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18417v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18417v2",
                "updated": "2025-02-03T12:12:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    12,
                    21,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-30T15:15:17Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    15,
                    15,
                    17,
                    3,
                    30,
                    0
                ],
                "title": "Real-Time Anomaly Detection with Synthetic Anomaly Monitoring (SAM)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Anomaly Detection with Synthetic Anomaly Monitoring (SAM)"
                },
                "summary": "Anomaly detection is essential for identifying rare and significant events\nacross diverse domains such as finance, cybersecurity, and network monitoring.\nThis paper presents Synthetic Anomaly Monitoring (SAM), an innovative approach\nthat applies synthetic control methods from causal inference to improve both\nthe accuracy and interpretability of anomaly detection processes. By modeling\nnormal behavior through the treatment of each feature as a control unit, SAM\nidentifies anomalies as deviations within this causal framework. We conducted\nextensive experiments comparing SAM with established benchmark models,\nincluding Isolation Forest, Local Outlier Factor (LOF), k-Nearest Neighbors\n(kNN), and One-Class Support Vector Machine (SVM), across five diverse\ndatasets, including Credit Card Fraud, HTTP Dataset CSIC 2010, and KDD Cup\n1999, among others. Our results demonstrate that SAM consistently delivers\nrobust performance, highlighting its potential as a powerful tool for real-time\nanomaly detection in dynamic and complex environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Anomaly detection is essential for identifying rare and significant events\nacross diverse domains such as finance, cybersecurity, and network monitoring.\nThis paper presents Synthetic Anomaly Monitoring (SAM), an innovative approach\nthat applies synthetic control methods from causal inference to improve both\nthe accuracy and interpretability of anomaly detection processes. By modeling\nnormal behavior through the treatment of each feature as a control unit, SAM\nidentifies anomalies as deviations within this causal framework. We conducted\nextensive experiments comparing SAM with established benchmark models,\nincluding Isolation Forest, Local Outlier Factor (LOF), k-Nearest Neighbors\n(kNN), and One-Class Support Vector Machine (SVM), across five diverse\ndatasets, including Credit Card Fraud, HTTP Dataset CSIC 2010, and KDD Cup\n1999, among others. Our results demonstrate that SAM consistently delivers\nrobust performance, highlighting its potential as a powerful tool for real-time\nanomaly detection in dynamic and complex environments."
                },
                "authors": [
                    {
                        "name": "Emanuele Luzio"
                    },
                    {
                        "name": "Moacir Antonelli Ponti"
                    }
                ],
                "author_detail": {
                    "name": "Moacir Antonelli Ponti"
                },
                "author": "Moacir Antonelli Ponti",
                "arxiv_comment": "19 pages, 3 figures, submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18417v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18417v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62H30, 68T05, 62G99, 91G80, 68M10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.5.4; K.6.5; I.2.6; H.4.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13720v2",
                "updated": "2025-02-03T10:52:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    10,
                    52,
                    15,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-23T14:50:37Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    50,
                    37,
                    3,
                    23,
                    0
                ],
                "title": "Musical ethnocentrism in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Musical ethnocentrism in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments."
                },
                "authors": [
                    {
                        "name": "Anna Kruspe"
                    }
                ],
                "author_detail": {
                    "name": "Anna Kruspe"
                },
                "author": "Anna Kruspe",
                "arxiv_journal_ref": "Proceedings of the 3rd Workshop on NLP for Music and Audio\n  (NLP4MusA) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18158v2",
                "updated": "2025-02-03T10:45:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    10,
                    45,
                    22,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-30T05:48:13Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    48,
                    13,
                    3,
                    30,
                    0
                ],
                "title": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study"
                },
                "summary": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data."
                },
                "authors": [
                    {
                        "name": "Yuchen Lei"
                    },
                    {
                        "name": "Yuexin Xiang"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Rafael Dowsley"
                    },
                    {
                        "name": "Tsz Hon Yuen"
                    },
                    {
                        "name": "Jiangshan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangshan Yu"
                },
                "author": "Jiangshan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14230v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14230v3",
                "updated": "2025-02-03T10:33:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    10,
                    33,
                    17,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-20T11:51:00Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    11,
                    51,
                    0,
                    3,
                    172,
                    0
                ],
                "title": "Raising the Bar: Investigating the Values of Large Language Models via\n  Generative Evolving Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raising the Bar: Investigating the Values of Large Language Models via\n  Generative Evolving Testing"
                },
                "summary": "Warning: Contains harmful model outputs.\n  Despite significant advancements, the propensity of Large Language Models\n(LLMs) to generate harmful and unethical content poses critical challenges.\nMeasuring value alignment of LLMs becomes crucial for their regulation and\nresponsible deployment. Although numerous benchmarks have been constructed to\nassess social bias, toxicity, and ethical issues in LLMs, those static\nbenchmarks suffer from evaluation chronoeffect, in which, as models rapidly\nevolve, existing benchmarks may leak into training data or become saturated,\noverestimating ever-developing LLMs. To tackle this problem, we propose GETA, a\nnovel generative evolving testing approach based on adaptive testing methods in\nmeasurement theory. Unlike traditional adaptive testing methods that rely on a\nstatic test item pool, GETA probes the underlying moral boundaries of LLMs by\ndynamically generating test items tailored to model capability. GETA co-evolves\nwith LLMs by learning a joint distribution of item difficulty and model value\nconformity, thus effectively addressing evaluation chronoeffect. We evaluated\nvarious popular LLMs with GETA and demonstrated that 1) GETA can dynamically\ncreate difficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warning: Contains harmful model outputs.\n  Despite significant advancements, the propensity of Large Language Models\n(LLMs) to generate harmful and unethical content poses critical challenges.\nMeasuring value alignment of LLMs becomes crucial for their regulation and\nresponsible deployment. Although numerous benchmarks have been constructed to\nassess social bias, toxicity, and ethical issues in LLMs, those static\nbenchmarks suffer from evaluation chronoeffect, in which, as models rapidly\nevolve, existing benchmarks may leak into training data or become saturated,\noverestimating ever-developing LLMs. To tackle this problem, we propose GETA, a\nnovel generative evolving testing approach based on adaptive testing methods in\nmeasurement theory. Unlike traditional adaptive testing methods that rely on a\nstatic test item pool, GETA probes the underlying moral boundaries of LLMs by\ndynamically generating test items tailored to model capability. GETA co-evolves\nwith LLMs by learning a joint distribution of item difficulty and model value\nconformity, thus effectively addressing evaluation chronoeffect. We evaluated\nvarious popular LLMs with GETA and demonstrated that 1) GETA can dynamically\ncreate difficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms."
                },
                "authors": [
                    {
                        "name": "Han Jiang"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Zhihua Wei"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14230v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14744v2",
                "updated": "2025-02-03T09:57:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    57,
                    1,
                    0,
                    34,
                    0
                ],
                "published": "2024-08-27T02:45:26Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    45,
                    26,
                    1,
                    240,
                    0
                ],
                "title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models"
                },
                "summary": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller."
                },
                "authors": [
                    {
                        "name": "Junyao Ge"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Kaitai Guo"
                    },
                    {
                        "name": "Jimin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Liang"
                },
                "author": "Jimin Liang",
                "arxiv_comment": "Submitted to ISPRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17600v2",
                "updated": "2025-02-03T09:48:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    48,
                    26,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-23T06:54:03Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    54,
                    3,
                    2,
                    297,
                    0
                ],
                "title": "Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective"
                },
                "summary": "Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Boming Yang"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Sixun Ouyang"
                    },
                    {
                        "name": "Moritz Blum"
                    },
                    {
                        "name": "Tianwei She"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Freddy Lecue"
                    },
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2407.10794",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11668v2",
                "updated": "2025-02-03T09:25:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    25,
                    8,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-17T15:51:01Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    51,
                    1,
                    0,
                    169,
                    0
                ],
                "title": "\"Not Aligned\" is Not \"Malicious\": Being Careful about Hallucinations of\n  Large Language Models' Jailbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Not Aligned\" is Not \"Malicious\": Being Careful about Hallucinations of\n  Large Language Models' Jailbreak"
                },
                "summary": "\"Jailbreak\" is a major safety concern of Large Language Models (LLMs), which\noccurs when malicious prompts lead LLMs to produce harmful outputs, raising\nissues about the reliability and safety of LLMs. Therefore, an effective\nevaluation of jailbreaks is very crucial to develop its mitigation strategies.\nHowever, our research reveals that many jailbreaks identified by current\nevaluations may actually be hallucinations-erroneous outputs that are mistaken\nfor genuine safety breaches. This finding suggests that some perceived\nvulnerabilities might not represent actual threats, indicating a need for more\nprecise red teaming benchmarks. To address this problem, we propose the\n$\\textbf{B}$enchmark for reli$\\textbf{AB}$ilit$\\textbf{Y}$ and\njail$\\textbf{B}$reak ha$\\textbf{L}$l$\\textbf{U}$cination $\\textbf{E}$valuation\n(BabyBLUE). BabyBLUE introduces a specialized validation framework including\nvarious evaluators to enhance existing jailbreak benchmarks, ensuring outputs\nare useful malicious instructions. Additionally, BabyBLUE presents a new\ndataset as an augmentation to the existing red teaming benchmarks, specifically\naddressing hallucinations in jailbreaks, aiming to evaluate the true potential\nof jailbroken LLM outputs to cause harm to human society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Jailbreak\" is a major safety concern of Large Language Models (LLMs), which\noccurs when malicious prompts lead LLMs to produce harmful outputs, raising\nissues about the reliability and safety of LLMs. Therefore, an effective\nevaluation of jailbreaks is very crucial to develop its mitigation strategies.\nHowever, our research reveals that many jailbreaks identified by current\nevaluations may actually be hallucinations-erroneous outputs that are mistaken\nfor genuine safety breaches. This finding suggests that some perceived\nvulnerabilities might not represent actual threats, indicating a need for more\nprecise red teaming benchmarks. To address this problem, we propose the\n$\\textbf{B}$enchmark for reli$\\textbf{AB}$ilit$\\textbf{Y}$ and\njail$\\textbf{B}$reak ha$\\textbf{L}$l$\\textbf{U}$cination $\\textbf{E}$valuation\n(BabyBLUE). BabyBLUE introduces a specialized validation framework including\nvarious evaluators to enhance existing jailbreak benchmarks, ensuring outputs\nare useful malicious instructions. Additionally, BabyBLUE presents a new\ndataset as an augmentation to the existing red teaming benchmarks, specifically\naddressing hallucinations in jailbreaks, aiming to evaluate the true potential\nof jailbroken LLM outputs to cause harm to human society."
                },
                "authors": [
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Jiayi Mao"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05855v3",
                "updated": "2025-02-03T09:13:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    13,
                    17,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-10T10:53:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    53,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability"
                },
                "summary": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim."
                },
                "authors": [
                    {
                        "name": "Antonin Poché"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Agustin Martin Picard"
                    },
                    {
                        "name": "Victor Boutin"
                    },
                    {
                        "name": "Fanny Jourdan"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Jourdan"
                },
                "arxiv_affiliation": "CERCO UMR5549, ANITI",
                "author": "Fanny Jourdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13327v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13327v3",
                "updated": "2025-02-03T09:12:19Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    12,
                    19,
                    0,
                    34,
                    0
                ],
                "published": "2023-11-22T11:47:40Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    11,
                    47,
                    40,
                    2,
                    326,
                    0
                ],
                "title": "Regressions under Adverse Conditions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Regressions under Adverse Conditions"
                },
                "summary": "We introduce a new regression method that relates the mean of an outcome\nvariable to covariates, under the \"adverse condition\" that a distress variable\nfalls in its tail. This allows to tailor classical mean regressions to adverse\nscenarios, which receive increasing interest in economics and finance, among\nmany others. In the terminology of the systemic risk literature, our method can\nbe interpreted as a regression for the Marginal Expected Shortfall. We propose\na two-step procedure to estimate the new models, show consistency and\nasymptotic normality of the estimator, and propose feasible inference under\nweak conditions that allow for cross-sectional and time series applications.\nSimulations verify the accuracy of the asymptotic approximations of the\ntwo-step estimator. Two empirical applications show that our regressions under\nadverse conditions are a valuable tool in such diverse fields as the study of\nthe relation between systemic risk and asset price bubbles, and dissecting\nmacroeconomic growth vulnerabilities into individual components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new regression method that relates the mean of an outcome\nvariable to covariates, under the \"adverse condition\" that a distress variable\nfalls in its tail. This allows to tailor classical mean regressions to adverse\nscenarios, which receive increasing interest in economics and finance, among\nmany others. In the terminology of the systemic risk literature, our method can\nbe interpreted as a regression for the Marginal Expected Shortfall. We propose\na two-step procedure to estimate the new models, show consistency and\nasymptotic normality of the estimator, and propose feasible inference under\nweak conditions that allow for cross-sectional and time series applications.\nSimulations verify the accuracy of the asymptotic approximations of the\ntwo-step estimator. Two empirical applications show that our regressions under\nadverse conditions are a valuable tool in such diverse fields as the study of\nthe relation between systemic risk and asset price bubbles, and dissecting\nmacroeconomic growth vulnerabilities into individual components."
                },
                "authors": [
                    {
                        "name": "Timo Dimitriadis"
                    },
                    {
                        "name": "Yannick Hoga"
                    }
                ],
                "author_detail": {
                    "name": "Yannick Hoga"
                },
                "author": "Yannick Hoga",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13327v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13327v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "econ.EM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "econ.EM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03376v2",
                "updated": "2025-02-03T09:11:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    11,
                    36,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-05T15:31:43Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    15,
                    31,
                    43,
                    2,
                    157,
                    0
                ],
                "title": "Log Parsing using LLMs with Self-Generated In-Context Learning and\n  Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log Parsing using LLMs with Self-Generated In-Context Learning and\n  Self-Correction"
                },
                "summary": "Log parsing transforms log messages into structured formats, serving as a\ncrucial step for log analysis. Despite a variety of log parsers that have been\nproposed, their performance on evolving log data remains unsatisfactory due to\nreliance on human-crafted rules or learning-based models with limited training\ndata. The recent emergence of large language models (LLMs) has demonstrated\nstrong abilities in understanding natural language and code, making it\npromising to apply LLMs for log parsing. Consequently, several studies have\nproposed LLM-based log parsers. However, LLMs may produce inaccurate templates,\nand existing LLM-based log parsers directly use the template generated by the\nLLM as the parsing result, hindering the accuracy of log parsing. Furthermore,\nthese log parsers depend heavily on historical log data as demonstrations,\nwhich poses challenges in maintaining accuracy when dealing with scarce\nhistorical log data or evolving log data. To address these challenges, we\npropose AdaParser, an effective and adaptive log parsing framework using LLMs\nwith self-generated in-context learning (SG-ICL) and self-correction. To\nfacilitate accurate log parsing, AdaParser incorporates a novel component, a\ntemplate corrector, which utilizes the LLM to correct potential parsing errors\nin the templates it generates. In addition, AdaParser maintains a dynamic\ncandidate set composed of previously generated templates as demonstrations to\nadapt evolving log data. Extensive experiments on public large-scale datasets\nindicate that AdaParser outperforms state-of-the-art methods across all\nmetrics, even in zero-shot scenarios. Moreover, when integrated with different\nLLMs, AdaParser consistently enhances the performance of the utilized LLMs by a\nlarge margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log parsing transforms log messages into structured formats, serving as a\ncrucial step for log analysis. Despite a variety of log parsers that have been\nproposed, their performance on evolving log data remains unsatisfactory due to\nreliance on human-crafted rules or learning-based models with limited training\ndata. The recent emergence of large language models (LLMs) has demonstrated\nstrong abilities in understanding natural language and code, making it\npromising to apply LLMs for log parsing. Consequently, several studies have\nproposed LLM-based log parsers. However, LLMs may produce inaccurate templates,\nand existing LLM-based log parsers directly use the template generated by the\nLLM as the parsing result, hindering the accuracy of log parsing. Furthermore,\nthese log parsers depend heavily on historical log data as demonstrations,\nwhich poses challenges in maintaining accuracy when dealing with scarce\nhistorical log data or evolving log data. To address these challenges, we\npropose AdaParser, an effective and adaptive log parsing framework using LLMs\nwith self-generated in-context learning (SG-ICL) and self-correction. To\nfacilitate accurate log parsing, AdaParser incorporates a novel component, a\ntemplate corrector, which utilizes the LLM to correct potential parsing errors\nin the templates it generates. In addition, AdaParser maintains a dynamic\ncandidate set composed of previously generated templates as demonstrations to\nadapt evolving log data. Extensive experiments on public large-scale datasets\nindicate that AdaParser outperforms state-of-the-art methods across all\nmetrics, even in zero-shot scenarios. Moreover, when integrated with different\nLLMs, AdaParser consistently enhances the performance of the utilized LLMs by a\nlarge margin."
                },
                "authors": [
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Siyu Yu"
                    },
                    {
                        "name": "Ying Li"
                    }
                ],
                "author_detail": {
                    "name": "Ying Li"
                },
                "author": "Ying Li",
                "arxiv_comment": "Accepted by the 33rd IEEE/ACM International Conference on Program\n  Comprehension (ICPC'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13979v2",
                "updated": "2025-02-03T08:54:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    54,
                    28,
                    0,
                    34,
                    0
                ],
                "published": "2024-09-21T02:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    2,
                    9,
                    13,
                    5,
                    265,
                    0
                ],
                "title": "Role-Play Paradox in Large Language Models: Reasoning Performance Gains\n  and Ethical Dilemmas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Play Paradox in Large Language Models: Reasoning Performance Gains\n  and Ethical Dilemmas"
                },
                "summary": "Role-play in large language models (LLMs) enhances their ability to generate\ncontextually relevant and high-quality responses by simulating diverse\ncognitive perspectives. However, our study identifies significant risks\nassociated with this technique. First, we demonstrate that autotuning, a method\nused to auto-select models' roles based on the question, can lead to the\ngeneration of harmful outputs, even when the model is tasked with adopting\nneutral roles. Second, we investigate how different roles affect the likelihood\nof generating biased or harmful content. Through testing on benchmarks\ncontaining stereotypical and harmful questions, we find that role-play\nconsistently amplifies the risk of biased outputs. Our results underscore the\nneed for careful consideration of both role simulation and tuning processes\nwhen deploying LLMs in sensitive or high-stakes contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-play in large language models (LLMs) enhances their ability to generate\ncontextually relevant and high-quality responses by simulating diverse\ncognitive perspectives. However, our study identifies significant risks\nassociated with this technique. First, we demonstrate that autotuning, a method\nused to auto-select models' roles based on the question, can lead to the\ngeneration of harmful outputs, even when the model is tasked with adopting\nneutral roles. Second, we investigate how different roles affect the likelihood\nof generating biased or harmful content. Through testing on benchmarks\ncontaining stereotypical and harmful questions, we find that role-play\nconsistently amplifies the risk of biased outputs. Our results underscore the\nneed for careful consideration of both role simulation and tuning processes\nwhen deploying LLMs in sensitive or high-stakes contexts."
                },
                "authors": [
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Zifan Qian"
                    },
                    {
                        "name": "Linbo Cao"
                    },
                    {
                        "name": "Yining Wang"
                    },
                    {
                        "name": "Yitian Ding"
                    },
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Zeyong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zeyong Jin"
                },
                "arxiv_affiliation": "The Australian National University",
                "author": "Zeyong Jin",
                "arxiv_comment": "9 pages, 7 figures, 3 tables, submitted to CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04350v2",
                "updated": "2025-02-03T08:28:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    28,
                    51,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-06T04:03:00Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    4,
                    3,
                    0,
                    6,
                    280,
                    0
                ],
                "title": "TIS-DPO: Token-level Importance Sampling for Direct Preference\n  Optimization With Estimated Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIS-DPO: Token-level Importance Sampling for Direct Preference\n  Optimization With Estimated Weights"
                },
                "summary": "Direct Preference Optimization (DPO) has been widely adopted for preference\nalignment of Large Language Models (LLMs) due to its simplicity and\neffectiveness. However, DPO is derived as a bandit problem in which the whole\nresponse is treated as a single arm, ignoring the importance differences\nbetween tokens, which may affect optimization efficiency and make it difficult\nto achieve optimal results. In this work, we propose that the optimal data for\nDPO has equal expected rewards for each token in winning and losing responses,\nas there is no difference in token importance. However, since the optimal\ndataset is unavailable in practice, we propose using the original dataset for\nimportance sampling to achieve unbiased optimization. Accordingly, we propose a\ntoken-level importance sampling DPO objective named TIS-DPO that assigns\nimportance weights to each token based on its reward. Inspired by previous\nworks, we estimate the token importance weights using the difference in\nprediction probabilities from a pair of contrastive LLMs. We explore three\nmethods to construct these contrastive LLMs: (1) guiding the original LLM with\ncontrastive prompts, (2) training two separate LLMs using winning and losing\nresponses, and (3) performing forward and reverse DPO training with winning and\nlosing responses. Experiments show that TIS-DPO significantly outperforms\nvarious baseline methods on harmlessness and helpfulness alignment and\nsummarization tasks. We also visualize the estimated weights, demonstrating\ntheir ability to identify key token positions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has been widely adopted for preference\nalignment of Large Language Models (LLMs) due to its simplicity and\neffectiveness. However, DPO is derived as a bandit problem in which the whole\nresponse is treated as a single arm, ignoring the importance differences\nbetween tokens, which may affect optimization efficiency and make it difficult\nto achieve optimal results. In this work, we propose that the optimal data for\nDPO has equal expected rewards for each token in winning and losing responses,\nas there is no difference in token importance. However, since the optimal\ndataset is unavailable in practice, we propose using the original dataset for\nimportance sampling to achieve unbiased optimization. Accordingly, we propose a\ntoken-level importance sampling DPO objective named TIS-DPO that assigns\nimportance weights to each token based on its reward. Inspired by previous\nworks, we estimate the token importance weights using the difference in\nprediction probabilities from a pair of contrastive LLMs. We explore three\nmethods to construct these contrastive LLMs: (1) guiding the original LLM with\ncontrastive prompts, (2) training two separate LLMs using winning and losing\nresponses, and (3) performing forward and reverse DPO training with winning and\nlosing responses. Experiments show that TIS-DPO significantly outperforms\nvarious baseline methods on harmlessness and helpfulness alignment and\nsummarization tasks. We also visualize the estimated weights, demonstrating\ntheir ability to identify key token positions."
                },
                "authors": [
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Haoping Bai"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Xiaojiang Liu"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Meng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Cao"
                },
                "author": "Meng Cao",
                "arxiv_comment": "30 pages, 8 figures, 8 tables, Published in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14744v3",
                "updated": "2025-02-03T08:22:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    22,
                    35,
                    0,
                    34,
                    0
                ],
                "published": "2024-05-23T16:13:33Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    13,
                    33,
                    3,
                    144,
                    0
                ],
                "title": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View"
                },
                "summary": "Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents."
                },
                "authors": [
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haoyang Shang"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09508v2",
                "updated": "2025-02-03T08:17:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    17,
                    43,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-12T12:10:14Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    12,
                    10,
                    14,
                    5,
                    286,
                    0
                ],
                "title": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing"
                },
                "summary": "Collaborative learning of large language models (LLMs) has emerged as a new\nparadigm for utilizing private data from different parties to guarantee\nefficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also\ngarnered increased attention due to its ability to manipulate the behaviors of\nLLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits\nof multiple parties are aggregated in a privacy-preserving and continual\nmanner) unexamined. To this end, this manuscript dives into the first\ninvestigation of collaborative KE, in which we start by carefully identifying\nthe unique three challenges therein, including knowledge overlap, knowledge\nconflict, and knowledge forgetting. We then propose a non-destructive\ncollaborative KE framework, COLLABEDIT, which employs a novel model merging\nmechanism to mimic the global KE behavior while preventing the severe\nperformance drop. Extensive experiments on two canonical datasets demonstrate\nthe superiority of COLLABEDIT compared to other destructive baselines, and\nresults shed light on addressing three collaborative KE challenges and future\napplications. Our code is available at https://github.com/LINs-lab/CollabEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative learning of large language models (LLMs) has emerged as a new\nparadigm for utilizing private data from different parties to guarantee\nefficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also\ngarnered increased attention due to its ability to manipulate the behaviors of\nLLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits\nof multiple parties are aggregated in a privacy-preserving and continual\nmanner) unexamined. To this end, this manuscript dives into the first\ninvestigation of collaborative KE, in which we start by carefully identifying\nthe unique three challenges therein, including knowledge overlap, knowledge\nconflict, and knowledge forgetting. We then propose a non-destructive\ncollaborative KE framework, COLLABEDIT, which employs a novel model merging\nmechanism to mimic the global KE behavior while preventing the severe\nperformance drop. Extensive experiments on two canonical datasets demonstrate\nthe superiority of COLLABEDIT compared to other destructive baselines, and\nresults shed light on addressing three collaborative KE challenges and future\napplications. Our code is available at https://github.com/LINs-lab/CollabEdit."
                },
                "authors": [
                    {
                        "name": "Jiamu Zheng"
                    },
                    {
                        "name": "Jinghuai Zhang"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "20 pages, 11 figures. Published as a conference paper at ICLR 2025.\n  Code at https://github.com/LINs-lab/CollabEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17006v2",
                "updated": "2025-02-03T07:23:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    7,
                    23,
                    13,
                    0,
                    34,
                    0
                ],
                "published": "2024-03-25T17:59:41Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    59,
                    41,
                    0,
                    85,
                    0
                ],
                "title": "Invertible Diffusion Models for Compressed Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invertible Diffusion Models for Compressed Sensing"
                },
                "summary": "While deep neural networks (NN) significantly advance image compressed\nsensing (CS) by improving reconstruction quality, the necessity of training\ncurrent CS NNs from scratch constrains their effectiveness and hampers rapid\ndeployment. Although recent methods utilize pre-trained diffusion models for\nimage reconstruction, they struggle with slow inference and restricted\nadaptability to CS. To tackle these challenges, this paper proposes Invertible\nDiffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS\nmethod. IDM repurposes a large-scale diffusion sampling process as a\nreconstruction model, and fine-tunes it end-to-end to recover original images\ndirectly from CS measurements, moving beyond the traditional paradigm of\none-step noise estimation learning. To enable such memory-intensive end-to-end\nfine-tuning, we propose a novel two-level invertible design to transform both\n(1) multi-step sampling process and (2) noise estimation U-Net in each step\ninto invertible networks. As a result, most intermediate features are cleared\nduring training to reduce up to 93.8% GPU memory. In addition, we develop a set\nof lightweight modules to inject measurements into noise estimator to further\nfacilitate reconstruction. Experiments demonstrate that IDM outperforms\nexisting state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the\nrecent diffusion-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain\nand 14.54 times faster inference. Code is available at\nhttps://github.com/Guaishou74851/IDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deep neural networks (NN) significantly advance image compressed\nsensing (CS) by improving reconstruction quality, the necessity of training\ncurrent CS NNs from scratch constrains their effectiveness and hampers rapid\ndeployment. Although recent methods utilize pre-trained diffusion models for\nimage reconstruction, they struggle with slow inference and restricted\nadaptability to CS. To tackle these challenges, this paper proposes Invertible\nDiffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS\nmethod. IDM repurposes a large-scale diffusion sampling process as a\nreconstruction model, and fine-tunes it end-to-end to recover original images\ndirectly from CS measurements, moving beyond the traditional paradigm of\none-step noise estimation learning. To enable such memory-intensive end-to-end\nfine-tuning, we propose a novel two-level invertible design to transform both\n(1) multi-step sampling process and (2) noise estimation U-Net in each step\ninto invertible networks. As a result, most intermediate features are cleared\nduring training to reduce up to 93.8% GPU memory. In addition, we develop a set\nof lightweight modules to inject measurements into noise estimator to further\nfacilitate reconstruction. Experiments demonstrate that IDM outperforms\nexisting state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the\nrecent diffusion-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain\nand 14.54 times faster inference. Code is available at\nhttps://github.com/Guaishou74851/IDM."
                },
                "authors": [
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Weiqi Li"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Jiwen Yu"
                    },
                    {
                        "name": "Shijie Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07641v2",
                "updated": "2025-02-03T07:19:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    7,
                    19,
                    24,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-13T19:04:57Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    19,
                    4,
                    57,
                    0,
                    13,
                    0
                ],
                "title": "GPT as a Monte Carlo Language Tree: A Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT as a Monte Carlo Language Tree: A Probabilistic Perspective"
                },
                "summary": "Large Language Models (LLMs), such as GPT, are considered to learn the latent\ndistributions within large-scale web-crawl datasets and accomplish natural\nlanguage processing (NLP) tasks by predicting the next token. However, this\nmechanism of latent distribution modeling lacks quantitative understanding and\nanalysis. In this paper, we propose a novel perspective that any language\ndataset can be represented by a Monte Carlo Language Tree (abbreviated as\n``Data-Tree''), where each node denotes a token, each edge denotes a token\ntransition probability, and each sequence has a unique path. Any GPT-like\nlanguage model can also be flattened into another Monte Carlo Language Tree\n(abbreviated as ``GPT-Tree''). Our experiments show that different GPT models\ntrained on the same dataset exhibit significant structural similarity in\nGPT-Tree visualization, and larger models converge more closely to the\nData-Tree. More than 87\\% GPT output tokens can be recalled by Data-Tree. These\nfindings may confirm that the reasoning process of LLMs is more likely to be\nprobabilistic pattern-matching rather than formal reasoning, as each model\ninference seems to find a context pattern with maximum probability from the\nData-Tree. Furthermore, we provide deeper insights into issues such as\nhallucination, Chain-of-Thought (CoT) reasoning, and token bias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, are considered to learn the latent\ndistributions within large-scale web-crawl datasets and accomplish natural\nlanguage processing (NLP) tasks by predicting the next token. However, this\nmechanism of latent distribution modeling lacks quantitative understanding and\nanalysis. In this paper, we propose a novel perspective that any language\ndataset can be represented by a Monte Carlo Language Tree (abbreviated as\n``Data-Tree''), where each node denotes a token, each edge denotes a token\ntransition probability, and each sequence has a unique path. Any GPT-like\nlanguage model can also be flattened into another Monte Carlo Language Tree\n(abbreviated as ``GPT-Tree''). Our experiments show that different GPT models\ntrained on the same dataset exhibit significant structural similarity in\nGPT-Tree visualization, and larger models converge more closely to the\nData-Tree. More than 87\\% GPT output tokens can be recalled by Data-Tree. These\nfindings may confirm that the reasoning process of LLMs is more likely to be\nprobabilistic pattern-matching rather than formal reasoning, as each model\ninference seems to find a context pattern with maximum probability from the\nData-Tree. Furthermore, we provide deeper insights into issues such as\nhallucination, Chain-of-Thought (CoT) reasoning, and token bias in LLMs."
                },
                "authors": [
                    {
                        "name": "Kun-Peng Ning"
                    },
                    {
                        "name": "Jia-Yu Yao"
                    },
                    {
                        "name": "Yu-Yang Liu"
                    },
                    {
                        "name": "Mu-Nan Ning"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14569v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14569v3",
                "updated": "2025-02-03T06:52:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    6,
                    52,
                    55,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-18T16:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, generated\nimpersonation posts where 93.9% of them were deemed authentic, and boosted\nclick rate of phishing links in spear phishing emails by 46.67%. Additionally,\nour findings underscore the limitations of existing safeguards in contemporary\ncommercial LLMs, emphasizing the urgent need for robust security measures to\nprevent the misuse of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, generated\nimpersonation posts where 93.9% of them were deemed authentic, and boosted\nclick rate of phishing links in spear phishing emails by 46.67%. Additionally,\nour findings underscore the limitations of existing safeguards in contemporary\ncommercial LLMs, emphasizing the urgent need for robust security measures to\nprevent the misuse of LLM agents."
                },
                "authors": [
                    {
                        "name": "Hanna Kim"
                    },
                    {
                        "name": "Minkyoo Song"
                    },
                    {
                        "name": "Seung Ho Na"
                    },
                    {
                        "name": "Seungwon Shin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_comment": "20 pages, To appear in Usenix Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14569v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18712v2",
                "updated": "2025-02-03T06:36:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    6,
                    36,
                    8,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-30T19:15:41Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    19,
                    15,
                    41,
                    3,
                    30,
                    0
                ],
                "title": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps"
                },
                "summary": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts."
                },
                "authors": [
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Naman Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Naman Mishra"
                },
                "author": "Naman Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.05661v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.05661v3",
                "updated": "2025-02-03T06:26:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    6,
                    26,
                    54,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-09T06:30:28Z",
                "published_parsed": [
                    2024,
                    6,
                    9,
                    6,
                    30,
                    28,
                    6,
                    161,
                    0
                ],
                "title": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked\n  Language Modelling methods for learning Speech Representations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MS-HuBERT: Mitigating Pre-training and Inference Mismatch in Masked\n  Language Modelling methods for learning Speech Representations"
                },
                "summary": "In recent years, self-supervised pre-training methods have gained significant\ntraction in learning high-level information from raw speech. Among these\nmethods, HuBERT has demonstrated SOTA performance in automatic speech\nrecognition (ASR). However, HuBERT's performance lags behind data2vec due to\ndisparities in pre-training strategies. In this paper, we propose (i) a Swap\nmethod to address pre-training and inference mismatch observed in HuBERT and\n(ii) incorporates Multicluster masked prediction loss for more effective\nutilization of the models capacity. The resulting method is, MS-HuBERT, an\nend-to-end self-supervised pre-training method for learning robust speech\nrepresentations. It beats vanilla HuBERT on the ASR Librispeech benchmark on\naverage by a 5% margin when evaluated on different finetuning splits.\nAdditionally, we demonstrate that the learned embeddings obtained during\npre-training encode essential information for improving performance of content\nbased tasks such as ASR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, self-supervised pre-training methods have gained significant\ntraction in learning high-level information from raw speech. Among these\nmethods, HuBERT has demonstrated SOTA performance in automatic speech\nrecognition (ASR). However, HuBERT's performance lags behind data2vec due to\ndisparities in pre-training strategies. In this paper, we propose (i) a Swap\nmethod to address pre-training and inference mismatch observed in HuBERT and\n(ii) incorporates Multicluster masked prediction loss for more effective\nutilization of the models capacity. The resulting method is, MS-HuBERT, an\nend-to-end self-supervised pre-training method for learning robust speech\nrepresentations. It beats vanilla HuBERT on the ASR Librispeech benchmark on\naverage by a 5% margin when evaluated on different finetuning splits.\nAdditionally, we demonstrate that the learned embeddings obtained during\npre-training encode essential information for improving performance of content\nbased tasks such as ASR."
                },
                "authors": [
                    {
                        "name": "Hemant Yadav"
                    },
                    {
                        "name": "Sunayana Sitaram"
                    },
                    {
                        "name": "Rajiv Ratn Shah"
                    }
                ],
                "author_detail": {
                    "name": "Rajiv Ratn Shah"
                },
                "author": "Rajiv Ratn Shah",
                "arxiv_comment": "4 pages, submitted to interspeech2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.05661v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.05661v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19306v2",
                "updated": "2025-02-03T06:21:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    6,
                    21,
                    8,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T17:03:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    3,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws."
                },
                "authors": [
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Chengrun Yang"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Sercan Ö Arık"
                    }
                ],
                "author_detail": {
                    "name": "Sercan Ö Arık"
                },
                "author": "Sercan Ö Arık",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11712v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11712v4",
                "updated": "2025-02-03T05:35:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    35,
                    7,
                    0,
                    34,
                    0
                ],
                "published": "2024-07-16T13:30:14Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    30,
                    14,
                    1,
                    198,
                    0
                ],
                "title": "Fine-tuning Multimodal Large Language Models for Product Bundling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Multimodal Large Language Models for Product Bundling"
                },
                "summary": "Recent advances in product bundling have leveraged multimodal information\nthrough sophisticated encoders, but remain constrained by limited semantic\nunderstanding and a narrow scope of knowledge. Therefore, some attempts employ\nIn-context Learning (ICL) to explore the potential of large language models\n(LLMs) for their extensive knowledge and complex reasoning abilities. However,\nthese efforts are inadequate in understanding mulitmodal data and exploiting\nLLMs' knowledge for product bundling. To bridge the gap, we introduce\nBundle-MLLM, a novel framework that fine-tunes LLMs through a hybrid item\ntokenization approach within a well-designed optimization strategy.\nSpecifically, we integrate textual, media, and relational data into a unified\ntokenization, introducing a soft separation token to distinguish between\ntextual and non-textual tokens. Additionally, a streamlined yet powerful\nmultimodal fusion module is employed to embed all non-textual features into a\nsingle, informative token, significantly boosting efficiency. To tailor product\nbundling tasks for LLMs, we reformulate the task as a multiple-choice question\nwith candidate items as options. We further propose a progressive optimization\nstrategy that fine-tunes LLMs for disentangled objectives: 1) learning bundle\npatterns and 2) enhancing multimodal semantic understanding specific to product\nbundling. Extensive experiments on four datasets across two domains demonstrate\nthat our approach outperforms a range of state-of-the-art (SOTA) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in product bundling have leveraged multimodal information\nthrough sophisticated encoders, but remain constrained by limited semantic\nunderstanding and a narrow scope of knowledge. Therefore, some attempts employ\nIn-context Learning (ICL) to explore the potential of large language models\n(LLMs) for their extensive knowledge and complex reasoning abilities. However,\nthese efforts are inadequate in understanding mulitmodal data and exploiting\nLLMs' knowledge for product bundling. To bridge the gap, we introduce\nBundle-MLLM, a novel framework that fine-tunes LLMs through a hybrid item\ntokenization approach within a well-designed optimization strategy.\nSpecifically, we integrate textual, media, and relational data into a unified\ntokenization, introducing a soft separation token to distinguish between\ntextual and non-textual tokens. Additionally, a streamlined yet powerful\nmultimodal fusion module is employed to embed all non-textual features into a\nsingle, informative token, significantly boosting efficiency. To tailor product\nbundling tasks for LLMs, we reformulate the task as a multiple-choice question\nwith candidate items as options. We further propose a progressive optimization\nstrategy that fine-tunes LLMs for disentangled objectives: 1) learning bundle\npatterns and 2) enhancing multimodal semantic understanding specific to product\nbundling. Extensive experiments on four datasets across two domains demonstrate\nthat our approach outperforms a range of state-of-the-art (SOTA) methods."
                },
                "authors": [
                    {
                        "name": "Xiaohao Liu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Zhulin Tao"
                    },
                    {
                        "name": "Yunshan Ma"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Tat-seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-seng Chua"
                },
                "author": "Tat-seng Chua",
                "arxiv_comment": "Accepted by KDD 2025 (CR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11712v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11712v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.16883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.16883v2",
                "updated": "2025-02-03T05:27:50Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    27,
                    50,
                    0,
                    34,
                    0
                ],
                "published": "2024-08-29T20:12:01Z",
                "published_parsed": [
                    2024,
                    8,
                    29,
                    20,
                    12,
                    1,
                    3,
                    242,
                    0
                ],
                "title": "Multimodal ELBO with Diffusion Decoders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal ELBO with Diffusion Decoders"
                },
                "summary": "Multimodal variational autoencoders have demonstrated their ability to learn\nthe relationships between different modalities by mapping them into a latent\nrepresentation. Their design and capacity to perform any-to-any conditional and\nunconditional generation make them appealing. However, different variants of\nmultimodal VAEs often suffer from generating low-quality output, particularly\nwhen complex modalities such as images are involved. In addition to that, they\nfrequently exhibit low coherence among the generated modalities when sampling\nfrom the joint distribution. To address these limitations, we propose a new\nvariant of the multimodal VAE ELBO that incorporates a better decoder using a\ndiffusion generative model. The diffusion decoder enables the model to learn\ncomplex modalities and generate high-quality outputs. The multimodal model can\nalso seamlessly integrate with a standard feed-forward decoder for different\ntypes of modality, facilitating end-to-end training and inference. Furthermore,\nwe introduce an auxiliary score-based model to enhance the unconditional\ngeneration capabilities of our proposed approach. This approach addresses the\nlimitations imposed by conventional multimodal VAEs and opens up new\npossibilities to improve multimodal generation tasks. Our model provides\nstate-of-the-art results compared to other multimodal VAEs in different\ndatasets with higher coherence and superior quality in the generated\nmodalities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal variational autoencoders have demonstrated their ability to learn\nthe relationships between different modalities by mapping them into a latent\nrepresentation. Their design and capacity to perform any-to-any conditional and\nunconditional generation make them appealing. However, different variants of\nmultimodal VAEs often suffer from generating low-quality output, particularly\nwhen complex modalities such as images are involved. In addition to that, they\nfrequently exhibit low coherence among the generated modalities when sampling\nfrom the joint distribution. To address these limitations, we propose a new\nvariant of the multimodal VAE ELBO that incorporates a better decoder using a\ndiffusion generative model. The diffusion decoder enables the model to learn\ncomplex modalities and generate high-quality outputs. The multimodal model can\nalso seamlessly integrate with a standard feed-forward decoder for different\ntypes of modality, facilitating end-to-end training and inference. Furthermore,\nwe introduce an auxiliary score-based model to enhance the unconditional\ngeneration capabilities of our proposed approach. This approach addresses the\nlimitations imposed by conventional multimodal VAEs and opens up new\npossibilities to improve multimodal generation tasks. Our model provides\nstate-of-the-art results compared to other multimodal VAEs in different\ndatasets with higher coherence and superior quality in the generated\nmodalities."
                },
                "authors": [
                    {
                        "name": "Daniel Wesego"
                    },
                    {
                        "name": "Pedram Rooshenas"
                    }
                ],
                "author_detail": {
                    "name": "Pedram Rooshenas"
                },
                "author": "Pedram Rooshenas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.16883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.16883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06224v2",
                "updated": "2025-02-03T05:23:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    23,
                    40,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-07T09:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    21,
                    20,
                    1,
                    7,
                    0
                ],
                "title": "Detection, Retrieval, and Explanation Unified: A Violence Detection\n  System Based on Knowledge Graphs and GAT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection, Retrieval, and Explanation Unified: A Violence Detection\n  System Based on Knowledge Graphs and GAT"
                },
                "summary": "Recently, violence detection systems developed using unified multimodal\nmodels have achieved significant success and attracted widespread attention.\nHowever, most of these systems face two critical challenges: the lack of\ninterpretability as black-box models and limited functionality, offering only\nclassification or retrieval capabilities. To address these challenges, this\npaper proposes a novel interpretable violence detection system, termed the\nThree-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and\ngraph attention networks (GAT) to provide three core functionalities:\ndetection, retrieval, and explanation. Specifically, the system processes each\nvideo frame along with text descriptions generated by a large language model\n(LLM) for videos containing potential violent behavior. It employs ImageBind to\ngenerate high-dimensional embeddings for constructing a knowledge graph, uses\nGAT for reasoning, and applies lightweight time series modules to extract video\nembedding features. The final step connects a classifier and retriever for\nmulti-functional outputs. The interpretability of KG enables the system to\nverify the reasoning process behind each output. Additionally, the paper\nintroduces several lightweight methods to reduce the resource consumption of\nthe TIO system and enhance its efficiency. Extensive experiments conducted on\nthe XD-Violence and UCF-Crime datasets validate the effectiveness of the\nproposed system. A case study further reveals an intriguing phenomenon: as the\nnumber of bystanders increases, the occurrence of violent behavior tends to\ndecrease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, violence detection systems developed using unified multimodal\nmodels have achieved significant success and attracted widespread attention.\nHowever, most of these systems face two critical challenges: the lack of\ninterpretability as black-box models and limited functionality, offering only\nclassification or retrieval capabilities. To address these challenges, this\npaper proposes a novel interpretable violence detection system, termed the\nThree-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and\ngraph attention networks (GAT) to provide three core functionalities:\ndetection, retrieval, and explanation. Specifically, the system processes each\nvideo frame along with text descriptions generated by a large language model\n(LLM) for videos containing potential violent behavior. It employs ImageBind to\ngenerate high-dimensional embeddings for constructing a knowledge graph, uses\nGAT for reasoning, and applies lightweight time series modules to extract video\nembedding features. The final step connects a classifier and retriever for\nmulti-functional outputs. The interpretability of KG enables the system to\nverify the reasoning process behind each output. Additionally, the paper\nintroduces several lightweight methods to reduce the resource consumption of\nthe TIO system and enhance its efficiency. Extensive experiments conducted on\nthe XD-Violence and UCF-Crime datasets validate the effectiveness of the\nproposed system. A case study further reveals an intriguing phenomenon: as the\nnumber of bystanders increases, the occurrence of violent behavior tends to\ndecrease."
                },
                "authors": [
                    {
                        "name": "Wen-Dong Jiang"
                    },
                    {
                        "name": "Chih-Yung Chang"
                    },
                    {
                        "name": "Diptendu Sinha Roy"
                    }
                ],
                "author_detail": {
                    "name": "Diptendu Sinha Roy"
                },
                "author": "Diptendu Sinha Roy",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14043v2",
                "updated": "2025-02-03T04:50:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    50,
                    39,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-17T21:35:55Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    21,
                    35,
                    55,
                    3,
                    291,
                    0
                ],
                "title": "Retrieval of Temporal Event Sequences from Textual Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval of Temporal Event Sequences from Textual Descriptions"
                },
                "summary": "Retrieving temporal event sequences from textual descriptions is crucial for\napplications such as analyzing e-commerce behavior, monitoring social media\nactivities, and tracking criminal incidents. To advance this task, we introduce\nTESRBench, a comprehensive benchmark for temporal event sequence retrieval\n(TESR) from textual descriptions. TESRBench includes diverse real-world\ndatasets with synthesized and reviewed textual descriptions, providing a strong\nfoundation for evaluating retrieval performance and addressing challenges in\nthis domain. Building on this benchmark, we propose TPP-Embedding, a novel\nmodel for embedding and retrieving event sequences. The model leverages the\nTPP-LLM framework, integrating large language models (LLMs) with temporal point\nprocesses (TPPs) to encode both event texts and times. By pooling\nrepresentations and applying a contrastive loss, it unifies temporal dynamics\nand event semantics in a shared embedding space, aligning sequence-level\nembeddings of event sequences and their descriptions. TPP-Embedding\ndemonstrates superior performance over baseline models across TESRBench\ndatasets, establishing it as a powerful solution for the temporal event\nsequence retrieval task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving temporal event sequences from textual descriptions is crucial for\napplications such as analyzing e-commerce behavior, monitoring social media\nactivities, and tracking criminal incidents. To advance this task, we introduce\nTESRBench, a comprehensive benchmark for temporal event sequence retrieval\n(TESR) from textual descriptions. TESRBench includes diverse real-world\ndatasets with synthesized and reviewed textual descriptions, providing a strong\nfoundation for evaluating retrieval performance and addressing challenges in\nthis domain. Building on this benchmark, we propose TPP-Embedding, a novel\nmodel for embedding and retrieving event sequences. The model leverages the\nTPP-LLM framework, integrating large language models (LLMs) with temporal point\nprocesses (TPPs) to encode both event texts and times. By pooling\nrepresentations and applying a contrastive loss, it unifies temporal dynamics\nand event semantics in a shared embedding space, aligning sequence-level\nembeddings of event sequences and their descriptions. TPP-Embedding\ndemonstrates superior performance over baseline models across TESRBench\ndatasets, establishing it as a powerful solution for the temporal event\nsequence retrieval task."
                },
                "authors": [
                    {
                        "name": "Zefang Liu"
                    },
                    {
                        "name": "Yinzhu Quan"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhu Quan"
                },
                "author": "Yinzhu Quan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2209.01870v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2209.01870v3",
                "updated": "2025-02-03T04:26:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    26,
                    20,
                    0,
                    34,
                    0
                ],
                "published": "2022-09-05T10:06:03Z",
                "published_parsed": [
                    2022,
                    9,
                    5,
                    10,
                    6,
                    3,
                    0,
                    248,
                    0
                ],
                "title": "Unsupervised Domain Adaptation via Style-Aware Self-intermediate Domain",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised Domain Adaptation via Style-Aware Self-intermediate Domain"
                },
                "summary": "Unsupervised domain adaptation (UDA) has attracted considerable attention,\nwhich transfers knowledge from a label-rich source domain to a related but\nunlabeled target domain. Reducing inter-domain differences has always been a\ncrucial factor to improve performance in UDA, especially for tasks where there\nis a large gap between source and target domains. To this end, we propose a\nnovel style-aware feature fusion method (SAFF) to bridge the large domain gap\nand transfer knowledge while alleviating the loss of class-discriminative\ninformation. Inspired by the human transitive inference and learning ability, a\nnovel style-aware self-intermediate domain (SSID) is investigated to link two\nseemingly unrelated concepts through a series of intermediate auxiliary\nsynthesized concepts. Specifically, we propose a novel learning strategy of\nSSID, which selects samples from both source and target domains as anchors, and\nthen randomly fuses the object and style features of these anchors to generate\nlabeled and style-rich intermediate auxiliary features for knowledge transfer.\nMoreover, we design an external memory bank to store and update specified\nlabeled features to obtain stable class features and class-wise style features.\nBased on the proposed memory bank, the intra- and inter-domain loss functions\nare designed to improve the class recognition ability and feature\ncompatibility, respectively. Meanwhile, we simulate the rich latent feature\nspace of SSID by infinite sampling and the convergence of the loss function by\nmathematical theory. Finally, we conduct comprehensive experiments on commonly\nused domain adaptive benchmarks to evaluate the proposed SAFF, and the\nexperimental results show that the proposed SAFF can be easily combined with\ndifferent backbone networks and obtain better performance as a plug-in-plug-out\nmodule.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unsupervised domain adaptation (UDA) has attracted considerable attention,\nwhich transfers knowledge from a label-rich source domain to a related but\nunlabeled target domain. Reducing inter-domain differences has always been a\ncrucial factor to improve performance in UDA, especially for tasks where there\nis a large gap between source and target domains. To this end, we propose a\nnovel style-aware feature fusion method (SAFF) to bridge the large domain gap\nand transfer knowledge while alleviating the loss of class-discriminative\ninformation. Inspired by the human transitive inference and learning ability, a\nnovel style-aware self-intermediate domain (SSID) is investigated to link two\nseemingly unrelated concepts through a series of intermediate auxiliary\nsynthesized concepts. Specifically, we propose a novel learning strategy of\nSSID, which selects samples from both source and target domains as anchors, and\nthen randomly fuses the object and style features of these anchors to generate\nlabeled and style-rich intermediate auxiliary features for knowledge transfer.\nMoreover, we design an external memory bank to store and update specified\nlabeled features to obtain stable class features and class-wise style features.\nBased on the proposed memory bank, the intra- and inter-domain loss functions\nare designed to improve the class recognition ability and feature\ncompatibility, respectively. Meanwhile, we simulate the rich latent feature\nspace of SSID by infinite sampling and the convergence of the loss function by\nmathematical theory. Finally, we conduct comprehensive experiments on commonly\nused domain adaptive benchmarks to evaluate the proposed SAFF, and the\nexperimental results show that the proposed SAFF can be easily combined with\ndifferent backbone networks and obtain better performance as a plug-in-plug-out\nmodule."
                },
                "authors": [
                    {
                        "name": "Lianyu Wang"
                    },
                    {
                        "name": "Meng Wang"
                    },
                    {
                        "name": "Daoqiang Zhang"
                    },
                    {
                        "name": "Huazhu Fu"
                    }
                ],
                "author_detail": {
                    "name": "Huazhu Fu"
                },
                "author": "Huazhu Fu",
                "arxiv_comment": "13 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2209.01870v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2209.01870v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T07",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.m",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.05273v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.05273v3",
                "updated": "2025-02-03T04:07:37Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    7,
                    37,
                    0,
                    34,
                    0
                ],
                "published": "2024-09-12T09:18:09Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    9,
                    18,
                    9,
                    3,
                    256,
                    0
                ],
                "title": "HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HiRT: Enhancing Robotic Control with Hierarchical Robot Transformers"
                },
                "summary": "Large Vision-Language-Action (VLA) models, leveraging powerful pre trained\nVision-Language Models (VLMs) backends, have shown promise in robotic control\ndue to their impressive generalization ability. However, the success comes at a\ncost. Their reliance on VLM backends with billions of parameters leads to high\ncomputational costs and inference latency, limiting the testing scenarios to\nmainly quasi-static tasks and hindering performance in dynamic tasks requiring\nrapid interactions. To address these limitations, this paper proposes HiRT, a\nHierarchical Robot Transformer framework that enables flexible frequency and\nperformance trade-off. HiRT keeps VLMs running at low frequencies to capture\ntemporarily invariant features while enabling real-time interaction through a\nhigh-frequency vision-based policy guided by the slowly updated features.\nExperiment results in both simulation and real-world settings demonstrate\nsignificant improvements over baseline methods. Empirically, in static tasks,\nwe double the control frequency and achieve comparable success rates.\nAdditionally, on novel real-world dynamic ma nipulation tasks which are\nchallenging for previous VLA models, HiRT improves the success rate from 48% to\n75%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Vision-Language-Action (VLA) models, leveraging powerful pre trained\nVision-Language Models (VLMs) backends, have shown promise in robotic control\ndue to their impressive generalization ability. However, the success comes at a\ncost. Their reliance on VLM backends with billions of parameters leads to high\ncomputational costs and inference latency, limiting the testing scenarios to\nmainly quasi-static tasks and hindering performance in dynamic tasks requiring\nrapid interactions. To address these limitations, this paper proposes HiRT, a\nHierarchical Robot Transformer framework that enables flexible frequency and\nperformance trade-off. HiRT keeps VLMs running at low frequencies to capture\ntemporarily invariant features while enabling real-time interaction through a\nhigh-frequency vision-based policy guided by the slowly updated features.\nExperiment results in both simulation and real-world settings demonstrate\nsignificant improvements over baseline methods. Empirically, in static tasks,\nwe double the control frequency and achieve comparable success rates.\nAdditionally, on novel real-world dynamic ma nipulation tasks which are\nchallenging for previous VLA models, HiRT improves the success rate from 48% to\n75%."
                },
                "authors": [
                    {
                        "name": "Jianke Zhang"
                    },
                    {
                        "name": "Yanjiang Guo"
                    },
                    {
                        "name": "Xiaoyu Chen"
                    },
                    {
                        "name": "Yen-Jen Wang"
                    },
                    {
                        "name": "Yucheng Hu"
                    },
                    {
                        "name": "Chengming Shi"
                    },
                    {
                        "name": "Jianyu Chen"
                    }
                ],
                "author_detail": {
                    "name": "Jianyu Chen"
                },
                "author": "Jianyu Chen",
                "arxiv_comment": "Accepted to CORL 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.05273v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.05273v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09136v2",
                "updated": "2025-02-03T04:01:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    1,
                    36,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-15T20:40:25Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    40,
                    25,
                    2,
                    15,
                    0
                ],
                "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG."
                },
                "authors": [
                    {
                        "name": "Aditi Singh"
                    },
                    {
                        "name": "Abul Ehtesham"
                    },
                    {
                        "name": "Saket Kumar"
                    },
                    {
                        "name": "Tala Talaei Khoei"
                    }
                ],
                "author_detail": {
                    "name": "Tala Talaei Khoei"
                },
                "author": "Tala Talaei Khoei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06564v3",
                "updated": "2025-02-03T03:59:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    3,
                    59,
                    0,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-09T15:17:36Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    17,
                    36,
                    0,
                    344,
                    0
                ],
                "title": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering"
                },
                "summary": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research."
                },
                "authors": [
                    {
                        "name": "Matheus de Morais Leça"
                    },
                    {
                        "name": "Lucas Valença"
                    },
                    {
                        "name": "Reydne Santos"
                    },
                    {
                        "name": "Ronnie de Souza Santos"
                    }
                ],
                "author_detail": {
                    "name": "Ronnie de Souza Santos"
                },
                "author": "Ronnie de Souza Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01565v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01565v4",
                "updated": "2025-02-03T03:27:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    3,
                    27,
                    20,
                    0,
                    34,
                    0
                ],
                "published": "2024-11-03T13:36:34Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    13,
                    36,
                    34,
                    6,
                    308,
                    0
                ],
                "title": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models"
                },
                "summary": "In recent years, the rapid development of large language models (LLMs) has\nbrought new vitality into various domains, generating substantial social and\neconomic benefits. However, this swift advancement has also introduced new\nvulnerabilities. Jailbreaking, a form of attack that induces LLMs to produce\nharmful content through carefully crafted prompts, presents a significant\nchallenge to the safe and trustworthy development of LLMs. Previous jailbreak\nmethods primarily exploited the internal properties or capabilities of LLMs,\nsuch as optimization-based jailbreak methods and methods that leveraged the\nmodel's context-learning abilities. In this paper, we introduce a novel\njailbreak method, SQL Injection Jailbreak (SIJ), which targets the external\nproperties of LLMs, specifically, the way LLMs construct input prompts. By\ninjecting jailbreak information into user prompts, SIJ successfully induces the\nmodel to output harmful content. Our SIJ method achieves near 100\\% attack\nsuccess rates on five well-known open-source LLMs on the AdvBench and HEx-PHI,\nwhile incurring lower time costs compared to previous methods. For\nclosed-source models, SIJ achieves near 100% attack success rate on\nGPT-3.5-turbo. Additionally, SIJ exposes a new vulnerability in LLMs that\nurgently requires mitigation. To address this, we propose a simple defense\nmethod called Self-Reminder-Key to counter SIJ and demonstrate its\neffectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large language models (LLMs) has\nbrought new vitality into various domains, generating substantial social and\neconomic benefits. However, this swift advancement has also introduced new\nvulnerabilities. Jailbreaking, a form of attack that induces LLMs to produce\nharmful content through carefully crafted prompts, presents a significant\nchallenge to the safe and trustworthy development of LLMs. Previous jailbreak\nmethods primarily exploited the internal properties or capabilities of LLMs,\nsuch as optimization-based jailbreak methods and methods that leveraged the\nmodel's context-learning abilities. In this paper, we introduce a novel\njailbreak method, SQL Injection Jailbreak (SIJ), which targets the external\nproperties of LLMs, specifically, the way LLMs construct input prompts. By\ninjecting jailbreak information into user prompts, SIJ successfully induces the\nmodel to output harmful content. Our SIJ method achieves near 100\\% attack\nsuccess rates on five well-known open-source LLMs on the AdvBench and HEx-PHI,\nwhile incurring lower time costs compared to previous methods. For\nclosed-source models, SIJ achieves near 100% attack success rate on\nGPT-3.5-turbo. Additionally, SIJ exposes a new vulnerability in LLMs that\nurgently requires mitigation. To address this, we propose a simple defense\nmethod called Self-Reminder-Key to counter SIJ and demonstrate its\neffectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01565v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01565v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15392v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15392v2",
                "updated": "2025-02-03T03:24:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    3,
                    24,
                    4,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-26T04:19:43Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    4,
                    19,
                    43,
                    6,
                    26,
                    0
                ],
                "title": "Faster Configuration Performance Bug Testing with Neural Dual-level\n  Prioritization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Faster Configuration Performance Bug Testing with Neural Dual-level\n  Prioritization"
                },
                "summary": "As software systems become more complex and configurable, more performance\nproblems tend to arise from the configuration designs. This has caused some\nconfiguration options to unexpectedly degrade performance which deviates from\ntheir original expectations designed by the developers. Such discrepancies,\nnamely configuration performance bugs (CPBugs), are devastating and can be\ndeeply hidden in the source code. Yet, efficiently testing CPBugs is difficult,\nnot only due to the test oracle is hard to set, but also because the\nconfiguration measurement is expensive and there are simply too many possible\nconfigurations to test. As such, existing testing tools suffer from lengthy\nruntime or have been ineffective in detecting CPBugs when the budget is\nlimited, compounded by inaccurate test oracle. In this paper, we seek to\nachieve significantly faster CPBug testing by neurally prioritizing the testing\nat both the configuration option and value range levels with automated oracle\nestimation. Our proposed tool, dubbed NDP, is a general framework that works\nwith different heuristic generators. The idea is to leverage two neural\nlanguage models: one to estimate the CPBug types that serve as the oracle\nwhile, more vitally, the other to infer the probabilities of an option being\nCPBug-related, based on which the options and the value ranges to be searched\ncan be prioritized. Experiments on several widely-used systems of different\nversions reveal that NDP can, in general, better predict CPBug type in 87%\ncases and find more CPBugs with up to 88.88x testing efficiency speedup over\nthe state-of-the-art tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As software systems become more complex and configurable, more performance\nproblems tend to arise from the configuration designs. This has caused some\nconfiguration options to unexpectedly degrade performance which deviates from\ntheir original expectations designed by the developers. Such discrepancies,\nnamely configuration performance bugs (CPBugs), are devastating and can be\ndeeply hidden in the source code. Yet, efficiently testing CPBugs is difficult,\nnot only due to the test oracle is hard to set, but also because the\nconfiguration measurement is expensive and there are simply too many possible\nconfigurations to test. As such, existing testing tools suffer from lengthy\nruntime or have been ineffective in detecting CPBugs when the budget is\nlimited, compounded by inaccurate test oracle. In this paper, we seek to\nachieve significantly faster CPBug testing by neurally prioritizing the testing\nat both the configuration option and value range levels with automated oracle\nestimation. Our proposed tool, dubbed NDP, is a general framework that works\nwith different heuristic generators. The idea is to leverage two neural\nlanguage models: one to estimate the CPBug types that serve as the oracle\nwhile, more vitally, the other to infer the probabilities of an option being\nCPBug-related, based on which the options and the value ranges to be searched\ncan be prioritized. Experiments on several widely-used systems of different\nversions reveal that NDP can, in general, better predict CPBug type in 87%\ncases and find more CPBugs with up to 88.88x testing efficiency speedup over\nthe state-of-the-art tools."
                },
                "authors": [
                    {
                        "name": "Youpeng Ma"
                    },
                    {
                        "name": "Tao Chen"
                    },
                    {
                        "name": "Ke Li"
                    }
                ],
                "author_detail": {
                    "name": "Ke Li"
                },
                "author": "Ke Li",
                "arxiv_comment": "accepted by ICSE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15392v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15392v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.16232v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.16232v2",
                "updated": "2025-02-03T02:54:56Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    2,
                    54,
                    56,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-19T02:38:31Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    2,
                    38,
                    31,
                    3,
                    354,
                    0
                ],
                "title": "Defeasible Visual Entailment: Benchmark, Evaluator, and Reward-Driven\n  Optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defeasible Visual Entailment: Benchmark, Evaluator, and Reward-Driven\n  Optimization"
                },
                "summary": "We introduce a new task called Defeasible Visual Entailment (DVE), where the\ngoal is to allow the modification of the entailment relationship between an\nimage premise and a text hypothesis based on an additional update. While this\nconcept is well-established in Natural Language Inference, it remains\nunexplored in visual entailment. At a high level, DVE enables models to refine\ntheir initial interpretations, leading to improved accuracy and reliability in\nvarious applications such as detecting misleading information in images,\nenhancing visual question answering, and refining decision-making processes in\nautonomous systems. Existing metrics do not adequately capture the change in\nthe entailment relationship brought by updates. To address this, we propose a\nnovel inference-aware evaluator designed to capture changes in entailment\nstrength induced by updates, using pairwise contrastive learning and\ncategorical information learning. Additionally, we introduce a reward-driven\nupdate optimization method to further enhance the quality of updates generated\nby multimodal models. Experimental results demonstrate the effectiveness of our\nproposed evaluator and optimization method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a new task called Defeasible Visual Entailment (DVE), where the\ngoal is to allow the modification of the entailment relationship between an\nimage premise and a text hypothesis based on an additional update. While this\nconcept is well-established in Natural Language Inference, it remains\nunexplored in visual entailment. At a high level, DVE enables models to refine\ntheir initial interpretations, leading to improved accuracy and reliability in\nvarious applications such as detecting misleading information in images,\nenhancing visual question answering, and refining decision-making processes in\nautonomous systems. Existing metrics do not adequately capture the change in\nthe entailment relationship brought by updates. To address this, we propose a\nnovel inference-aware evaluator designed to capture changes in entailment\nstrength induced by updates, using pairwise contrastive learning and\ncategorical information learning. Additionally, we introduce a reward-driven\nupdate optimization method to further enhance the quality of updates generated\nby multimodal models. Experimental results demonstrate the effectiveness of our\nproposed evaluator and optimization method."
                },
                "authors": [
                    {
                        "name": "Yue Zhang"
                    },
                    {
                        "name": "Liqiang Jing"
                    },
                    {
                        "name": "Vibhav Gogate"
                    }
                ],
                "author_detail": {
                    "name": "Vibhav Gogate"
                },
                "author": "Vibhav Gogate",
                "arxiv_comment": "Accepted by AAAI 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.16232v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.16232v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12433v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12433v4",
                "updated": "2025-02-03T02:14:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    2,
                    14,
                    26,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-18T09:29:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    9,
                    29,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations"
                },
                "summary": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12433v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12433v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.13686v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.13686v3",
                "updated": "2025-02-03T01:52:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    1,
                    52,
                    59,
                    0,
                    34,
                    0
                ],
                "published": "2022-12-28T03:48:01Z",
                "published_parsed": [
                    2022,
                    12,
                    28,
                    3,
                    48,
                    1,
                    2,
                    362,
                    0
                ],
                "title": "Statistical inference for high-dimensional spectral density matrix",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Statistical inference for high-dimensional spectral density matrix"
                },
                "summary": "The spectral density matrix is a fundamental object of interest in time\nseries analysis, and it encodes both contemporary and dynamic linear\nrelationships between component processes of the multivariate system. In this\npaper we develop novel inference procedures for the spectral density matrix in\nthe high-dimensional setting. Specifically, we introduce a new global testing\nprocedure to test the nullity of the cross-spectral density for a given set of\nfrequencies and across pairs of component indices. For the first time, both\nGaussian approximation and parametric bootstrap methodologies are employed to\nconduct inference for a high-dimensional parameter formulated in the frequency\ndomain, and new technical tools are developed to provide asymptotic guarantees\nof the size accuracy and power for global testing. We further propose a\nmultiple testing procedure for simultaneously testing the nullity of the\ncross-spectral density at a given set of frequencies. The method is shown to\ncontrol the false discovery rate. Both numerical simulations and a real data\nillustration demonstrate the usefulness of the proposed testing methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The spectral density matrix is a fundamental object of interest in time\nseries analysis, and it encodes both contemporary and dynamic linear\nrelationships between component processes of the multivariate system. In this\npaper we develop novel inference procedures for the spectral density matrix in\nthe high-dimensional setting. Specifically, we introduce a new global testing\nprocedure to test the nullity of the cross-spectral density for a given set of\nfrequencies and across pairs of component indices. For the first time, both\nGaussian approximation and parametric bootstrap methodologies are employed to\nconduct inference for a high-dimensional parameter formulated in the frequency\ndomain, and new technical tools are developed to provide asymptotic guarantees\nof the size accuracy and power for global testing. We further propose a\nmultiple testing procedure for simultaneously testing the nullity of the\ncross-spectral density at a given set of frequencies. The method is shown to\ncontrol the false discovery rate. Both numerical simulations and a real data\nillustration demonstrate the usefulness of the proposed testing methods."
                },
                "authors": [
                    {
                        "name": "Jinyuan Chang"
                    },
                    {
                        "name": "Qing Jiang"
                    },
                    {
                        "name": "Tucker S. McElroy"
                    },
                    {
                        "name": "Xiaofeng Shao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaofeng Shao"
                },
                "author": "Xiaofeng Shao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.13686v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.13686v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10027v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10027v4",
                "updated": "2025-02-03T01:26:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    1,
                    26,
                    49,
                    0,
                    34,
                    0
                ],
                "published": "2024-09-16T06:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    35,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models"
                },
                "summary": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/."
                },
                "authors": [
                    {
                        "name": "Chan Kim"
                    },
                    {
                        "name": "Keonwoo Kim"
                    },
                    {
                        "name": "Mintaek Oh"
                    },
                    {
                        "name": "Hanbi Baek"
                    },
                    {
                        "name": "Jiyang Lee"
                    },
                    {
                        "name": "Donghwi Jung"
                    },
                    {
                        "name": "Soojin Woo"
                    },
                    {
                        "name": "Younkyung Woo"
                    },
                    {
                        "name": "John Tucker"
                    },
                    {
                        "name": "Roya Firoozi"
                    },
                    {
                        "name": "Seung-Woo Seo"
                    },
                    {
                        "name": "Mac Schwager"
                    },
                    {
                        "name": "Seong-Woo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Woo Kim"
                },
                "author": "Seong-Woo Kim",
                "arxiv_comment": "19 pages, 28 figures. Project page: https://e2map.github.io. Accepted\n  to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10027v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10027v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05887v2",
                "updated": "2025-02-03T01:11:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    1,
                    11,
                    7,
                    0,
                    34,
                    0
                ],
                "published": "2024-03-09T11:59:10Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    11,
                    59,
                    10,
                    5,
                    69,
                    0
                ],
                "title": "Aligning Speech to Languages to Enhance Code-switching Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Speech to Languages to Enhance Code-switching Speech\n  Recognition"
                },
                "summary": "Code-switching (CS) refers to the switching of languages within a speech\nsignal and results in language confusion for automatic speech recognition\n(ASR). To address language confusion, we introduce a novel language alignment\nloss into ASR training to align acoustic features to pseudo-language labels\nlearned from the ASR decoder. This approach enables frame-level language\nidentification without the need for frame-level language annotations. To\nfurther tackle the complex token alternatives for language modeling in\nbilingual scenarios, we propose to employ large language models via a\ngenerative error correction method. A linguistic hint, which is derived from\nLAL outputs and decoded hypotheses, is introduced to guide the prompting and\nenhance the LLM-based generative error correction for CS-ASR. The proposed\nmethods are evaluated on the SEAME dataset and data from the ASRU 2019\nMandarin-English code-switching speech recognition challenge. The incorporation\nof the proposed language alignment loss improves the CS-ASR performance for\nboth hybrid CTC/attention and Whisper models on both datasets, with only a\nnegligible increase in the number of parameters. This work also highlights the\nefficacy of language alignment loss in balancing primary-language-dominant\nbilingual data during training, with an 8.6% relative improvement on the ASRU\ndataset compared to the baseline model. Performance evaluation using large\nlanguage models reveals the advantage of the linguistic hint by achieving 14.1%\nand 5.5% relative improvement on test sets of the ASRU and SEAME datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-switching (CS) refers to the switching of languages within a speech\nsignal and results in language confusion for automatic speech recognition\n(ASR). To address language confusion, we introduce a novel language alignment\nloss into ASR training to align acoustic features to pseudo-language labels\nlearned from the ASR decoder. This approach enables frame-level language\nidentification without the need for frame-level language annotations. To\nfurther tackle the complex token alternatives for language modeling in\nbilingual scenarios, we propose to employ large language models via a\ngenerative error correction method. A linguistic hint, which is derived from\nLAL outputs and decoded hypotheses, is introduced to guide the prompting and\nenhance the LLM-based generative error correction for CS-ASR. The proposed\nmethods are evaluated on the SEAME dataset and data from the ASRU 2019\nMandarin-English code-switching speech recognition challenge. The incorporation\nof the proposed language alignment loss improves the CS-ASR performance for\nboth hybrid CTC/attention and Whisper models on both datasets, with only a\nnegligible increase in the number of parameters. This work also highlights the\nefficacy of language alignment loss in balancing primary-language-dominant\nbilingual data during training, with an 8.6% relative improvement on the ASRU\ndataset compared to the baseline model. Performance evaluation using large\nlanguage models reveals the advantage of the linguistic hint by achieving 14.1%\nand 5.5% relative improvement on test sets of the ASRU and SEAME datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Leibny Paola Garcia"
                    },
                    {
                        "name": "Andy W. H. Khong"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "arxiv_comment": "Manuscript submitted to IEEE/ACM Transactions on Audio, Speech, and\n  Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.04811v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.04811v2",
                "updated": "2025-02-03T00:30:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    0,
                    30,
                    24,
                    0,
                    34,
                    0
                ],
                "published": "2024-01-09T20:43:20Z",
                "published_parsed": [
                    2024,
                    1,
                    9,
                    20,
                    43,
                    20,
                    1,
                    9,
                    0
                ],
                "title": "Monte Carlo, blocking, and inference: How to measure the renormalization\n  group flow",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Monte Carlo, blocking, and inference: How to measure the renormalization\n  group flow"
                },
                "summary": "Renormalization group theory is a powerful and intriguing technique with a\nwide range of applications. One of the main successes of renormalization group\ntheory is the description of continuous phase transitions and the development\nof scaling theory. Most courses on phase transitions focus on scaling and\ncritical exponents, while less attention is paid to universality,\nrenormalization group flow, and the existence of a unique fixed point, which\nare the ultimate reasons why scaling theory is so effective in describing\ncontinuous phase transitions. We use a combination of Monte Carlo simulations\nand real space renormalization group theory to determine the renormalization\ngroup flow and to show the existence of a universal fixed point in the context\nof the ferromagnetic Ising model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Renormalization group theory is a powerful and intriguing technique with a\nwide range of applications. One of the main successes of renormalization group\ntheory is the description of continuous phase transitions and the development\nof scaling theory. Most courses on phase transitions focus on scaling and\ncritical exponents, while less attention is paid to universality,\nrenormalization group flow, and the existence of a unique fixed point, which\nare the ultimate reasons why scaling theory is so effective in describing\ncontinuous phase transitions. We use a combination of Monte Carlo simulations\nand real space renormalization group theory to determine the renormalization\ngroup flow and to show the existence of a universal fixed point in the context\nof the ferromagnetic Ising model."
                },
                "authors": [
                    {
                        "name": "Luca Di Carlo"
                    }
                ],
                "author_detail": {
                    "name": "Luca Di Carlo"
                },
                "author": "Luca Di Carlo",
                "arxiv_doi": "10.1119/5.0179365",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1119/5.0179365",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.04811v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.04811v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11060v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11060v3",
                "updated": "2025-02-03T00:23:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    0,
                    23,
                    45,
                    0,
                    34,
                    0
                ],
                "published": "2024-02-16T20:20:43Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    20,
                    20,
                    43,
                    4,
                    47,
                    0
                ],
                "title": "Persona-DB: Efficient Large Language Model Personalization for Response\n  Prediction with Collaborative Data Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona-DB: Efficient Large Language Model Personalization for Response\n  Prediction with Collaborative Data Refinement"
                },
                "summary": "The increasing demand for personalized interactions with large language\nmodels (LLMs) calls for methodologies capable of accurately and efficiently\nidentifying user opinions and preferences. Retrieval augmentation emerges as an\neffective strategy, as it can accommodate a vast number of users without the\ncosts from fine-tuning. Existing research, however, has largely focused on\nenhancing the retrieval stage and devoted limited exploration toward optimizing\nthe representation of the database, a crucial aspect for tasks such as\npersonalization. In this work, we examine the problem from a novel angle,\nfocusing on how data can be better represented for more data-efficient\nretrieval in the context of LLM customization. To tackle this challenge, we\nintroduce Persona-DB, a simple yet effective framework consisting of a\nhierarchical construction process to improve generalization across task\ncontexts and collaborative refinement to effectively bridge knowledge gaps\namong users. In the evaluation of response prediction, Persona-DB demonstrates\nsuperior context efficiency in maintaining accuracy with a significantly\nreduced retrieval size, a critical advantage in scenarios with extensive\nhistories or limited context windows. Our experiments also indicate a marked\nimprovement of over 10% under cold-start scenarios, when users have extremely\nsparse data. Furthermore, our analysis reveals the increasing importance of\ncollaborative knowledge as the retrieval capacity expands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for personalized interactions with large language\nmodels (LLMs) calls for methodologies capable of accurately and efficiently\nidentifying user opinions and preferences. Retrieval augmentation emerges as an\neffective strategy, as it can accommodate a vast number of users without the\ncosts from fine-tuning. Existing research, however, has largely focused on\nenhancing the retrieval stage and devoted limited exploration toward optimizing\nthe representation of the database, a crucial aspect for tasks such as\npersonalization. In this work, we examine the problem from a novel angle,\nfocusing on how data can be better represented for more data-efficient\nretrieval in the context of LLM customization. To tackle this challenge, we\nintroduce Persona-DB, a simple yet effective framework consisting of a\nhierarchical construction process to improve generalization across task\ncontexts and collaborative refinement to effectively bridge knowledge gaps\namong users. In the evaluation of response prediction, Persona-DB demonstrates\nsuperior context efficiency in maintaining accuracy with a significantly\nreduced retrieval size, a critical advantage in scenarios with extensive\nhistories or limited context windows. Our experiments also indicate a marked\nimprovement of over 10% under cold-start scenarios, when users have extremely\nsparse data. Furthermore, our analysis reveals the increasing importance of\ncollaborative knowledge as the retrieval capacity expands."
                },
                "authors": [
                    {
                        "name": "Chenkai Sun"
                    },
                    {
                        "name": "Ke Yang"
                    },
                    {
                        "name": "Revanth Gangi Reddy"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Kevin Small"
                    },
                    {
                        "name": "ChengXiang Zhai"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11060v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11060v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06484v2",
                "updated": "2025-02-02T23:58:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    23,
                    58,
                    48,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-09T13:34:23Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    13,
                    34,
                    23,
                    0,
                    344,
                    0
                ],
                "title": "Small Languages, Big Models: A Study of Continual Training on Languages\n  of Norway",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Small Languages, Big Models: A Study of Continual Training on Languages\n  of Norway"
                },
                "summary": "Training large language models requires vast amounts of data, posing a\nchallenge for less widely spoken languages like Norwegian and even more so for\ntruly low-resource languages like Northern S\\'ami. To address this issue, we\npresent a novel three-stage continual training approach that substantially\nimproves the downstream performance together with the inference efficiency for\nthe target languages. Based on our findings, we train, evaluate, and openly\nrelease a new generative language model for Norwegian Bokm\\r{a}l, Nynorsk, and\nNorthern S\\'ami with 11.4 billion parameters: NorMistral-11B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Training large language models requires vast amounts of data, posing a\nchallenge for less widely spoken languages like Norwegian and even more so for\ntruly low-resource languages like Northern S\\'ami. To address this issue, we\npresent a novel three-stage continual training approach that substantially\nimproves the downstream performance together with the inference efficiency for\nthe target languages. Based on our findings, we train, evaluate, and openly\nrelease a new generative language model for Norwegian Bokm\\r{a}l, Nynorsk, and\nNorthern S\\'ami with 11.4 billion parameters: NorMistral-11B."
                },
                "authors": [
                    {
                        "name": "David Samuel"
                    },
                    {
                        "name": "Vladislav Mikhailov"
                    },
                    {
                        "name": "Erik Velldal"
                    },
                    {
                        "name": "Lilja Øvrelid"
                    },
                    {
                        "name": "Lucas Georges Gabriel Charpentier"
                    },
                    {
                        "name": "Andrey Kutuzov"
                    },
                    {
                        "name": "Stephan Oepen"
                    }
                ],
                "author_detail": {
                    "name": "Stephan Oepen"
                },
                "author": "Stephan Oepen",
                "arxiv_comment": "Published at NoDaLiDa 2025",
                "arxiv_journal_ref": "Proceedings of the 25th Nordic Conference on Computational\n  Linguistics (NoDaLiDa 2025). Tallinn, Estonia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02626v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02626v3",
                "updated": "2025-02-02T22:13:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    22,
                    13,
                    29,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-03T17:54:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    54,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Reversal Provides Unsupervised Feedback to LLMs"
                },
                "summary": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard."
                },
                "authors": [
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Rahul Madhavan"
                    },
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "Accepted as a spotlight in NeurIPS 2024",
                "arxiv_journal_ref": "The Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02626v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02626v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12004v2",
                "updated": "2025-02-02T21:27:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    27,
                    6,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-16T17:32:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    32,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "The Open Source Advantage in Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Source Advantage in Large Language Models (LLMs)"
                },
                "summary": "Large language models (LLMs) have rapidly advanced natural language\nprocessing, driving significant breakthroughs in tasks such as text generation,\nmachine translation, and domain-specific reasoning. The field now faces a\ncritical dilemma in its approach: closed-source models like GPT-4 deliver\nstate-of-the-art performance but restrict reproducibility, accessibility, and\nexternal oversight, while open-source frameworks like LLaMA and Mixtral\ndemocratize access, foster collaboration, and support diverse applications,\nachieving competitive results through techniques like instruction tuning and\nLoRA. Hybrid approaches address challenges like bias mitigation and resource\naccessibility by combining the scalability of closed-source systems with the\ntransparency and inclusivity of open-source framework. However, in this\nposition paper, we argue that open-source remains the most robust path for\nadvancing LLM research and ethical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly advanced natural language\nprocessing, driving significant breakthroughs in tasks such as text generation,\nmachine translation, and domain-specific reasoning. The field now faces a\ncritical dilemma in its approach: closed-source models like GPT-4 deliver\nstate-of-the-art performance but restrict reproducibility, accessibility, and\nexternal oversight, while open-source frameworks like LLaMA and Mixtral\ndemocratize access, foster collaboration, and support diverse applications,\nachieving competitive results through techniques like instruction tuning and\nLoRA. Hybrid approaches address challenges like bias mitigation and resource\naccessibility by combining the scalability of closed-source systems with the\ntransparency and inclusivity of open-source framework. However, in this\nposition paper, we argue that open-source remains the most robust path for\nadvancing LLM research and ethical deployment."
                },
                "authors": [
                    {
                        "name": "Jiya Manchanda"
                    },
                    {
                        "name": "Laura Boettcher"
                    },
                    {
                        "name": "Matheus Westphalen"
                    },
                    {
                        "name": "Jasser Jasser"
                    }
                ],
                "author_detail": {
                    "name": "Jasser Jasser"
                },
                "author": "Jasser Jasser",
                "arxiv_comment": "9 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11781v2",
                "updated": "2025-02-02T20:27:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    20,
                    27,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2024-10-15T17:00:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    0,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Language Models Encode Numbers Using Digit Representations in Base 10",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Encode Numbers Using Digit Representations in Base 10"
                },
                "summary": "Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across the digits of the answer rather than normally\naround its numeric value. Through a series of probing experiments and causal\ninterventions, we show that LLMs internally represent numbers with individual\ncircular representations per-digit in base 10. This digit-wise representation,\nas opposed to a value representation, sheds light on the error patterns of\nmodels on tasks involving numerical reasoning and could serve as a basis for\nfuture studies on analyzing numerical mechanisms in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across the digits of the answer rather than normally\naround its numeric value. Through a series of probing experiments and causal\ninterventions, we show that LLMs internally represent numbers with individual\ncircular representations per-digit in base 10. This digit-wise representation,\nas opposed to a value representation, sheds light on the error patterns of\nmodels on tasks involving numerical reasoning and could serve as a basis for\nfuture studies on analyzing numerical mechanisms in LLMs."
                },
                "authors": [
                    {
                        "name": "Amit Arnold Levy"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16452v2",
                "updated": "2025-02-02T20:20:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    20,
                    20,
                    48,
                    6,
                    33,
                    0
                ],
                "published": "2024-09-24T20:44:30Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    20,
                    44,
                    30,
                    1,
                    268,
                    0
                ],
                "title": "FMDLlama: Financial Misinformation Detection based on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMDLlama: Financial Misinformation Detection based on Large Language\n  Models"
                },
                "summary": "The emergence of social media has made the spread of misinformation easier.\nIn the financial domain, the accuracy of information is crucial for various\naspects of financial market, which has made financial misinformation detection\n(FMD) an urgent problem that needs to be addressed. Large language models\n(LLMs) have demonstrated outstanding performance in various fields. However,\ncurrent studies mostly rely on traditional methods and have not explored the\napplication of LLMs in the field of FMD. The main reason is the lack of FMD\ninstruction tuning datasets and evaluation benchmarks. In this paper, we\npropose FMDLlama, the first open-sourced instruction-following LLMs for FMD\ntask based on fine-tuning Llama3.1 with instruction data, the first multi-task\nFMD instruction dataset (FMDID) to support LLM instruction tuning, and a\ncomprehensive FMD evaluation benchmark (FMD-B) with classification and\nexplanation generation tasks to test the FMD ability of LLMs. We compare our\nmodels with a variety of LLMs on FMD-B, where our model outperforms other\nopen-sourced LLMs as well as OpenAI's products. This project is available at\nhttps://github.com/lzw108/FMD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of social media has made the spread of misinformation easier.\nIn the financial domain, the accuracy of information is crucial for various\naspects of financial market, which has made financial misinformation detection\n(FMD) an urgent problem that needs to be addressed. Large language models\n(LLMs) have demonstrated outstanding performance in various fields. However,\ncurrent studies mostly rely on traditional methods and have not explored the\napplication of LLMs in the field of FMD. The main reason is the lack of FMD\ninstruction tuning datasets and evaluation benchmarks. In this paper, we\npropose FMDLlama, the first open-sourced instruction-following LLMs for FMD\ntask based on fine-tuning Llama3.1 with instruction data, the first multi-task\nFMD instruction dataset (FMDID) to support LLM instruction tuning, and a\ncomprehensive FMD evaluation benchmark (FMD-B) with classification and\nexplanation generation tasks to test the FMD ability of LLMs. We compare our\nmodels with a variety of LLMs on FMD-B, where our model outperforms other\nopen-sourced LLMs as well as OpenAI's products. This project is available at\nhttps://github.com/lzw108/FMD."
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "Accepted by The Web Conference (WWW) 2025 Short Paper Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05541v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05541v3",
                "updated": "2025-02-02T20:04:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    20,
                    4,
                    51,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-09T19:27:28Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    27,
                    28,
                    3,
                    9,
                    0
                ],
                "title": "Customizable LLM-Powered Chatbot for Behavioral Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizable LLM-Powered Chatbot for Behavioral Science Research"
                },
                "summary": "The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general."
                },
                "authors": [
                    {
                        "name": "Zenon Lamprou"
                    },
                    {
                        "name": "Yashar Moshfeghi"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Moshfeghi"
                },
                "author": "Yashar Moshfeghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05541v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05541v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.04802v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.04802v5",
                "updated": "2025-02-02T19:38:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    38,
                    20,
                    6,
                    33,
                    0
                ],
                "published": "2023-06-07T21:51:56Z",
                "published_parsed": [
                    2023,
                    6,
                    7,
                    21,
                    51,
                    56,
                    2,
                    158,
                    0
                ],
                "title": "A Review on Knowledge Graphs for Healthcare: Resources, Applications,\n  and Promises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review on Knowledge Graphs for Healthcare: Resources, Applications,\n  and Promises"
                },
                "summary": "This comprehensive review aims to provide an overview of the current state of\nHealthcare Knowledge Graphs (HKGs), including their construction, utilization\nmodels, and applications across various healthcare and biomedical research\ndomains. We thoroughly analyzed existing literature on HKGs, covering their\nconstruction methodologies, utilization techniques, and applications in basic\nscience research, pharmaceutical research and development, clinical decision\nsupport, and public health. The review encompasses both model-free and\nmodel-based utilization approaches and the integration of HKGs with large\nlanguage models (LLMs). We searched Google Scholar for relevant papers on HKGs\nand classified them into the following topics: HKG construction, HKG\nutilization, and their downstream applications in various domains. We also\ndiscussed their special challenges and the promise for future work. The review\nhighlights the potential of HKGs to significantly impact biomedical research\nand clinical practice by integrating vast amounts of biomedical knowledge from\nmultiple domains. The synergy between HKGs and LLMs offers promising\nopportunities for constructing more comprehensive knowledge graphs and\nimproving the accuracy of healthcare applications. HKGs have emerged as a\npowerful tool for structuring medical knowledge, with broad applications across\nbiomedical research, clinical decision-making, and public health. This survey\nserves as a roadmap for future research and development in the field of HKGs,\nhighlighting the potential of combining knowledge graphs with advanced machine\nlearning models for healthcare transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review aims to provide an overview of the current state of\nHealthcare Knowledge Graphs (HKGs), including their construction, utilization\nmodels, and applications across various healthcare and biomedical research\ndomains. We thoroughly analyzed existing literature on HKGs, covering their\nconstruction methodologies, utilization techniques, and applications in basic\nscience research, pharmaceutical research and development, clinical decision\nsupport, and public health. The review encompasses both model-free and\nmodel-based utilization approaches and the integration of HKGs with large\nlanguage models (LLMs). We searched Google Scholar for relevant papers on HKGs\nand classified them into the following topics: HKG construction, HKG\nutilization, and their downstream applications in various domains. We also\ndiscussed their special challenges and the promise for future work. The review\nhighlights the potential of HKGs to significantly impact biomedical research\nand clinical practice by integrating vast amounts of biomedical knowledge from\nmultiple domains. The synergy between HKGs and LLMs offers promising\nopportunities for constructing more comprehensive knowledge graphs and\nimproving the accuracy of healthcare applications. HKGs have emerged as a\npowerful tool for structuring medical knowledge, with broad applications across\nbiomedical research, clinical decision-making, and public health. This survey\nserves as a roadmap for future research and development in the field of HKGs,\nhighlighting the potential of combining knowledge graphs with advanced machine\nlearning models for healthcare transformation."
                },
                "authors": [
                    {
                        "name": "Hejie Cui"
                    },
                    {
                        "name": "Jiaying Lu"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Wenjing Ma"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Shaojun Yu"
                    },
                    {
                        "name": "Xuan Kan"
                    },
                    {
                        "name": "Chen Ling"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Zhaohui S. Qin"
                    },
                    {
                        "name": "Joyce C. Ho"
                    },
                    {
                        "name": "Tianfan Fu"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Mengdi Huai"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Carl Yang"
                    }
                ],
                "author_detail": {
                    "name": "Carl Yang"
                },
                "author": "Carl Yang",
                "arxiv_comment": "Preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.04802v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.04802v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30, 68T50, 68T09",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; I.2.6; I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02688v3",
                "updated": "2025-02-02T19:28:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    28,
                    39,
                    6,
                    33,
                    0
                ],
                "published": "2024-11-05T00:16:01Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    0,
                    16,
                    1,
                    1,
                    310,
                    0
                ],
                "title": "On the Loss of Context-awareness in General Instruction Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Loss of Context-awareness in General Instruction Fine-tuning"
                },
                "summary": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We identify and demonstrate that the loss of context\nawareness, particularly in open-source models, occurs in instruction fine-tuned\nLLMs when the chat template is applied to input prompts. We identify that the\nperformance decline is associated with a bias toward different roles learned\nduring conversational instruction fine-tuning. We demonstrate this correlation\nby visualizing changes in attention allocation after the chat template is\napplied and manually steering the attention heads. The bias can be learned from\ntraining examples that align with the model's internal knowledge and rely less\non the user-provided context to generate correct responses. Based on these\nobservations, we propose a metric to identify context-dependent examples from\ngeneral instruction fine-tuning datasets. We then apply conditional instruction\nfine-tuning with a context-dependency indicator, enabling the model to preserve\ncontext awareness after SFT. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We identify and demonstrate that the loss of context\nawareness, particularly in open-source models, occurs in instruction fine-tuned\nLLMs when the chat template is applied to input prompts. We identify that the\nperformance decline is associated with a bias toward different roles learned\nduring conversational instruction fine-tuning. We demonstrate this correlation\nby visualizing changes in attention allocation after the chat template is\napplied and manually steering the attention heads. The bias can be learned from\ntraining examples that align with the model's internal knowledge and rely less\non the user-provided context to generate correct responses. Based on these\nobservations, we propose a metric to identify context-dependent examples from\ngeneral instruction fine-tuning datasets. We then apply conditional instruction\nfine-tuning with a context-dependency indicator, enabling the model to preserve\ncontext awareness after SFT. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Cho-Jui Hsieh"
                },
                "author": "Cho-Jui Hsieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08368v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08368v2",
                "updated": "2025-02-02T19:28:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    28,
                    27,
                    6,
                    33,
                    0
                ],
                "published": "2024-10-10T20:54:15Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    20,
                    54,
                    15,
                    3,
                    284,
                    0
                ],
                "title": "ElasticTok: Adaptive Tokenization for Image and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ElasticTok: Adaptive Tokenization for Image and Video"
                },
                "summary": "Efficient video tokenization remains a key bottleneck in learning general\npurpose vision models that are capable of processing long video sequences.\nPrevailing approaches are restricted to encoding videos to a fixed number of\ntokens, where too few tokens will result in overly lossy encodings, and too\nmany tokens will result in prohibitively long sequence lengths. In this work,\nwe introduce ElasticTok, a method that conditions on prior frames to adaptively\nencode a frame into a variable number of tokens. To enable this in a\ncomputationally scalable way, we propose a masking technique that drops a\nrandom number of tokens at the end of each frames's token encoding. During\ninference, ElasticTok can dynamically allocate tokens when needed -- more\ncomplex data can leverage more tokens, while simpler data only needs a few\ntokens. Our empirical evaluations on images and video demonstrate the\neffectiveness of our approach in efficient token usage, paving the way for\nfuture development of more powerful multimodal models, world models, and\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient video tokenization remains a key bottleneck in learning general\npurpose vision models that are capable of processing long video sequences.\nPrevailing approaches are restricted to encoding videos to a fixed number of\ntokens, where too few tokens will result in overly lossy encodings, and too\nmany tokens will result in prohibitively long sequence lengths. In this work,\nwe introduce ElasticTok, a method that conditions on prior frames to adaptively\nencode a frame into a variable number of tokens. To enable this in a\ncomputationally scalable way, we propose a masking technique that drops a\nrandom number of tokens at the end of each frames's token encoding. During\ninference, ElasticTok can dynamically allocate tokens when needed -- more\ncomplex data can leverage more tokens, while simpler data only needs a few\ntokens. Our empirical evaluations on images and video demonstrate the\neffectiveness of our approach in efficient token usage, paving the way for\nfuture development of more powerful multimodal models, world models, and\nagents."
                },
                "authors": [
                    {
                        "name": "Wilson Yan"
                    },
                    {
                        "name": "Volodymyr Mnih"
                    },
                    {
                        "name": "Aleksandra Faust"
                    },
                    {
                        "name": "Matei Zaharia"
                    },
                    {
                        "name": "Pieter Abbeel"
                    },
                    {
                        "name": "Hao Liu"
                    }
                ],
                "author_detail": {
                    "name": "Hao Liu"
                },
                "author": "Hao Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08368v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08368v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09594v3",
                "updated": "2025-02-02T18:40:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    18,
                    40,
                    22,
                    6,
                    33,
                    0
                ],
                "published": "2024-08-18T20:59:59Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    20,
                    59,
                    59,
                    6,
                    231,
                    0
                ],
                "title": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models"
                },
                "summary": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels."
                },
                "authors": [
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Michael Middleton"
                    },
                    {
                        "name": "Tim Merino"
                    },
                    {
                        "name": "Nidhushan Kanagaraja"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Zhan Zhuang"
                    },
                    {
                        "name": "Julian Togelius"
                    }
                ],
                "author_detail": {
                    "name": "Julian Togelius"
                },
                "author": "Julian Togelius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00210v2",
                "updated": "2025-02-02T18:15:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    18,
                    15,
                    42,
                    6,
                    33,
                    0
                ],
                "published": "2024-10-31T21:14:22Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    14,
                    22,
                    3,
                    305,
                    0
                ],
                "title": "Scale-Aware Recognition in Satellite Images under Resource Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-Aware Recognition in Satellite Images under Resource Constraints"
                },
                "summary": "Recognition of features in satellite imagery (forests, swimming pools, etc.)\ndepends strongly on the spatial scale of the concept and therefore the\nresolution of the images. This poses two challenges: Which resolution is best\nsuited for recognizing a given concept, and where and when should the costlier\nhigher-resolution (HR) imagery be acquired?\n  We present a novel scheme to address these challenges by introducing three\ncomponents: (1) A technique to distill knowledge from models trained on HR\nimagery to recognition models that operate on imagery of lower resolution (LR),\n(2) a sampling strategy for HR imagery based on model disagreement, and (3) an\nLLM-based approach for inferring concept \"scale\". With these components we\npresent a system to efficiently perform scale-aware recognition in satellite\nimagery, improving accuracy over single-scale inference while following budget\nconstraints. Our novel approach offers up to a 26.3% improvement over entirely\nHR baselines, using 76.3% fewer HR images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognition of features in satellite imagery (forests, swimming pools, etc.)\ndepends strongly on the spatial scale of the concept and therefore the\nresolution of the images. This poses two challenges: Which resolution is best\nsuited for recognizing a given concept, and where and when should the costlier\nhigher-resolution (HR) imagery be acquired?\n  We present a novel scheme to address these challenges by introducing three\ncomponents: (1) A technique to distill knowledge from models trained on HR\nimagery to recognition models that operate on imagery of lower resolution (LR),\n(2) a sampling strategy for HR imagery based on model disagreement, and (3) an\nLLM-based approach for inferring concept \"scale\". With these components we\npresent a system to efficiently perform scale-aware recognition in satellite\nimagery, improving accuracy over single-scale inference while following budget\nconstraints. Our novel approach offers up to a 26.3% improvement over entirely\nHR baselines, using 76.3% fewer HR images."
                },
                "authors": [
                    {
                        "name": "Shreelekha Revankar"
                    },
                    {
                        "name": "Cheng Perng Phoo"
                    },
                    {
                        "name": "Utkarsh Mall"
                    },
                    {
                        "name": "Bharath Hariharan"
                    },
                    {
                        "name": "Kavita Bala"
                    }
                ],
                "author_detail": {
                    "name": "Kavita Bala"
                },
                "author": "Kavita Bala",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v8",
                "updated": "2025-02-02T17:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    17,
                    8,
                    44,
                    6,
                    33,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02956v2",
                "updated": "2025-02-02T16:51:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    16,
                    51,
                    13,
                    6,
                    33,
                    0
                ],
                "published": "2024-07-03T09:49:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    49,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "IncogniText: Privacy-enhancing Conditional Text Anonymization via\n  LLM-based Private Attribute Randomization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IncogniText: Privacy-enhancing Conditional Text Anonymization via\n  LLM-based Private Attribute Randomization"
                },
                "summary": "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%\nacross 8 different private attributes. Finally, we demonstrate the maturity of\nIncogniText for real-world applications by distilling its anonymization\ncapability into a set of LoRA parameters associated with an on-device model.\nOur results show the possibility of reducing privacy leakage by more than half\nwith limited impact on utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%\nacross 8 different private attributes. Finally, we demonstrate the maturity of\nIncogniText for real-world applications by distilling its anonymization\ncapability into a set of LoRA parameters associated with an on-device model.\nOur results show the possibility of reducing privacy leakage by more than half\nwith limited impact on utility."
                },
                "authors": [
                    {
                        "name": "Ahmed Frikha"
                    },
                    {
                        "name": "Nassim Walha"
                    },
                    {
                        "name": "Krishna Kanth Nakka"
                    },
                    {
                        "name": "Ricardo Mendes"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Xuebing Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuebing Zhou"
                },
                "author": "Xuebing Zhou",
                "arxiv_comment": "Accepted at NeurIPS 2024 - Safe GenAI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01626v2",
                "updated": "2025-02-02T16:34:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    16,
                    34,
                    25,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-02T15:44:19Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    44,
                    19,
                    0,
                    337,
                    0
                ],
                "title": "WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation"
                },
                "summary": "The use of Large Language Models (LLMs) has increased significantly with\nusers frequently asking questions to chatbots. In the time when information is\nreadily accessible, it is crucial to stimulate and preserve human cognitive\nabilities and maintain strong reasoning skills. This paper addresses such\nchallenges by promoting the use of hints as an alternative or a supplement to\ndirect answers. We first introduce a manually constructed hint dataset,\nWikiHint, which is based on Wikipedia and includes 5,000 hints created for\n1,000 questions. We then finetune open-source LLMs such as LLaMA-3.1 for hint\ngeneration in answer-aware and answeragnostic contexts. We assess the\neffectiveness of the hints with human participants who answer questions with\nand without the aid of hints. Additionally, we introduce a lightweight\nevaluation method, HintRank, to evaluate and rank hints in both answeraware and\nanswer-agnostic settings. Our findings show that (a) the dataset helps generate\nmore effective hints, (b) including answer information along with questions\ngenerally improves quality of generated hints, and (c) encoder-based models\nperform better than decoder-based models in hint ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) has increased significantly with\nusers frequently asking questions to chatbots. In the time when information is\nreadily accessible, it is crucial to stimulate and preserve human cognitive\nabilities and maintain strong reasoning skills. This paper addresses such\nchallenges by promoting the use of hints as an alternative or a supplement to\ndirect answers. We first introduce a manually constructed hint dataset,\nWikiHint, which is based on Wikipedia and includes 5,000 hints created for\n1,000 questions. We then finetune open-source LLMs such as LLaMA-3.1 for hint\ngeneration in answer-aware and answeragnostic contexts. We assess the\neffectiveness of the hints with human participants who answer questions with\nand without the aid of hints. Additionally, we introduce a lightweight\nevaluation method, HintRank, to evaluate and rank hints in both answeraware and\nanswer-agnostic settings. Our findings show that (a) the dataset helps generate\nmore effective hints, (b) including answer information along with questions\ngenerally improves quality of generated hints, and (c) encoder-based models\nperform better than decoder-based models in hint ranking."
                },
                "authors": [
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Florian Gerhold"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Submitted to SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02724v2",
                "updated": "2025-02-02T15:57:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    15,
                    57,
                    1,
                    6,
                    33,
                    0
                ],
                "published": "2024-10-03T17:45:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    45,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Large Language Models as Markov Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Markov Chains"
                },
                "summary": "Large language models (LLMs) are remarkably efficient across a wide range of\nnatural language processing tasks and well beyond them. However, a\ncomprehensive theoretical analysis of the LLMs' generalization capabilities\nremains elusive. In our paper, we approach this task by drawing an equivalence\nbetween autoregressive transformer-based language models and Markov chains\ndefined on a finite state space. This allows us to study the multi-step\ninference mechanism of LLMs from first principles. We relate the obtained\nresults to the pathological behavior observed with LLMs such as repetitions and\nincoherent replies with high temperature. Finally, we leverage the proposed\nformalization to derive pre-training and in-context learning generalization\nbounds for LLMs under realistic data and model assumptions. Experiments with\nthe most recent Llama and Gemma herds of models show that our theory correctly\ncaptures their behavior in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are remarkably efficient across a wide range of\nnatural language processing tasks and well beyond them. However, a\ncomprehensive theoretical analysis of the LLMs' generalization capabilities\nremains elusive. In our paper, we approach this task by drawing an equivalence\nbetween autoregressive transformer-based language models and Markov chains\ndefined on a finite state space. This allows us to study the multi-step\ninference mechanism of LLMs from first principles. We relate the obtained\nresults to the pathological behavior observed with LLMs such as repetitions and\nincoherent replies with high temperature. Finally, we leverage the proposed\nformalization to derive pre-training and in-context learning generalization\nbounds for LLMs under realistic data and model assumptions. Experiments with\nthe most recent Llama and Gemma herds of models show that our theory correctly\ncaptures their behavior in practice."
                },
                "authors": [
                    {
                        "name": "Oussama Zekri"
                    },
                    {
                        "name": "Ambroise Odonnat"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Linus Bleistein"
                    },
                    {
                        "name": "Nicolas Boullé"
                    },
                    {
                        "name": "Ievgen Redko"
                    }
                ],
                "author_detail": {
                    "name": "Ievgen Redko"
                },
                "author": "Ievgen Redko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17876v2",
                "updated": "2025-02-02T15:29:21Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    15,
                    29,
                    21,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-18T05:56:38Z",
                "published_parsed": [
                    2025,
                    1,
                    18,
                    5,
                    56,
                    38,
                    5,
                    18,
                    0
                ],
                "title": "SCDM: Score-Based Channel Denoising Model for Digital Semantic\n  Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCDM: Score-Based Channel Denoising Model for Digital Semantic\n  Communications"
                },
                "summary": "Score-based diffusion models represent a significant variant within the\ndiffusion model family and have seen extensive application in the increasingly\npopular domain of generative tasks. Recent investigations have explored the\ndenoising potential of diffusion models in semantic communications. However, in\nprevious paradigms, noise distortion in the diffusion process does not match\nprecisely with digital channel noise characteristics. In this work, we\nintroduce the Score-Based Channel Denoising Model (SCDM) for Digital Semantic\nCommunications (DSC). SCDM views the distortion of constellation symbol\nsequences in digital transmission as a score-based forward diffusion process.\nWe design a tailored forward noise corruption to align digital channel noise\nproperties in the training phase. During the inference stage, the well-trained\nSCDM can effectively denoise received semantic symbols under various SNR\nconditions, reducing the difficulty for the semantic decoder in extracting\nsemantic information from the received noisy symbols and thereby enhancing the\nrobustness of the reconstructed semantic information. Experimental results show\nthat SCDM outperforms the baseline model in PSNR, SSIM, and MSE metrics,\nparticularly at low SNR levels. Moreover, SCDM reduces storage requirements by\na factor of 7.8. This efficiency in storage, combined with its robust denoising\ncapability, makes SCDM a practical solution for DSC across diverse channel\nconditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Score-based diffusion models represent a significant variant within the\ndiffusion model family and have seen extensive application in the increasingly\npopular domain of generative tasks. Recent investigations have explored the\ndenoising potential of diffusion models in semantic communications. However, in\nprevious paradigms, noise distortion in the diffusion process does not match\nprecisely with digital channel noise characteristics. In this work, we\nintroduce the Score-Based Channel Denoising Model (SCDM) for Digital Semantic\nCommunications (DSC). SCDM views the distortion of constellation symbol\nsequences in digital transmission as a score-based forward diffusion process.\nWe design a tailored forward noise corruption to align digital channel noise\nproperties in the training phase. During the inference stage, the well-trained\nSCDM can effectively denoise received semantic symbols under various SNR\nconditions, reducing the difficulty for the semantic decoder in extracting\nsemantic information from the received noisy symbols and thereby enhancing the\nrobustness of the reconstructed semantic information. Experimental results show\nthat SCDM outperforms the baseline model in PSNR, SSIM, and MSE metrics,\nparticularly at low SNR levels. Moreover, SCDM reduces storage requirements by\na factor of 7.8. This efficiency in storage, combined with its robust denoising\ncapability, makes SCDM a practical solution for DSC across diverse channel\nconditions."
                },
                "authors": [
                    {
                        "name": "Hao Mo"
                    },
                    {
                        "name": "Yaping Sun"
                    },
                    {
                        "name": "Shumin Yao"
                    },
                    {
                        "name": "Hao Chen"
                    },
                    {
                        "name": "Zhiyong Chen"
                    },
                    {
                        "name": "Xiaodong Xu"
                    },
                    {
                        "name": "Nan Ma"
                    },
                    {
                        "name": "Meixia Tao"
                    },
                    {
                        "name": "Shuguang Cui"
                    }
                ],
                "author_detail": {
                    "name": "Shuguang Cui"
                },
                "author": "Shuguang Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14844v2",
                "updated": "2025-02-02T14:32:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    32,
                    41,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-24T09:10:02Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    10,
                    2,
                    4,
                    24,
                    0
                ],
                "title": "Unmasking Conversational Bias in AI Multiagent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking Conversational Bias in AI Multiagent Systems"
                },
                "summary": "Detecting biases in the outputs produced by generative models is essential to\nreduce the potential risks associated with their application in critical\nsettings. However, the majority of existing methodologies for identifying\nbiases in generated text consider the models in isolation and neglect their\ncontextual applications. Specifically, the biases that may arise in multi-agent\nsystems involving generative models remain under-researched. To address this\ngap, we present a framework designed to quantify biases within multi-agent\nsystems of conversational Large Language Models (LLMs). Our approach involves\nsimulating small echo chambers, where pairs of LLMs, initialized with aligned\nperspectives on a polarizing topic, engage in discussions. Contrary to\nexpectations, we observe significant shifts in the stance expressed in the\ngenerated messages, particularly within echo chambers where all agents\ninitially express conservative viewpoints, in line with the well-documented\npolitical bias of many LLMs toward liberal positions. Crucially, the bias\nobserved in the echo-chamber experiment remains undetected by current\nstate-of-the-art bias detection methods that rely on questionnaires. This\nhighlights a critical need for the development of a more sophisticated toolkit\nfor bias detection and mitigation for AI multi-agent systems. The code to\nperform the experiments is publicly available at\nhttps://anonymous.4open.science/r/LLMsConversationalBias-7725.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting biases in the outputs produced by generative models is essential to\nreduce the potential risks associated with their application in critical\nsettings. However, the majority of existing methodologies for identifying\nbiases in generated text consider the models in isolation and neglect their\ncontextual applications. Specifically, the biases that may arise in multi-agent\nsystems involving generative models remain under-researched. To address this\ngap, we present a framework designed to quantify biases within multi-agent\nsystems of conversational Large Language Models (LLMs). Our approach involves\nsimulating small echo chambers, where pairs of LLMs, initialized with aligned\nperspectives on a polarizing topic, engage in discussions. Contrary to\nexpectations, we observe significant shifts in the stance expressed in the\ngenerated messages, particularly within echo chambers where all agents\ninitially express conservative viewpoints, in line with the well-documented\npolitical bias of many LLMs toward liberal positions. Crucially, the bias\nobserved in the echo-chamber experiment remains undetected by current\nstate-of-the-art bias detection methods that rely on questionnaires. This\nhighlights a critical need for the development of a more sophisticated toolkit\nfor bias detection and mitigation for AI multi-agent systems. The code to\nperform the experiments is publicly available at\nhttps://anonymous.4open.science/r/LLMsConversationalBias-7725."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Giuseppe Manco"
                    },
                    {
                        "name": "Luca Maria Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Luca Maria Aiello"
                },
                "author": "Luca Maria Aiello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18124v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18124v2",
                "updated": "2025-02-02T14:32:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    32,
                    1,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-30T03:58:41Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    3,
                    58,
                    41,
                    3,
                    30,
                    0
                ],
                "title": "REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via\n  Multimodal Visual Feature Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REMOTE: Real-time Ego-motion Tracking for Various Endoscopes via\n  Multimodal Visual Feature Learning"
                },
                "summary": "Real-time ego-motion tracking for endoscope is a significant task for\nefficient navigation and robotic automation of endoscopy. In this paper, a\nnovel framework is proposed to perform real-time ego-motion tracking for\nendoscope. Firstly, a multi-modal visual feature learning network is proposed\nto perform relative pose prediction, in which the motion feature from the\noptical flow, the scene features and the joint feature from two adjacent\nobservations are all extracted for prediction. Due to more correlation\ninformation in the channel dimension of the concatenated image, a novel feature\nextractor is designed based on an attention mechanism to integrate\nmulti-dimensional information from the concatenation of two continuous frames.\nTo extract more complete feature representation from the fused features, a\nnovel pose decoder is proposed to predict the pose transformation from the\nconcatenated feature map at the end of the framework. At last, the absolute\npose of endoscope is calculated based on relative poses. The experiment is\nconducted on three datasets of various endoscopic scenes and the results\ndemonstrate that the proposed method outperforms state-of-the-art methods.\nBesides, the inference speed of the proposed method is over 30 frames per\nsecond, which meets the real-time requirement. The project page is here:\nremote-bmxs.netlify.app",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time ego-motion tracking for endoscope is a significant task for\nefficient navigation and robotic automation of endoscopy. In this paper, a\nnovel framework is proposed to perform real-time ego-motion tracking for\nendoscope. Firstly, a multi-modal visual feature learning network is proposed\nto perform relative pose prediction, in which the motion feature from the\noptical flow, the scene features and the joint feature from two adjacent\nobservations are all extracted for prediction. Due to more correlation\ninformation in the channel dimension of the concatenated image, a novel feature\nextractor is designed based on an attention mechanism to integrate\nmulti-dimensional information from the concatenation of two continuous frames.\nTo extract more complete feature representation from the fused features, a\nnovel pose decoder is proposed to predict the pose transformation from the\nconcatenated feature map at the end of the framework. At last, the absolute\npose of endoscope is calculated based on relative poses. The experiment is\nconducted on three datasets of various endoscopic scenes and the results\ndemonstrate that the proposed method outperforms state-of-the-art methods.\nBesides, the inference speed of the proposed method is over 30 frames per\nsecond, which meets the real-time requirement. The project page is here:\nremote-bmxs.netlify.app"
                },
                "authors": [
                    {
                        "name": "Liangjing Shao"
                    },
                    {
                        "name": "Benshuang Chen"
                    },
                    {
                        "name": "Shuting Zhao"
                    },
                    {
                        "name": "Xinrong Chen"
                    }
                ],
                "author_detail": {
                    "name": "Xinrong Chen"
                },
                "author": "Xinrong Chen",
                "arxiv_comment": "Accepted by ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18124v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18124v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14931v2",
                "updated": "2025-02-02T13:51:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    13,
                    51,
                    2,
                    6,
                    33,
                    0
                ],
                "published": "2024-01-26T15:10:23Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    15,
                    10,
                    23,
                    4,
                    26,
                    0
                ],
                "title": "Do LLMs Dream of Ontologies?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Dream of Ontologies?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse natural language processing tasks, yet their ability to memorize\nstructured knowledge remains underexplored. In this paper, we investigate the\nextent to which general-purpose pre-trained LLMs retain and correctly reproduce\nconcept identifier (ID)-label associations from publicly available ontologies.\nWe conduct a systematic evaluation across multiple ontological resources,\nincluding the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as\nPythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only\na small fraction of ontological concepts is accurately memorized, with GPT-4\ndemonstrating the highest performance. To understand why certain concepts are\nmemorized more effectively than others, we analyze the relationship between\nmemorization accuracy and concept popularity on the Web. Our results indicate a\nstrong correlation between the frequency of a concept's occurrence online and\nthe likelihood of accurately retrieving its ID from the label. This suggests\nthat LLMs primarily acquire such knowledge through indirect textual exposure\nrather than directly from structured ontological resources. Furthermore, we\nintroduce new metrics to quantify prediction invariance, demonstrating that the\nstability of model responses across variations in prompt language and\ntemperature settings can serve as a proxy for estimating memorization\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse natural language processing tasks, yet their ability to memorize\nstructured knowledge remains underexplored. In this paper, we investigate the\nextent to which general-purpose pre-trained LLMs retain and correctly reproduce\nconcept identifier (ID)-label associations from publicly available ontologies.\nWe conduct a systematic evaluation across multiple ontological resources,\nincluding the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as\nPythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only\na small fraction of ontological concepts is accurately memorized, with GPT-4\ndemonstrating the highest performance. To understand why certain concepts are\nmemorized more effectively than others, we analyze the relationship between\nmemorization accuracy and concept popularity on the Web. Our results indicate a\nstrong correlation between the frequency of a concept's occurrence online and\nthe likelihood of accurately retrieving its ID from the label. This suggests\nthat LLMs primarily acquire such knowledge through indirect textual exposure\nrather than directly from structured ontological resources. Furthermore, we\nintroduce new metrics to quantify prediction invariance, demonstrating that the\nstability of model responses across variations in prompt language and\ntemperature settings can serve as a proxy for estimating memorization\nrobustness."
                },
                "authors": [
                    {
                        "name": "Marco Bombieri"
                    },
                    {
                        "name": "Paolo Fiorini"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    },
                    {
                        "name": "Marco Rospocher"
                    }
                ],
                "author_detail": {
                    "name": "Marco Rospocher"
                },
                "author": "Marco Rospocher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01292v2",
                "updated": "2025-02-02T11:49:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    49,
                    25,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-02T09:07:57Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    7,
                    57,
                    0,
                    337,
                    0
                ],
                "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual\n  Preferences"
                },
                "summary": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing\nattention, which is crucial for developing embodied AI within 3D scenes, such\nas visual navigation and embodied question answering. Due to the high density\nof visual features, especially in large 3D scenes, accurately locating\ntask-relevant visual information is challenging. Existing works attempt to\nsegment all objects and consider their features as scene representations.\nHowever, these task-agnostic object features include much redundant information\nand missing details for the task-relevant area. To tackle these problems, we\npropose LSceneLLM, an adaptive framework that automatically identifies\ntask-relevant areas by leveraging LLM's visual preference for different tasks,\nfollowed by a plug-and-play scene magnifier module to capture fine-grained\ndetails in focused areas. Specifically, a dense token selector examines the\nattention map of LLM to identify visual preferences for the instruction input.\nIt then magnifies fine-grained details of the focusing area. An adaptive\nself-attention module is leveraged to fuse the coarse-grained and selected\nfine-grained visual information. To comprehensively evaluate the large scene\nunderstanding ability of 3D-VLMs, we further introduce a cross-room\nunderstanding benchmark, XR-Scene, which contains a series of large scene\nunderstanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption.\nExperiments show that our method surpasses existing methods on both large scene\nunderstanding and existing scene understanding benchmarks. Plunging our scene\nmagnifier module into the existing 3D-VLMs also brings significant improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing\nattention, which is crucial for developing embodied AI within 3D scenes, such\nas visual navigation and embodied question answering. Due to the high density\nof visual features, especially in large 3D scenes, accurately locating\ntask-relevant visual information is challenging. Existing works attempt to\nsegment all objects and consider their features as scene representations.\nHowever, these task-agnostic object features include much redundant information\nand missing details for the task-relevant area. To tackle these problems, we\npropose LSceneLLM, an adaptive framework that automatically identifies\ntask-relevant areas by leveraging LLM's visual preference for different tasks,\nfollowed by a plug-and-play scene magnifier module to capture fine-grained\ndetails in focused areas. Specifically, a dense token selector examines the\nattention map of LLM to identify visual preferences for the instruction input.\nIt then magnifies fine-grained details of the focusing area. An adaptive\nself-attention module is leveraged to fuse the coarse-grained and selected\nfine-grained visual information. To comprehensively evaluate the large scene\nunderstanding ability of 3D-VLMs, we further introduce a cross-room\nunderstanding benchmark, XR-Scene, which contains a series of large scene\nunderstanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption.\nExperiments show that our method surpasses existing methods on both large scene\nunderstanding and existing scene understanding benchmarks. Plunging our scene\nmagnifier module into the existing 3D-VLMs also brings significant improvement."
                },
                "authors": [
                    {
                        "name": "Hongyan Zhi"
                    },
                    {
                        "name": "Peihao Chen"
                    },
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Shuailei Ma"
                    },
                    {
                        "name": "Xinyu Sun"
                    },
                    {
                        "name": "Tianhang Xiang"
                    },
                    {
                        "name": "Yinjie Lei"
                    },
                    {
                        "name": "Mingkui Tan"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07927v2",
                "updated": "2025-02-02T11:30:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    30,
                    27,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-14T08:30:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    30,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "Gandalf the Red: Adaptive Security for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gandalf the Red: Adaptive Security for LLMs"
                },
                "summary": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications."
                },
                "authors": [
                    {
                        "name": "Niklas Pfister"
                    },
                    {
                        "name": "Václav Volhejn"
                    },
                    {
                        "name": "Manuel Knott"
                    },
                    {
                        "name": "Santiago Arias"
                    },
                    {
                        "name": "Julia Bazińska"
                    },
                    {
                        "name": "Mykhailo Bichurin"
                    },
                    {
                        "name": "Alan Commike"
                    },
                    {
                        "name": "Janet Darling"
                    },
                    {
                        "name": "Peter Dienes"
                    },
                    {
                        "name": "Matthew Fiedler"
                    },
                    {
                        "name": "David Haber"
                    },
                    {
                        "name": "Matthias Kraft"
                    },
                    {
                        "name": "Marco Lancini"
                    },
                    {
                        "name": "Max Mathys"
                    },
                    {
                        "name": "Damián Pascual-Ortiz"
                    },
                    {
                        "name": "Jakub Podolak"
                    },
                    {
                        "name": "Adrià Romero-López"
                    },
                    {
                        "name": "Kyriacos Shiarlis"
                    },
                    {
                        "name": "Andreas Signer"
                    },
                    {
                        "name": "Zsolt Terek"
                    },
                    {
                        "name": "Athanasios Theocharis"
                    },
                    {
                        "name": "Daniel Timbrell"
                    },
                    {
                        "name": "Samuel Trautwein"
                    },
                    {
                        "name": "Samuel Watts"
                    },
                    {
                        "name": "Yun-Han Wu"
                    },
                    {
                        "name": "Mateo Rojas-Carulla"
                    }
                ],
                "author_detail": {
                    "name": "Mateo Rojas-Carulla"
                },
                "author": "Mateo Rojas-Carulla",
                "arxiv_comment": "Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05014v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05014v3",
                "updated": "2025-02-02T11:07:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    7,
                    35,
                    6,
                    33,
                    0
                ],
                "published": "2023-10-08T05:13:25Z",
                "published_parsed": [
                    2023,
                    10,
                    8,
                    5,
                    13,
                    25,
                    6,
                    281,
                    0
                ],
                "title": "Congruence Closure Modulo Groups",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Congruence Closure Modulo Groups"
                },
                "summary": "This paper presents a new framework for constructing congruence closure of a\nfinite set of ground equations over uninterpreted symbols and interpreted\nsymbols for the group axioms. In this framework, ground equations are flattened\ninto certain forms by introducing new constants, and a completion procedure is\nperformed on ground flat equations. The proposed completion procedure uses\nequational inference rules and constructs a ground convergent rewrite system\nfor congruence closure with such interpreted symbols. If the completion\nprocedure terminates, then it yields a decision procedure for the word problem\nfor a finite set of ground equations with respect to the group axioms. This\npaper also provides a sufficient terminating condition of the completion\nprocedure for constructing a ground convergent rewrite system from ground flat\nequations containing interpreted symbols for the group axioms. In addition,\nthis paper presents a new method for constructing congruence closure of a\nfinite set of ground equations containing interpreted symbols for the\nsemigroup, monoid, and the multiple disjoint sets of group axioms,\nrespectively, using the proposed framework.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new framework for constructing congruence closure of a\nfinite set of ground equations over uninterpreted symbols and interpreted\nsymbols for the group axioms. In this framework, ground equations are flattened\ninto certain forms by introducing new constants, and a completion procedure is\nperformed on ground flat equations. The proposed completion procedure uses\nequational inference rules and constructs a ground convergent rewrite system\nfor congruence closure with such interpreted symbols. If the completion\nprocedure terminates, then it yields a decision procedure for the word problem\nfor a finite set of ground equations with respect to the group axioms. This\npaper also provides a sufficient terminating condition of the completion\nprocedure for constructing a ground convergent rewrite system from ground flat\nequations containing interpreted symbols for the group axioms. In addition,\nthis paper presents a new method for constructing congruence closure of a\nfinite set of ground equations containing interpreted symbols for the\nsemigroup, monoid, and the multiple disjoint sets of group axioms,\nrespectively, using the proposed framework."
                },
                "authors": [
                    {
                        "name": "Dohan Kim"
                    }
                ],
                "author_detail": {
                    "name": "Dohan Kim"
                },
                "author": "Dohan Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2310.05014v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05014v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.14896v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.14896v2",
                "updated": "2025-02-02T10:48:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    10,
                    48,
                    5,
                    6,
                    33,
                    0
                ],
                "published": "2024-07-20T15:11:04Z",
                "published_parsed": [
                    2024,
                    7,
                    20,
                    15,
                    11,
                    4,
                    5,
                    202,
                    0
                ],
                "title": "Probing 3D magnetic fields using starlight polarization and grain\n  alignment theory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Probing 3D magnetic fields using starlight polarization and grain\n  alignment theory"
                },
                "summary": "Polarization of starlight induced by dust grains aligned with the magnetic\nfield (hereafter B-field) is widely used to measure the two-dimensional\nB-fields projected onto the plane-of-sky. Here, we introduce a new method to\ninfer three-dimensional B-fields using starlight polarization. We show that the\ninclination angle or line-of-sight (LOS) component of B-fields can be\nconstrained by the starlight polarization efficiency from observations, the\nalignment degree provided by the magnetically enhanced radiative torque (MRAT)\nalignment theory, and the effect of B-field tangling. We first perform\nsynthetic observations of starlight polarization of magnetohydrodynamic (MHD)\nsimulations of a filamentary cloud with our updated POLARIS code incorporating\nthe modern MRAT theory. We test the new technique with synthetic observations\nand find that the B-field inclination angles can be accurately determined by\nthe synthetic starlight polarization efficiency once the effects of grain\nalignment, dust properties, and B-field fluctuations are well characterized.\nThe technique can provide an accurate constraint on B-field inclination angles\nusing optical polarization in low-density regions $A_{\\rm V}< 3$ with efficient\nMRAT alignment, whereas the technique can infer further to high-density regions\nwith significant alignment loss at $A_{\\rm V} \\sim 8 - 30$ by using\nnear-infrared polarization. Our new technique unlocks the full potential of\ntracing 3D B-fields and constraining dust properties and grain alignment\nphysics on multiple scales of the diffuse interstellar medium and star-forming\nregions using multi-wavelength starlight polarization observations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Polarization of starlight induced by dust grains aligned with the magnetic\nfield (hereafter B-field) is widely used to measure the two-dimensional\nB-fields projected onto the plane-of-sky. Here, we introduce a new method to\ninfer three-dimensional B-fields using starlight polarization. We show that the\ninclination angle or line-of-sight (LOS) component of B-fields can be\nconstrained by the starlight polarization efficiency from observations, the\nalignment degree provided by the magnetically enhanced radiative torque (MRAT)\nalignment theory, and the effect of B-field tangling. We first perform\nsynthetic observations of starlight polarization of magnetohydrodynamic (MHD)\nsimulations of a filamentary cloud with our updated POLARIS code incorporating\nthe modern MRAT theory. We test the new technique with synthetic observations\nand find that the B-field inclination angles can be accurately determined by\nthe synthetic starlight polarization efficiency once the effects of grain\nalignment, dust properties, and B-field fluctuations are well characterized.\nThe technique can provide an accurate constraint on B-field inclination angles\nusing optical polarization in low-density regions $A_{\\rm V}< 3$ with efficient\nMRAT alignment, whereas the technique can infer further to high-density regions\nwith significant alignment loss at $A_{\\rm V} \\sim 8 - 30$ by using\nnear-infrared polarization. Our new technique unlocks the full potential of\ntracing 3D B-fields and constraining dust properties and grain alignment\nphysics on multiple scales of the diffuse interstellar medium and star-forming\nregions using multi-wavelength starlight polarization observations."
                },
                "authors": [
                    {
                        "name": "Bao Truong"
                    },
                    {
                        "name": "Thiem Hoang"
                    }
                ],
                "author_detail": {
                    "name": "Thiem Hoang"
                },
                "author": "Thiem Hoang",
                "arxiv_comment": "37 pages, 35 figures, 4 tables, accepted by ApJ",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.14896v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.14896v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2301.09522v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2301.09522v4",
                "updated": "2025-02-02T09:21:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    9,
                    21,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2023-01-23T16:14:09Z",
                "published_parsed": [
                    2023,
                    1,
                    23,
                    16,
                    14,
                    9,
                    0,
                    23,
                    0
                ],
                "title": "Optimising Event-Driven Spiking Neural Network with Regularisation and\n  Cutoff",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimising Event-Driven Spiking Neural Network with Regularisation and\n  Cutoff"
                },
                "summary": "Spiking neural network (SNN), as the next generation of artificial neural\nnetwork (ANN), offer a closer mimicry of natural neural networks and hold\npromise for significant improvements in computational efficiency. However, the\ncurrent SNN is trained to infer over a fixed duration, overlooking the\npotential of dynamic inference in SNN. In this paper, we strengthen the\nmarriage between SNN and event-driven processing with a proposal to consider a\ncutoff in SNN, which can terminate SNN anytime during inference to achieve\nefficient inference. Two novel optimisation techniques are presented to achieve\ninference efficient SNN: a Top-K cutoff and a regularisation.The proposed\nregularisation influences the training process, optimising SNN for the cutoff,\nwhile the Top-K cutoff technique optimises the inference phase. We conduct an\nextensive set of experiments on multiple benchmark frame-based datasets, such\nasCIFAR10/100, Tiny-ImageNet, and event-based datasets, including CIFAR10-DVS,\nN-Caltech101 and DVS128 Gesture. The experimental results demonstrate the\neffectiveness of our techniques in both ANN-to-SNN conversion and direct\ntraining, enabling SNNs to require 1.76 to 2.76x fewer timesteps for CIFAR-10,\nwhile achieving 1.64 to 1.95x fewer timesteps across all event-based datasets,\nwith near-zero accuracy loss. These findings affirms the compatibility and\npotential benefits of our techniques in enhancing accuracy and reducing\ninference latency when integrated with existing methods. Code available:\nhttps://github.com/Dengyu-Wu/SNNCutoff",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking neural network (SNN), as the next generation of artificial neural\nnetwork (ANN), offer a closer mimicry of natural neural networks and hold\npromise for significant improvements in computational efficiency. However, the\ncurrent SNN is trained to infer over a fixed duration, overlooking the\npotential of dynamic inference in SNN. In this paper, we strengthen the\nmarriage between SNN and event-driven processing with a proposal to consider a\ncutoff in SNN, which can terminate SNN anytime during inference to achieve\nefficient inference. Two novel optimisation techniques are presented to achieve\ninference efficient SNN: a Top-K cutoff and a regularisation.The proposed\nregularisation influences the training process, optimising SNN for the cutoff,\nwhile the Top-K cutoff technique optimises the inference phase. We conduct an\nextensive set of experiments on multiple benchmark frame-based datasets, such\nasCIFAR10/100, Tiny-ImageNet, and event-based datasets, including CIFAR10-DVS,\nN-Caltech101 and DVS128 Gesture. The experimental results demonstrate the\neffectiveness of our techniques in both ANN-to-SNN conversion and direct\ntraining, enabling SNNs to require 1.76 to 2.76x fewer timesteps for CIFAR-10,\nwhile achieving 1.64 to 1.95x fewer timesteps across all event-based datasets,\nwith near-zero accuracy loss. These findings affirms the compatibility and\npotential benefits of our techniques in enhancing accuracy and reducing\ninference latency when integrated with existing methods. Code available:\nhttps://github.com/Dengyu-Wu/SNNCutoff"
                },
                "authors": [
                    {
                        "name": "Dengyu Wu"
                    },
                    {
                        "name": "Gaojie Jin"
                    },
                    {
                        "name": "Han Yu"
                    },
                    {
                        "name": "Xinping Yi"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "arxiv_doi": "10.3389/fnins.2025.1522788",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3389/fnins.2025.1522788",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2301.09522v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2301.09522v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Frontiers in Neuroscience, 19 (2025)",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10559v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10559v2",
                "updated": "2025-02-02T08:46:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    8,
                    46,
                    53,
                    6,
                    33,
                    0
                ],
                "published": "2024-10-14T14:38:57Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    38,
                    57,
                    0,
                    288,
                    0
                ],
                "title": "Scale-dependence in $Λ$CDM parameters inferred from the CMB: a\n  possible sign of Early Dark Energy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-dependence in $Λ$CDM parameters inferred from the CMB: a\n  possible sign of Early Dark Energy"
                },
                "summary": "The early dark energy (EDE) model is one of the promising solutions to the\nHubble tension. One of the successes of the EDE model is that it can provide a\nsimilar fit to the $\\Lambda$CDM model for the CMB power spectrum. In this work,\nI analyze the phenomenology of the EDE and $\\Lambda$CDM parameters on the CMB\ntemperature power spectrum and notice that this cannot hold on all scales.\nThus, if the real cosmology is as described by the EDE model, the $\\Lambda$CDM\nparameters will be scale-dependent when fitting the CMB power spectrum with the\n$\\Lambda$CDM model, which can be hints for the EDE model. I examine CMB-S4-like\nobservations through mock data analysis and find that parameter shifts are\nnotable. As observations include smaller scales, I find lower $H_0$, $n_s$,\n$\\omega_b$ and higher $\\omega_m$, $A_s e^{-2\\tau}$, which will also constitute\nnew tensions with other observations. They can serve as a possible signal for\nthe EDE model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The early dark energy (EDE) model is one of the promising solutions to the\nHubble tension. One of the successes of the EDE model is that it can provide a\nsimilar fit to the $\\Lambda$CDM model for the CMB power spectrum. In this work,\nI analyze the phenomenology of the EDE and $\\Lambda$CDM parameters on the CMB\ntemperature power spectrum and notice that this cannot hold on all scales.\nThus, if the real cosmology is as described by the EDE model, the $\\Lambda$CDM\nparameters will be scale-dependent when fitting the CMB power spectrum with the\n$\\Lambda$CDM model, which can be hints for the EDE model. I examine CMB-S4-like\nobservations through mock data analysis and find that parameter shifts are\nnotable. As observations include smaller scales, I find lower $H_0$, $n_s$,\n$\\omega_b$ and higher $\\omega_m$, $A_s e^{-2\\tau}$, which will also constitute\nnew tensions with other observations. They can serve as a possible signal for\nthe EDE model."
                },
                "authors": [
                    {
                        "name": "Jun-Qian Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Jun-Qian Jiang"
                },
                "author": "Jun-Qian Jiang",
                "arxiv_comment": "26 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10559v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10559v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-th",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00101v2",
                "updated": "2025-02-02T08:36:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    8,
                    36,
                    36,
                    6,
                    33,
                    0
                ],
                "published": "2024-08-27T12:07:09Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    12,
                    7,
                    9,
                    1,
                    240,
                    0
                ],
                "title": "NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals"
                },
                "summary": "Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm."
                },
                "authors": [
                    {
                        "name": "Wei-Bang Jiang"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Bao-Liang Lu"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13164v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13164v3",
                "updated": "2025-02-02T08:18:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    8,
                    18,
                    38,
                    6,
                    33,
                    0
                ],
                "published": "2024-03-19T21:31:56Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    21,
                    31,
                    56,
                    1,
                    79,
                    0
                ],
                "title": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning"
                },
                "summary": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL."
                },
                "authors": [
                    {
                        "name": "Yongshuo Zong"
                    },
                    {
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "name": "Timothy Hospedales"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Hospedales"
                },
                "author": "Timothy Hospedales",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13164v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13164v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04901v2",
                "updated": "2025-02-02T07:38:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    7,
                    38,
                    12,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-09T01:26:59Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    26,
                    59,
                    3,
                    9,
                    0
                ],
                "title": "ThriftLLM: On Cost-Effective Selection of Large Language Models for\n  Classification Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThriftLLM: On Cost-Effective Selection of Large Language Models for\n  Classification Queries"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated remarkable\ncapabilities in comprehending and generating natural language content. An\nincreasing number of services offer LLMs for various tasks via APIs. Different\nLLMs demonstrate expertise in different domains of queries (e.g., text\nclassification queries). Meanwhile, LLMs of different scales, complexity, and\nperformance are priced diversely. Driven by this, several researchers are\ninvestigating strategies for selecting an ensemble of LLMs, aiming to decrease\noverall usage costs while enhancing performance. However, to the best of our\nknowledge, none of the existing works addresses the problem, how to find an LLM\nensemble subject to a cost budget, which maximizes the ensemble performance\nwith guarantees.\n  In this paper, we formalize the performance of an ensemble of models (LLMs)\nusing the notion of prediction accuracy which we formally define. We develop an\napproach for aggregating responses from multiple LLMs to enhance ensemble\nperformance. Building on this, we formulate the Optimal Ensemble Selection\nproblem of selecting a set of LLMs subject to a cost budget that maximizes the\noverall prediction accuracy. We show that prediction accuracy is non-decreasing\nand non-submodular and provide evidence that the Optimal Ensemble Selection\nproblem is likely to be NP-hard. By leveraging a submodular function that upper\nbounds prediction accuracy, we develop an algorithm called ThriftLLM and prove\nthat it achieves an instance-dependent approximation guarantee with high\nprobability. In addition, it achieves state-of-the-art performance for text\nclassification and entity matching queries on multiple real-world datasets\nagainst various baselines in our extensive experimental evaluation, while using\na relatively lower cost budget, strongly supporting the effectiveness and\nsuperiority of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated remarkable\ncapabilities in comprehending and generating natural language content. An\nincreasing number of services offer LLMs for various tasks via APIs. Different\nLLMs demonstrate expertise in different domains of queries (e.g., text\nclassification queries). Meanwhile, LLMs of different scales, complexity, and\nperformance are priced diversely. Driven by this, several researchers are\ninvestigating strategies for selecting an ensemble of LLMs, aiming to decrease\noverall usage costs while enhancing performance. However, to the best of our\nknowledge, none of the existing works addresses the problem, how to find an LLM\nensemble subject to a cost budget, which maximizes the ensemble performance\nwith guarantees.\n  In this paper, we formalize the performance of an ensemble of models (LLMs)\nusing the notion of prediction accuracy which we formally define. We develop an\napproach for aggregating responses from multiple LLMs to enhance ensemble\nperformance. Building on this, we formulate the Optimal Ensemble Selection\nproblem of selecting a set of LLMs subject to a cost budget that maximizes the\noverall prediction accuracy. We show that prediction accuracy is non-decreasing\nand non-submodular and provide evidence that the Optimal Ensemble Selection\nproblem is likely to be NP-hard. By leveraging a submodular function that upper\nbounds prediction accuracy, we develop an algorithm called ThriftLLM and prove\nthat it achieves an instance-dependent approximation guarantee with high\nprobability. In addition, it achieves state-of-the-art performance for text\nclassification and entity matching queries on multiple real-world datasets\nagainst various baselines in our extensive experimental evaluation, while using\na relatively lower cost budget, strongly supporting the effectiveness and\nsuperiority of our method."
                },
                "authors": [
                    {
                        "name": "Keke Huang"
                    },
                    {
                        "name": "Yimin Shi"
                    },
                    {
                        "name": "Dujian Ding"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Yang Fei"
                    },
                    {
                        "name": "Laks Lakshmanan"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14427v2",
                "updated": "2025-02-02T06:55:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    6,
                    55,
                    25,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-24T11:55:57Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    55,
                    57,
                    4,
                    24,
                    0
                ],
                "title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand\n  Graphs Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand\n  Graphs Better"
                },
                "summary": "The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe a counter-intuitive fact that when\nthe order of nodes or edges in the natural language description of a graph is\nshuffled, despite describing the same graph, model performance fluctuates\nbetween high performance and random guessing. Additionally, due to LLMs'\nlimited input context length, current methods typically randomly sample\nneighbors of target nodes as representatives of their neighborhood, which may\nnot always be effective for accurate reasoning. To address these gaps, we\nintroduce GraphSOS (Graph Sampling and Order Selection). This novel model\nframework features an Order Selector Module to ensure proper serialization\norder of the graph and a Subgraph Sampling Module to sample subgraphs with\nbetter structure for better reasoning. Furthermore, we propose Graph CoT\nobtained through distillation, and enhance LLM's reasoning and zero-shot\nlearning capabilities for graph tasks through instruction tuning. Experiments\non multiple datasets for node classification and graph question-answering\ndemonstrate that GraphSOS improves LLMs' performance and generalization ability\non graph tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe a counter-intuitive fact that when\nthe order of nodes or edges in the natural language description of a graph is\nshuffled, despite describing the same graph, model performance fluctuates\nbetween high performance and random guessing. Additionally, due to LLMs'\nlimited input context length, current methods typically randomly sample\nneighbors of target nodes as representatives of their neighborhood, which may\nnot always be effective for accurate reasoning. To address these gaps, we\nintroduce GraphSOS (Graph Sampling and Order Selection). This novel model\nframework features an Order Selector Module to ensure proper serialization\norder of the graph and a Subgraph Sampling Module to sample subgraphs with\nbetter structure for better reasoning. Furthermore, we propose Graph CoT\nobtained through distillation, and enhance LLM's reasoning and zero-shot\nlearning capabilities for graph tasks through instruction tuning. Experiments\non multiple datasets for node classification and graph question-answering\ndemonstrate that GraphSOS improves LLMs' performance and generalization ability\non graph tasks."
                },
                "authors": [
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Hanlin Xue"
                    },
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Bingce Wang"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Weiping Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Li"
                },
                "author": "Weiping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18060v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18060v5",
                "updated": "2025-02-02T06:31:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    6,
                    31,
                    14,
                    6,
                    33,
                    0
                ],
                "published": "2024-02-28T05:44:41Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    5,
                    44,
                    41,
                    2,
                    59,
                    0
                ],
                "title": "Benchmarking Large Language Models on Answering and Explaining\n  Challenging Medical Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models on Answering and Explaining\n  Challenging Medical Questions"
                },
                "summary": "LLMs have demonstrated impressive performance in answering medical questions,\nsuch as achieving passing scores on medical licensing examinations. However,\nmedical board exams or general clinical questions do not capture the complexity\nof realistic clinical cases. Moreover, the lack of reference explanations means\nwe cannot easily evaluate the reasoning of model decisions, a crucial component\nof supporting doctors in making complex medical decisions. To address these\nchallenges, we construct two new datasets: JAMA Clinical Challenge and\nMedbullets.\\footnote{Datasets and code are available at\n\\url{https://github.com/HanjieChen/ChallengeClinicalQA}.} JAMA Clinical\nChallenge consists of questions based on challenging clinical cases, while\nMedbullets comprises simulated clinical questions. Both datasets are structured\nas multiple-choice question-answering tasks, accompanied by expert-written\nexplanations. We evaluate seven LLMs on the two datasets using various prompts.\nExperiments demonstrate that our datasets are harder than previous benchmarks.\nIn-depth automatic and human evaluations of model-generated explanations\nprovide insights into the promise and deficiency of LLMs for explainable\nmedical QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated impressive performance in answering medical questions,\nsuch as achieving passing scores on medical licensing examinations. However,\nmedical board exams or general clinical questions do not capture the complexity\nof realistic clinical cases. Moreover, the lack of reference explanations means\nwe cannot easily evaluate the reasoning of model decisions, a crucial component\nof supporting doctors in making complex medical decisions. To address these\nchallenges, we construct two new datasets: JAMA Clinical Challenge and\nMedbullets.\\footnote{Datasets and code are available at\n\\url{https://github.com/HanjieChen/ChallengeClinicalQA}.} JAMA Clinical\nChallenge consists of questions based on challenging clinical cases, while\nMedbullets comprises simulated clinical questions. Both datasets are structured\nas multiple-choice question-answering tasks, accompanied by expert-written\nexplanations. We evaluate seven LLMs on the two datasets using various prompts.\nExperiments demonstrate that our datasets are harder than previous benchmarks.\nIn-depth automatic and human evaluations of model-generated explanations\nprovide insights into the promise and deficiency of LLMs for explainable\nmedical QA."
                },
                "authors": [
                    {
                        "name": "Hanjie Chen"
                    },
                    {
                        "name": "Zhouxiang Fang"
                    },
                    {
                        "name": "Yash Singla"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18060v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18060v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.13876v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.13876v2",
                "updated": "2025-02-02T05:26:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    5,
                    26,
                    25,
                    6,
                    33,
                    0
                ],
                "published": "2024-06-19T22:57:31Z",
                "published_parsed": [
                    2024,
                    6,
                    19,
                    22,
                    57,
                    31,
                    2,
                    171,
                    0
                ],
                "title": "An Empirical Bayes Jackknife Regression Framework for Covariance Matrix\n  Estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Bayes Jackknife Regression Framework for Covariance Matrix\n  Estimation"
                },
                "summary": "Covariance matrix estimation, a classical statistical topic, poses\nsignificant challenges when the sample size is comparable to or smaller than\nthe number of features. In this paper, we frame covariance matrix estimation as\na compound decision problem and apply an optimal decision rule to estimate\ncovariance parameters. To approximate this rule, we introduce an algorithm that\nintegrates jackknife techniques with machine learning regression methods. This\nalgorithm exhibits adaptability across diverse scenarios without relying on\nassumptions about data distribution. Simulation results and gene network\ninference from an RNA-seq experiment in mice demonstrate that our approach\neither matches or surpasses several state-of-the-art methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Covariance matrix estimation, a classical statistical topic, poses\nsignificant challenges when the sample size is comparable to or smaller than\nthe number of features. In this paper, we frame covariance matrix estimation as\na compound decision problem and apply an optimal decision rule to estimate\ncovariance parameters. To approximate this rule, we introduce an algorithm that\nintegrates jackknife techniques with machine learning regression methods. This\nalgorithm exhibits adaptability across diverse scenarios without relying on\nassumptions about data distribution. Simulation results and gene network\ninference from an RNA-seq experiment in mice demonstrate that our approach\neither matches or surpasses several state-of-the-art methods"
                },
                "authors": [
                    {
                        "name": "Huqin Xin"
                    },
                    {
                        "name": "Sihai Dave Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sihai Dave Zhao"
                },
                "author": "Sihai Dave Zhao",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.13876v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.13876v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "62C25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2501.14723v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14723v2",
                "updated": "2025-02-03T18:57:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    57,
                    5,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-24T18:58:40Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    18,
                    58,
                    40,
                    4,
                    24,
                    0
                ],
                "title": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CodeMonkeys: Scaling Test-Time Compute for Software Engineering"
                },
                "summary": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling test-time compute is a promising axis for improving LLM capabilities.\nHowever, test-time compute can be scaled in a variety of ways, and effectively\ncombining different approaches remains an active area of research. Here, we\nexplore this problem in the context of solving real-world GitHub issues from\nthe SWE-bench dataset. Our system, named CodeMonkeys, allows models to\niteratively edit a codebase by jointly generating and running a testing script\nalongside their draft edit. We sample many of these multi-turn trajectories for\nevery issue to generate a collection of candidate edits. This approach lets us\nscale \"serial\" test-time compute by increasing the number of iterations per\ntrajectory and \"parallel\" test-time compute by increasing the number of\ntrajectories per problem. With parallel scaling, we can amortize up-front costs\nacross multiple downstream samples, allowing us to identify relevant codebase\ncontext using the simple method of letting an LLM read every file. In order to\nselect between candidate edits, we combine voting using model-generated tests\nwith a final multi-turn trajectory dedicated to selection. Overall, CodeMonkeys\nresolves 57.4% of issues from SWE-bench Verified using a budget of\napproximately 2300 USD. Our selection method can also be used to combine\ncandidates from different sources. Selecting over an ensemble of edits from\nexisting top SWE-bench Verified submissions obtains a score of 66.2% and\noutperforms the best member of the ensemble on its own. We fully release our\ncode and data at https://scalingintelligence.stanford.edu/pubs/codemonkeys."
                },
                "authors": [
                    {
                        "name": "Ryan Ehrlich"
                    },
                    {
                        "name": "Bradley Brown"
                    },
                    {
                        "name": "Jordan Juravsky"
                    },
                    {
                        "name": "Ronald Clark"
                    },
                    {
                        "name": "Christopher Ré"
                    },
                    {
                        "name": "Azalia Mirhoseini"
                    }
                ],
                "author_detail": {
                    "name": "Azalia Mirhoseini"
                },
                "author": "Azalia Mirhoseini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14723v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14723v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18626v2",
                "updated": "2025-02-03T18:19:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    19,
                    4,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-27T12:48:47Z",
                "published_parsed": [
                    2025,
                    1,
                    27,
                    12,
                    48,
                    47,
                    0,
                    27,
                    0
                ],
                "title": "The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt\n  Adversarial Attacks on LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The TIP of the Iceberg: Revealing a Hidden Class of Task-In-Prompt\n  Adversarial Attacks on LLMs"
                },
                "summary": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel class of jailbreak adversarial attacks on LLMs, termed\nTask-in-Prompt (TIP) attacks. Our approach embeds sequence-to-sequence tasks\n(e.g., cipher decoding, riddles, code execution) into the model's prompt to\nindirectly generate prohibited inputs. To systematically assess the\neffectiveness of these attacks, we introduce the PHRYGE benchmark. We\ndemonstrate that our techniques successfully circumvent safeguards in six\nstate-of-the-art language models, including GPT-4o and LLaMA 3.2. Our findings\nhighlight critical weaknesses in current LLM safety alignments and underscore\nthe urgent need for more sophisticated defence strategies.\n  Warning: this paper contains examples of unethical inquiries used solely for\nresearch purposes."
                },
                "authors": [
                    {
                        "name": "Sergey Berezin"
                    },
                    {
                        "name": "Reza Farahbakhsh"
                    },
                    {
                        "name": "Noel Crespi"
                    }
                ],
                "author_detail": {
                    "name": "Noel Crespi"
                },
                "author": "Noel Crespi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11004v2",
                "updated": "2025-02-03T18:17:53Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    17,
                    53,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-25T17:58:26Z",
                "published_parsed": [
                    2024,
                    6,
                    25,
                    17,
                    58,
                    26,
                    1,
                    177,
                    0
                ],
                "title": "The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The ALCHEmist: Automated Labeling 500x CHEaper Than LLM Data Annotators"
                },
                "summary": "Large pretrained models can be used as annotators, helping replace or augment\ncrowdworkers and enabling distilling generalist models into smaller specialist\nmodels. Unfortunately, this comes at a cost: employing top-of-the-line models\noften requires paying thousands of dollars for API calls, while the resulting\ndatasets are static and challenging to audit. To address these challenges, we\npropose a simple alternative: rather than directly querying labels from\npretrained models, we task models to generate programs that can produce labels.\nThese programs can be stored and applied locally, re-used and extended, and\ncost orders of magnitude less. Our system, Alchemist, obtains comparable to or\nbetter performance than large language model-based annotation in a range of\ntasks for a fraction of the cost: on average, improvements amount to a 12.9%\nenhancement while the total labeling costs across all datasets are reduced by a\nfactor of approximately 500x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pretrained models can be used as annotators, helping replace or augment\ncrowdworkers and enabling distilling generalist models into smaller specialist\nmodels. Unfortunately, this comes at a cost: employing top-of-the-line models\noften requires paying thousands of dollars for API calls, while the resulting\ndatasets are static and challenging to audit. To address these challenges, we\npropose a simple alternative: rather than directly querying labels from\npretrained models, we task models to generate programs that can produce labels.\nThese programs can be stored and applied locally, re-used and extended, and\ncost orders of magnitude less. Our system, Alchemist, obtains comparable to or\nbetter performance than large language model-based annotation in a range of\ntasks for a fraction of the cost: on average, improvements amount to a 12.9%\nenhancement while the total labeling costs across all datasets are reduced by a\nfactor of approximately 500x."
                },
                "authors": [
                    {
                        "name": "Tzu-Heng Huang"
                    },
                    {
                        "name": "Catherine Cao"
                    },
                    {
                        "name": "Vaishnavi Bhargava"
                    },
                    {
                        "name": "Frederic Sala"
                    }
                ],
                "author_detail": {
                    "name": "Frederic Sala"
                },
                "author": "Frederic Sala",
                "arxiv_comment": "NeurIPS 2024 Spotlight Paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13843v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13843v2",
                "updated": "2025-02-03T18:06:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    18,
                    6,
                    34,
                    0,
                    34,
                    0
                ],
                "published": "2024-09-20T18:34:38Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    18,
                    34,
                    38,
                    4,
                    264,
                    0
                ],
                "title": "STOP! Benchmarking Large Language Models with Sensitivity Testing on\n  Offensive Progressions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "STOP! Benchmarking Large Language Models with Sensitivity Testing on\n  Offensive Progressions"
                },
                "summary": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has\nbecome a critical focus in the field of natural language processing. However,\nmany current methodologies evaluate scenarios in isolation, without considering\nthe broader context or the spectrum of potential biases within each situation.\nTo address this, we introduce the Sensitivity Testing on Offensive Progressions\n(STOP) dataset, which includes 450 offensive progressions containing 2,700\nunique sentences of varying severity that progressively escalate from less to\nmore explicitly offensive. Covering a broad spectrum of 9 demographics and 46\nsub-demographics, STOP ensures inclusivity and comprehensive coverage. We\nevaluate several leading closed- and open-source models, including GPT-4,\nMixtral, and Llama 3. Our findings reveal that even the best-performing models\ndetect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We\nalso demonstrate how aligning models with human judgments on STOP can improve\nmodel answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs\nby up to 191%, while maintaining or even improving performance. STOP presents a\nnovel framework for assessing the complex nature of biases in LLMs, which will\nenable more effective bias mitigation strategies and facilitates the creation\nof fairer language models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating explicit and implicit biases in Large Language Models (LLMs) has\nbecome a critical focus in the field of natural language processing. However,\nmany current methodologies evaluate scenarios in isolation, without considering\nthe broader context or the spectrum of potential biases within each situation.\nTo address this, we introduce the Sensitivity Testing on Offensive Progressions\n(STOP) dataset, which includes 450 offensive progressions containing 2,700\nunique sentences of varying severity that progressively escalate from less to\nmore explicitly offensive. Covering a broad spectrum of 9 demographics and 46\nsub-demographics, STOP ensures inclusivity and comprehensive coverage. We\nevaluate several leading closed- and open-source models, including GPT-4,\nMixtral, and Llama 3. Our findings reveal that even the best-performing models\ndetect bias inconsistently, with success rates ranging from 19.3% to 69.8%. We\nalso demonstrate how aligning models with human judgments on STOP can improve\nmodel answer rates on sensitive tasks such as BBQ, StereoSet, and CrowS-Pairs\nby up to 191%, while maintaining or even improving performance. STOP presents a\nnovel framework for assessing the complex nature of biases in LLMs, which will\nenable more effective bias mitigation strategies and facilitates the creation\nof fairer language models."
                },
                "authors": [
                    {
                        "name": "Robert Morabito"
                    },
                    {
                        "name": "Sangmitra Madhusudan"
                    },
                    {
                        "name": "Tyler McDonald"
                    },
                    {
                        "name": "Ali Emami"
                    }
                ],
                "author_detail": {
                    "name": "Ali Emami"
                },
                "author": "Ali Emami",
                "arxiv_doi": "10.18653/v1/2024.emnlp-main.243",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.18653/v1/2024.emnlp-main.243",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2409.13843v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13843v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages (excluding references), accepted to EMNLP 2024 Main\n  Conference",
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19377v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19377v2",
                "updated": "2025-02-03T17:35:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    17,
                    35,
                    35,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T18:30:36Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    18,
                    30,
                    36,
                    4,
                    31,
                    0
                ],
                "title": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SELMA: A Speech-Enabled Language Model for Virtual Assistant\n  Interactions"
                },
                "summary": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present and evaluate SELMA, a Speech-Enabled Language Model\nfor virtual Assistant interactions that integrates audio and text as inputs to\na Large Language Model (LLM). SELMA is designed to handle three primary and two\nauxiliary tasks related to interactions with virtual assistants simultaneously\nwithin a single end-to-end model. We employ low-rank adaptation modules for\nparameter-efficient training of both the audio encoder and the LLM.\nAdditionally, we implement a feature pooling strategy enabling the system to\nrecognize global patterns and improve accuracy on tasks less reliant on\nindividual sequence elements. Experimental results on Voice Trigger (VT)\ndetection, Device-Directed Speech Detection (DDSD), and Automatic Speech\nRecognition (ASR), demonstrate that our approach both simplifies the typical\ninput processing pipeline of virtual assistants significantly and also improves\nperformance compared to dedicated models for each individual task. SELMA yields\nrelative Equal-Error Rate improvements of 64% on the VT detection task, and 22%\non DDSD, while also achieving word error rates close to the baseline."
                },
                "authors": [
                    {
                        "name": "Dominik Wagner"
                    },
                    {
                        "name": "Alexander Churchill"
                    },
                    {
                        "name": "Siddharth Sigtia"
                    },
                    {
                        "name": "Erik Marchi"
                    }
                ],
                "author_detail": {
                    "name": "Erik Marchi"
                },
                "author": "Erik Marchi",
                "arxiv_comment": "Accepted at ICASSP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19377v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19377v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08959v2",
                "updated": "2025-02-03T16:03:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    3,
                    18,
                    0,
                    34,
                    0
                ],
                "published": "2024-08-16T18:07:48Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    18,
                    7,
                    48,
                    4,
                    229,
                    0
                ],
                "title": "Trust-Oriented Adaptive Guardrails for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trust-Oriented Adaptive Guardrails for Large Language Models"
                },
                "summary": "Guardrail, an emerging mechanism designed to ensure that large language\nmodels (LLMs) align with human values by moderating harmful or toxic responses,\nrequires a sociotechnical approach in their design. This paper addresses a\ncritical issue: existing guardrails lack a well-founded methodology to\naccommodate the diverse needs of different user groups, particularly concerning\naccess rights. Supported by trust modeling (primarily on `social' aspect) and\nenhanced with online in-context learning via retrieval-augmented generation (on\n`technical' aspect), we introduce an adaptive guardrail mechanism, to\ndynamically moderate access to sensitive content based on user trust metrics.\nUser trust metrics, defined as a novel combination of direct interaction trust\nand authority-verified trust, enable the system to precisely tailor the\nstrictness of content moderation by aligning with the user's credibility and\nthe specific context of their inquiries. Our empirical evaluation demonstrates\nthe effectiveness of the adaptive guardrail in meeting diverse user needs,\noutperforming existing guardrails while securing sensitive information and\nprecisely managing potentially hazardous content through a context-aware\nknowledge base. To the best of our knowledge, this work is the first to\nintroduce trust-oriented concept into a guardrail system, offering a scalable\nsolution that enriches the discourse on ethical deployment for next-generation\nLLM service.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guardrail, an emerging mechanism designed to ensure that large language\nmodels (LLMs) align with human values by moderating harmful or toxic responses,\nrequires a sociotechnical approach in their design. This paper addresses a\ncritical issue: existing guardrails lack a well-founded methodology to\naccommodate the diverse needs of different user groups, particularly concerning\naccess rights. Supported by trust modeling (primarily on `social' aspect) and\nenhanced with online in-context learning via retrieval-augmented generation (on\n`technical' aspect), we introduce an adaptive guardrail mechanism, to\ndynamically moderate access to sensitive content based on user trust metrics.\nUser trust metrics, defined as a novel combination of direct interaction trust\nand authority-verified trust, enable the system to precisely tailor the\nstrictness of content moderation by aligning with the user's credibility and\nthe specific context of their inquiries. Our empirical evaluation demonstrates\nthe effectiveness of the adaptive guardrail in meeting diverse user needs,\noutperforming existing guardrails while securing sensitive information and\nprecisely managing potentially hazardous content through a context-aware\nknowledge base. To the best of our knowledge, this work is the first to\nintroduce trust-oriented concept into a guardrail system, offering a scalable\nsolution that enriches the discourse on ethical deployment for next-generation\nLLM service."
                },
                "authors": [
                    {
                        "name": "Jinwei Hu"
                    },
                    {
                        "name": "Yi Dong"
                    },
                    {
                        "name": "Xiaowei Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaowei Huang"
                },
                "author": "Xiaowei Huang",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.13846v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.13846v2",
                "updated": "2025-02-03T16:03:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    16,
                    3,
                    13,
                    0,
                    34,
                    0
                ],
                "published": "2024-02-21T14:44:00Z",
                "published_parsed": [
                    2024,
                    2,
                    21,
                    14,
                    44,
                    0,
                    2,
                    52,
                    0
                ],
                "title": "Large Language Models are Advanced Anonymizers",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are Advanced Anonymizers"
                },
                "summary": "Recent privacy research on large language models (LLMs) has shown that they\nachieve near-human-level performance at inferring personal data from online\ntexts. With ever-increasing model capabilities, existing text anonymization\nmethods are currently lacking behind regulatory requirements and adversarial\nthreats. In this work, we take two steps to bridge this gap: First, we present\na new setting for evaluating anonymization in the face of adversarial LLM\ninferences, allowing for a natural measurement of anonymization performance\nwhile remedying some of the shortcomings of previous metrics. Then, within this\nsetting, we develop a novel LLM-based adversarial anonymization framework\nleveraging the strong inferential capabilities of LLMs to inform our\nanonymization procedure. We conduct a comprehensive experimental evaluation of\nadversarial anonymization across 13 LLMs on real-world and synthetic online\ntexts, comparing it against multiple baselines and industry-grade anonymizers.\nOur evaluation shows that adversarial anonymization outperforms current\ncommercial anonymizers both in terms of the resulting utility and privacy. We\nsupport our findings with a human study (n=50) highlighting a strong and\nconsistent human preference for LLM-anonymized texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent privacy research on large language models (LLMs) has shown that they\nachieve near-human-level performance at inferring personal data from online\ntexts. With ever-increasing model capabilities, existing text anonymization\nmethods are currently lacking behind regulatory requirements and adversarial\nthreats. In this work, we take two steps to bridge this gap: First, we present\na new setting for evaluating anonymization in the face of adversarial LLM\ninferences, allowing for a natural measurement of anonymization performance\nwhile remedying some of the shortcomings of previous metrics. Then, within this\nsetting, we develop a novel LLM-based adversarial anonymization framework\nleveraging the strong inferential capabilities of LLMs to inform our\nanonymization procedure. We conduct a comprehensive experimental evaluation of\nadversarial anonymization across 13 LLMs on real-world and synthetic online\ntexts, comparing it against multiple baselines and industry-grade anonymizers.\nOur evaluation shows that adversarial anonymization outperforms current\ncommercial anonymizers both in terms of the resulting utility and privacy. We\nsupport our findings with a human study (n=50) highlighting a strong and\nconsistent human preference for LLM-anonymized texts."
                },
                "authors": [
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "arxiv_comment": "International Conference on Learning Representations (ICLR 2024)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.13846v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.13846v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00154v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00154v2",
                "updated": "2025-02-03T15:33:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    33,
                    59,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-31T18:59:46Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    18,
                    59,
                    46,
                    3,
                    305,
                    0
                ],
                "title": "Scaling Up Membership Inference: When and How Attacks Succeed on Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling Up Membership Inference: When and How Attacks Succeed on Large\n  Language Models"
                },
                "summary": "Membership inference attacks (MIA) attempt to verify the membership of a\ngiven data sample in the training set for a model. MIA has become relevant in\nrecent years, following the rapid development of large language models (LLM).\nMany are concerned about the usage of copyrighted materials for training them\nand call for methods for detecting such usage. However, recent research has\nlargely concluded that current MIA methods do not work on LLMs. Even when they\nseem to work, it is usually because of the ill-designed experimental setup\nwhere other shortcut features enable \"cheating.\" In this work, we argue that\nMIA still works on LLMs, but only when multiple documents are presented for\ntesting. We construct new benchmarks that measure the MIA performances at a\ncontinuous scale of data samples, from sentences (n-grams) to a collection of\ndocuments (multiple chunks of tokens). To validate the efficacy of current MIA\napproaches at greater scales, we adapt a recent work on Dataset Inference (DI)\nfor the task of binary membership detection that aggregates paragraph-level MIA\nfeatures to enable MIA at document and collection of documents level. This\nbaseline achieves the first successful MIA on pre-trained and fine-tuned LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Membership inference attacks (MIA) attempt to verify the membership of a\ngiven data sample in the training set for a model. MIA has become relevant in\nrecent years, following the rapid development of large language models (LLM).\nMany are concerned about the usage of copyrighted materials for training them\nand call for methods for detecting such usage. However, recent research has\nlargely concluded that current MIA methods do not work on LLMs. Even when they\nseem to work, it is usually because of the ill-designed experimental setup\nwhere other shortcut features enable \"cheating.\" In this work, we argue that\nMIA still works on LLMs, but only when multiple documents are presented for\ntesting. We construct new benchmarks that measure the MIA performances at a\ncontinuous scale of data samples, from sentences (n-grams) to a collection of\ndocuments (multiple chunks of tokens). To validate the efficacy of current MIA\napproaches at greater scales, we adapt a recent work on Dataset Inference (DI)\nfor the task of binary membership detection that aggregates paragraph-level MIA\nfeatures to enable MIA at document and collection of documents level. This\nbaseline achieves the first successful MIA on pre-trained and fine-tuned LLMs."
                },
                "authors": [
                    {
                        "name": "Haritz Puerto"
                    },
                    {
                        "name": "Martin Gubri"
                    },
                    {
                        "name": "Sangdoo Yun"
                    },
                    {
                        "name": "Seong Joon Oh"
                    }
                ],
                "author_detail": {
                    "name": "Seong Joon Oh"
                },
                "author": "Seong Joon Oh",
                "arxiv_comment": "Findings of NAACL 2025. Our code is available at\n  https://github.com/parameterlab/mia-scaling",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00154v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00154v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.10342v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.10342v2",
                "updated": "2025-02-03T15:23:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    23,
                    2,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-13T18:40:10Z",
                "published_parsed": [
                    2024,
                    12,
                    13,
                    18,
                    40,
                    10,
                    4,
                    348,
                    0
                ],
                "title": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining"
                },
                "summary": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital agents are increasingly employed to automate tasks in interactive\ndigital environments such as web pages, software applications, and operating\nsystems. While text-based agents built on Large Language Models (LLMs) often\nrequire frequent updates due to platform-specific APIs, visual agents\nleveraging Multimodal Large Language Models (MLLMs) offer enhanced adaptability\nby interacting directly with Graphical User Interfaces (GUIs). However, these\nagents face significant challenges in visual perception, particularly when\nhandling high-resolution, visually complex digital environments. This paper\nintroduces Iris, a foundational visual agent that addresses these challenges\nthrough two key innovations: Information-Sensitive Cropping (ISC) and\nSelf-Refining Dual Learning (SRDL). ISC dynamically identifies and prioritizes\nvisually dense regions using a edge detection algorithm, enabling efficient\nprocessing by allocating more computational resources to areas with higher\ninformation density. SRDL enhances the agent's ability to handle complex tasks\nby leveraging a dual-learning loop, where improvements in referring (describing\nUI elements) reinforce grounding (locating elements) and vice versa, all\nwithout requiring additional annotated data. Empirical evaluations demonstrate\nthat Iris achieves state-of-the-art performance across multiple benchmarks with\nonly 850K GUI annotations, outperforming methods using 10x more training data.\nThese improvements further translate to significant gains in both web and OS\nagent downstream tasks."
                },
                "authors": [
                    {
                        "name": "Zhiqi Ge"
                    },
                    {
                        "name": "Juncheng Li"
                    },
                    {
                        "name": "Xinglei Pang"
                    },
                    {
                        "name": "Minghe Gao"
                    },
                    {
                        "name": "Kaihang Pan"
                    },
                    {
                        "name": "Wang Lin"
                    },
                    {
                        "name": "Hao Fei"
                    },
                    {
                        "name": "Wenqiao Zhang"
                    },
                    {
                        "name": "Siliang Tang"
                    },
                    {
                        "name": "Yueting Zhuang"
                    }
                ],
                "author_detail": {
                    "name": "Yueting Zhuang"
                },
                "author": "Yueting Zhuang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.10342v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.10342v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.17115v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.17115v2",
                "updated": "2025-02-03T15:18:05Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    18,
                    5,
                    0,
                    34,
                    0
                ],
                "published": "2024-07-24T09:24:49Z",
                "published_parsed": [
                    2024,
                    7,
                    24,
                    9,
                    24,
                    49,
                    2,
                    206,
                    0
                ],
                "title": "Reinforced Prompt Personalization for Recommendation with Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforced Prompt Personalization for Recommendation with Large Language\n  Models"
                },
                "summary": "Designing effective prompts can empower LLMs to understand user preferences\nand provide recommendations with intent comprehension and knowledge utilization\ncapabilities. Nevertheless, recent studies predominantly concentrate on\ntask-wise prompting, developing fixed prompt templates shared across all users\nin a given recommendation task (e.g., rating or ranking). Although convenient,\ntask-wise prompting overlooks individual user differences, leading to\ninaccurate analysis of user interests. In this work, we introduce the concept\nof instance-wise prompting, aiming at personalizing discrete prompts for\nindividual users. Toward this end, we propose Reinforced Prompt Personalization\n(RPP) to realize it automatically. To improve efficiency and quality, RPP\npersonalizes prompts at the sentence level rather than searching in the vast\nvocabulary word-by-word. Specifically, RPP breaks down the prompt into four\npatterns, tailoring patterns based on multi-agent and combining them. Then the\npersonalized prompts interact with LLMs (environment) iteratively, to boost\nLLMs' recommending performance (reward). In addition to RPP, to improve the\nscalability of action space, our proposal of RPP+ dynamically refines the\nselected actions with LLMs throughout the iterative process. Extensive\nexperiments on various datasets demonstrate the superiority of RPP/RPP+ over\ntraditional recommender models, few-shot methods, and other prompt-based\nmethods, underscoring the significance of instance-wise prompting in LLMs for\nrecommendation. Our code is available at https://github.com/maowenyu-11/RPP.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing effective prompts can empower LLMs to understand user preferences\nand provide recommendations with intent comprehension and knowledge utilization\ncapabilities. Nevertheless, recent studies predominantly concentrate on\ntask-wise prompting, developing fixed prompt templates shared across all users\nin a given recommendation task (e.g., rating or ranking). Although convenient,\ntask-wise prompting overlooks individual user differences, leading to\ninaccurate analysis of user interests. In this work, we introduce the concept\nof instance-wise prompting, aiming at personalizing discrete prompts for\nindividual users. Toward this end, we propose Reinforced Prompt Personalization\n(RPP) to realize it automatically. To improve efficiency and quality, RPP\npersonalizes prompts at the sentence level rather than searching in the vast\nvocabulary word-by-word. Specifically, RPP breaks down the prompt into four\npatterns, tailoring patterns based on multi-agent and combining them. Then the\npersonalized prompts interact with LLMs (environment) iteratively, to boost\nLLMs' recommending performance (reward). In addition to RPP, to improve the\nscalability of action space, our proposal of RPP+ dynamically refines the\nselected actions with LLMs throughout the iterative process. Extensive\nexperiments on various datasets demonstrate the superiority of RPP/RPP+ over\ntraditional recommender models, few-shot methods, and other prompt-based\nmethods, underscoring the significance of instance-wise prompting in LLMs for\nrecommendation. Our code is available at https://github.com/maowenyu-11/RPP."
                },
                "authors": [
                    {
                        "name": "Wenyu Mao"
                    },
                    {
                        "name": "Jiancan Wu"
                    },
                    {
                        "name": "Weijian Chen"
                    },
                    {
                        "name": "Chongming Gao"
                    },
                    {
                        "name": "Xiang Wang"
                    },
                    {
                        "name": "Xiangnan He"
                    }
                ],
                "author_detail": {
                    "name": "Xiangnan He"
                },
                "author": "Xiangnan He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.17115v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.17115v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14201v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14201v2",
                "updated": "2025-02-03T15:15:58Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    15,
                    58,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-15T21:02:16Z",
                "published_parsed": [
                    2024,
                    12,
                    15,
                    21,
                    2,
                    16,
                    6,
                    350,
                    0
                ],
                "title": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The \"Huh?\" Button: Improving Understanding in Educational Videos with\n  Large Language Models"
                },
                "summary": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a simple way to use large language models (LLMs) in education.\nSpecifically, our method aims to improve individual comprehension by adding a\nnovel feature to online videos. We combine the low threshold for interactivity\nin digital experiences with the benefits of rephrased and elaborated\nexplanations typical of face-to-face interactions, thereby supporting to close\nknowledge gaps at scale. To demonstrate the technical feasibility of our\napproach, we conducted a proof-of-concept experiment and implemented a\nprototype which is available for testing online. Through the use case, we also\nshow how caching can be applied in LLM-powered applications to reduce their\ncarbon footprint."
                },
                "authors": [
                    {
                        "name": "Boris Ruf"
                    },
                    {
                        "name": "Marcin Detyniecki"
                    }
                ],
                "author_detail": {
                    "name": "Marcin Detyniecki"
                },
                "author": "Marcin Detyniecki",
                "arxiv_comment": "Presented at the 18th IEEE International Workshop on Multimedia\n  Technologies for E-Learning (MTEL), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14201v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14201v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02980v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02980v2",
                "updated": "2025-02-03T15:04:47Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    15,
                    4,
                    47,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-03T20:43:59Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    20,
                    43,
                    59,
                    3,
                    277,
                    0
                ],
                "title": "DecTrain: Deciding When to Train a Monocular Depth DNN Online",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DecTrain: Deciding When to Train a Monocular Depth DNN Online"
                },
                "summary": "Deep neural networks (DNNs) can deteriorate in accuracy when deployment data\ndiffers from training data. While performing online training at all timesteps\ncan improve accuracy, it is computationally expensive. We propose DecTrain, a\nnew algorithm that decides when to train a monocular depth DNN online using\nself-supervision with low overhead. To make the decision at each timestep,\nDecTrain compares the cost of training with the predicted accuracy gain. We\nevaluate DecTrain on out-of-distribution data, and find DecTrain maintains\naccuracy compared to online training at all timesteps, while training only 44%\nof the time on average. We also compare the recovery of a low inference cost\nDNN using DecTrain and a more generalizable high inference cost DNN on various\nsequences. DecTrain recovers the majority (97%) of the accuracy gain of online\ntraining at all timesteps while reducing computation compared to the high\ninference cost DNN which recovers only 66%. With an even smaller DNN, we\nachieve 89% recovery while reducing computation by 56%. DecTrain enables\nlow-cost online training for a smaller DNN to have competitive accuracy with a\nlarger, more generalizable DNN at a lower overall computational cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep neural networks (DNNs) can deteriorate in accuracy when deployment data\ndiffers from training data. While performing online training at all timesteps\ncan improve accuracy, it is computationally expensive. We propose DecTrain, a\nnew algorithm that decides when to train a monocular depth DNN online using\nself-supervision with low overhead. To make the decision at each timestep,\nDecTrain compares the cost of training with the predicted accuracy gain. We\nevaluate DecTrain on out-of-distribution data, and find DecTrain maintains\naccuracy compared to online training at all timesteps, while training only 44%\nof the time on average. We also compare the recovery of a low inference cost\nDNN using DecTrain and a more generalizable high inference cost DNN on various\nsequences. DecTrain recovers the majority (97%) of the accuracy gain of online\ntraining at all timesteps while reducing computation compared to the high\ninference cost DNN which recovers only 66%. With an even smaller DNN, we\nachieve 89% recovery while reducing computation by 56%. DecTrain enables\nlow-cost online training for a smaller DNN to have competitive accuracy with a\nlarger, more generalizable DNN at a lower overall computational cost."
                },
                "authors": [
                    {
                        "name": "Zih-Sing Fu"
                    },
                    {
                        "name": "Soumya Sudhakar"
                    },
                    {
                        "name": "Sertac Karaman"
                    },
                    {
                        "name": "Vivienne Sze"
                    }
                ],
                "author_detail": {
                    "name": "Vivienne Sze"
                },
                "author": "Vivienne Sze",
                "arxiv_doi": "10.1109/LRA.2025.3536206",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LRA.2025.3536206",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.02980v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02980v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07959v2",
                "updated": "2025-02-03T14:51:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    51,
                    44,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-10T14:23:51Z",
                "published_parsed": [
                    2024,
                    10,
                    10,
                    14,
                    23,
                    51,
                    3,
                    284,
                    0
                ],
                "title": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "COMPL-AI Framework: A Technical Interpretation and LLM Benchmarking\n  Suite for the EU Artificial Intelligence Act"
                },
                "summary": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The EU's Artificial Intelligence Act (AI Act) is a significant step towards\nresponsible AI development, but lacks clear technical interpretation, making it\ndifficult to assess models' compliance. This work presents COMPL-AI, a\ncomprehensive framework consisting of (i) the first technical interpretation of\nthe EU AI Act, translating its broad regulatory requirements into measurable\ntechnical requirements, with the focus on large language models (LLMs), and\n(ii) an open-source Act-centered benchmarking suite, based on thorough\nsurveying and implementation of state-of-the-art LLM benchmarks. By evaluating\n12 prominent LLMs in the context of COMPL-AI, we reveal shortcomings in\nexisting models and benchmarks, particularly in areas like robustness, safety,\ndiversity, and fairness. This work highlights the need for a shift in focus\ntowards these aspects, encouraging balanced development of LLMs and more\ncomprehensive regulation-aligned benchmarks. Simultaneously, COMPL-AI for the\nfirst time demonstrates the possibilities and difficulties of bringing the\nAct's obligations to a more concrete, technical level. As such, our work can\nserve as a useful first step towards having actionable recommendations for\nmodel providers, and contributes to ongoing efforts of the EU to enable\napplication of the Act, such as the drafting of the GPAI Code of Practice."
                },
                "authors": [
                    {
                        "name": "Philipp Guldimann"
                    },
                    {
                        "name": "Alexander Spiridonov"
                    },
                    {
                        "name": "Robin Staab"
                    },
                    {
                        "name": "Nikola Jovanović"
                    },
                    {
                        "name": "Mark Vero"
                    },
                    {
                        "name": "Velko Vechev"
                    },
                    {
                        "name": "Anna-Maria Gueorguieva"
                    },
                    {
                        "name": "Mislav Balunović"
                    },
                    {
                        "name": "Nikola Konstantinov"
                    },
                    {
                        "name": "Pavol Bielik"
                    },
                    {
                        "name": "Petar Tsankov"
                    },
                    {
                        "name": "Martin Vechev"
                    }
                ],
                "author_detail": {
                    "name": "Martin Vechev"
                },
                "author": "Martin Vechev",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.06635v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.06635v2",
                "updated": "2025-02-03T14:24:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    24,
                    38,
                    0,
                    34,
                    0
                ],
                "published": "2024-04-09T22:12:39Z",
                "published_parsed": [
                    2024,
                    4,
                    9,
                    22,
                    12,
                    39,
                    1,
                    100,
                    0
                ],
                "title": "Current Affairs: A Security Measurement Study of CCS EV Charging\n  Deployments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current Affairs: A Security Measurement Study of CCS EV Charging\n  Deployments"
                },
                "summary": "Since its introduction in 2012, the Combined Charging System (CCS) has\nemerged as the leading technology for EV fast charging in Europe, North America\nand parts of Asia. The charging communication of CCS is defined by the ISO\n15118 standards, which have been improved over the years. Most notably, in\n2014, important security features such as Transport Layer Security (TLS) and\nusability enhancements such as Plug and Charge were introduced.\n  In this paper, we conduct the first measurement study of publicly deployed\nCCS DC charging stations to capture the state of deployment for different\nprotocol versions and to better understand the attack surface of the EV\ncharging infrastructure. In our evaluation, we examine 325 chargers\nmanufactured between April 2013 and June 2023, and installed as late as May\n2024 by 26 manufacturers across 4 European countries. We find that only 12% of\nthe charging stations we analyzed implement TLS at all, leaving all others\nvulnerable to attacks that have already been demonstrated many years ago. We\nobserve an increasing trend in support for ISO 15118-2 over the years, reaching\n70% of chargers manufactured in 2023. We further notice that most chargers use\na decade-old firmware for their HomePlug modems, which could contain\nvulnerabilities that have been patched since. Finally, we discuss design flaws\nwith the Public Key Infrastructure system used in EV charging, and propose\nchanges to improve the adoption and availability of TLS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Since its introduction in 2012, the Combined Charging System (CCS) has\nemerged as the leading technology for EV fast charging in Europe, North America\nand parts of Asia. The charging communication of CCS is defined by the ISO\n15118 standards, which have been improved over the years. Most notably, in\n2014, important security features such as Transport Layer Security (TLS) and\nusability enhancements such as Plug and Charge were introduced.\n  In this paper, we conduct the first measurement study of publicly deployed\nCCS DC charging stations to capture the state of deployment for different\nprotocol versions and to better understand the attack surface of the EV\ncharging infrastructure. In our evaluation, we examine 325 chargers\nmanufactured between April 2013 and June 2023, and installed as late as May\n2024 by 26 manufacturers across 4 European countries. We find that only 12% of\nthe charging stations we analyzed implement TLS at all, leaving all others\nvulnerable to attacks that have already been demonstrated many years ago. We\nobserve an increasing trend in support for ISO 15118-2 over the years, reaching\n70% of chargers manufactured in 2023. We further notice that most chargers use\na decade-old firmware for their HomePlug modems, which could contain\nvulnerabilities that have been patched since. Finally, we discuss design flaws\nwith the Public Key Infrastructure system used in EV charging, and propose\nchanges to improve the adoption and availability of TLS."
                },
                "authors": [
                    {
                        "name": "Marcell Szakály"
                    },
                    {
                        "name": "Sebastian Köhler"
                    },
                    {
                        "name": "Ivan Martinovic"
                    }
                ],
                "author_detail": {
                    "name": "Ivan Martinovic"
                },
                "author": "Ivan Martinovic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.06635v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.06635v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19202v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19202v2",
                "updated": "2025-02-03T14:05:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    14,
                    5,
                    36,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T15:12:20Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    12,
                    20,
                    4,
                    31,
                    0
                ],
                "title": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving the Robustness of Representation Misdirection for Large\n  Language Model Unlearning"
                },
                "summary": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Representation Misdirection (RM) and variants are established large language\nmodel (LLM) unlearning methods with state-of-the-art performance. In this\npaper, we show that RM methods inherently reduce models' robustness, causing\nthem to misbehave even when a single non-adversarial forget-token is in the\nretain-query. Toward understanding underlying causes, we reframe the unlearning\nprocess as backdoor attacks and defenses: forget-tokens act as backdoor\ntriggers that, when activated in retain-queries, cause disruptions in RM\nmodels' behaviors, similar to successful backdoor attacks. To mitigate this\nvulnerability, we propose Random Noise Augmentation -- a model and method\nagnostic approach with theoretical guarantees for improving the robustness of\nRM methods. Extensive experiments demonstrate that RNA significantly improves\nthe robustness of RM models while enhancing the unlearning performances."
                },
                "authors": [
                    {
                        "name": "Dang Huu-Tien"
                    },
                    {
                        "name": "Hoang Thanh-Tung"
                    },
                    {
                        "name": "Le-Minh Nguyen"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "12 pages, 4 figures, 1 table",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19202v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19202v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.08782v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.08782v4",
                "updated": "2025-02-03T13:13:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    13,
                    44,
                    0,
                    34,
                    0
                ],
                "published": "2024-08-16T14:54:41Z",
                "published_parsed": [
                    2024,
                    8,
                    16,
                    14,
                    54,
                    41,
                    4,
                    229,
                    0
                ],
                "title": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EmoDynamiX: Emotional Support Dialogue Strategy Prediction by Modelling\n  MiXed Emotions and Discourse Dynamics"
                },
                "summary": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing emotionally intelligent conversational systems to provide comfort\nand advice to people experiencing distress is a compelling area of research.\nRecently, with advancements in large language models (LLMs), end-to-end\ndialogue agents without explicit strategy prediction steps have become\nprevalent. However, implicit strategy planning lacks transparency, and recent\nstudies show that LLMs' inherent preference bias towards certain\nsocio-emotional strategies hinders the delivery of high-quality emotional\nsupport. To address this challenge, we propose decoupling strategy prediction\nfrom language generation, and introduce a novel dialogue strategy prediction\nframework, EmoDynamiX, which models the discourse dynamics between user\nfine-grained emotions and system strategies using a heterogeneous graph for\nbetter performance and transparency. Experimental results on two ESC datasets\nshow EmoDynamiX outperforms previous state-of-the-art methods with a\nsignificant margin (better proficiency and lower preference bias). Our approach\nalso exhibits better transparency by allowing backtracing of decision making."
                },
                "authors": [
                    {
                        "name": "Chenwei Wan"
                    },
                    {
                        "name": "Matthieu Labeau"
                    },
                    {
                        "name": "Chloé Clavel"
                    }
                ],
                "author_detail": {
                    "name": "Chloé Clavel"
                },
                "author": "Chloé Clavel",
                "arxiv_comment": "Accepted to NAACL 2025 main, long paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.08782v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.08782v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.06159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.06159v3",
                "updated": "2025-02-03T13:11:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    13,
                    11,
                    25,
                    0,
                    34,
                    0
                ],
                "published": "2024-11-09T12:06:40Z",
                "published_parsed": [
                    2024,
                    11,
                    9,
                    12,
                    6,
                    40,
                    5,
                    314,
                    0
                ],
                "title": "Mixture of Knowledge Minigraph Agents for Literature Review Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Knowledge Minigraph Agents for Literature Review Generation"
                },
                "summary": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Literature reviews play a crucial role in scientific research for\nunderstanding the current state of research, identifying gaps, and guiding\nfuture studies on specific topics. However, the process of conducting a\ncomprehensive literature review is yet time-consuming. This paper proposes a\nnovel framework, collaborative knowledge minigraph agents (CKMAs), to automate\nscholarly literature reviews. A novel prompt-based algorithm, the knowledge\nminigraph construction agent (KMCA), is designed to identify relations between\nconcepts from academic literature and automatically constructs knowledge\nminigraphs. By leveraging the capabilities of large language models on\nconstructed knowledge minigraphs, the multiple path summarization agent (MPSA)\nefficiently organizes concepts and relations from different viewpoints to\ngenerate literature review paragraphs. We evaluate CKMAs on three benchmark\ndatasets. Experimental results show the effectiveness of the proposed method,\nfurther revealing promising applications of LLMs in scientific research."
                },
                "authors": [
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Yan Liu"
                    },
                    {
                        "name": "Sheng-hua Zhong"
                    },
                    {
                        "name": "Gong Chen"
                    },
                    {
                        "name": "Yu Yang"
                    },
                    {
                        "name": "Jiannong Cao"
                    }
                ],
                "author_detail": {
                    "name": "Jiannong Cao"
                },
                "author": "Jiannong Cao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.06159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.06159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.10537v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.10537v2",
                "updated": "2025-02-03T12:57:09Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    57,
                    9,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-14T14:17:52Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    14,
                    17,
                    52,
                    0,
                    288,
                    0
                ],
                "title": "Reproducible Machine Learning-based Voice Pathology Detection:\n  Introducing the Pitch Difference Feature",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reproducible Machine Learning-based Voice Pathology Detection:\n  Introducing the Pitch Difference Feature"
                },
                "summary": "This study introduces a novel methodology for voice pathology detection using\nthe publicly available Saarbr\\\"ucken Voice Database (SVD) database and a robust\nfeature set combining commonly used acoustic handcrafted features with two\nnovel ones: pitch difference (relative variation in fundamental frequency) and\na NaN feature (failed fundamental frequency estimation).\n  We evaluate six machine learning (ML) classifiers - support vector machine,\nk-nearest neighbors, naive Bayes, decision tree, random forest, and AdaBoost -\nusing grid search for feasible hyperparameters of selected classifiers and\n20480 different feature subsets. Top 1000 classifier-feature subset\ncombinations for each classifier type are validated with repeated stratified\ncross-validation. To address class imbalance, we apply K-Means SMOTE to augment\nthe training data.\n  Our approach achieves outstanding performance, reaching 85.61%, 84.69% and\n85.22% unweighted average recall (UAR) for females, males and combined results\nrespectivelly. We intentionally omit accuracy as it is a highly biased metric\nfor imbalanced data. This advancement demonstrates significant potential for\nclinical deployment of ML methods, offering a valuable supportive tool for an\nobjective examination of voice pathologies. To enable an easier use of our\nmethodology and to support our claims, we provide a publicly available GitHub\nrepository with DOI 10.5281/zenodo.13771573. Finally, we provide a REFORMS\nchecklist to enhance readability, reproducibility and justification of our\napproach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study introduces a novel methodology for voice pathology detection using\nthe publicly available Saarbr\\\"ucken Voice Database (SVD) database and a robust\nfeature set combining commonly used acoustic handcrafted features with two\nnovel ones: pitch difference (relative variation in fundamental frequency) and\na NaN feature (failed fundamental frequency estimation).\n  We evaluate six machine learning (ML) classifiers - support vector machine,\nk-nearest neighbors, naive Bayes, decision tree, random forest, and AdaBoost -\nusing grid search for feasible hyperparameters of selected classifiers and\n20480 different feature subsets. Top 1000 classifier-feature subset\ncombinations for each classifier type are validated with repeated stratified\ncross-validation. To address class imbalance, we apply K-Means SMOTE to augment\nthe training data.\n  Our approach achieves outstanding performance, reaching 85.61%, 84.69% and\n85.22% unweighted average recall (UAR) for females, males and combined results\nrespectivelly. We intentionally omit accuracy as it is a highly biased metric\nfor imbalanced data. This advancement demonstrates significant potential for\nclinical deployment of ML methods, offering a valuable supportive tool for an\nobjective examination of voice pathologies. To enable an easier use of our\nmethodology and to support our claims, we provide a publicly available GitHub\nrepository with DOI 10.5281/zenodo.13771573. Finally, we provide a REFORMS\nchecklist to enhance readability, reproducibility and justification of our\napproach."
                },
                "authors": [
                    {
                        "name": "Jan Vrba"
                    },
                    {
                        "name": "Jakub Steinbach"
                    },
                    {
                        "name": "Tomáš Jirsa"
                    },
                    {
                        "name": "Laura Verde"
                    },
                    {
                        "name": "Roberta De Fazio"
                    },
                    {
                        "name": "Yuwen Zeng"
                    },
                    {
                        "name": "Kei Ichiji"
                    },
                    {
                        "name": "Lukáš Hájek"
                    },
                    {
                        "name": "Zuzana Sedláková"
                    },
                    {
                        "name": "Zuzana Urbániová"
                    },
                    {
                        "name": "Martin Chovanec"
                    },
                    {
                        "name": "Jan Mareš"
                    },
                    {
                        "name": "Noriyasu Homma"
                    }
                ],
                "author_detail": {
                    "name": "Noriyasu Homma"
                },
                "author": "Noriyasu Homma",
                "arxiv_comment": "Code repository:\n  https://github.com/aailab-uct/Automated-Robust-and-Reproducible-Voice-Pathology-Detection,\n  Supplementary materials: https://doi.org/10.5281/zenodo.14793017",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.10537v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.10537v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15606v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15606v2",
                "updated": "2025-02-03T12:56:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    56,
                    55,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-20T07:00:46Z",
                "published_parsed": [
                    2024,
                    12,
                    20,
                    7,
                    0,
                    46,
                    4,
                    355,
                    0
                ],
                "title": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal Agent Tuning: Building a VLM-Driven Agent for Efficient Tool\n  Usage"
                },
                "summary": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The advancement of large language models (LLMs) prompts the development of\nmulti-modal agents, which are used as a controller to call external tools,\nproviding a feasible way to solve practical tasks. In this paper, we propose a\nmulti-modal agent tuning method that automatically generates multi-modal\ntool-usage data and tunes a vision-language model (VLM) as the controller for\npowerful tool-usage reasoning. To preserve the data quality, we prompt the\nGPT-4o mini model to generate queries, files, and trajectories, followed by\nquery-file and trajectory verifiers. Based on the data synthesis pipeline, we\ncollect the MM-Traj dataset that contains 20K tasks with trajectories of tool\nusage. Then, we develop the T3-Agent via \\underline{T}rajectory\n\\underline{T}uning on VLMs for \\underline{T}ool usage using MM-Traj.\nEvaluations on the GTA and GAIA benchmarks show that the T3-Agent consistently\nachieves improvements on two popular VLMs: MiniCPM-V-8.5B and {Qwen2-VL-7B},\nwhich outperforms untrained VLMs by $20\\%$, showing the effectiveness of the\nproposed data synthesis pipeline, leading to high-quality data for tool-usage\ncapabilities."
                },
                "authors": [
                    {
                        "name": "Zhi Gao"
                    },
                    {
                        "name": "Bofei Zhang"
                    },
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Xiaojian Ma"
                    },
                    {
                        "name": "Tao Yuan"
                    },
                    {
                        "name": "Yue Fan"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Yunde Jia"
                    },
                    {
                        "name": "Song-Chun Zhu"
                    },
                    {
                        "name": "Qing Li"
                    }
                ],
                "author_detail": {
                    "name": "Qing Li"
                },
                "author": "Qing Li",
                "arxiv_comment": "ICLR 2025, https://mat-agent.github.io/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15606v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15606v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.21009v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.21009v4",
                "updated": "2025-02-03T12:53:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    53,
                    41,
                    0,
                    34,
                    0
                ],
                "published": "2024-07-30T17:55:36Z",
                "published_parsed": [
                    2024,
                    7,
                    30,
                    17,
                    55,
                    36,
                    1,
                    212,
                    0
                ],
                "title": "AI-Assisted Generation of Difficult Math Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Assisted Generation of Difficult Math Questions"
                },
                "summary": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current LLM training positions mathematical reasoning as a core capability.\nWith publicly available sources fully tapped, there is unmet demand for diverse\nand challenging math questions. Relying solely on human experts is both\ntime-consuming and costly, while LLM-generated questions often lack the\nrequisite diversity and difficulty. We present a design framework that combines\nthe strengths of LLMs with a human-in-the-loop approach to generate a diverse\narray of challenging math questions. We leverage LLM metacognition skills\n[Didolkar et al., 2024] of a strong LLM to extract core \"skills\" from existing\nmath datasets. These skills serve as the basis for generating novel and\ndifficult questions by prompting the LLM with random pairs of core skills. The\nuse of two different skills within each question makes finding such questions\nan \"out of distribution\" task for both LLMs and humans. Our pipeline employs\nLLMs to iteratively generate and refine questions and solutions through\nmultiturn prompting. Human annotators then verify and further refine the\nquestions, with their efficiency enhanced via further LLM interactions.\nApplying this pipeline on skills extracted from the MATH dataset [Hendrycks et\nal., 2021] resulted in MATH$^2$ - a dataset of higher-quality math questions,\nas evidenced by: (a) Lower performance of all models on MATH$^2$ than on MATH\n(b) Higher performance on MATH when using MATH$^2$ questions as in-context\nexamples. Although focused on mathematics, our methodology seems applicable to\nother domains requiring structured reasoning, and potentially as a component of\nscalable oversight. Also of interest is a striking relationship observed\nbetween models' performance on the new dataset: the success rate on MATH$^2$ is\nthe square on MATH, suggesting that successfully solving the question in\nMATH$^2$ requires a nontrivial combination of two distinct math skills."
                },
                "authors": [
                    {
                        "name": "Vedant Shah"
                    },
                    {
                        "name": "Dingli Yu"
                    },
                    {
                        "name": "Kaifeng Lyu"
                    },
                    {
                        "name": "Simon Park"
                    },
                    {
                        "name": "Jiatong Yu"
                    },
                    {
                        "name": "Yinghui He"
                    },
                    {
                        "name": "Nan Rosemary Ke"
                    },
                    {
                        "name": "Michael Mozer"
                    },
                    {
                        "name": "Yoshua Bengio"
                    },
                    {
                        "name": "Sanjeev Arora"
                    },
                    {
                        "name": "Anirudh Goyal"
                    }
                ],
                "author_detail": {
                    "name": "Anirudh Goyal"
                },
                "author": "Anirudh Goyal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.21009v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.21009v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19060v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19060v2",
                "updated": "2025-02-03T12:12:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    12,
                    12,
                    30,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T11:47:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    11,
                    47,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contrast-Aware Calibration for Fine-Tuned CLIP: Leveraging Image-Text\n  Alignment"
                },
                "summary": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-language models (VLMs), such as CLIP, have demonstrated exceptional\ngeneralization capabilities and can quickly adapt to downstream tasks through\nprompt fine-tuning. Unfortunately, in classification tasks involving\nnon-training classes, known as open-vocabulary setting, fine-tuned VLMs often\noverfit to train classes, resulting in a misalignment between confidence scores\nand actual accuracy on unseen classes, which significantly undermines their\nreliability in real-world deployments. Existing confidence calibration methods\ntypically require training parameters or analyzing features from the training\ndataset, restricting their ability to generalize unseen classes without\ncorresponding train data. Moreover, VLM-specific calibration methods rely\nsolely on text features from train classes as calibration indicators, which\ninherently limits their ability to calibrate train classes. To address these\nchallenges, we propose an effective multimodal calibration method\nContrast-Aware Calibration (CAC). Building on the original CLIP's zero-shot\nadaptability and the conclusion from empirical analysis that poor intra-class\nand inter-class discriminative ability on unseen classes is the root cause, we\ncalculate calibration weights based on the contrastive difference between the\noriginal and fine-tuned CLIP. This method not only adapts to calibrating unseen\nclasses but also overcomes the limitations of previous VLM calibration methods\nthat could not calibrate train classes. In experiments involving 11 datasets\nwith 5 fine-tuning methods, CAC consistently achieved the best calibration\neffect on both train and unseen classes without sacrificing accuracy and\ninference speed."
                },
                "authors": [
                    {
                        "name": "Song-Lin Lv"
                    },
                    {
                        "name": "Yu-Yang Chen"
                    },
                    {
                        "name": "Zhi Zhou"
                    },
                    {
                        "name": "Yu-Feng Li"
                    },
                    {
                        "name": "Lan-Zhe Guo"
                    }
                ],
                "author_detail": {
                    "name": "Lan-Zhe Guo"
                },
                "author": "Lan-Zhe Guo",
                "arxiv_comment": "We are withdrawing the paper due to comments indicating overlap with\n  parts of another paper. We will revise the appendix and submit a new version\n  after addressing the issue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19060v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19060v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.05892v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.05892v3",
                "updated": "2025-02-03T11:44:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    11,
                    44,
                    59,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-08T11:14:16Z",
                "published_parsed": [
                    2024,
                    12,
                    8,
                    11,
                    14,
                    16,
                    6,
                    343,
                    0
                ],
                "title": "PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack\n  for Toxicity Maximization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PBI-Attack: Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack\n  for Toxicity Maximization"
                },
                "summary": "Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to\njailbreak attacks is essential for their responsible real-world deployment.\nMost previous work requires access to model gradients, or is based on human\nknowledge (prompt engineering) to complete jailbreak, and they hardly consider\nthe interaction of images and text, resulting in inability to jailbreak in\nblack box scenarios or poor performance. To overcome these limitations, we\npropose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for\ntoxicity maximization, referred to as PBI-Attack. Our method begins by\nextracting malicious features from a harmful corpus using an alternative LVLM\nand embedding these features into a benign image as prior information.\nSubsequently, we enhance these features through bidirectional cross-modal\ninteraction optimization, which iteratively optimizes the bimodal perturbations\nin an alternating manner through greedy search, aiming to maximize the toxicity\nof the generated response. The toxicity level is quantified using a\nwell-trained evaluation model. Experiments demonstrate that PBI-Attack\noutperforms previous state-of-the-art jailbreak methods, achieving an average\nattack success rate of 92.5% across three open-source LVLMs and around 67.3% on\nthree closed-source LVLMs. Disclaimer: This paper contains potentially\ndisturbing and offensive content.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the vulnerabilities of Large Vision Language Models (LVLMs) to\njailbreak attacks is essential for their responsible real-world deployment.\nMost previous work requires access to model gradients, or is based on human\nknowledge (prompt engineering) to complete jailbreak, and they hardly consider\nthe interaction of images and text, resulting in inability to jailbreak in\nblack box scenarios or poor performance. To overcome these limitations, we\npropose a Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for\ntoxicity maximization, referred to as PBI-Attack. Our method begins by\nextracting malicious features from a harmful corpus using an alternative LVLM\nand embedding these features into a benign image as prior information.\nSubsequently, we enhance these features through bidirectional cross-modal\ninteraction optimization, which iteratively optimizes the bimodal perturbations\nin an alternating manner through greedy search, aiming to maximize the toxicity\nof the generated response. The toxicity level is quantified using a\nwell-trained evaluation model. Experiments demonstrate that PBI-Attack\noutperforms previous state-of-the-art jailbreak methods, achieving an average\nattack success rate of 92.5% across three open-source LVLMs and around 67.3% on\nthree closed-source LVLMs. Disclaimer: This paper contains potentially\ndisturbing and offensive content."
                },
                "authors": [
                    {
                        "name": "Ruoxi Cheng"
                    },
                    {
                        "name": "Yizhong Ding"
                    },
                    {
                        "name": "Shuirong Cao"
                    },
                    {
                        "name": "Ranjie Duan"
                    },
                    {
                        "name": "Xiaoshuang Jia"
                    },
                    {
                        "name": "Shaowei Yuan"
                    },
                    {
                        "name": "Zhiqiang Wang"
                    },
                    {
                        "name": "Xiaojun Jia"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojun Jia"
                },
                "author": "Xiaojun Jia",
                "arxiv_comment": "Prior-Guided Bimodal Interactive Black-Box Jailbreak Attack for\n  Toxicity Maximization",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.05892v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.05892v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14566v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14566v2",
                "updated": "2025-02-03T11:32:57Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    11,
                    32,
                    57,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-24T15:19:04Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    15,
                    19,
                    4,
                    4,
                    24,
                    0
                ],
                "title": "Calibrating Wireless AI via Meta-Learned Context-Dependent Conformal\n  Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Calibrating Wireless AI via Meta-Learned Context-Dependent Conformal\n  Prediction"
                },
                "summary": "Modern software-defined networks, such as Open Radio Access Network (O-RAN)\nsystems, rely on artificial intelligence (AI)-powered applications running on\ncontrollers interfaced with the radio access network. To ensure that these AI\napplications operate reliably at runtime, they must be properly calibrated\nbefore deployment. A promising and theoretically grounded approach to\ncalibration is conformal prediction (CP), which enhances any AI model by\ntransforming it into a provably reliable set predictor that provides error bars\nfor estimates and decisions. CP requires calibration data that matches the\ndistribution of the environment encountered during runtime. However, in\npractical scenarios, network controllers often have access only to data\ncollected under different contexts -- such as varying traffic patterns and\nnetwork conditions -- leading to a mismatch between the calibration and runtime\ndistributions. This paper introduces a novel methodology to address this\ncalibration-test distribution shift. The approach leverages meta-learning to\ndevelop a zero-shot estimator of distribution shifts, relying solely on\ncontextual information. The proposed method, called meta-learned\ncontext-dependent weighted conformal prediction (ML-WCP), enables effective\ncalibration of AI applications without requiring data from the current context.\nAdditionally, it can incorporate data from multiple contexts to further enhance\ncalibration reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern software-defined networks, such as Open Radio Access Network (O-RAN)\nsystems, rely on artificial intelligence (AI)-powered applications running on\ncontrollers interfaced with the radio access network. To ensure that these AI\napplications operate reliably at runtime, they must be properly calibrated\nbefore deployment. A promising and theoretically grounded approach to\ncalibration is conformal prediction (CP), which enhances any AI model by\ntransforming it into a provably reliable set predictor that provides error bars\nfor estimates and decisions. CP requires calibration data that matches the\ndistribution of the environment encountered during runtime. However, in\npractical scenarios, network controllers often have access only to data\ncollected under different contexts -- such as varying traffic patterns and\nnetwork conditions -- leading to a mismatch between the calibration and runtime\ndistributions. This paper introduces a novel methodology to address this\ncalibration-test distribution shift. The approach leverages meta-learning to\ndevelop a zero-shot estimator of distribution shifts, relying solely on\ncontextual information. The proposed method, called meta-learned\ncontext-dependent weighted conformal prediction (ML-WCP), enables effective\ncalibration of AI applications without requiring data from the current context.\nAdditionally, it can incorporate data from multiple contexts to further enhance\ncalibration reliability."
                },
                "authors": [
                    {
                        "name": "Seonghoon Yoo"
                    },
                    {
                        "name": "Sangwoo Park"
                    },
                    {
                        "name": "Petar Popovski"
                    },
                    {
                        "name": "Joonhyuk Kang"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14566v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14566v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13720v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13720v2",
                "updated": "2025-02-03T10:52:15Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    10,
                    52,
                    15,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-23T14:50:37Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    14,
                    50,
                    37,
                    3,
                    23,
                    0
                ],
                "title": "Musical ethnocentrism in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Musical ethnocentrism in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) reflect the biases in their training data and,\nby extension, those of the people who created this training data. Detecting,\nanalyzing, and mitigating such biases is becoming a focus of research. One type\nof bias that has been understudied so far are geocultural biases. Those can be\ncaused by an imbalance in the representation of different geographic regions\nand cultures in the training data, but also by value judgments contained\ntherein. In this paper, we make a first step towards analyzing musical biases\nin LLMs, particularly ChatGPT and Mixtral. We conduct two experiments. In the\nfirst, we prompt LLMs to provide lists of the \"Top 100\" musical contributors of\nvarious categories and analyze their countries of origin. In the second\nexperiment, we ask the LLMs to numerically rate various aspects of the musical\ncultures of different countries. Our results indicate a strong preference of\nthe LLMs for Western music cultures in both experiments."
                },
                "authors": [
                    {
                        "name": "Anna Kruspe"
                    }
                ],
                "author_detail": {
                    "name": "Anna Kruspe"
                },
                "author": "Anna Kruspe",
                "arxiv_journal_ref": "Proceedings of the 3rd Workshop on NLP for Music and Audio\n  (NLP4MusA) 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13720v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13720v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18158v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18158v2",
                "updated": "2025-02-03T10:45:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    10,
                    45,
                    22,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-30T05:48:13Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    5,
                    48,
                    13,
                    3,
                    30,
                    0
                ],
                "title": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cryptocurrency Transaction Analysis: A Bitcoin\n  Case Study"
                },
                "summary": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cryptocurrencies are widely used, yet current methods for analyzing\ntransactions heavily rely on opaque, black-box models. These lack\ninterpretability and adaptability, failing to effectively capture behavioral\npatterns. Many researchers, including us, believe that Large Language Models\n(LLMs) could bridge this gap due to their robust reasoning abilities for\ncomplex tasks. In this paper, we test this hypothesis by applying LLMs to\nreal-world cryptocurrency transaction graphs, specifically within the Bitcoin\nnetwork. We introduce a three-tiered framework to assess LLM capabilities:\nfoundational metrics, characteristic overview, and contextual interpretation.\nThis includes a new, human-readable graph representation format, LLM4TG, and a\nconnectivity-enhanced sampling algorithm, CETraS, which simplifies larger\ntransaction graphs. Experimental results show that LLMs excel at foundational\nmetrics and offer detailed characteristic overviews. Their effectiveness in\ncontextual interpretation suggests they can provide useful explanations of\ntransaction behaviors, even with limited labeled data."
                },
                "authors": [
                    {
                        "name": "Yuchen Lei"
                    },
                    {
                        "name": "Yuexin Xiang"
                    },
                    {
                        "name": "Qin Wang"
                    },
                    {
                        "name": "Rafael Dowsley"
                    },
                    {
                        "name": "Tsz Hon Yuen"
                    },
                    {
                        "name": "Jiangshan Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jiangshan Yu"
                },
                "author": "Jiangshan Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18158v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18158v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.14230v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.14230v3",
                "updated": "2025-02-03T10:33:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    10,
                    33,
                    17,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-20T11:51:00Z",
                "published_parsed": [
                    2024,
                    6,
                    20,
                    11,
                    51,
                    0,
                    3,
                    172,
                    0
                ],
                "title": "Raising the Bar: Investigating the Values of Large Language Models via\n  Generative Evolving Testing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Raising the Bar: Investigating the Values of Large Language Models via\n  Generative Evolving Testing"
                },
                "summary": "Warning: Contains harmful model outputs.\n  Despite significant advancements, the propensity of Large Language Models\n(LLMs) to generate harmful and unethical content poses critical challenges.\nMeasuring value alignment of LLMs becomes crucial for their regulation and\nresponsible deployment. Although numerous benchmarks have been constructed to\nassess social bias, toxicity, and ethical issues in LLMs, those static\nbenchmarks suffer from evaluation chronoeffect, in which, as models rapidly\nevolve, existing benchmarks may leak into training data or become saturated,\noverestimating ever-developing LLMs. To tackle this problem, we propose GETA, a\nnovel generative evolving testing approach based on adaptive testing methods in\nmeasurement theory. Unlike traditional adaptive testing methods that rely on a\nstatic test item pool, GETA probes the underlying moral boundaries of LLMs by\ndynamically generating test items tailored to model capability. GETA co-evolves\nwith LLMs by learning a joint distribution of item difficulty and model value\nconformity, thus effectively addressing evaluation chronoeffect. We evaluated\nvarious popular LLMs with GETA and demonstrated that 1) GETA can dynamically\ncreate difficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Warning: Contains harmful model outputs.\n  Despite significant advancements, the propensity of Large Language Models\n(LLMs) to generate harmful and unethical content poses critical challenges.\nMeasuring value alignment of LLMs becomes crucial for their regulation and\nresponsible deployment. Although numerous benchmarks have been constructed to\nassess social bias, toxicity, and ethical issues in LLMs, those static\nbenchmarks suffer from evaluation chronoeffect, in which, as models rapidly\nevolve, existing benchmarks may leak into training data or become saturated,\noverestimating ever-developing LLMs. To tackle this problem, we propose GETA, a\nnovel generative evolving testing approach based on adaptive testing methods in\nmeasurement theory. Unlike traditional adaptive testing methods that rely on a\nstatic test item pool, GETA probes the underlying moral boundaries of LLMs by\ndynamically generating test items tailored to model capability. GETA co-evolves\nwith LLMs by learning a joint distribution of item difficulty and model value\nconformity, thus effectively addressing evaluation chronoeffect. We evaluated\nvarious popular LLMs with GETA and demonstrated that 1) GETA can dynamically\ncreate difficulty-tailored test items and 2) GETA's evaluation results are more\nconsistent with models' performance on unseen OOD and i.i.d. items, laying the\ngroundwork for future evaluation paradigms."
                },
                "authors": [
                    {
                        "name": "Han Jiang"
                    },
                    {
                        "name": "Xiaoyuan Yi"
                    },
                    {
                        "name": "Zhihua Wei"
                    },
                    {
                        "name": "Ziang Xiao"
                    },
                    {
                        "name": "Shu Wang"
                    },
                    {
                        "name": "Xing Xie"
                    }
                ],
                "author_detail": {
                    "name": "Xing Xie"
                },
                "author": "Xing Xie",
                "arxiv_comment": "Under Review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.14230v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.14230v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.02503v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.02503v2",
                "updated": "2025-02-03T10:00:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    10,
                    0,
                    6,
                    0,
                    34,
                    0
                ],
                "published": "2024-05-03T22:30:15Z",
                "published_parsed": [
                    2024,
                    5,
                    3,
                    22,
                    30,
                    15,
                    4,
                    124,
                    0
                ],
                "title": "Axiomatic Causal Interventions for Reverse Engineering Relevance\n  Computation in Neural Retrieval Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Axiomatic Causal Interventions for Reverse Engineering Relevance\n  Computation in Neural Retrieval Models"
                },
                "summary": "Neural models have demonstrated remarkable performance across diverse ranking\ntasks. However, the processes and internal mechanisms along which they\ndetermine relevance are still largely unknown. Existing approaches for\nanalyzing neural ranker behavior with respect to IR properties rely either on\nassessing overall model behavior or employing probing methods that may offer an\nincomplete understanding of causal mechanisms. To provide a more granular\nunderstanding of internal model decision-making processes, we propose the use\nof causal interventions to reverse engineer neural rankers, and demonstrate how\nmechanistic interpretability methods can be used to isolate components\nsatisfying term-frequency axioms within a ranking model. We identify a group of\nattention heads that detect duplicate tokens in earlier layers of the model,\nthen communicate with downstream heads to compute overall document relevance.\nMore generally, we propose that this style of mechanistic analysis opens up\navenues for reverse engineering the processes neural retrieval models use to\ncompute relevance. This work aims to initiate granular interpretability efforts\nthat will not only benefit retrieval model development and training, but\nultimately ensure safer deployment of these models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural models have demonstrated remarkable performance across diverse ranking\ntasks. However, the processes and internal mechanisms along which they\ndetermine relevance are still largely unknown. Existing approaches for\nanalyzing neural ranker behavior with respect to IR properties rely either on\nassessing overall model behavior or employing probing methods that may offer an\nincomplete understanding of causal mechanisms. To provide a more granular\nunderstanding of internal model decision-making processes, we propose the use\nof causal interventions to reverse engineer neural rankers, and demonstrate how\nmechanistic interpretability methods can be used to isolate components\nsatisfying term-frequency axioms within a ranking model. We identify a group of\nattention heads that detect duplicate tokens in earlier layers of the model,\nthen communicate with downstream heads to compute overall document relevance.\nMore generally, we propose that this style of mechanistic analysis opens up\navenues for reverse engineering the processes neural retrieval models use to\ncompute relevance. This work aims to initiate granular interpretability efforts\nthat will not only benefit retrieval model development and training, but\nultimately ensure safer deployment of these models."
                },
                "authors": [
                    {
                        "name": "Catherine Chen"
                    },
                    {
                        "name": "Jack Merullo"
                    },
                    {
                        "name": "Carsten Eickhoff"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Eickhoff"
                },
                "author": "Carsten Eickhoff",
                "arxiv_comment": "10 pages, 10 figures, updated version of SIGIR 2024 perspective paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.02503v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.02503v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.14744v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.14744v2",
                "updated": "2025-02-03T09:57:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    57,
                    1,
                    0,
                    34,
                    0
                ],
                "published": "2024-08-27T02:45:26Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    2,
                    45,
                    26,
                    1,
                    240,
                    0
                ],
                "title": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RSTeller: Scaling Up Visual Language Modeling in Remote Sensing with\n  Rich Linguistic Semantics from Openly Available Data and Large Language\n  Models"
                },
                "summary": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Abundant, well-annotated multimodal data in remote sensing are pivotal for\naligning complex visual remote sensing (RS) scenes with human language,\nenabling the development of specialized vision language models across diverse\nRS interpretation tasks. However, annotating RS images with rich linguistic\nsemantics at scale demands expertise in RS and substantial human labor, making\nit costly and often impractical. In this study, we propose a workflow that\nleverages large language models (LLMs) to generate multimodal datasets with\nsemantically rich captions at scale from plain OpenStreetMap (OSM) data for\nimages sourced from the Google Earth Engine (GEE) platform. This approach\nfacilitates the generation of paired remote sensing data and can be readily\nscaled up using openly available data. Within this framework, we present\nRSTeller, a multimodal dataset comprising over 1.3 million RS images, each\naccompanied by two descriptive captions. Extensive experiments demonstrate that\nRSTeller enhances the performance of multiple existing vision language models\nfor RS scene understanding through continual pre-training. Our methodology\nsignificantly reduces the manual effort and expertise needed for annotating\nremote sensing imagery while democratizing access to high-quality annotated\ndata. This advancement fosters progress in visual language modeling and\nencourages broader participation in remote sensing research and applications.\nThe RSTeller dataset is available at https://github.com/SlytherinGe/RSTeller."
                },
                "authors": [
                    {
                        "name": "Junyao Ge"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Yang Zheng"
                    },
                    {
                        "name": "Kaitai Guo"
                    },
                    {
                        "name": "Jimin Liang"
                    }
                ],
                "author_detail": {
                    "name": "Jimin Liang"
                },
                "author": "Jimin Liang",
                "arxiv_comment": "Submitted to ISPRS",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.14744v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.14744v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.4.8; I.2.10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.17600v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.17600v2",
                "updated": "2025-02-03T09:48:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    48,
                    26,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-23T06:54:03Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    6,
                    54,
                    3,
                    2,
                    297,
                    0
                ],
                "title": "Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphusion: A RAG Framework for Knowledge Graph Construction with a\n  Global Perspective"
                },
                "summary": "Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge Graphs (KGs) are crucial in the field of artificial intelligence\nand are widely used in downstream tasks, such as question-answering (QA). The\nconstruction of KGs typically requires significant effort from domain experts.\nLarge Language Models (LLMs) have recently been used for Knowledge Graph\nConstruction (KGC). However, most existing approaches focus on a local\nperspective, extracting knowledge triplets from individual sentences or\ndocuments, missing a fusion process to combine the knowledge in a global KG.\nThis work introduces Graphusion, a zero-shot KGC framework from free text. It\ncontains three steps: in Step 1, we extract a list of seed entities using topic\nmodeling to guide the final KG includes the most relevant entities; in Step 2,\nwe conduct candidate triplet extraction using LLMs; in Step 3, we design the\nnovel fusion module that provides a global view of the extracted knowledge,\nincorporating entity merging, conflict resolution, and novel triplet discovery.\nResults show that Graphusion achieves scores of 2.92 and 2.37 out of 3 for\nentity extraction and relation recognition, respectively. Moreover, we showcase\nhow Graphusion could be applied to the Natural Language Processing (NLP) domain\nand validate it in an educational scenario. Specifically, we introduce TutorQA,\na new expert-verified benchmark for QA, comprising six tasks and a total of\n1,200 QA pairs. Using the Graphusion-constructed KG, we achieve a significant\nimprovement on the benchmark, for example, a 9.2% accuracy improvement on\nsub-graph completion."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Boming Yang"
                    },
                    {
                        "name": "Aosong Feng"
                    },
                    {
                        "name": "Sixun Ouyang"
                    },
                    {
                        "name": "Moritz Blum"
                    },
                    {
                        "name": "Tianwei She"
                    },
                    {
                        "name": "Yuang Jiang"
                    },
                    {
                        "name": "Freddy Lecue"
                    },
                    {
                        "name": "Jinghui Lu"
                    },
                    {
                        "name": "Irene Li"
                    }
                ],
                "author_detail": {
                    "name": "Irene Li"
                },
                "author": "Irene Li",
                "arxiv_comment": "arXiv admin note: substantial text overlap with arXiv:2407.10794",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.17600v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.17600v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11668v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11668v2",
                "updated": "2025-02-03T09:25:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    25,
                    8,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-17T15:51:01Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    51,
                    1,
                    0,
                    169,
                    0
                ],
                "title": "\"Not Aligned\" is Not \"Malicious\": Being Careful about Hallucinations of\n  Large Language Models' Jailbreak",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Not Aligned\" is Not \"Malicious\": Being Careful about Hallucinations of\n  Large Language Models' Jailbreak"
                },
                "summary": "\"Jailbreak\" is a major safety concern of Large Language Models (LLMs), which\noccurs when malicious prompts lead LLMs to produce harmful outputs, raising\nissues about the reliability and safety of LLMs. Therefore, an effective\nevaluation of jailbreaks is very crucial to develop its mitigation strategies.\nHowever, our research reveals that many jailbreaks identified by current\nevaluations may actually be hallucinations-erroneous outputs that are mistaken\nfor genuine safety breaches. This finding suggests that some perceived\nvulnerabilities might not represent actual threats, indicating a need for more\nprecise red teaming benchmarks. To address this problem, we propose the\n$\\textbf{B}$enchmark for reli$\\textbf{AB}$ilit$\\textbf{Y}$ and\njail$\\textbf{B}$reak ha$\\textbf{L}$l$\\textbf{U}$cination $\\textbf{E}$valuation\n(BabyBLUE). BabyBLUE introduces a specialized validation framework including\nvarious evaluators to enhance existing jailbreak benchmarks, ensuring outputs\nare useful malicious instructions. Additionally, BabyBLUE presents a new\ndataset as an augmentation to the existing red teaming benchmarks, specifically\naddressing hallucinations in jailbreaks, aiming to evaluate the true potential\nof jailbroken LLM outputs to cause harm to human society.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "\"Jailbreak\" is a major safety concern of Large Language Models (LLMs), which\noccurs when malicious prompts lead LLMs to produce harmful outputs, raising\nissues about the reliability and safety of LLMs. Therefore, an effective\nevaluation of jailbreaks is very crucial to develop its mitigation strategies.\nHowever, our research reveals that many jailbreaks identified by current\nevaluations may actually be hallucinations-erroneous outputs that are mistaken\nfor genuine safety breaches. This finding suggests that some perceived\nvulnerabilities might not represent actual threats, indicating a need for more\nprecise red teaming benchmarks. To address this problem, we propose the\n$\\textbf{B}$enchmark for reli$\\textbf{AB}$ilit$\\textbf{Y}$ and\njail$\\textbf{B}$reak ha$\\textbf{L}$l$\\textbf{U}$cination $\\textbf{E}$valuation\n(BabyBLUE). BabyBLUE introduces a specialized validation framework including\nvarious evaluators to enhance existing jailbreak benchmarks, ensuring outputs\nare useful malicious instructions. Additionally, BabyBLUE presents a new\ndataset as an augmentation to the existing red teaming benchmarks, specifically\naddressing hallucinations in jailbreaks, aiming to evaluate the true potential\nof jailbroken LLM outputs to cause harm to human society."
                },
                "authors": [
                    {
                        "name": "Lingrui Mei"
                    },
                    {
                        "name": "Shenghua Liu"
                    },
                    {
                        "name": "Yiwei Wang"
                    },
                    {
                        "name": "Baolong Bi"
                    },
                    {
                        "name": "Jiayi Mao"
                    },
                    {
                        "name": "Xueqi Cheng"
                    }
                ],
                "author_detail": {
                    "name": "Xueqi Cheng"
                },
                "author": "Xueqi Cheng",
                "arxiv_comment": "COLING 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11668v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11668v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05855v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05855v3",
                "updated": "2025-02-03T09:13:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    13,
                    17,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-10T10:53:48Z",
                "published_parsed": [
                    2025,
                    1,
                    10,
                    10,
                    53,
                    48,
                    4,
                    10,
                    0
                ],
                "title": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ConSim: Measuring Concept-Based Explanations' Effectiveness with\n  Automated Simulatability"
                },
                "summary": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concept-based explanations work by mapping complex model computations to\nhuman-understandable concepts. Evaluating such explanations is very difficult,\nas it includes not only the quality of the induced space of possible concepts\nbut also how effectively the chosen concepts are communicated to users.\nExisting evaluation metrics often focus solely on the former, neglecting the\nlatter. We introduce an evaluation framework for measuring concept explanations\nvia automated simulatability: a simulator's ability to predict the explained\nmodel's outputs based on the provided explanations. This approach accounts for\nboth the concept space and its interpretation in an end-to-end evaluation.\nHuman studies for simulatability are notoriously difficult to enact,\nparticularly at the scale of a wide, comprehensive empirical evaluation (which\nis the subject of this work). We propose using large language models (LLMs) as\nsimulators to approximate the evaluation and report various analyses to make\nsuch approximations reliable. Our method allows for scalable and consistent\nevaluation across various models and datasets. We report a comprehensive\nempirical evaluation using this framework and show that LLMs provide consistent\nrankings of explanation methods. Code available at\nhttps://github.com/AnonymousConSim/ConSim."
                },
                "authors": [
                    {
                        "name": "Antonin Poché"
                    },
                    {
                        "name": "Alon Jacovi"
                    },
                    {
                        "name": "Agustin Martin Picard"
                    },
                    {
                        "name": "Victor Boutin"
                    },
                    {
                        "name": "Fanny Jourdan"
                    }
                ],
                "author_detail": {
                    "name": "Fanny Jourdan"
                },
                "arxiv_affiliation": "CERCO UMR5549, ANITI",
                "author": "Fanny Jourdan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05855v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05855v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.03376v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.03376v2",
                "updated": "2025-02-03T09:11:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    9,
                    11,
                    36,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-05T15:31:43Z",
                "published_parsed": [
                    2024,
                    6,
                    5,
                    15,
                    31,
                    43,
                    2,
                    157,
                    0
                ],
                "title": "Log Parsing using LLMs with Self-Generated In-Context Learning and\n  Self-Correction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log Parsing using LLMs with Self-Generated In-Context Learning and\n  Self-Correction"
                },
                "summary": "Log parsing transforms log messages into structured formats, serving as a\ncrucial step for log analysis. Despite a variety of log parsers that have been\nproposed, their performance on evolving log data remains unsatisfactory due to\nreliance on human-crafted rules or learning-based models with limited training\ndata. The recent emergence of large language models (LLMs) has demonstrated\nstrong abilities in understanding natural language and code, making it\npromising to apply LLMs for log parsing. Consequently, several studies have\nproposed LLM-based log parsers. However, LLMs may produce inaccurate templates,\nand existing LLM-based log parsers directly use the template generated by the\nLLM as the parsing result, hindering the accuracy of log parsing. Furthermore,\nthese log parsers depend heavily on historical log data as demonstrations,\nwhich poses challenges in maintaining accuracy when dealing with scarce\nhistorical log data or evolving log data. To address these challenges, we\npropose AdaParser, an effective and adaptive log parsing framework using LLMs\nwith self-generated in-context learning (SG-ICL) and self-correction. To\nfacilitate accurate log parsing, AdaParser incorporates a novel component, a\ntemplate corrector, which utilizes the LLM to correct potential parsing errors\nin the templates it generates. In addition, AdaParser maintains a dynamic\ncandidate set composed of previously generated templates as demonstrations to\nadapt evolving log data. Extensive experiments on public large-scale datasets\nindicate that AdaParser outperforms state-of-the-art methods across all\nmetrics, even in zero-shot scenarios. Moreover, when integrated with different\nLLMs, AdaParser consistently enhances the performance of the utilized LLMs by a\nlarge margin.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log parsing transforms log messages into structured formats, serving as a\ncrucial step for log analysis. Despite a variety of log parsers that have been\nproposed, their performance on evolving log data remains unsatisfactory due to\nreliance on human-crafted rules or learning-based models with limited training\ndata. The recent emergence of large language models (LLMs) has demonstrated\nstrong abilities in understanding natural language and code, making it\npromising to apply LLMs for log parsing. Consequently, several studies have\nproposed LLM-based log parsers. However, LLMs may produce inaccurate templates,\nand existing LLM-based log parsers directly use the template generated by the\nLLM as the parsing result, hindering the accuracy of log parsing. Furthermore,\nthese log parsers depend heavily on historical log data as demonstrations,\nwhich poses challenges in maintaining accuracy when dealing with scarce\nhistorical log data or evolving log data. To address these challenges, we\npropose AdaParser, an effective and adaptive log parsing framework using LLMs\nwith self-generated in-context learning (SG-ICL) and self-correction. To\nfacilitate accurate log parsing, AdaParser incorporates a novel component, a\ntemplate corrector, which utilizes the LLM to correct potential parsing errors\nin the templates it generates. In addition, AdaParser maintains a dynamic\ncandidate set composed of previously generated templates as demonstrations to\nadapt evolving log data. Extensive experiments on public large-scale datasets\nindicate that AdaParser outperforms state-of-the-art methods across all\nmetrics, even in zero-shot scenarios. Moreover, when integrated with different\nLLMs, AdaParser consistently enhances the performance of the utilized LLMs by a\nlarge margin."
                },
                "authors": [
                    {
                        "name": "Yifan Wu"
                    },
                    {
                        "name": "Siyu Yu"
                    },
                    {
                        "name": "Ying Li"
                    }
                ],
                "author_detail": {
                    "name": "Ying Li"
                },
                "author": "Ying Li",
                "arxiv_comment": "Accepted by the 33rd IEEE/ACM International Conference on Program\n  Comprehension (ICPC'25)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.03376v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.03376v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13979v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13979v2",
                "updated": "2025-02-03T08:54:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    54,
                    28,
                    0,
                    34,
                    0
                ],
                "published": "2024-09-21T02:09:13Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    2,
                    9,
                    13,
                    5,
                    265,
                    0
                ],
                "title": "Role-Play Paradox in Large Language Models: Reasoning Performance Gains\n  and Ethical Dilemmas",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-Play Paradox in Large Language Models: Reasoning Performance Gains\n  and Ethical Dilemmas"
                },
                "summary": "Role-play in large language models (LLMs) enhances their ability to generate\ncontextually relevant and high-quality responses by simulating diverse\ncognitive perspectives. However, our study identifies significant risks\nassociated with this technique. First, we demonstrate that autotuning, a method\nused to auto-select models' roles based on the question, can lead to the\ngeneration of harmful outputs, even when the model is tasked with adopting\nneutral roles. Second, we investigate how different roles affect the likelihood\nof generating biased or harmful content. Through testing on benchmarks\ncontaining stereotypical and harmful questions, we find that role-play\nconsistently amplifies the risk of biased outputs. Our results underscore the\nneed for careful consideration of both role simulation and tuning processes\nwhen deploying LLMs in sensitive or high-stakes contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-play in large language models (LLMs) enhances their ability to generate\ncontextually relevant and high-quality responses by simulating diverse\ncognitive perspectives. However, our study identifies significant risks\nassociated with this technique. First, we demonstrate that autotuning, a method\nused to auto-select models' roles based on the question, can lead to the\ngeneration of harmful outputs, even when the model is tasked with adopting\nneutral roles. Second, we investigate how different roles affect the likelihood\nof generating biased or harmful content. Through testing on benchmarks\ncontaining stereotypical and harmful questions, we find that role-play\nconsistently amplifies the risk of biased outputs. Our results underscore the\nneed for careful consideration of both role simulation and tuning processes\nwhen deploying LLMs in sensitive or high-stakes contexts."
                },
                "authors": [
                    {
                        "name": "Jinman Zhao"
                    },
                    {
                        "name": "Zifan Qian"
                    },
                    {
                        "name": "Linbo Cao"
                    },
                    {
                        "name": "Yining Wang"
                    },
                    {
                        "name": "Yitian Ding"
                    },
                    {
                        "name": "Yulan Hu"
                    },
                    {
                        "name": "Zeyu Zhang"
                    },
                    {
                        "name": "Zeyong Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zeyong Jin"
                },
                "arxiv_affiliation": "The Australian National University",
                "author": "Zeyong Jin",
                "arxiv_comment": "9 pages, 7 figures, 3 tables, submitted to CogSci 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13979v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13979v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19245v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19245v2",
                "updated": "2025-02-03T08:41:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    41,
                    43,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T15:59:50Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    59,
                    50,
                    4,
                    31,
                    0
                ],
                "title": "SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI\n  Interaction Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHARPIE: A Modular Framework for Reinforcement Learning and Human-AI\n  Interaction Experiments"
                },
                "summary": "Reinforcement learning (RL) offers a general approach for modeling and\ntraining AI agents, including human-AI interaction scenarios. In this paper, we\npropose SHARPIE (Shared Human-AI Reinforcement Learning Platform for\nInteractive Experiments) to address the need for a generic framework to support\nexperiments with RL agents and humans. Its modular design consists of a\nversatile wrapper for RL environments and algorithm libraries, a\nparticipant-facing web interface, logging utilities, deployment on popular\ncloud and participant recruitment platforms. It empowers researchers to study a\nwide variety of research questions related to the interaction between humans\nand RL agents, including those related to interactive reward specification and\nlearning, learning from human feedback, action delegation, preference\nelicitation, user-modeling, and human-AI teaming. The platform is based on a\ngeneric interface for human-RL interactions that aims to standardize the field\nof study on RL in human contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) offers a general approach for modeling and\ntraining AI agents, including human-AI interaction scenarios. In this paper, we\npropose SHARPIE (Shared Human-AI Reinforcement Learning Platform for\nInteractive Experiments) to address the need for a generic framework to support\nexperiments with RL agents and humans. Its modular design consists of a\nversatile wrapper for RL environments and algorithm libraries, a\nparticipant-facing web interface, logging utilities, deployment on popular\ncloud and participant recruitment platforms. It empowers researchers to study a\nwide variety of research questions related to the interaction between humans\nand RL agents, including those related to interactive reward specification and\nlearning, learning from human feedback, action delegation, preference\nelicitation, user-modeling, and human-AI teaming. The platform is based on a\ngeneric interface for human-RL interactions that aims to standardize the field\nof study on RL in human contexts."
                },
                "authors": [
                    {
                        "name": "Hüseyin Aydın"
                    },
                    {
                        "name": "Kevin Godin-Dubois"
                    },
                    {
                        "name": "Libio Goncalvez Braz"
                    },
                    {
                        "name": "Floris den Hengst"
                    },
                    {
                        "name": "Kim Baraka"
                    },
                    {
                        "name": "Mustafa Mert Çelikok"
                    },
                    {
                        "name": "Andreas Sauter"
                    },
                    {
                        "name": "Shihan Wang"
                    },
                    {
                        "name": "Frans A. Oliehoek"
                    }
                ],
                "author_detail": {
                    "name": "Frans A. Oliehoek"
                },
                "author": "Frans A. Oliehoek",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19245v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19245v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.04350v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.04350v2",
                "updated": "2025-02-03T08:28:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    28,
                    51,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-06T04:03:00Z",
                "published_parsed": [
                    2024,
                    10,
                    6,
                    4,
                    3,
                    0,
                    6,
                    280,
                    0
                ],
                "title": "TIS-DPO: Token-level Importance Sampling for Direct Preference\n  Optimization With Estimated Weights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TIS-DPO: Token-level Importance Sampling for Direct Preference\n  Optimization With Estimated Weights"
                },
                "summary": "Direct Preference Optimization (DPO) has been widely adopted for preference\nalignment of Large Language Models (LLMs) due to its simplicity and\neffectiveness. However, DPO is derived as a bandit problem in which the whole\nresponse is treated as a single arm, ignoring the importance differences\nbetween tokens, which may affect optimization efficiency and make it difficult\nto achieve optimal results. In this work, we propose that the optimal data for\nDPO has equal expected rewards for each token in winning and losing responses,\nas there is no difference in token importance. However, since the optimal\ndataset is unavailable in practice, we propose using the original dataset for\nimportance sampling to achieve unbiased optimization. Accordingly, we propose a\ntoken-level importance sampling DPO objective named TIS-DPO that assigns\nimportance weights to each token based on its reward. Inspired by previous\nworks, we estimate the token importance weights using the difference in\nprediction probabilities from a pair of contrastive LLMs. We explore three\nmethods to construct these contrastive LLMs: (1) guiding the original LLM with\ncontrastive prompts, (2) training two separate LLMs using winning and losing\nresponses, and (3) performing forward and reverse DPO training with winning and\nlosing responses. Experiments show that TIS-DPO significantly outperforms\nvarious baseline methods on harmlessness and helpfulness alignment and\nsummarization tasks. We also visualize the estimated weights, demonstrating\ntheir ability to identify key token positions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Direct Preference Optimization (DPO) has been widely adopted for preference\nalignment of Large Language Models (LLMs) due to its simplicity and\neffectiveness. However, DPO is derived as a bandit problem in which the whole\nresponse is treated as a single arm, ignoring the importance differences\nbetween tokens, which may affect optimization efficiency and make it difficult\nto achieve optimal results. In this work, we propose that the optimal data for\nDPO has equal expected rewards for each token in winning and losing responses,\nas there is no difference in token importance. However, since the optimal\ndataset is unavailable in practice, we propose using the original dataset for\nimportance sampling to achieve unbiased optimization. Accordingly, we propose a\ntoken-level importance sampling DPO objective named TIS-DPO that assigns\nimportance weights to each token based on its reward. Inspired by previous\nworks, we estimate the token importance weights using the difference in\nprediction probabilities from a pair of contrastive LLMs. We explore three\nmethods to construct these contrastive LLMs: (1) guiding the original LLM with\ncontrastive prompts, (2) training two separate LLMs using winning and losing\nresponses, and (3) performing forward and reverse DPO training with winning and\nlosing responses. Experiments show that TIS-DPO significantly outperforms\nvarious baseline methods on harmlessness and helpfulness alignment and\nsummarization tasks. We also visualize the estimated weights, demonstrating\ntheir ability to identify key token positions."
                },
                "authors": [
                    {
                        "name": "Aiwei Liu"
                    },
                    {
                        "name": "Haoping Bai"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Xiaojiang Liu"
                    },
                    {
                        "name": "Lijie Wen"
                    },
                    {
                        "name": "Philip S. Yu"
                    },
                    {
                        "name": "Meng Cao"
                    }
                ],
                "author_detail": {
                    "name": "Meng Cao"
                },
                "author": "Meng Cao",
                "arxiv_comment": "30 pages, 8 figures, 8 tables, Published in ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.04350v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.04350v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.14744v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.14744v3",
                "updated": "2025-02-03T08:22:35Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    22,
                    35,
                    0,
                    34,
                    0
                ],
                "published": "2024-05-23T16:13:33Z",
                "published_parsed": [
                    2024,
                    5,
                    23,
                    16,
                    13,
                    33,
                    3,
                    144,
                    0
                ],
                "title": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Prosocial Irrationality for LLM Agents: A Social Cognition\n  View"
                },
                "summary": "Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been shown to face hallucination issues due\nto the data they trained on often containing human bias; whether this is\nreflected in the decision-making process of LLM Agents remains under-explored.\nAs LLM Agents are increasingly employed in intricate social environments, a\npressing and natural question emerges: Can we utilize LLM Agents' systematic\nhallucinations to mirror human cognitive biases, thus exhibiting irrational\nsocial intelligence? In this paper, we probe the irrational behavior among\ncontemporary LLM Agents by melding practical social science experiments with\ntheoretical insights. Specifically, We propose CogMir, an open-ended Multi-LLM\nAgents framework that utilizes hallucination properties to assess and enhance\nLLM Agents' social intelligence through cognitive biases. Experimental results\non CogMir subsets show that LLM Agents and humans exhibit high consistency in\nirrational and prosocial decision-making under uncertain conditions,\nunderscoring the prosociality of LLM Agents as social entities and highlighting\nthe significance of hallucination properties. Additionally, the CogMir\nframework demonstrates its potential as a valuable platform for encouraging\nmore research into the social intelligence of LLM Agents."
                },
                "authors": [
                    {
                        "name": "Xuan Liu"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Song Guo"
                    },
                    {
                        "name": "Haoyang Shang"
                    },
                    {
                        "name": "Chengxu Yang"
                    },
                    {
                        "name": "Quanyan Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Quanyan Zhu"
                },
                "author": "Quanyan Zhu",
                "arxiv_comment": "Accepted by ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.14744v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.14744v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.09508v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.09508v2",
                "updated": "2025-02-03T08:17:43Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    8,
                    17,
                    43,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-12T12:10:14Z",
                "published_parsed": [
                    2024,
                    10,
                    12,
                    12,
                    10,
                    14,
                    5,
                    286,
                    0
                ],
                "title": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CollabEdit: Towards Non-destructive Collaborative Knowledge Editing"
                },
                "summary": "Collaborative learning of large language models (LLMs) has emerged as a new\nparadigm for utilizing private data from different parties to guarantee\nefficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also\ngarnered increased attention due to its ability to manipulate the behaviors of\nLLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits\nof multiple parties are aggregated in a privacy-preserving and continual\nmanner) unexamined. To this end, this manuscript dives into the first\ninvestigation of collaborative KE, in which we start by carefully identifying\nthe unique three challenges therein, including knowledge overlap, knowledge\nconflict, and knowledge forgetting. We then propose a non-destructive\ncollaborative KE framework, COLLABEDIT, which employs a novel model merging\nmechanism to mimic the global KE behavior while preventing the severe\nperformance drop. Extensive experiments on two canonical datasets demonstrate\nthe superiority of COLLABEDIT compared to other destructive baselines, and\nresults shed light on addressing three collaborative KE challenges and future\napplications. Our code is available at https://github.com/LINs-lab/CollabEdit.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative learning of large language models (LLMs) has emerged as a new\nparadigm for utilizing private data from different parties to guarantee\nefficiency and privacy. Meanwhile, Knowledge Editing (KE) for LLMs has also\ngarnered increased attention due to its ability to manipulate the behaviors of\nLLMs explicitly, yet leaves the collaborative KE case (in which knowledge edits\nof multiple parties are aggregated in a privacy-preserving and continual\nmanner) unexamined. To this end, this manuscript dives into the first\ninvestigation of collaborative KE, in which we start by carefully identifying\nthe unique three challenges therein, including knowledge overlap, knowledge\nconflict, and knowledge forgetting. We then propose a non-destructive\ncollaborative KE framework, COLLABEDIT, which employs a novel model merging\nmechanism to mimic the global KE behavior while preventing the severe\nperformance drop. Extensive experiments on two canonical datasets demonstrate\nthe superiority of COLLABEDIT compared to other destructive baselines, and\nresults shed light on addressing three collaborative KE challenges and future\napplications. Our code is available at https://github.com/LINs-lab/CollabEdit."
                },
                "authors": [
                    {
                        "name": "Jiamu Zheng"
                    },
                    {
                        "name": "Jinghuai Zhang"
                    },
                    {
                        "name": "Tianyu Du"
                    },
                    {
                        "name": "Xuhong Zhang"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Tao Lin"
                    }
                ],
                "author_detail": {
                    "name": "Tao Lin"
                },
                "author": "Tao Lin",
                "arxiv_comment": "20 pages, 11 figures. Published as a conference paper at ICLR 2025.\n  Code at https://github.com/LINs-lab/CollabEdit",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.09508v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.09508v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.17006v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.17006v2",
                "updated": "2025-02-03T07:23:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    7,
                    23,
                    13,
                    0,
                    34,
                    0
                ],
                "published": "2024-03-25T17:59:41Z",
                "published_parsed": [
                    2024,
                    3,
                    25,
                    17,
                    59,
                    41,
                    0,
                    85,
                    0
                ],
                "title": "Invertible Diffusion Models for Compressed Sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invertible Diffusion Models for Compressed Sensing"
                },
                "summary": "While deep neural networks (NN) significantly advance image compressed\nsensing (CS) by improving reconstruction quality, the necessity of training\ncurrent CS NNs from scratch constrains their effectiveness and hampers rapid\ndeployment. Although recent methods utilize pre-trained diffusion models for\nimage reconstruction, they struggle with slow inference and restricted\nadaptability to CS. To tackle these challenges, this paper proposes Invertible\nDiffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS\nmethod. IDM repurposes a large-scale diffusion sampling process as a\nreconstruction model, and fine-tunes it end-to-end to recover original images\ndirectly from CS measurements, moving beyond the traditional paradigm of\none-step noise estimation learning. To enable such memory-intensive end-to-end\nfine-tuning, we propose a novel two-level invertible design to transform both\n(1) multi-step sampling process and (2) noise estimation U-Net in each step\ninto invertible networks. As a result, most intermediate features are cleared\nduring training to reduce up to 93.8% GPU memory. In addition, we develop a set\nof lightweight modules to inject measurements into noise estimator to further\nfacilitate reconstruction. Experiments demonstrate that IDM outperforms\nexisting state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the\nrecent diffusion-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain\nand 14.54 times faster inference. Code is available at\nhttps://github.com/Guaishou74851/IDM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While deep neural networks (NN) significantly advance image compressed\nsensing (CS) by improving reconstruction quality, the necessity of training\ncurrent CS NNs from scratch constrains their effectiveness and hampers rapid\ndeployment. Although recent methods utilize pre-trained diffusion models for\nimage reconstruction, they struggle with slow inference and restricted\nadaptability to CS. To tackle these challenges, this paper proposes Invertible\nDiffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS\nmethod. IDM repurposes a large-scale diffusion sampling process as a\nreconstruction model, and fine-tunes it end-to-end to recover original images\ndirectly from CS measurements, moving beyond the traditional paradigm of\none-step noise estimation learning. To enable such memory-intensive end-to-end\nfine-tuning, we propose a novel two-level invertible design to transform both\n(1) multi-step sampling process and (2) noise estimation U-Net in each step\ninto invertible networks. As a result, most intermediate features are cleared\nduring training to reduce up to 93.8% GPU memory. In addition, we develop a set\nof lightweight modules to inject measurements into noise estimator to further\nfacilitate reconstruction. Experiments demonstrate that IDM outperforms\nexisting state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the\nrecent diffusion-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain\nand 14.54 times faster inference. Code is available at\nhttps://github.com/Guaishou74851/IDM."
                },
                "authors": [
                    {
                        "name": "Bin Chen"
                    },
                    {
                        "name": "Zhenyu Zhang"
                    },
                    {
                        "name": "Weiqi Li"
                    },
                    {
                        "name": "Chen Zhao"
                    },
                    {
                        "name": "Jiwen Yu"
                    },
                    {
                        "name": "Shijie Zhao"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Jian Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jian Zhang"
                },
                "author": "Jian Zhang",
                "arxiv_comment": "Accepted for publication in IEEE Transactions on Pattern Analysis and\n  Machine Intelligence (TPAMI)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.17006v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.17006v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07641v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07641v2",
                "updated": "2025-02-03T07:19:24Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    7,
                    19,
                    24,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-13T19:04:57Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    19,
                    4,
                    57,
                    0,
                    13,
                    0
                ],
                "title": "GPT as a Monte Carlo Language Tree: A Probabilistic Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPT as a Monte Carlo Language Tree: A Probabilistic Perspective"
                },
                "summary": "Large Language Models (LLMs), such as GPT, are considered to learn the latent\ndistributions within large-scale web-crawl datasets and accomplish natural\nlanguage processing (NLP) tasks by predicting the next token. However, this\nmechanism of latent distribution modeling lacks quantitative understanding and\nanalysis. In this paper, we propose a novel perspective that any language\ndataset can be represented by a Monte Carlo Language Tree (abbreviated as\n``Data-Tree''), where each node denotes a token, each edge denotes a token\ntransition probability, and each sequence has a unique path. Any GPT-like\nlanguage model can also be flattened into another Monte Carlo Language Tree\n(abbreviated as ``GPT-Tree''). Our experiments show that different GPT models\ntrained on the same dataset exhibit significant structural similarity in\nGPT-Tree visualization, and larger models converge more closely to the\nData-Tree. More than 87\\% GPT output tokens can be recalled by Data-Tree. These\nfindings may confirm that the reasoning process of LLMs is more likely to be\nprobabilistic pattern-matching rather than formal reasoning, as each model\ninference seems to find a context pattern with maximum probability from the\nData-Tree. Furthermore, we provide deeper insights into issues such as\nhallucination, Chain-of-Thought (CoT) reasoning, and token bias in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), such as GPT, are considered to learn the latent\ndistributions within large-scale web-crawl datasets and accomplish natural\nlanguage processing (NLP) tasks by predicting the next token. However, this\nmechanism of latent distribution modeling lacks quantitative understanding and\nanalysis. In this paper, we propose a novel perspective that any language\ndataset can be represented by a Monte Carlo Language Tree (abbreviated as\n``Data-Tree''), where each node denotes a token, each edge denotes a token\ntransition probability, and each sequence has a unique path. Any GPT-like\nlanguage model can also be flattened into another Monte Carlo Language Tree\n(abbreviated as ``GPT-Tree''). Our experiments show that different GPT models\ntrained on the same dataset exhibit significant structural similarity in\nGPT-Tree visualization, and larger models converge more closely to the\nData-Tree. More than 87\\% GPT output tokens can be recalled by Data-Tree. These\nfindings may confirm that the reasoning process of LLMs is more likely to be\nprobabilistic pattern-matching rather than formal reasoning, as each model\ninference seems to find a context pattern with maximum probability from the\nData-Tree. Furthermore, we provide deeper insights into issues such as\nhallucination, Chain-of-Thought (CoT) reasoning, and token bias in LLMs."
                },
                "authors": [
                    {
                        "name": "Kun-Peng Ning"
                    },
                    {
                        "name": "Jia-Yu Yao"
                    },
                    {
                        "name": "Yu-Yang Liu"
                    },
                    {
                        "name": "Mu-Nan Ning"
                    },
                    {
                        "name": "Li Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Li Yuan"
                },
                "author": "Li Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07641v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07641v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14569v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14569v3",
                "updated": "2025-02-03T06:52:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    6,
                    52,
                    55,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-18T16:16:34Z",
                "published_parsed": [
                    2024,
                    10,
                    18,
                    16,
                    16,
                    34,
                    4,
                    292,
                    0
                ],
                "title": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "When LLMs Go Online: The Emerging Threat of Web-Enabled LLMs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, generated\nimpersonation posts where 93.9% of them were deemed authentic, and boosted\nclick rate of phishing links in spear phishing emails by 46.67%. Additionally,\nour findings underscore the limitations of existing safeguards in contemporary\ncommercial LLMs, emphasizing the urgent need for robust security measures to\nprevent the misuse of LLM agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have established them as\nagentic systems capable of planning and interacting with various tools. These\nLLM agents are often paired with web-based tools, enabling access to diverse\nsources and real-time information. Although these advancements offer\nsignificant benefits across various applications, they also increase the risk\nof malicious use, particularly in cyberattacks involving personal information.\nIn this work, we investigate the risks associated with misuse of LLM agents in\ncyberattacks involving personal data. Specifically, we aim to understand: 1)\nhow potent LLM agents can be when directed to conduct cyberattacks, 2) how\ncyberattacks are enhanced by web-based tools, and 3) how affordable and easy it\nbecomes to launch cyberattacks using LLM agents. We examine three attack\nscenarios: the collection of Personally Identifiable Information (PII), the\ngeneration of impersonation posts, and the creation of spear-phishing emails.\nOur experiments reveal the effectiveness of LLM agents in these attacks: LLM\nagents achieved a precision of up to 95.9% in collecting PII, generated\nimpersonation posts where 93.9% of them were deemed authentic, and boosted\nclick rate of phishing links in spear phishing emails by 46.67%. Additionally,\nour findings underscore the limitations of existing safeguards in contemporary\ncommercial LLMs, emphasizing the urgent need for robust security measures to\nprevent the misuse of LLM agents."
                },
                "authors": [
                    {
                        "name": "Hanna Kim"
                    },
                    {
                        "name": "Minkyoo Song"
                    },
                    {
                        "name": "Seung Ho Na"
                    },
                    {
                        "name": "Seungwon Shin"
                    },
                    {
                        "name": "Kimin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kimin Lee"
                },
                "author": "Kimin Lee",
                "arxiv_comment": "20 pages, To appear in Usenix Security 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14569v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14569v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.18712v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.18712v2",
                "updated": "2025-02-03T06:36:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    6,
                    36,
                    8,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-30T19:15:41Z",
                "published_parsed": [
                    2025,
                    1,
                    30,
                    19,
                    15,
                    41,
                    3,
                    30,
                    0
                ],
                "title": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Invisible Traces: Using Hybrid Fingerprinting to identify underlying\n  LLMs in GenAI Apps"
                },
                "summary": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fingerprinting refers to the process of identifying underlying Machine\nLearning (ML) models of AI Systemts, such as Large Language Models (LLMs), by\nanalyzing their unique characteristics or patterns, much like a human\nfingerprint. The fingerprinting of Large Language Models (LLMs) has become\nessential for ensuring the security and transparency of AI-integrated\napplications. While existing methods primarily rely on access to direct\ninteractions with the application to infer model identity, they often fail in\nreal-world scenarios involving multi-agent systems, frequent model updates, and\nrestricted access to model internals. In this paper, we introduce a novel\nfingerprinting framework designed to address these challenges by integrating\nstatic and dynamic fingerprinting techniques. Our approach identifies\narchitectural features and behavioral traits, enabling accurate and robust\nfingerprinting of LLMs in dynamic environments. We also highlight new threat\nscenarios where traditional fingerprinting methods are ineffective, bridging\nthe gap between theoretical techniques and practical application. To validate\nour framework, we present an extensive evaluation setup that simulates\nreal-world conditions and demonstrate the effectiveness of our methods in\nidentifying and monitoring LLMs in Gen-AI applications. Our results highlight\nthe framework's adaptability to diverse and evolving deployment contexts."
                },
                "authors": [
                    {
                        "name": "Devansh Bhardwaj"
                    },
                    {
                        "name": "Naman Mishra"
                    }
                ],
                "author_detail": {
                    "name": "Naman Mishra"
                },
                "author": "Naman Mishra",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.18712v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.18712v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19306v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19306v2",
                "updated": "2025-02-03T06:21:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    6,
                    21,
                    8,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-31T17:03:16Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    17,
                    3,
                    16,
                    4,
                    31,
                    0
                ],
                "title": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SETS: Leveraging Self-Verification and Self-Correction for Improved\n  Test-Time Scaling"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have created new\nopportunities to enhance performance on complex reasoning tasks by leveraging\ntest-time computation. However, conventional approaches such as repeated\nsampling with majority voting or reward model scoring, often face diminishing\nreturns as test-time compute scales, in addition to requiring costly\ntask-specific reward model training. In this paper, we present Self-Enhanced\nTest-Time Scaling (SETS), a novel method that leverages the self-verification\nand self-correction capabilities of recent advanced LLMs to overcome these\nlimitations. SETS integrates sampling, self-verification, and self-correction\ninto a unified framework, enabling efficient and scalable test-time computation\nfor improved capabilities at complex tasks. Through extensive experiments on\nchallenging planning and reasoning benchmarks, compared to the alternatives, we\ndemonstrate that SETS achieves significant performance improvements and more\nfavorable test-time scaling laws."
                },
                "authors": [
                    {
                        "name": "Jiefeng Chen"
                    },
                    {
                        "name": "Jie Ren"
                    },
                    {
                        "name": "Xinyun Chen"
                    },
                    {
                        "name": "Chengrun Yang"
                    },
                    {
                        "name": "Ruoxi Sun"
                    },
                    {
                        "name": "Sercan Ö Arık"
                    }
                ],
                "author_detail": {
                    "name": "Sercan Ö Arık"
                },
                "author": "Sercan Ö Arık",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19306v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19306v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.11712v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.11712v4",
                "updated": "2025-02-03T05:35:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    35,
                    7,
                    0,
                    34,
                    0
                ],
                "published": "2024-07-16T13:30:14Z",
                "published_parsed": [
                    2024,
                    7,
                    16,
                    13,
                    30,
                    14,
                    1,
                    198,
                    0
                ],
                "title": "Fine-tuning Multimodal Large Language Models for Product Bundling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning Multimodal Large Language Models for Product Bundling"
                },
                "summary": "Recent advances in product bundling have leveraged multimodal information\nthrough sophisticated encoders, but remain constrained by limited semantic\nunderstanding and a narrow scope of knowledge. Therefore, some attempts employ\nIn-context Learning (ICL) to explore the potential of large language models\n(LLMs) for their extensive knowledge and complex reasoning abilities. However,\nthese efforts are inadequate in understanding mulitmodal data and exploiting\nLLMs' knowledge for product bundling. To bridge the gap, we introduce\nBundle-MLLM, a novel framework that fine-tunes LLMs through a hybrid item\ntokenization approach within a well-designed optimization strategy.\nSpecifically, we integrate textual, media, and relational data into a unified\ntokenization, introducing a soft separation token to distinguish between\ntextual and non-textual tokens. Additionally, a streamlined yet powerful\nmultimodal fusion module is employed to embed all non-textual features into a\nsingle, informative token, significantly boosting efficiency. To tailor product\nbundling tasks for LLMs, we reformulate the task as a multiple-choice question\nwith candidate items as options. We further propose a progressive optimization\nstrategy that fine-tunes LLMs for disentangled objectives: 1) learning bundle\npatterns and 2) enhancing multimodal semantic understanding specific to product\nbundling. Extensive experiments on four datasets across two domains demonstrate\nthat our approach outperforms a range of state-of-the-art (SOTA) methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in product bundling have leveraged multimodal information\nthrough sophisticated encoders, but remain constrained by limited semantic\nunderstanding and a narrow scope of knowledge. Therefore, some attempts employ\nIn-context Learning (ICL) to explore the potential of large language models\n(LLMs) for their extensive knowledge and complex reasoning abilities. However,\nthese efforts are inadequate in understanding mulitmodal data and exploiting\nLLMs' knowledge for product bundling. To bridge the gap, we introduce\nBundle-MLLM, a novel framework that fine-tunes LLMs through a hybrid item\ntokenization approach within a well-designed optimization strategy.\nSpecifically, we integrate textual, media, and relational data into a unified\ntokenization, introducing a soft separation token to distinguish between\ntextual and non-textual tokens. Additionally, a streamlined yet powerful\nmultimodal fusion module is employed to embed all non-textual features into a\nsingle, informative token, significantly boosting efficiency. To tailor product\nbundling tasks for LLMs, we reformulate the task as a multiple-choice question\nwith candidate items as options. We further propose a progressive optimization\nstrategy that fine-tunes LLMs for disentangled objectives: 1) learning bundle\npatterns and 2) enhancing multimodal semantic understanding specific to product\nbundling. Extensive experiments on four datasets across two domains demonstrate\nthat our approach outperforms a range of state-of-the-art (SOTA) methods."
                },
                "authors": [
                    {
                        "name": "Xiaohao Liu"
                    },
                    {
                        "name": "Jie Wu"
                    },
                    {
                        "name": "Zhulin Tao"
                    },
                    {
                        "name": "Yunshan Ma"
                    },
                    {
                        "name": "Yinwei Wei"
                    },
                    {
                        "name": "Tat-seng Chua"
                    }
                ],
                "author_detail": {
                    "name": "Tat-seng Chua"
                },
                "author": "Tat-seng Chua",
                "arxiv_comment": "Accepted by KDD 2025 (CR)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.11712v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.11712v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.06224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.06224v2",
                "updated": "2025-02-03T05:23:40Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    5,
                    23,
                    40,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-07T09:21:20Z",
                "published_parsed": [
                    2025,
                    1,
                    7,
                    9,
                    21,
                    20,
                    1,
                    7,
                    0
                ],
                "title": "Detection, Retrieval, and Explanation Unified: A Violence Detection\n  System Based on Knowledge Graphs and GAT",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detection, Retrieval, and Explanation Unified: A Violence Detection\n  System Based on Knowledge Graphs and GAT"
                },
                "summary": "Recently, violence detection systems developed using unified multimodal\nmodels have achieved significant success and attracted widespread attention.\nHowever, most of these systems face two critical challenges: the lack of\ninterpretability as black-box models and limited functionality, offering only\nclassification or retrieval capabilities. To address these challenges, this\npaper proposes a novel interpretable violence detection system, termed the\nThree-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and\ngraph attention networks (GAT) to provide three core functionalities:\ndetection, retrieval, and explanation. Specifically, the system processes each\nvideo frame along with text descriptions generated by a large language model\n(LLM) for videos containing potential violent behavior. It employs ImageBind to\ngenerate high-dimensional embeddings for constructing a knowledge graph, uses\nGAT for reasoning, and applies lightweight time series modules to extract video\nembedding features. The final step connects a classifier and retriever for\nmulti-functional outputs. The interpretability of KG enables the system to\nverify the reasoning process behind each output. Additionally, the paper\nintroduces several lightweight methods to reduce the resource consumption of\nthe TIO system and enhance its efficiency. Extensive experiments conducted on\nthe XD-Violence and UCF-Crime datasets validate the effectiveness of the\nproposed system. A case study further reveals an intriguing phenomenon: as the\nnumber of bystanders increases, the occurrence of violent behavior tends to\ndecrease.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, violence detection systems developed using unified multimodal\nmodels have achieved significant success and attracted widespread attention.\nHowever, most of these systems face two critical challenges: the lack of\ninterpretability as black-box models and limited functionality, offering only\nclassification or retrieval capabilities. To address these challenges, this\npaper proposes a novel interpretable violence detection system, termed the\nThree-in-One (TIO) System. The TIO system integrates knowledge graphs (KG) and\ngraph attention networks (GAT) to provide three core functionalities:\ndetection, retrieval, and explanation. Specifically, the system processes each\nvideo frame along with text descriptions generated by a large language model\n(LLM) for videos containing potential violent behavior. It employs ImageBind to\ngenerate high-dimensional embeddings for constructing a knowledge graph, uses\nGAT for reasoning, and applies lightweight time series modules to extract video\nembedding features. The final step connects a classifier and retriever for\nmulti-functional outputs. The interpretability of KG enables the system to\nverify the reasoning process behind each output. Additionally, the paper\nintroduces several lightweight methods to reduce the resource consumption of\nthe TIO system and enhance its efficiency. Extensive experiments conducted on\nthe XD-Violence and UCF-Crime datasets validate the effectiveness of the\nproposed system. A case study further reveals an intriguing phenomenon: as the\nnumber of bystanders increases, the occurrence of violent behavior tends to\ndecrease."
                },
                "authors": [
                    {
                        "name": "Wen-Dong Jiang"
                    },
                    {
                        "name": "Chih-Yung Chang"
                    },
                    {
                        "name": "Diptendu Sinha Roy"
                    }
                ],
                "author_detail": {
                    "name": "Diptendu Sinha Roy"
                },
                "author": "Diptendu Sinha Roy",
                "arxiv_comment": "This work has been submitted to the IEEE for possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.06224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.06224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14043v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14043v2",
                "updated": "2025-02-03T04:50:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    50,
                    39,
                    0,
                    34,
                    0
                ],
                "published": "2024-10-17T21:35:55Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    21,
                    35,
                    55,
                    3,
                    291,
                    0
                ],
                "title": "Retrieval of Temporal Event Sequences from Textual Descriptions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval of Temporal Event Sequences from Textual Descriptions"
                },
                "summary": "Retrieving temporal event sequences from textual descriptions is crucial for\napplications such as analyzing e-commerce behavior, monitoring social media\nactivities, and tracking criminal incidents. To advance this task, we introduce\nTESRBench, a comprehensive benchmark for temporal event sequence retrieval\n(TESR) from textual descriptions. TESRBench includes diverse real-world\ndatasets with synthesized and reviewed textual descriptions, providing a strong\nfoundation for evaluating retrieval performance and addressing challenges in\nthis domain. Building on this benchmark, we propose TPP-Embedding, a novel\nmodel for embedding and retrieving event sequences. The model leverages the\nTPP-LLM framework, integrating large language models (LLMs) with temporal point\nprocesses (TPPs) to encode both event texts and times. By pooling\nrepresentations and applying a contrastive loss, it unifies temporal dynamics\nand event semantics in a shared embedding space, aligning sequence-level\nembeddings of event sequences and their descriptions. TPP-Embedding\ndemonstrates superior performance over baseline models across TESRBench\ndatasets, establishing it as a powerful solution for the temporal event\nsequence retrieval task.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieving temporal event sequences from textual descriptions is crucial for\napplications such as analyzing e-commerce behavior, monitoring social media\nactivities, and tracking criminal incidents. To advance this task, we introduce\nTESRBench, a comprehensive benchmark for temporal event sequence retrieval\n(TESR) from textual descriptions. TESRBench includes diverse real-world\ndatasets with synthesized and reviewed textual descriptions, providing a strong\nfoundation for evaluating retrieval performance and addressing challenges in\nthis domain. Building on this benchmark, we propose TPP-Embedding, a novel\nmodel for embedding and retrieving event sequences. The model leverages the\nTPP-LLM framework, integrating large language models (LLMs) with temporal point\nprocesses (TPPs) to encode both event texts and times. By pooling\nrepresentations and applying a contrastive loss, it unifies temporal dynamics\nand event semantics in a shared embedding space, aligning sequence-level\nembeddings of event sequences and their descriptions. TPP-Embedding\ndemonstrates superior performance over baseline models across TESRBench\ndatasets, establishing it as a powerful solution for the temporal event\nsequence retrieval task."
                },
                "authors": [
                    {
                        "name": "Zefang Liu"
                    },
                    {
                        "name": "Yinzhu Quan"
                    }
                ],
                "author_detail": {
                    "name": "Yinzhu Quan"
                },
                "author": "Yinzhu Quan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.14043v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14043v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.09136v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.09136v2",
                "updated": "2025-02-03T04:01:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    4,
                    1,
                    36,
                    0,
                    34,
                    0
                ],
                "published": "2025-01-15T20:40:25Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    20,
                    40,
                    25,
                    2,
                    15,
                    0
                ],
                "title": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Retrieval-Augmented Generation: A Survey on Agentic RAG"
                },
                "summary": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized artificial intelligence (AI)\nby enabling human like text generation and natural language understanding.\nHowever, their reliance on static training data limits their ability to respond\nto dynamic, real time queries, resulting in outdated or inaccurate outputs.\nRetrieval Augmented Generation (RAG) has emerged as a solution, enhancing LLMs\nby integrating real time data retrieval to provide contextually relevant and\nup-to-date responses. Despite its promise, traditional RAG systems are\nconstrained by static workflows and lack the adaptability required for\nmultistep reasoning and complex task management.\n  Agentic Retrieval-Augmented Generation (Agentic RAG) transcends these\nlimitations by embedding autonomous AI agents into the RAG pipeline. These\nagents leverage agentic design patterns reflection, planning, tool use, and\nmultiagent collaboration to dynamically manage retrieval strategies,\niteratively refine contextual understanding, and adapt workflows to meet\ncomplex task requirements. This integration enables Agentic RAG systems to\ndeliver unparalleled flexibility, scalability, and context awareness across\ndiverse applications.\n  This survey provides a comprehensive exploration of Agentic RAG, beginning\nwith its foundational principles and the evolution of RAG paradigms. It\npresents a detailed taxonomy of Agentic RAG architectures, highlights key\napplications in industries such as healthcare, finance, and education, and\nexamines practical implementation strategies. Additionally, it addresses\nchallenges in scaling these systems, ensuring ethical decision making, and\noptimizing performance for real-world applications, while providing detailed\ninsights into frameworks and tools for implementing Agentic RAG."
                },
                "authors": [
                    {
                        "name": "Aditi Singh"
                    },
                    {
                        "name": "Abul Ehtesham"
                    },
                    {
                        "name": "Saket Kumar"
                    },
                    {
                        "name": "Tala Talaei Khoei"
                    }
                ],
                "author_detail": {
                    "name": "Tala Talaei Khoei"
                },
                "author": "Tala Talaei Khoei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.09136v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.09136v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.06564v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.06564v3",
                "updated": "2025-02-03T03:59:00Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    3,
                    59,
                    0,
                    0,
                    34,
                    0
                ],
                "published": "2024-12-09T15:17:36Z",
                "published_parsed": [
                    2024,
                    12,
                    9,
                    15,
                    17,
                    36,
                    0,
                    344,
                    0
                ],
                "title": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Applications and Implications of Large Language Models in Qualitative\n  Analysis: A New Frontier for Empirical Software Engineering"
                },
                "summary": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of large language models (LLMs) for qualitative analysis is gaining\nattention in various fields, including software engineering, where qualitative\nmethods are essential for understanding human and social factors. This study\naimed to investigate how LLMs are currently used in qualitative analysis and\ntheir potential applications in software engineering research, focusing on the\nbenefits, limitations, and practices associated with their use. A systematic\nmapping study was conducted, analyzing 21 relevant studies to explore reported\nuses of LLMs for qualitative analysis. The findings indicate that LLMs are\nprimarily used for tasks such as coding, thematic analysis, and data\ncategorization, offering benefits like increased efficiency and support for new\nresearchers. However, limitations such as output variability, challenges in\ncapturing nuanced perspectives, and ethical concerns related to privacy and\ntransparency were also identified. The study emphasizes the need for structured\nstrategies and guidelines to optimize LLM use in qualitative research within\nsoftware engineering, enhancing their effectiveness while addressing ethical\nconsiderations. While LLMs show promise in supporting qualitative analysis,\nhuman expertise remains crucial for interpreting data, and ongoing exploration\nof best practices will be vital for their successful integration into empirical\nsoftware engineering research."
                },
                "authors": [
                    {
                        "name": "Matheus de Morais Leça"
                    },
                    {
                        "name": "Lucas Valença"
                    },
                    {
                        "name": "Reydne Santos"
                    },
                    {
                        "name": "Ronnie de Souza Santos"
                    }
                ],
                "author_detail": {
                    "name": "Ronnie de Souza Santos"
                },
                "author": "Ronnie de Souza Santos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.06564v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.06564v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01565v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01565v4",
                "updated": "2025-02-03T03:27:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    3,
                    27,
                    20,
                    0,
                    34,
                    0
                ],
                "published": "2024-11-03T13:36:34Z",
                "published_parsed": [
                    2024,
                    11,
                    3,
                    13,
                    36,
                    34,
                    6,
                    308,
                    0
                ],
                "title": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQL Injection Jailbreak: A Structural Disaster of Large Language Models"
                },
                "summary": "In recent years, the rapid development of large language models (LLMs) has\nbrought new vitality into various domains, generating substantial social and\neconomic benefits. However, this swift advancement has also introduced new\nvulnerabilities. Jailbreaking, a form of attack that induces LLMs to produce\nharmful content through carefully crafted prompts, presents a significant\nchallenge to the safe and trustworthy development of LLMs. Previous jailbreak\nmethods primarily exploited the internal properties or capabilities of LLMs,\nsuch as optimization-based jailbreak methods and methods that leveraged the\nmodel's context-learning abilities. In this paper, we introduce a novel\njailbreak method, SQL Injection Jailbreak (SIJ), which targets the external\nproperties of LLMs, specifically, the way LLMs construct input prompts. By\ninjecting jailbreak information into user prompts, SIJ successfully induces the\nmodel to output harmful content. Our SIJ method achieves near 100\\% attack\nsuccess rates on five well-known open-source LLMs on the AdvBench and HEx-PHI,\nwhile incurring lower time costs compared to previous methods. For\nclosed-source models, SIJ achieves near 100% attack success rate on\nGPT-3.5-turbo. Additionally, SIJ exposes a new vulnerability in LLMs that\nurgently requires mitigation. To address this, we propose a simple defense\nmethod called Self-Reminder-Key to counter SIJ and demonstrate its\neffectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, the rapid development of large language models (LLMs) has\nbrought new vitality into various domains, generating substantial social and\neconomic benefits. However, this swift advancement has also introduced new\nvulnerabilities. Jailbreaking, a form of attack that induces LLMs to produce\nharmful content through carefully crafted prompts, presents a significant\nchallenge to the safe and trustworthy development of LLMs. Previous jailbreak\nmethods primarily exploited the internal properties or capabilities of LLMs,\nsuch as optimization-based jailbreak methods and methods that leveraged the\nmodel's context-learning abilities. In this paper, we introduce a novel\njailbreak method, SQL Injection Jailbreak (SIJ), which targets the external\nproperties of LLMs, specifically, the way LLMs construct input prompts. By\ninjecting jailbreak information into user prompts, SIJ successfully induces the\nmodel to output harmful content. Our SIJ method achieves near 100\\% attack\nsuccess rates on five well-known open-source LLMs on the AdvBench and HEx-PHI,\nwhile incurring lower time costs compared to previous methods. For\nclosed-source models, SIJ achieves near 100% attack success rate on\nGPT-3.5-turbo. Additionally, SIJ exposes a new vulnerability in LLMs that\nurgently requires mitigation. To address this, we propose a simple defense\nmethod called Self-Reminder-Key to counter SIJ and demonstrate its\neffectiveness through experimental results. Our code is available at\nhttps://github.com/weiyezhimeng/SQL-Injection-Jailbreak."
                },
                "authors": [
                    {
                        "name": "Jiawei Zhao"
                    },
                    {
                        "name": "Kejiang Chen"
                    },
                    {
                        "name": "Weiming Zhang"
                    },
                    {
                        "name": "Nenghai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Nenghai Yu"
                },
                "author": "Nenghai Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01565v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01565v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12433v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12433v4",
                "updated": "2025-02-03T02:14:26Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    2,
                    14,
                    26,
                    0,
                    34,
                    0
                ],
                "published": "2024-06-18T09:29:18Z",
                "published_parsed": [
                    2024,
                    6,
                    18,
                    9,
                    29,
                    18,
                    1,
                    170,
                    0
                ],
                "title": "LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM4Rerank: LLM-based Auto-Reranking Framework for Recommendations"
                },
                "summary": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reranking is a critical component in recommender systems, playing an\nessential role in refining the output of recommendation algorithms. Traditional\nreranking models have focused predominantly on accuracy, but modern\napplications demand consideration of additional criteria such as diversity and\nfairness. Existing reranking approaches often fail to harmonize these diverse\ncriteria effectively at the model level. Moreover, these models frequently\nencounter challenges with scalability and personalization due to their\ncomplexity and the varying significance of different reranking criteria in\ndiverse scenarios. In response, we introduce a comprehensive reranking\nframework enhanced by LLM, designed to seamlessly integrate various reranking\ncriteria while maintaining scalability and facilitating personalized\nrecommendations. This framework employs a fully connected graph structure,\nallowing the LLM to simultaneously consider multiple aspects such as accuracy,\ndiversity, and fairness through a coherent Chain-of-Thought (CoT) process. A\ncustomizable input mechanism is also integrated, enabling the tuning of the\nlanguage model's focus to meet specific reranking needs. We validate our\napproach using three popular public datasets, where our framework demonstrates\nsuperior performance over existing state-of-the-art reranking models in\nbalancing multiple criteria."
                },
                "authors": [
                    {
                        "name": "Jingtong Gao"
                    },
                    {
                        "name": "Bo Chen"
                    },
                    {
                        "name": "Weiwen Liu"
                    },
                    {
                        "name": "Xiangyang Li"
                    },
                    {
                        "name": "Yichao Wang"
                    },
                    {
                        "name": "Wanyu Wang"
                    },
                    {
                        "name": "Huifeng Guo"
                    },
                    {
                        "name": "Ruiming Tang"
                    },
                    {
                        "name": "Xiangyu Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhao"
                },
                "author": "Xiangyu Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12433v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12433v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.10027v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.10027v4",
                "updated": "2025-02-03T01:26:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    1,
                    26,
                    49,
                    0,
                    34,
                    0
                ],
                "published": "2024-09-16T06:35:18Z",
                "published_parsed": [
                    2024,
                    9,
                    16,
                    6,
                    35,
                    18,
                    0,
                    260,
                    0
                ],
                "title": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "E2Map: Experience-and-Emotion Map for Self-Reflective Robot Navigation\n  with Language Models"
                },
                "summary": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have shown significant potential in guiding\nembodied agents to execute language instructions across a range of tasks,\nincluding robotic manipulation and navigation. However, existing methods are\nprimarily designed for static environments and do not leverage the agent's own\nexperiences to refine its initial plans. Given that real-world environments are\ninherently stochastic, initial plans based solely on LLMs' general knowledge\nmay fail to achieve their objectives, unlike in static scenarios. To address\nthis limitation, this study introduces the Experience-and-Emotion Map (E2Map),\nwhich integrates not only LLM knowledge but also the agent's real-world\nexperiences, drawing inspiration from human emotional responses. The proposed\nmethodology enables one-shot behavior adjustments by updating the E2Map based\non the agent's experiences. Our evaluation in stochastic navigation\nenvironments, including both simulations and real-world scenarios, demonstrates\nthat the proposed method significantly enhances performance in stochastic\nenvironments compared to existing LLM-based approaches. Code and supplementary\nmaterials are available at https://e2map.github.io/."
                },
                "authors": [
                    {
                        "name": "Chan Kim"
                    },
                    {
                        "name": "Keonwoo Kim"
                    },
                    {
                        "name": "Mintaek Oh"
                    },
                    {
                        "name": "Hanbi Baek"
                    },
                    {
                        "name": "Jiyang Lee"
                    },
                    {
                        "name": "Donghwi Jung"
                    },
                    {
                        "name": "Soojin Woo"
                    },
                    {
                        "name": "Younkyung Woo"
                    },
                    {
                        "name": "John Tucker"
                    },
                    {
                        "name": "Roya Firoozi"
                    },
                    {
                        "name": "Seung-Woo Seo"
                    },
                    {
                        "name": "Mac Schwager"
                    },
                    {
                        "name": "Seong-Woo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Woo Kim"
                },
                "author": "Seong-Woo Kim",
                "arxiv_comment": "19 pages, 28 figures. Project page: https://e2map.github.io. Accepted\n  to ICRA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.10027v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.10027v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.05887v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.05887v2",
                "updated": "2025-02-03T01:11:07Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    1,
                    11,
                    7,
                    0,
                    34,
                    0
                ],
                "published": "2024-03-09T11:59:10Z",
                "published_parsed": [
                    2024,
                    3,
                    9,
                    11,
                    59,
                    10,
                    5,
                    69,
                    0
                ],
                "title": "Aligning Speech to Languages to Enhance Code-switching Speech\n  Recognition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aligning Speech to Languages to Enhance Code-switching Speech\n  Recognition"
                },
                "summary": "Code-switching (CS) refers to the switching of languages within a speech\nsignal and results in language confusion for automatic speech recognition\n(ASR). To address language confusion, we introduce a novel language alignment\nloss into ASR training to align acoustic features to pseudo-language labels\nlearned from the ASR decoder. This approach enables frame-level language\nidentification without the need for frame-level language annotations. To\nfurther tackle the complex token alternatives for language modeling in\nbilingual scenarios, we propose to employ large language models via a\ngenerative error correction method. A linguistic hint, which is derived from\nLAL outputs and decoded hypotheses, is introduced to guide the prompting and\nenhance the LLM-based generative error correction for CS-ASR. The proposed\nmethods are evaluated on the SEAME dataset and data from the ASRU 2019\nMandarin-English code-switching speech recognition challenge. The incorporation\nof the proposed language alignment loss improves the CS-ASR performance for\nboth hybrid CTC/attention and Whisper models on both datasets, with only a\nnegligible increase in the number of parameters. This work also highlights the\nefficacy of language alignment loss in balancing primary-language-dominant\nbilingual data during training, with an 8.6% relative improvement on the ASRU\ndataset compared to the baseline model. Performance evaluation using large\nlanguage models reveals the advantage of the linguistic hint by achieving 14.1%\nand 5.5% relative improvement on test sets of the ASRU and SEAME datasets,\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code-switching (CS) refers to the switching of languages within a speech\nsignal and results in language confusion for automatic speech recognition\n(ASR). To address language confusion, we introduce a novel language alignment\nloss into ASR training to align acoustic features to pseudo-language labels\nlearned from the ASR decoder. This approach enables frame-level language\nidentification without the need for frame-level language annotations. To\nfurther tackle the complex token alternatives for language modeling in\nbilingual scenarios, we propose to employ large language models via a\ngenerative error correction method. A linguistic hint, which is derived from\nLAL outputs and decoded hypotheses, is introduced to guide the prompting and\nenhance the LLM-based generative error correction for CS-ASR. The proposed\nmethods are evaluated on the SEAME dataset and data from the ASRU 2019\nMandarin-English code-switching speech recognition challenge. The incorporation\nof the proposed language alignment loss improves the CS-ASR performance for\nboth hybrid CTC/attention and Whisper models on both datasets, with only a\nnegligible increase in the number of parameters. This work also highlights the\nefficacy of language alignment loss in balancing primary-language-dominant\nbilingual data during training, with an 8.6% relative improvement on the ASRU\ndataset compared to the baseline model. Performance evaluation using large\nlanguage models reveals the advantage of the linguistic hint by achieving 14.1%\nand 5.5% relative improvement on test sets of the ASRU and SEAME datasets,\nrespectively."
                },
                "authors": [
                    {
                        "name": "Hexin Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Haoyang Zhang"
                    },
                    {
                        "name": "Leibny Paola Garcia"
                    },
                    {
                        "name": "Andy W. H. Khong"
                    },
                    {
                        "name": "Eng Siong Chng"
                    },
                    {
                        "name": "Shinji Watanabe"
                    }
                ],
                "author_detail": {
                    "name": "Shinji Watanabe"
                },
                "author": "Shinji Watanabe",
                "arxiv_comment": "Manuscript submitted to IEEE/ACM Transactions on Audio, Speech, and\n  Language Processing",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.05887v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.05887v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.11060v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.11060v3",
                "updated": "2025-02-03T00:23:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    3,
                    0,
                    23,
                    45,
                    0,
                    34,
                    0
                ],
                "published": "2024-02-16T20:20:43Z",
                "published_parsed": [
                    2024,
                    2,
                    16,
                    20,
                    20,
                    43,
                    4,
                    47,
                    0
                ],
                "title": "Persona-DB: Efficient Large Language Model Personalization for Response\n  Prediction with Collaborative Data Refinement",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persona-DB: Efficient Large Language Model Personalization for Response\n  Prediction with Collaborative Data Refinement"
                },
                "summary": "The increasing demand for personalized interactions with large language\nmodels (LLMs) calls for methodologies capable of accurately and efficiently\nidentifying user opinions and preferences. Retrieval augmentation emerges as an\neffective strategy, as it can accommodate a vast number of users without the\ncosts from fine-tuning. Existing research, however, has largely focused on\nenhancing the retrieval stage and devoted limited exploration toward optimizing\nthe representation of the database, a crucial aspect for tasks such as\npersonalization. In this work, we examine the problem from a novel angle,\nfocusing on how data can be better represented for more data-efficient\nretrieval in the context of LLM customization. To tackle this challenge, we\nintroduce Persona-DB, a simple yet effective framework consisting of a\nhierarchical construction process to improve generalization across task\ncontexts and collaborative refinement to effectively bridge knowledge gaps\namong users. In the evaluation of response prediction, Persona-DB demonstrates\nsuperior context efficiency in maintaining accuracy with a significantly\nreduced retrieval size, a critical advantage in scenarios with extensive\nhistories or limited context windows. Our experiments also indicate a marked\nimprovement of over 10% under cold-start scenarios, when users have extremely\nsparse data. Furthermore, our analysis reveals the increasing importance of\ncollaborative knowledge as the retrieval capacity expands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for personalized interactions with large language\nmodels (LLMs) calls for methodologies capable of accurately and efficiently\nidentifying user opinions and preferences. Retrieval augmentation emerges as an\neffective strategy, as it can accommodate a vast number of users without the\ncosts from fine-tuning. Existing research, however, has largely focused on\nenhancing the retrieval stage and devoted limited exploration toward optimizing\nthe representation of the database, a crucial aspect for tasks such as\npersonalization. In this work, we examine the problem from a novel angle,\nfocusing on how data can be better represented for more data-efficient\nretrieval in the context of LLM customization. To tackle this challenge, we\nintroduce Persona-DB, a simple yet effective framework consisting of a\nhierarchical construction process to improve generalization across task\ncontexts and collaborative refinement to effectively bridge knowledge gaps\namong users. In the evaluation of response prediction, Persona-DB demonstrates\nsuperior context efficiency in maintaining accuracy with a significantly\nreduced retrieval size, a critical advantage in scenarios with extensive\nhistories or limited context windows. Our experiments also indicate a marked\nimprovement of over 10% under cold-start scenarios, when users have extremely\nsparse data. Furthermore, our analysis reveals the increasing importance of\ncollaborative knowledge as the retrieval capacity expands."
                },
                "authors": [
                    {
                        "name": "Chenkai Sun"
                    },
                    {
                        "name": "Ke Yang"
                    },
                    {
                        "name": "Revanth Gangi Reddy"
                    },
                    {
                        "name": "Yi R. Fung"
                    },
                    {
                        "name": "Hou Pong Chan"
                    },
                    {
                        "name": "Kevin Small"
                    },
                    {
                        "name": "ChengXiang Zhai"
                    },
                    {
                        "name": "Heng Ji"
                    }
                ],
                "author_detail": {
                    "name": "Heng Ji"
                },
                "author": "Heng Ji",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.11060v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.11060v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02626v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02626v3",
                "updated": "2025-02-02T22:13:29Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    22,
                    13,
                    29,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-03T17:54:12Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    17,
                    54,
                    12,
                    1,
                    338,
                    0
                ],
                "title": "Time-Reversal Provides Unsupervised Feedback to LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Reversal Provides Unsupervised Feedback to LLMs"
                },
                "summary": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are typically trained to predict in the forward\ndirection of time. However, recent works have shown that prompting these models\nto look back and critique their own generations can produce useful feedback.\nMotivated by this, we explore the question of whether LLMs can be empowered to\nthink (predict and score) backwards to provide unsupervised feedback that\ncomplements forward LLMs. Towards this, we introduce Time Reversed Language\nModels (TRLMs), which can score and generate queries when conditioned on\nresponses, effectively functioning in the reverse direction of time. Further,\nto effectively infer in the response to query direction, we pre-train and\nfine-tune a language model (TRLM-Ba) in the reverse token order from scratch.\nWe show empirically (and theoretically in a stylized setting) that\ntime-reversed models can indeed complement forward model predictions when used\nto score the query given response for re-ranking multiple forward generations.\nWe obtain up to 5\\% improvement on the widely used AlpacaEval Leaderboard over\nthe competent baseline of best-of-N re-ranking using self log-perplexity\nscores. We further show that TRLM scoring outperforms conventional forward\nscoring of response given query, resulting in significant gains in applications\nsuch as citation generation and passage retrieval. We next leverage the\ngenerative ability of TRLM to augment or provide unsupervised feedback to input\nsafety filters of LLMs, demonstrating a drastic reduction in false negative\nrate with negligible impact on false positive rates against several attacks\npublished on the popular JailbreakBench leaderboard."
                },
                "authors": [
                    {
                        "name": "Yerram Varun"
                    },
                    {
                        "name": "Rahul Madhavan"
                    },
                    {
                        "name": "Sravanti Addepalli"
                    },
                    {
                        "name": "Arun Suggala"
                    },
                    {
                        "name": "Karthikeyan Shanmugam"
                    },
                    {
                        "name": "Prateek Jain"
                    }
                ],
                "author_detail": {
                    "name": "Prateek Jain"
                },
                "author": "Prateek Jain",
                "arxiv_comment": "Accepted as a spotlight in NeurIPS 2024",
                "arxiv_journal_ref": "The Thirty-Eighth Annual Conference on Neural Information\n  Processing Systems (NeurIPS), 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02626v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02626v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.12004v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.12004v2",
                "updated": "2025-02-02T21:27:06Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    21,
                    27,
                    6,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-16T17:32:11Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    17,
                    32,
                    11,
                    0,
                    351,
                    0
                ],
                "title": "The Open Source Advantage in Large Language Models (LLMs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Source Advantage in Large Language Models (LLMs)"
                },
                "summary": "Large language models (LLMs) have rapidly advanced natural language\nprocessing, driving significant breakthroughs in tasks such as text generation,\nmachine translation, and domain-specific reasoning. The field now faces a\ncritical dilemma in its approach: closed-source models like GPT-4 deliver\nstate-of-the-art performance but restrict reproducibility, accessibility, and\nexternal oversight, while open-source frameworks like LLaMA and Mixtral\ndemocratize access, foster collaboration, and support diverse applications,\nachieving competitive results through techniques like instruction tuning and\nLoRA. Hybrid approaches address challenges like bias mitigation and resource\naccessibility by combining the scalability of closed-source systems with the\ntransparency and inclusivity of open-source framework. However, in this\nposition paper, we argue that open-source remains the most robust path for\nadvancing LLM research and ethical deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have rapidly advanced natural language\nprocessing, driving significant breakthroughs in tasks such as text generation,\nmachine translation, and domain-specific reasoning. The field now faces a\ncritical dilemma in its approach: closed-source models like GPT-4 deliver\nstate-of-the-art performance but restrict reproducibility, accessibility, and\nexternal oversight, while open-source frameworks like LLaMA and Mixtral\ndemocratize access, foster collaboration, and support diverse applications,\nachieving competitive results through techniques like instruction tuning and\nLoRA. Hybrid approaches address challenges like bias mitigation and resource\naccessibility by combining the scalability of closed-source systems with the\ntransparency and inclusivity of open-source framework. However, in this\nposition paper, we argue that open-source remains the most robust path for\nadvancing LLM research and ethical deployment."
                },
                "authors": [
                    {
                        "name": "Jiya Manchanda"
                    },
                    {
                        "name": "Laura Boettcher"
                    },
                    {
                        "name": "Matheus Westphalen"
                    },
                    {
                        "name": "Jasser Jasser"
                    }
                ],
                "author_detail": {
                    "name": "Jasser Jasser"
                },
                "author": "Jasser Jasser",
                "arxiv_comment": "9 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.12004v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.12004v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.11781v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.11781v2",
                "updated": "2025-02-02T20:27:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    20,
                    27,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2024-10-15T17:00:15Z",
                "published_parsed": [
                    2024,
                    10,
                    15,
                    17,
                    0,
                    15,
                    1,
                    289,
                    0
                ],
                "title": "Language Models Encode Numbers Using Digit Representations in Base 10",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Models Encode Numbers Using Digit Representations in Base 10"
                },
                "summary": "Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across the digits of the answer rather than normally\naround its numeric value. Through a series of probing experiments and causal\ninterventions, we show that LLMs internally represent numbers with individual\ncircular representations per-digit in base 10. This digit-wise representation,\nas opposed to a value representation, sheds light on the error patterns of\nmodels on tasks involving numerical reasoning and could serve as a basis for\nfuture studies on analyzing numerical mechanisms in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) frequently make errors when handling even simple\nnumerical problems, such as comparing two small numbers. A natural hypothesis\nis that these errors stem from how LLMs represent numbers, and specifically,\nwhether their representations of numbers capture their numeric values. We\ntackle this question from the observation that LLM errors on numerical tasks\nare often distributed across the digits of the answer rather than normally\naround its numeric value. Through a series of probing experiments and causal\ninterventions, we show that LLMs internally represent numbers with individual\ncircular representations per-digit in base 10. This digit-wise representation,\nas opposed to a value representation, sheds light on the error patterns of\nmodels on tasks involving numerical reasoning and could serve as a basis for\nfuture studies on analyzing numerical mechanisms in LLMs."
                },
                "authors": [
                    {
                        "name": "Amit Arnold Levy"
                    },
                    {
                        "name": "Mor Geva"
                    }
                ],
                "author_detail": {
                    "name": "Mor Geva"
                },
                "author": "Mor Geva",
                "arxiv_comment": "Accepted at NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.11781v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.11781v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.16452v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.16452v2",
                "updated": "2025-02-02T20:20:48Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    20,
                    20,
                    48,
                    6,
                    33,
                    0
                ],
                "published": "2024-09-24T20:44:30Z",
                "published_parsed": [
                    2024,
                    9,
                    24,
                    20,
                    44,
                    30,
                    1,
                    268,
                    0
                ],
                "title": "FMDLlama: Financial Misinformation Detection based on Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FMDLlama: Financial Misinformation Detection based on Large Language\n  Models"
                },
                "summary": "The emergence of social media has made the spread of misinformation easier.\nIn the financial domain, the accuracy of information is crucial for various\naspects of financial market, which has made financial misinformation detection\n(FMD) an urgent problem that needs to be addressed. Large language models\n(LLMs) have demonstrated outstanding performance in various fields. However,\ncurrent studies mostly rely on traditional methods and have not explored the\napplication of LLMs in the field of FMD. The main reason is the lack of FMD\ninstruction tuning datasets and evaluation benchmarks. In this paper, we\npropose FMDLlama, the first open-sourced instruction-following LLMs for FMD\ntask based on fine-tuning Llama3.1 with instruction data, the first multi-task\nFMD instruction dataset (FMDID) to support LLM instruction tuning, and a\ncomprehensive FMD evaluation benchmark (FMD-B) with classification and\nexplanation generation tasks to test the FMD ability of LLMs. We compare our\nmodels with a variety of LLMs on FMD-B, where our model outperforms other\nopen-sourced LLMs as well as OpenAI's products. This project is available at\nhttps://github.com/lzw108/FMD.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of social media has made the spread of misinformation easier.\nIn the financial domain, the accuracy of information is crucial for various\naspects of financial market, which has made financial misinformation detection\n(FMD) an urgent problem that needs to be addressed. Large language models\n(LLMs) have demonstrated outstanding performance in various fields. However,\ncurrent studies mostly rely on traditional methods and have not explored the\napplication of LLMs in the field of FMD. The main reason is the lack of FMD\ninstruction tuning datasets and evaluation benchmarks. In this paper, we\npropose FMDLlama, the first open-sourced instruction-following LLMs for FMD\ntask based on fine-tuning Llama3.1 with instruction data, the first multi-task\nFMD instruction dataset (FMDID) to support LLM instruction tuning, and a\ncomprehensive FMD evaluation benchmark (FMD-B) with classification and\nexplanation generation tasks to test the FMD ability of LLMs. We compare our\nmodels with a variety of LLMs on FMD-B, where our model outperforms other\nopen-sourced LLMs as well as OpenAI's products. This project is available at\nhttps://github.com/lzw108/FMD."
                },
                "authors": [
                    {
                        "name": "Zhiwei Liu"
                    },
                    {
                        "name": "Xin Zhang"
                    },
                    {
                        "name": "Kailai Yang"
                    },
                    {
                        "name": "Qianqian Xie"
                    },
                    {
                        "name": "Jimin Huang"
                    },
                    {
                        "name": "Sophia Ananiadou"
                    }
                ],
                "author_detail": {
                    "name": "Sophia Ananiadou"
                },
                "author": "Sophia Ananiadou",
                "arxiv_comment": "Accepted by The Web Conference (WWW) 2025 Short Paper Track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.16452v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.16452v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.05541v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.05541v3",
                "updated": "2025-02-02T20:04:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    20,
                    4,
                    51,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-09T19:27:28Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    19,
                    27,
                    28,
                    3,
                    9,
                    0
                ],
                "title": "Customizable LLM-Powered Chatbot for Behavioral Science Research",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Customizable LLM-Powered Chatbot for Behavioral Science Research"
                },
                "summary": "The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Artificial Intelligence has resulted in the advent\nof Large Language Models (LLMs) with the capacity to produce text that closely\nresembles human communication. These models have been seamlessly integrated\ninto diverse applications, enabling interactive and responsive communication\nacross multiple platforms. The potential utility of chatbots transcends these\ntraditional applications, particularly in research contexts, wherein they can\noffer valuable insights and facilitate the design of innovative experiments. In\nthis study, we present a Customizable LLM-Powered Chatbot (CLPC), a web-based\nchatbot system designed to assist in behavioral science research. The system is\nmeticulously designed to function as an experimental instrument rather than a\nconventional chatbot, necessitating users to input a username and experiment\ncode upon access. This setup facilitates precise data cross-referencing,\nthereby augmenting the integrity and applicability of the data collected for\nresearch purposes. It can be easily expanded to accommodate new basic events as\nneeded; and it allows researchers to integrate their own logging events without\nthe necessity of implementing a separate logging mechanism. It is worth noting\nthat our system was built to assist primarily behavioral science research but\nis not limited to it, it can easily be adapted to assist information retrieval\nresearch or interacting with chat bot agents in general."
                },
                "authors": [
                    {
                        "name": "Zenon Lamprou"
                    },
                    {
                        "name": "Yashar Moshfeghi"
                    }
                ],
                "author_detail": {
                    "name": "Yashar Moshfeghi"
                },
                "author": "Yashar Moshfeghi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.05541v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.05541v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2306.04802v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2306.04802v5",
                "updated": "2025-02-02T19:38:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    38,
                    20,
                    6,
                    33,
                    0
                ],
                "published": "2023-06-07T21:51:56Z",
                "published_parsed": [
                    2023,
                    6,
                    7,
                    21,
                    51,
                    56,
                    2,
                    158,
                    0
                ],
                "title": "A Review on Knowledge Graphs for Healthcare: Resources, Applications,\n  and Promises",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Review on Knowledge Graphs for Healthcare: Resources, Applications,\n  and Promises"
                },
                "summary": "This comprehensive review aims to provide an overview of the current state of\nHealthcare Knowledge Graphs (HKGs), including their construction, utilization\nmodels, and applications across various healthcare and biomedical research\ndomains. We thoroughly analyzed existing literature on HKGs, covering their\nconstruction methodologies, utilization techniques, and applications in basic\nscience research, pharmaceutical research and development, clinical decision\nsupport, and public health. The review encompasses both model-free and\nmodel-based utilization approaches and the integration of HKGs with large\nlanguage models (LLMs). We searched Google Scholar for relevant papers on HKGs\nand classified them into the following topics: HKG construction, HKG\nutilization, and their downstream applications in various domains. We also\ndiscussed their special challenges and the promise for future work. The review\nhighlights the potential of HKGs to significantly impact biomedical research\nand clinical practice by integrating vast amounts of biomedical knowledge from\nmultiple domains. The synergy between HKGs and LLMs offers promising\nopportunities for constructing more comprehensive knowledge graphs and\nimproving the accuracy of healthcare applications. HKGs have emerged as a\npowerful tool for structuring medical knowledge, with broad applications across\nbiomedical research, clinical decision-making, and public health. This survey\nserves as a roadmap for future research and development in the field of HKGs,\nhighlighting the potential of combining knowledge graphs with advanced machine\nlearning models for healthcare transformation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This comprehensive review aims to provide an overview of the current state of\nHealthcare Knowledge Graphs (HKGs), including their construction, utilization\nmodels, and applications across various healthcare and biomedical research\ndomains. We thoroughly analyzed existing literature on HKGs, covering their\nconstruction methodologies, utilization techniques, and applications in basic\nscience research, pharmaceutical research and development, clinical decision\nsupport, and public health. The review encompasses both model-free and\nmodel-based utilization approaches and the integration of HKGs with large\nlanguage models (LLMs). We searched Google Scholar for relevant papers on HKGs\nand classified them into the following topics: HKG construction, HKG\nutilization, and their downstream applications in various domains. We also\ndiscussed their special challenges and the promise for future work. The review\nhighlights the potential of HKGs to significantly impact biomedical research\nand clinical practice by integrating vast amounts of biomedical knowledge from\nmultiple domains. The synergy between HKGs and LLMs offers promising\nopportunities for constructing more comprehensive knowledge graphs and\nimproving the accuracy of healthcare applications. HKGs have emerged as a\npowerful tool for structuring medical knowledge, with broad applications across\nbiomedical research, clinical decision-making, and public health. This survey\nserves as a roadmap for future research and development in the field of HKGs,\nhighlighting the potential of combining knowledge graphs with advanced machine\nlearning models for healthcare transformation."
                },
                "authors": [
                    {
                        "name": "Hejie Cui"
                    },
                    {
                        "name": "Jiaying Lu"
                    },
                    {
                        "name": "Ran Xu"
                    },
                    {
                        "name": "Shiyu Wang"
                    },
                    {
                        "name": "Wenjing Ma"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Shaojun Yu"
                    },
                    {
                        "name": "Xuan Kan"
                    },
                    {
                        "name": "Chen Ling"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Zhaohui S. Qin"
                    },
                    {
                        "name": "Joyce C. Ho"
                    },
                    {
                        "name": "Tianfan Fu"
                    },
                    {
                        "name": "Jing Ma"
                    },
                    {
                        "name": "Mengdi Huai"
                    },
                    {
                        "name": "Fei Wang"
                    },
                    {
                        "name": "Carl Yang"
                    }
                ],
                "author_detail": {
                    "name": "Carl Yang"
                },
                "author": "Carl Yang",
                "arxiv_comment": "Preprint under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2306.04802v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2306.04802v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T30, 68T50, 68T09",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; I.2.6; I.2.7; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.02688v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.02688v3",
                "updated": "2025-02-02T19:28:39Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    19,
                    28,
                    39,
                    6,
                    33,
                    0
                ],
                "published": "2024-11-05T00:16:01Z",
                "published_parsed": [
                    2024,
                    11,
                    5,
                    0,
                    16,
                    1,
                    1,
                    310,
                    0
                ],
                "title": "On the Loss of Context-awareness in General Instruction Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On the Loss of Context-awareness in General Instruction Fine-tuning"
                },
                "summary": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We identify and demonstrate that the loss of context\nawareness, particularly in open-source models, occurs in instruction fine-tuned\nLLMs when the chat template is applied to input prompts. We identify that the\nperformance decline is associated with a bias toward different roles learned\nduring conversational instruction fine-tuning. We demonstrate this correlation\nby visualizing changes in attention allocation after the chat template is\napplied and manually steering the attention heads. The bias can be learned from\ntraining examples that align with the model's internal knowledge and rely less\non the user-provided context to generate correct responses. Based on these\nobservations, we propose a metric to identify context-dependent examples from\ngeneral instruction fine-tuning datasets. We then apply conditional instruction\nfine-tuning with a context-dependency indicator, enabling the model to preserve\ncontext awareness after SFT. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pre-trained Large Language Models (LLMs) require post-training methods such\nas supervised fine-tuning (SFT) on instruction-response pairs to enable\ninstruction following. However, this process can potentially harm existing\ncapabilities learned during pre-training. In this paper, we investigate the\nloss of context awareness after SFT, where context awareness is defined as the\nability to extract and understand information from user-provided context and\nrespond accordingly. We identify and demonstrate that the loss of context\nawareness, particularly in open-source models, occurs in instruction fine-tuned\nLLMs when the chat template is applied to input prompts. We identify that the\nperformance decline is associated with a bias toward different roles learned\nduring conversational instruction fine-tuning. We demonstrate this correlation\nby visualizing changes in attention allocation after the chat template is\napplied and manually steering the attention heads. The bias can be learned from\ntraining examples that align with the model's internal knowledge and rely less\non the user-provided context to generate correct responses. Based on these\nobservations, we propose a metric to identify context-dependent examples from\ngeneral instruction fine-tuning datasets. We then apply conditional instruction\nfine-tuning with a context-dependency indicator, enabling the model to preserve\ncontext awareness after SFT. Empirical experiments on four context-dependent\ndownstream tasks and three pre-trained LLMs of different sizes show that our\nmethod effectively mitigates the loss of context awareness without compromising\ngeneral instruction-following capabilities."
                },
                "authors": [
                    {
                        "name": "Yihan Wang"
                    },
                    {
                        "name": "Andrew Bai"
                    },
                    {
                        "name": "Nanyun Peng"
                    },
                    {
                        "name": "Cho-Jui Hsieh"
                    }
                ],
                "author_detail": {
                    "name": "Cho-Jui Hsieh"
                },
                "author": "Cho-Jui Hsieh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.02688v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.02688v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14119v2",
                "updated": "2025-02-02T18:43:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    18,
                    43,
                    34,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-18T18:06:58Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    18,
                    6,
                    58,
                    2,
                    353,
                    0
                ],
                "title": "Learning and Reconstructing Conflicts in O-RAN: A Graph Neural Network\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning and Reconstructing Conflicts in O-RAN: A Graph Neural Network\n  Approach"
                },
                "summary": "The Open Radio Access Network (O-RAN) architecture enables the deployment of\nthird-party applications on the RAN Intelligent Controllers (RICs). However,\nthe operation of third-party applications in the Near Real-Time RIC (Near-RT\nRIC), known as xApps, may result in conflicting interactions. Each xApp can\nindependently modify the same control parameters to achieve distinct outcomes,\nwhich has the potential to cause performance degradation and network\ninstability. The current conflict detection and mitigation solutions in the\nliterature assume that all conflicts are known a priori, which does not always\nhold due to complex and often hidden relationships between control parameters\nand Key Performance Indicators (KPIs). In this paper, we introduce the first\ndata-driven method for reconstructing and labeling conflict graphs in O-RAN.\nSpecifically, we leverage GraphSAGE, an inductive learning framework, to\ndynamically learn the hidden relationships between xApps, parameters, and KPIs.\nOur numerical results, based on a conflict model used in the O-RAN conflict\nmanagement literature, demonstrate that our proposed method can effectively\nreconstruct conflict graphs and identify the conflicts defined by the O-RAN\nAlliance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open Radio Access Network (O-RAN) architecture enables the deployment of\nthird-party applications on the RAN Intelligent Controllers (RICs). However,\nthe operation of third-party applications in the Near Real-Time RIC (Near-RT\nRIC), known as xApps, may result in conflicting interactions. Each xApp can\nindependently modify the same control parameters to achieve distinct outcomes,\nwhich has the potential to cause performance degradation and network\ninstability. The current conflict detection and mitigation solutions in the\nliterature assume that all conflicts are known a priori, which does not always\nhold due to complex and often hidden relationships between control parameters\nand Key Performance Indicators (KPIs). In this paper, we introduce the first\ndata-driven method for reconstructing and labeling conflict graphs in O-RAN.\nSpecifically, we leverage GraphSAGE, an inductive learning framework, to\ndynamically learn the hidden relationships between xApps, parameters, and KPIs.\nOur numerical results, based on a conflict model used in the O-RAN conflict\nmanagement literature, demonstrate that our proposed method can effectively\nreconstruct conflict graphs and identify the conflicts defined by the O-RAN\nAlliance."
                },
                "authors": [
                    {
                        "name": "Arshia Zolghadr"
                    },
                    {
                        "name": "Joao F. Santos"
                    },
                    {
                        "name": "Luiz A. DaSilva"
                    },
                    {
                        "name": "Jacek Kibiłda"
                    }
                ],
                "author_detail": {
                    "name": "Jacek Kibiłda"
                },
                "author": "Jacek Kibiłda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2408.09594v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2408.09594v3",
                "updated": "2025-02-02T18:40:22Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    18,
                    40,
                    22,
                    6,
                    33,
                    0
                ],
                "published": "2024-08-18T20:59:59Z",
                "published_parsed": [
                    2024,
                    8,
                    18,
                    20,
                    59,
                    59,
                    6,
                    231,
                    0
                ],
                "title": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Moonshine: Distilling Game Content Generators into Steerable Generative\n  Models"
                },
                "summary": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Procedural Content Generation via Machine Learning (PCGML) has enhanced game\ncontent creation, yet challenges in controllability and limited training data\npersist. This study addresses these issues by distilling a constructive PCG\nalgorithm into a controllable PCGML model. We first generate a large amount of\ncontent with a constructive algorithm and label it using a Large Language Model\n(LLM). We use these synthetic labels to condition two PCGML models for\ncontent-specific generation, a diffusion model and the five-dollar model. This\nneural network distillation process ensures that the generation aligns with the\noriginal algorithm while introducing controllability through plain text. We\ndefine this text-conditioned PCGML as a Text-to-game-Map (T2M) task, offering\nan alternative to prevalent text-to-image multi-modal tasks. We compare our\ndistilled models with the baseline constructive algorithm. Our analysis of the\nvariety, accuracy, and quality of our generation demonstrates the efficacy of\ndistilling constructive methods into controllable text-conditioned PCGML\nmodels."
                },
                "authors": [
                    {
                        "name": "Yuhe Nie"
                    },
                    {
                        "name": "Michael Middleton"
                    },
                    {
                        "name": "Tim Merino"
                    },
                    {
                        "name": "Nidhushan Kanagaraja"
                    },
                    {
                        "name": "Ashutosh Kumar"
                    },
                    {
                        "name": "Zhan Zhuang"
                    },
                    {
                        "name": "Julian Togelius"
                    }
                ],
                "author_detail": {
                    "name": "Julian Togelius"
                },
                "author": "Julian Togelius",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2408.09594v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2408.09594v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00210v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00210v2",
                "updated": "2025-02-02T18:15:42Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    18,
                    15,
                    42,
                    6,
                    33,
                    0
                ],
                "published": "2024-10-31T21:14:22Z",
                "published_parsed": [
                    2024,
                    10,
                    31,
                    21,
                    14,
                    22,
                    3,
                    305,
                    0
                ],
                "title": "Scale-Aware Recognition in Satellite Images under Resource Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scale-Aware Recognition in Satellite Images under Resource Constraints"
                },
                "summary": "Recognition of features in satellite imagery (forests, swimming pools, etc.)\ndepends strongly on the spatial scale of the concept and therefore the\nresolution of the images. This poses two challenges: Which resolution is best\nsuited for recognizing a given concept, and where and when should the costlier\nhigher-resolution (HR) imagery be acquired?\n  We present a novel scheme to address these challenges by introducing three\ncomponents: (1) A technique to distill knowledge from models trained on HR\nimagery to recognition models that operate on imagery of lower resolution (LR),\n(2) a sampling strategy for HR imagery based on model disagreement, and (3) an\nLLM-based approach for inferring concept \"scale\". With these components we\npresent a system to efficiently perform scale-aware recognition in satellite\nimagery, improving accuracy over single-scale inference while following budget\nconstraints. Our novel approach offers up to a 26.3% improvement over entirely\nHR baselines, using 76.3% fewer HR images.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recognition of features in satellite imagery (forests, swimming pools, etc.)\ndepends strongly on the spatial scale of the concept and therefore the\nresolution of the images. This poses two challenges: Which resolution is best\nsuited for recognizing a given concept, and where and when should the costlier\nhigher-resolution (HR) imagery be acquired?\n  We present a novel scheme to address these challenges by introducing three\ncomponents: (1) A technique to distill knowledge from models trained on HR\nimagery to recognition models that operate on imagery of lower resolution (LR),\n(2) a sampling strategy for HR imagery based on model disagreement, and (3) an\nLLM-based approach for inferring concept \"scale\". With these components we\npresent a system to efficiently perform scale-aware recognition in satellite\nimagery, improving accuracy over single-scale inference while following budget\nconstraints. Our novel approach offers up to a 26.3% improvement over entirely\nHR baselines, using 76.3% fewer HR images."
                },
                "authors": [
                    {
                        "name": "Shreelekha Revankar"
                    },
                    {
                        "name": "Cheng Perng Phoo"
                    },
                    {
                        "name": "Utkarsh Mall"
                    },
                    {
                        "name": "Bharath Hariharan"
                    },
                    {
                        "name": "Kavita Bala"
                    }
                ],
                "author_detail": {
                    "name": "Kavita Bala"
                },
                "author": "Kavita Bala",
                "arxiv_comment": "16 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00210v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00210v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.18279v8",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.18279v8",
                "updated": "2025-02-02T17:08:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    17,
                    8,
                    44,
                    6,
                    33,
                    0
                ],
                "published": "2024-11-27T12:13:39Z",
                "published_parsed": [
                    2024,
                    11,
                    27,
                    12,
                    13,
                    39,
                    2,
                    332,
                    0
                ],
                "title": "Large Language Model-Brained GUI Agents: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model-Brained GUI Agents: A Survey"
                },
                "summary": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GUIs have long been central to human-computer interaction, providing an\nintuitive and visually-driven way to access and interact with digital systems.\nThe advent of LLMs, particularly multimodal models, has ushered in a new era of\nGUI automation. They have demonstrated exceptional capabilities in natural\nlanguage understanding, code generation, and visual processing. This has paved\nthe way for a new generation of LLM-brained GUI agents capable of interpreting\ncomplex GUI elements and autonomously executing actions based on natural\nlanguage instructions. These agents represent a paradigm shift, enabling users\nto perform intricate, multi-step tasks through simple conversational commands.\nTheir applications span across web navigation, mobile app interactions, and\ndesktop automation, offering a transformative user experience that\nrevolutionizes how individuals interact with software. This emerging field is\nrapidly advancing, with significant progress in both research and industry.\n  To provide a structured understanding of this trend, this paper presents a\ncomprehensive survey of LLM-brained GUI agents, exploring their historical\nevolution, core components, and advanced techniques. We address research\nquestions such as existing GUI agent frameworks, the collection and utilization\nof data for training specialized GUI agents, the development of large action\nmodels tailored for GUI tasks, and the evaluation metrics and benchmarks\nnecessary to assess their effectiveness. Additionally, we examine emerging\napplications powered by these agents. Through a detailed analysis, this survey\nidentifies key research gaps and outlines a roadmap for future advancements in\nthe field. By consolidating foundational knowledge and state-of-the-art\ndevelopments, this work aims to guide both researchers and practitioners in\novercoming challenges and unlocking the full potential of LLM-brained GUI\nagents."
                },
                "authors": [
                    {
                        "name": "Chaoyun Zhang"
                    },
                    {
                        "name": "Shilin He"
                    },
                    {
                        "name": "Jiaxu Qian"
                    },
                    {
                        "name": "Bowen Li"
                    },
                    {
                        "name": "Liqun Li"
                    },
                    {
                        "name": "Si Qin"
                    },
                    {
                        "name": "Yu Kang"
                    },
                    {
                        "name": "Minghua Ma"
                    },
                    {
                        "name": "Guyue Liu"
                    },
                    {
                        "name": "Qingwei Lin"
                    },
                    {
                        "name": "Saravan Rajmohan"
                    },
                    {
                        "name": "Dongmei Zhang"
                    },
                    {
                        "name": "Qi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Qi Zhang"
                },
                "author": "Qi Zhang",
                "arxiv_comment": "The collection of papers reviewed in this survey will be hosted and\n  regularly updated on the GitHub repository:\n  https://github.com/vyokky/LLM-Brained-GUI-Agents-Survey Additionally, a\n  searchable webpage is available at https://aka.ms/gui-agent for easier access\n  and exploration",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.18279v8",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.18279v8",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02956v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02956v2",
                "updated": "2025-02-02T16:51:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    16,
                    51,
                    13,
                    6,
                    33,
                    0
                ],
                "published": "2024-07-03T09:49:03Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    49,
                    3,
                    2,
                    185,
                    0
                ],
                "title": "IncogniText: Privacy-enhancing Conditional Text Anonymization via\n  LLM-based Private Attribute Randomization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IncogniText: Privacy-enhancing Conditional Text Anonymization via\n  LLM-based Private Attribute Randomization"
                },
                "summary": "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%\nacross 8 different private attributes. Finally, we demonstrate the maturity of\nIncogniText for real-world applications by distilling its anonymization\ncapability into a set of LoRA parameters associated with an on-device model.\nOur results show the possibility of reducing privacy leakage by more than half\nwith limited impact on utility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we address the problem of text anonymization where the goal is\nto prevent adversaries from correctly inferring private attributes of the\nauthor, while keeping the text utility, i.e., meaning and semantics. We propose\nIncogniText, a technique that anonymizes the text to mislead a potential\nadversary into predicting a wrong private attribute value. Our empirical\nevaluation shows a reduction of private attribute leakage by more than 90%\nacross 8 different private attributes. Finally, we demonstrate the maturity of\nIncogniText for real-world applications by distilling its anonymization\ncapability into a set of LoRA parameters associated with an on-device model.\nOur results show the possibility of reducing privacy leakage by more than half\nwith limited impact on utility."
                },
                "authors": [
                    {
                        "name": "Ahmed Frikha"
                    },
                    {
                        "name": "Nassim Walha"
                    },
                    {
                        "name": "Krishna Kanth Nakka"
                    },
                    {
                        "name": "Ricardo Mendes"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Xuebing Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xuebing Zhou"
                },
                "author": "Xuebing Zhou",
                "arxiv_comment": "Accepted at NeurIPS 2024 - Safe GenAI Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02956v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02956v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01626v2",
                "updated": "2025-02-02T16:34:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    16,
                    34,
                    25,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-02T15:44:19Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    15,
                    44,
                    19,
                    0,
                    337,
                    0
                ],
                "title": "WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WikiHint: A Human-Annotated Dataset for Hint Ranking and Generation"
                },
                "summary": "The use of Large Language Models (LLMs) has increased significantly with\nusers frequently asking questions to chatbots. In the time when information is\nreadily accessible, it is crucial to stimulate and preserve human cognitive\nabilities and maintain strong reasoning skills. This paper addresses such\nchallenges by promoting the use of hints as an alternative or a supplement to\ndirect answers. We first introduce a manually constructed hint dataset,\nWikiHint, which is based on Wikipedia and includes 5,000 hints created for\n1,000 questions. We then finetune open-source LLMs such as LLaMA-3.1 for hint\ngeneration in answer-aware and answeragnostic contexts. We assess the\neffectiveness of the hints with human participants who answer questions with\nand without the aid of hints. Additionally, we introduce a lightweight\nevaluation method, HintRank, to evaluate and rank hints in both answeraware and\nanswer-agnostic settings. Our findings show that (a) the dataset helps generate\nmore effective hints, (b) including answer information along with questions\ngenerally improves quality of generated hints, and (c) encoder-based models\nperform better than decoder-based models in hint ranking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The use of Large Language Models (LLMs) has increased significantly with\nusers frequently asking questions to chatbots. In the time when information is\nreadily accessible, it is crucial to stimulate and preserve human cognitive\nabilities and maintain strong reasoning skills. This paper addresses such\nchallenges by promoting the use of hints as an alternative or a supplement to\ndirect answers. We first introduce a manually constructed hint dataset,\nWikiHint, which is based on Wikipedia and includes 5,000 hints created for\n1,000 questions. We then finetune open-source LLMs such as LLaMA-3.1 for hint\ngeneration in answer-aware and answeragnostic contexts. We assess the\neffectiveness of the hints with human participants who answer questions with\nand without the aid of hints. Additionally, we introduce a lightweight\nevaluation method, HintRank, to evaluate and rank hints in both answeraware and\nanswer-agnostic settings. Our findings show that (a) the dataset helps generate\nmore effective hints, (b) including answer information along with questions\ngenerally improves quality of generated hints, and (c) encoder-based models\nperform better than decoder-based models in hint ranking."
                },
                "authors": [
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Florian Gerhold"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "arxiv_comment": "Submitted to SIGIR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.02724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.02724v2",
                "updated": "2025-02-02T15:57:01Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    15,
                    57,
                    1,
                    6,
                    33,
                    0
                ],
                "published": "2024-10-03T17:45:31Z",
                "published_parsed": [
                    2024,
                    10,
                    3,
                    17,
                    45,
                    31,
                    3,
                    277,
                    0
                ],
                "title": "Large Language Models as Markov Chains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models as Markov Chains"
                },
                "summary": "Large language models (LLMs) are remarkably efficient across a wide range of\nnatural language processing tasks and well beyond them. However, a\ncomprehensive theoretical analysis of the LLMs' generalization capabilities\nremains elusive. In our paper, we approach this task by drawing an equivalence\nbetween autoregressive transformer-based language models and Markov chains\ndefined on a finite state space. This allows us to study the multi-step\ninference mechanism of LLMs from first principles. We relate the obtained\nresults to the pathological behavior observed with LLMs such as repetitions and\nincoherent replies with high temperature. Finally, we leverage the proposed\nformalization to derive pre-training and in-context learning generalization\nbounds for LLMs under realistic data and model assumptions. Experiments with\nthe most recent Llama and Gemma herds of models show that our theory correctly\ncaptures their behavior in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are remarkably efficient across a wide range of\nnatural language processing tasks and well beyond them. However, a\ncomprehensive theoretical analysis of the LLMs' generalization capabilities\nremains elusive. In our paper, we approach this task by drawing an equivalence\nbetween autoregressive transformer-based language models and Markov chains\ndefined on a finite state space. This allows us to study the multi-step\ninference mechanism of LLMs from first principles. We relate the obtained\nresults to the pathological behavior observed with LLMs such as repetitions and\nincoherent replies with high temperature. Finally, we leverage the proposed\nformalization to derive pre-training and in-context learning generalization\nbounds for LLMs under realistic data and model assumptions. Experiments with\nthe most recent Llama and Gemma herds of models show that our theory correctly\ncaptures their behavior in practice."
                },
                "authors": [
                    {
                        "name": "Oussama Zekri"
                    },
                    {
                        "name": "Ambroise Odonnat"
                    },
                    {
                        "name": "Abdelhakim Benechehab"
                    },
                    {
                        "name": "Linus Bleistein"
                    },
                    {
                        "name": "Nicolas Boullé"
                    },
                    {
                        "name": "Ievgen Redko"
                    }
                ],
                "author_detail": {
                    "name": "Ievgen Redko"
                },
                "author": "Ievgen Redko",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.02724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.02724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.09641v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.09641v3",
                "updated": "2025-02-02T14:59:13Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    59,
                    13,
                    6,
                    33,
                    0
                ],
                "published": "2024-09-15T07:23:07Z",
                "published_parsed": [
                    2024,
                    9,
                    15,
                    7,
                    23,
                    7,
                    6,
                    259,
                    0
                ],
                "title": "AACessTalk: Fostering Communication between Minimally Verbal Autistic\n  Children and Parents with Contextual Guidance and Card Recommendation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AACessTalk: Fostering Communication between Minimally Verbal Autistic\n  Children and Parents with Contextual Guidance and Card Recommendation"
                },
                "summary": "As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As minimally verbal autistic (MVA) children communicate with parents through\nfew words and nonverbal cues, parents often struggle to encourage their\nchildren to express subtle emotions and needs and to grasp their nuanced\nsignals. We present AACessTalk, a tablet-based, AI-mediated communication\nsystem that facilitates meaningful exchanges between an MVA child and a parent.\nAACessTalk provides real-time guides to the parent to engage the child in\nconversation and, in turn, recommends contextual vocabulary cards to the child.\nThrough a two-week deployment study with 11 MVA child-parent dyads, we examine\nhow AACessTalk fosters everyday conversation practice and mutual engagement.\nOur findings show high engagement from all dyads, leading to increased\nfrequency of conversation and turn-taking. AACessTalk also encouraged parents\nto explore their own interaction strategies and empowered the children to have\nmore agency in communication. We discuss the implications of designing\ntechnologies for balanced communication dynamics in parent-MVA child\ninteraction."
                },
                "authors": [
                    {
                        "name": "Dasom Choi"
                    },
                    {
                        "name": "SoHyun Park"
                    },
                    {
                        "name": "Kyungah Lee"
                    },
                    {
                        "name": "Hwajung Hong"
                    },
                    {
                        "name": "Young-Ho Kim"
                    }
                ],
                "author_detail": {
                    "name": "Young-Ho Kim"
                },
                "author": "Young-Ho Kim",
                "arxiv_comment": "21 pages excluding reference. Accepted at ACM CHI 2025.\n  https://naver-ai.github.io/aacesstalk/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.09641v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.09641v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14844v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14844v2",
                "updated": "2025-02-02T14:32:41Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    14,
                    32,
                    41,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-24T09:10:02Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    9,
                    10,
                    2,
                    4,
                    24,
                    0
                ],
                "title": "Unmasking Conversational Bias in AI Multiagent Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unmasking Conversational Bias in AI Multiagent Systems"
                },
                "summary": "Detecting biases in the outputs produced by generative models is essential to\nreduce the potential risks associated with their application in critical\nsettings. However, the majority of existing methodologies for identifying\nbiases in generated text consider the models in isolation and neglect their\ncontextual applications. Specifically, the biases that may arise in multi-agent\nsystems involving generative models remain under-researched. To address this\ngap, we present a framework designed to quantify biases within multi-agent\nsystems of conversational Large Language Models (LLMs). Our approach involves\nsimulating small echo chambers, where pairs of LLMs, initialized with aligned\nperspectives on a polarizing topic, engage in discussions. Contrary to\nexpectations, we observe significant shifts in the stance expressed in the\ngenerated messages, particularly within echo chambers where all agents\ninitially express conservative viewpoints, in line with the well-documented\npolitical bias of many LLMs toward liberal positions. Crucially, the bias\nobserved in the echo-chamber experiment remains undetected by current\nstate-of-the-art bias detection methods that rely on questionnaires. This\nhighlights a critical need for the development of a more sophisticated toolkit\nfor bias detection and mitigation for AI multi-agent systems. The code to\nperform the experiments is publicly available at\nhttps://anonymous.4open.science/r/LLMsConversationalBias-7725.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting biases in the outputs produced by generative models is essential to\nreduce the potential risks associated with their application in critical\nsettings. However, the majority of existing methodologies for identifying\nbiases in generated text consider the models in isolation and neglect their\ncontextual applications. Specifically, the biases that may arise in multi-agent\nsystems involving generative models remain under-researched. To address this\ngap, we present a framework designed to quantify biases within multi-agent\nsystems of conversational Large Language Models (LLMs). Our approach involves\nsimulating small echo chambers, where pairs of LLMs, initialized with aligned\nperspectives on a polarizing topic, engage in discussions. Contrary to\nexpectations, we observe significant shifts in the stance expressed in the\ngenerated messages, particularly within echo chambers where all agents\ninitially express conservative viewpoints, in line with the well-documented\npolitical bias of many LLMs toward liberal positions. Crucially, the bias\nobserved in the echo-chamber experiment remains undetected by current\nstate-of-the-art bias detection methods that rely on questionnaires. This\nhighlights a critical need for the development of a more sophisticated toolkit\nfor bias detection and mitigation for AI multi-agent systems. The code to\nperform the experiments is publicly available at\nhttps://anonymous.4open.science/r/LLMsConversationalBias-7725."
                },
                "authors": [
                    {
                        "name": "Erica Coppolillo"
                    },
                    {
                        "name": "Giuseppe Manco"
                    },
                    {
                        "name": "Luca Maria Aiello"
                    }
                ],
                "author_detail": {
                    "name": "Luca Maria Aiello"
                },
                "author": "Luca Maria Aiello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14844v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14844v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.14931v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.14931v2",
                "updated": "2025-02-02T13:51:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    13,
                    51,
                    2,
                    6,
                    33,
                    0
                ],
                "published": "2024-01-26T15:10:23Z",
                "published_parsed": [
                    2024,
                    1,
                    26,
                    15,
                    10,
                    23,
                    4,
                    26,
                    0
                ],
                "title": "Do LLMs Dream of Ontologies?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Dream of Ontologies?"
                },
                "summary": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse natural language processing tasks, yet their ability to memorize\nstructured knowledge remains underexplored. In this paper, we investigate the\nextent to which general-purpose pre-trained LLMs retain and correctly reproduce\nconcept identifier (ID)-label associations from publicly available ontologies.\nWe conduct a systematic evaluation across multiple ontological resources,\nincluding the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as\nPythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only\na small fraction of ontological concepts is accurately memorized, with GPT-4\ndemonstrating the highest performance. To understand why certain concepts are\nmemorized more effectively than others, we analyze the relationship between\nmemorization accuracy and concept popularity on the Web. Our results indicate a\nstrong correlation between the frequency of a concept's occurrence online and\nthe likelihood of accurately retrieving its ID from the label. This suggests\nthat LLMs primarily acquire such knowledge through indirect textual exposure\nrather than directly from structured ontological resources. Furthermore, we\nintroduce new metrics to quantify prediction invariance, demonstrating that the\nstability of model responses across variations in prompt language and\ntemperature settings can serve as a proxy for estimating memorization\nrobustness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated remarkable performance across\ndiverse natural language processing tasks, yet their ability to memorize\nstructured knowledge remains underexplored. In this paper, we investigate the\nextent to which general-purpose pre-trained LLMs retain and correctly reproduce\nconcept identifier (ID)-label associations from publicly available ontologies.\nWe conduct a systematic evaluation across multiple ontological resources,\nincluding the Gene Ontology, Uberon, Wikidata, and ICD-10, using LLMs such as\nPythia-12B, Gemini-1.5-Flash, GPT-3.5, and GPT-4. Our findings reveal that only\na small fraction of ontological concepts is accurately memorized, with GPT-4\ndemonstrating the highest performance. To understand why certain concepts are\nmemorized more effectively than others, we analyze the relationship between\nmemorization accuracy and concept popularity on the Web. Our results indicate a\nstrong correlation between the frequency of a concept's occurrence online and\nthe likelihood of accurately retrieving its ID from the label. This suggests\nthat LLMs primarily acquire such knowledge through indirect textual exposure\nrather than directly from structured ontological resources. Furthermore, we\nintroduce new metrics to quantify prediction invariance, demonstrating that the\nstability of model responses across variations in prompt language and\ntemperature settings can serve as a proxy for estimating memorization\nrobustness."
                },
                "authors": [
                    {
                        "name": "Marco Bombieri"
                    },
                    {
                        "name": "Paolo Fiorini"
                    },
                    {
                        "name": "Simone Paolo Ponzetto"
                    },
                    {
                        "name": "Marco Rospocher"
                    }
                ],
                "author_detail": {
                    "name": "Marco Rospocher"
                },
                "author": "Marco Rospocher",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2401.14931v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.14931v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.01292v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.01292v2",
                "updated": "2025-02-02T11:49:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    49,
                    25,
                    6,
                    33,
                    0
                ],
                "published": "2024-12-02T09:07:57Z",
                "published_parsed": [
                    2024,
                    12,
                    2,
                    9,
                    7,
                    57,
                    0,
                    337,
                    0
                ],
                "title": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual\n  Preferences",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LSceneLLM: Enhancing Large 3D Scene Understanding Using Adaptive Visual\n  Preferences"
                },
                "summary": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing\nattention, which is crucial for developing embodied AI within 3D scenes, such\nas visual navigation and embodied question answering. Due to the high density\nof visual features, especially in large 3D scenes, accurately locating\ntask-relevant visual information is challenging. Existing works attempt to\nsegment all objects and consider their features as scene representations.\nHowever, these task-agnostic object features include much redundant information\nand missing details for the task-relevant area. To tackle these problems, we\npropose LSceneLLM, an adaptive framework that automatically identifies\ntask-relevant areas by leveraging LLM's visual preference for different tasks,\nfollowed by a plug-and-play scene magnifier module to capture fine-grained\ndetails in focused areas. Specifically, a dense token selector examines the\nattention map of LLM to identify visual preferences for the instruction input.\nIt then magnifies fine-grained details of the focusing area. An adaptive\nself-attention module is leveraged to fuse the coarse-grained and selected\nfine-grained visual information. To comprehensively evaluate the large scene\nunderstanding ability of 3D-VLMs, we further introduce a cross-room\nunderstanding benchmark, XR-Scene, which contains a series of large scene\nunderstanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption.\nExperiments show that our method surpasses existing methods on both large scene\nunderstanding and existing scene understanding benchmarks. Plunging our scene\nmagnifier module into the existing 3D-VLMs also brings significant improvement.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on 3D Vision-Language Models (3D-VLMs) is gaining increasing\nattention, which is crucial for developing embodied AI within 3D scenes, such\nas visual navigation and embodied question answering. Due to the high density\nof visual features, especially in large 3D scenes, accurately locating\ntask-relevant visual information is challenging. Existing works attempt to\nsegment all objects and consider their features as scene representations.\nHowever, these task-agnostic object features include much redundant information\nand missing details for the task-relevant area. To tackle these problems, we\npropose LSceneLLM, an adaptive framework that automatically identifies\ntask-relevant areas by leveraging LLM's visual preference for different tasks,\nfollowed by a plug-and-play scene magnifier module to capture fine-grained\ndetails in focused areas. Specifically, a dense token selector examines the\nattention map of LLM to identify visual preferences for the instruction input.\nIt then magnifies fine-grained details of the focusing area. An adaptive\nself-attention module is leveraged to fuse the coarse-grained and selected\nfine-grained visual information. To comprehensively evaluate the large scene\nunderstanding ability of 3D-VLMs, we further introduce a cross-room\nunderstanding benchmark, XR-Scene, which contains a series of large scene\nunderstanding tasks including XR-QA, XR-EmbodiedPlanning, and XR-SceneCaption.\nExperiments show that our method surpasses existing methods on both large scene\nunderstanding and existing scene understanding benchmarks. Plunging our scene\nmagnifier module into the existing 3D-VLMs also brings significant improvement."
                },
                "authors": [
                    {
                        "name": "Hongyan Zhi"
                    },
                    {
                        "name": "Peihao Chen"
                    },
                    {
                        "name": "Junyan Li"
                    },
                    {
                        "name": "Shuailei Ma"
                    },
                    {
                        "name": "Xinyu Sun"
                    },
                    {
                        "name": "Tianhang Xiang"
                    },
                    {
                        "name": "Yinjie Lei"
                    },
                    {
                        "name": "Mingkui Tan"
                    },
                    {
                        "name": "Chuang Gan"
                    }
                ],
                "author_detail": {
                    "name": "Chuang Gan"
                },
                "author": "Chuang Gan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.01292v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.01292v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07927v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07927v2",
                "updated": "2025-02-02T11:30:27Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    11,
                    30,
                    27,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-14T08:30:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    30,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "Gandalf the Red: Adaptive Security for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gandalf the Red: Adaptive Security for LLMs"
                },
                "summary": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications."
                },
                "authors": [
                    {
                        "name": "Niklas Pfister"
                    },
                    {
                        "name": "Václav Volhejn"
                    },
                    {
                        "name": "Manuel Knott"
                    },
                    {
                        "name": "Santiago Arias"
                    },
                    {
                        "name": "Julia Bazińska"
                    },
                    {
                        "name": "Mykhailo Bichurin"
                    },
                    {
                        "name": "Alan Commike"
                    },
                    {
                        "name": "Janet Darling"
                    },
                    {
                        "name": "Peter Dienes"
                    },
                    {
                        "name": "Matthew Fiedler"
                    },
                    {
                        "name": "David Haber"
                    },
                    {
                        "name": "Matthias Kraft"
                    },
                    {
                        "name": "Marco Lancini"
                    },
                    {
                        "name": "Max Mathys"
                    },
                    {
                        "name": "Damián Pascual-Ortiz"
                    },
                    {
                        "name": "Jakub Podolak"
                    },
                    {
                        "name": "Adrià Romero-López"
                    },
                    {
                        "name": "Kyriacos Shiarlis"
                    },
                    {
                        "name": "Andreas Signer"
                    },
                    {
                        "name": "Zsolt Terek"
                    },
                    {
                        "name": "Athanasios Theocharis"
                    },
                    {
                        "name": "Daniel Timbrell"
                    },
                    {
                        "name": "Samuel Trautwein"
                    },
                    {
                        "name": "Samuel Watts"
                    },
                    {
                        "name": "Yun-Han Wu"
                    },
                    {
                        "name": "Mateo Rojas-Carulla"
                    }
                ],
                "author_detail": {
                    "name": "Mateo Rojas-Carulla"
                },
                "author": "Mateo Rojas-Carulla",
                "arxiv_comment": "Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07927v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07927v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.00101v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.00101v2",
                "updated": "2025-02-02T08:36:36Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    8,
                    36,
                    36,
                    6,
                    33,
                    0
                ],
                "published": "2024-08-27T12:07:09Z",
                "published_parsed": [
                    2024,
                    8,
                    27,
                    12,
                    7,
                    9,
                    1,
                    240,
                    0
                ],
                "title": "NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuroLM: A Universal Multi-task Foundation Model for Bridging the Gap\n  between Language and EEG Signals"
                },
                "summary": "Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements for large-scale pre-training with neural signals such as\nelectroencephalogram (EEG) have shown promising results, significantly boosting\nthe development of brain-computer interfaces (BCIs) and healthcare. However,\nthese pre-trained models often require full fine-tuning on each downstream task\nto achieve substantial improvements, limiting their versatility and usability,\nand leading to considerable resource wastage. To tackle these challenges, we\npropose NeuroLM, the first multi-task foundation model that leverages the\ncapabilities of Large Language Models (LLMs) by regarding EEG signals as a\nforeign language, endowing the model with multi-task learning and inference\ncapabilities. Our approach begins with learning a text-aligned neural tokenizer\nthrough vector-quantized temporal-frequency prediction, which encodes EEG\nsignals into discrete neural tokens. These EEG tokens, generated by the frozen\nvector-quantized (VQ) encoder, are then fed into an LLM that learns causal EEG\ninformation via multi-channel autoregression. Consequently, NeuroLM can\nunderstand both EEG and language modalities. Finally, multi-task instruction\ntuning adapts NeuroLM to various downstream tasks. We are the first to\ndemonstrate that, by specific incorporation with LLMs, NeuroLM unifies diverse\nEEG tasks within a single model through instruction tuning. The largest variant\nNeuroLM-XL has record-breaking 1.7B parameters for EEG signal processing, and\nis pre-trained on a large-scale corpus comprising approximately 25,000-hour EEG\ndata. When evaluated on six diverse downstream datasets, NeuroLM showcases the\nhuge potential of this multi-task learning paradigm."
                },
                "authors": [
                    {
                        "name": "Wei-Bang Jiang"
                    },
                    {
                        "name": "Yansen Wang"
                    },
                    {
                        "name": "Bao-Liang Lu"
                    },
                    {
                        "name": "Dongsheng Li"
                    }
                ],
                "author_detail": {
                    "name": "Dongsheng Li"
                },
                "author": "Dongsheng Li",
                "arxiv_comment": "The Thirteenth International Conference on Learning Representations",
                "arxiv_journal_ref": "The Thirteenth International Conference on Learning\n  Representations, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.00101v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.00101v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13164v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13164v3",
                "updated": "2025-02-02T08:18:38Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    8,
                    18,
                    38,
                    6,
                    33,
                    0
                ],
                "published": "2024-03-19T21:31:56Z",
                "published_parsed": [
                    2024,
                    3,
                    19,
                    21,
                    31,
                    56,
                    1,
                    79,
                    0
                ],
                "title": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VL-ICL Bench: The Devil in the Details of Multimodal In-Context Learning"
                },
                "summary": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) famously exhibit emergent in-context learning\n(ICL) -- the ability to rapidly adapt to new tasks using few-shot examples\nprovided as a prompt, without updating the model's weights. Built on top of\nLLMs, vision large language models (VLLMs) have advanced significantly in areas\nsuch as recognition, reasoning, and grounding. However, investigations into\n\\emph{multimodal ICL} have predominantly focused on few-shot visual question\nanswering (VQA), and image captioning, which we will show neither exploit the\nstrengths of ICL, nor test its limitations. The broader capabilities and\nlimitations of multimodal ICL remain under-explored. In this study, we\nintroduce a comprehensive benchmark VL-ICL Bench for multimodal in-context\nlearning, encompassing a broad spectrum of tasks that involve both images and\ntext as inputs and outputs, and different types of challenges, from {perception\nto reasoning and long context length}. We evaluate the abilities of\nstate-of-the-art VLLMs against this benchmark suite, revealing their diverse\nstrengths and weaknesses, and showing that even the most advanced models, such\nas GPT-4, find the tasks challenging. By highlighting a range of new ICL tasks,\nand the associated strengths and limitations of existing models, we hope that\nour dataset will inspire future work on enhancing the in-context learning\ncapabilities of VLLMs, as well as inspire new applications that leverage VLLM\nICL. The code and dataset are available at https://github.com/ys-zong/VL-ICL."
                },
                "authors": [
                    {
                        "name": "Yongshuo Zong"
                    },
                    {
                        "name": "Ondrej Bohdal"
                    },
                    {
                        "name": "Timothy Hospedales"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Hospedales"
                },
                "author": "Timothy Hospedales",
                "arxiv_comment": "ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13164v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13164v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.04901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.04901v2",
                "updated": "2025-02-02T07:38:12Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    7,
                    38,
                    12,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-09T01:26:59Z",
                "published_parsed": [
                    2025,
                    1,
                    9,
                    1,
                    26,
                    59,
                    3,
                    9,
                    0
                ],
                "title": "ThriftLLM: On Cost-Effective Selection of Large Language Models for\n  Classification Queries",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ThriftLLM: On Cost-Effective Selection of Large Language Models for\n  Classification Queries"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated remarkable\ncapabilities in comprehending and generating natural language content. An\nincreasing number of services offer LLMs for various tasks via APIs. Different\nLLMs demonstrate expertise in different domains of queries (e.g., text\nclassification queries). Meanwhile, LLMs of different scales, complexity, and\nperformance are priced diversely. Driven by this, several researchers are\ninvestigating strategies for selecting an ensemble of LLMs, aiming to decrease\noverall usage costs while enhancing performance. However, to the best of our\nknowledge, none of the existing works addresses the problem, how to find an LLM\nensemble subject to a cost budget, which maximizes the ensemble performance\nwith guarantees.\n  In this paper, we formalize the performance of an ensemble of models (LLMs)\nusing the notion of prediction accuracy which we formally define. We develop an\napproach for aggregating responses from multiple LLMs to enhance ensemble\nperformance. Building on this, we formulate the Optimal Ensemble Selection\nproblem of selecting a set of LLMs subject to a cost budget that maximizes the\noverall prediction accuracy. We show that prediction accuracy is non-decreasing\nand non-submodular and provide evidence that the Optimal Ensemble Selection\nproblem is likely to be NP-hard. By leveraging a submodular function that upper\nbounds prediction accuracy, we develop an algorithm called ThriftLLM and prove\nthat it achieves an instance-dependent approximation guarantee with high\nprobability. In addition, it achieves state-of-the-art performance for text\nclassification and entity matching queries on multiple real-world datasets\nagainst various baselines in our extensive experimental evaluation, while using\na relatively lower cost budget, strongly supporting the effectiveness and\nsuperiority of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated remarkable\ncapabilities in comprehending and generating natural language content. An\nincreasing number of services offer LLMs for various tasks via APIs. Different\nLLMs demonstrate expertise in different domains of queries (e.g., text\nclassification queries). Meanwhile, LLMs of different scales, complexity, and\nperformance are priced diversely. Driven by this, several researchers are\ninvestigating strategies for selecting an ensemble of LLMs, aiming to decrease\noverall usage costs while enhancing performance. However, to the best of our\nknowledge, none of the existing works addresses the problem, how to find an LLM\nensemble subject to a cost budget, which maximizes the ensemble performance\nwith guarantees.\n  In this paper, we formalize the performance of an ensemble of models (LLMs)\nusing the notion of prediction accuracy which we formally define. We develop an\napproach for aggregating responses from multiple LLMs to enhance ensemble\nperformance. Building on this, we formulate the Optimal Ensemble Selection\nproblem of selecting a set of LLMs subject to a cost budget that maximizes the\noverall prediction accuracy. We show that prediction accuracy is non-decreasing\nand non-submodular and provide evidence that the Optimal Ensemble Selection\nproblem is likely to be NP-hard. By leveraging a submodular function that upper\nbounds prediction accuracy, we develop an algorithm called ThriftLLM and prove\nthat it achieves an instance-dependent approximation guarantee with high\nprobability. In addition, it achieves state-of-the-art performance for text\nclassification and entity matching queries on multiple real-world datasets\nagainst various baselines in our extensive experimental evaluation, while using\na relatively lower cost budget, strongly supporting the effectiveness and\nsuperiority of our method."
                },
                "authors": [
                    {
                        "name": "Keke Huang"
                    },
                    {
                        "name": "Yimin Shi"
                    },
                    {
                        "name": "Dujian Ding"
                    },
                    {
                        "name": "Yifei Li"
                    },
                    {
                        "name": "Yang Fei"
                    },
                    {
                        "name": "Laks Lakshmanan"
                    },
                    {
                        "name": "Xiaokui Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaokui Xiao"
                },
                "author": "Xiaokui Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.04901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.04901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14427v2",
                "updated": "2025-02-02T06:55:25Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    6,
                    55,
                    25,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-24T11:55:57Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    11,
                    55,
                    57,
                    4,
                    24,
                    0
                ],
                "title": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand\n  Graphs Better",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphSOS: Graph Sampling and Order Selection to Help LLMs Understand\n  Graphs Better"
                },
                "summary": "The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe a counter-intuitive fact that when\nthe order of nodes or edges in the natural language description of a graph is\nshuffled, despite describing the same graph, model performance fluctuates\nbetween high performance and random guessing. Additionally, due to LLMs'\nlimited input context length, current methods typically randomly sample\nneighbors of target nodes as representatives of their neighborhood, which may\nnot always be effective for accurate reasoning. To address these gaps, we\nintroduce GraphSOS (Graph Sampling and Order Selection). This novel model\nframework features an Order Selector Module to ensure proper serialization\norder of the graph and a Subgraph Sampling Module to sample subgraphs with\nbetter structure for better reasoning. Furthermore, we propose Graph CoT\nobtained through distillation, and enhance LLM's reasoning and zero-shot\nlearning capabilities for graph tasks through instruction tuning. Experiments\non multiple datasets for node classification and graph question-answering\ndemonstrate that GraphSOS improves LLMs' performance and generalization ability\non graph tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of Large Language Models (LLMs) in various domains has led\nresearchers to apply them to graph-related problems by converting graph data\ninto natural language text. However, unlike graph data, natural language\ninherently has sequential order. We observe a counter-intuitive fact that when\nthe order of nodes or edges in the natural language description of a graph is\nshuffled, despite describing the same graph, model performance fluctuates\nbetween high performance and random guessing. Additionally, due to LLMs'\nlimited input context length, current methods typically randomly sample\nneighbors of target nodes as representatives of their neighborhood, which may\nnot always be effective for accurate reasoning. To address these gaps, we\nintroduce GraphSOS (Graph Sampling and Order Selection). This novel model\nframework features an Order Selector Module to ensure proper serialization\norder of the graph and a Subgraph Sampling Module to sample subgraphs with\nbetter structure for better reasoning. Furthermore, we propose Graph CoT\nobtained through distillation, and enhance LLM's reasoning and zero-shot\nlearning capabilities for graph tasks through instruction tuning. Experiments\non multiple datasets for node classification and graph question-answering\ndemonstrate that GraphSOS improves LLMs' performance and generalization ability\non graph tasks."
                },
                "authors": [
                    {
                        "name": "Xu Chu"
                    },
                    {
                        "name": "Hanlin Xue"
                    },
                    {
                        "name": "Zhijie Tan"
                    },
                    {
                        "name": "Bingce Wang"
                    },
                    {
                        "name": "Tong Mo"
                    },
                    {
                        "name": "Weiping Li"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Li"
                },
                "author": "Weiping Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.18060v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.18060v5",
                "updated": "2025-02-02T06:31:14Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    6,
                    31,
                    14,
                    6,
                    33,
                    0
                ],
                "published": "2024-02-28T05:44:41Z",
                "published_parsed": [
                    2024,
                    2,
                    28,
                    5,
                    44,
                    41,
                    2,
                    59,
                    0
                ],
                "title": "Benchmarking Large Language Models on Answering and Explaining\n  Challenging Medical Questions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Large Language Models on Answering and Explaining\n  Challenging Medical Questions"
                },
                "summary": "LLMs have demonstrated impressive performance in answering medical questions,\nsuch as achieving passing scores on medical licensing examinations. However,\nmedical board exams or general clinical questions do not capture the complexity\nof realistic clinical cases. Moreover, the lack of reference explanations means\nwe cannot easily evaluate the reasoning of model decisions, a crucial component\nof supporting doctors in making complex medical decisions. To address these\nchallenges, we construct two new datasets: JAMA Clinical Challenge and\nMedbullets.\\footnote{Datasets and code are available at\n\\url{https://github.com/HanjieChen/ChallengeClinicalQA}.} JAMA Clinical\nChallenge consists of questions based on challenging clinical cases, while\nMedbullets comprises simulated clinical questions. Both datasets are structured\nas multiple-choice question-answering tasks, accompanied by expert-written\nexplanations. We evaluate seven LLMs on the two datasets using various prompts.\nExperiments demonstrate that our datasets are harder than previous benchmarks.\nIn-depth automatic and human evaluations of model-generated explanations\nprovide insights into the promise and deficiency of LLMs for explainable\nmedical QA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs have demonstrated impressive performance in answering medical questions,\nsuch as achieving passing scores on medical licensing examinations. However,\nmedical board exams or general clinical questions do not capture the complexity\nof realistic clinical cases. Moreover, the lack of reference explanations means\nwe cannot easily evaluate the reasoning of model decisions, a crucial component\nof supporting doctors in making complex medical decisions. To address these\nchallenges, we construct two new datasets: JAMA Clinical Challenge and\nMedbullets.\\footnote{Datasets and code are available at\n\\url{https://github.com/HanjieChen/ChallengeClinicalQA}.} JAMA Clinical\nChallenge consists of questions based on challenging clinical cases, while\nMedbullets comprises simulated clinical questions. Both datasets are structured\nas multiple-choice question-answering tasks, accompanied by expert-written\nexplanations. We evaluate seven LLMs on the two datasets using various prompts.\nExperiments demonstrate that our datasets are harder than previous benchmarks.\nIn-depth automatic and human evaluations of model-generated explanations\nprovide insights into the promise and deficiency of LLMs for explainable\nmedical QA."
                },
                "authors": [
                    {
                        "name": "Hanjie Chen"
                    },
                    {
                        "name": "Zhouxiang Fang"
                    },
                    {
                        "name": "Yash Singla"
                    },
                    {
                        "name": "Mark Dredze"
                    }
                ],
                "author_detail": {
                    "name": "Mark Dredze"
                },
                "author": "Mark Dredze",
                "arxiv_comment": "NAACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.18060v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.18060v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.02787v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.02787v3",
                "updated": "2025-02-02T04:40:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    4,
                    40,
                    51,
                    6,
                    33,
                    0
                ],
                "published": "2023-11-05T22:43:29Z",
                "published_parsed": [
                    2023,
                    11,
                    5,
                    22,
                    43,
                    29,
                    6,
                    309,
                    0
                ],
                "title": "Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable\n  Manipulation with Tools",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Make a Donut: Hierarchical EMD-Space Planning for Zero-Shot Deformable\n  Manipulation with Tools"
                },
                "summary": "Deformable object manipulation stands as one of the most captivating yet\nformidable challenges in robotics. While previous techniques have predominantly\nrelied on learning latent dynamics through demonstrations, typically\nrepresented as either particles or images, there exists a pertinent limitation:\nacquiring suitable demonstrations, especially for long-horizon tasks, can be\nelusive. Moreover, basing learning entirely on demonstrations can hamper the\nmodel's ability to generalize beyond the demonstrated tasks. In this work, we\nintroduce a demonstration-free hierarchical planning approach capable of\ntackling intricate long-horizon tasks without necessitating any training. We\nemploy large language models (LLMs) to articulate a high-level, stage-by-stage\nplan corresponding to a specified task. For every individual stage, the LLM\nprovides both the tool's name and the Python code to craft intermediate subgoal\npoint clouds. With the tool and subgoal for a particular stage at our disposal,\nwe present a granular closed-loop model predictive control strategy. This\nleverages Differentiable Physics with Point-to-Point correspondence\n(DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied\niteratively. Experimental findings affirm that our technique surpasses multiple\nbenchmarks in dough manipulation, spanning both short and long horizons.\nRemarkably, our model demonstrates robust generalization capabilities to novel\nand previously unencountered complex tasks without any preliminary\ndemonstrations. We further substantiate our approach with experimental trials\non real-world robotic platforms. Our project page:\nhttps://qq456cvb.github.io/projects/donut.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deformable object manipulation stands as one of the most captivating yet\nformidable challenges in robotics. While previous techniques have predominantly\nrelied on learning latent dynamics through demonstrations, typically\nrepresented as either particles or images, there exists a pertinent limitation:\nacquiring suitable demonstrations, especially for long-horizon tasks, can be\nelusive. Moreover, basing learning entirely on demonstrations can hamper the\nmodel's ability to generalize beyond the demonstrated tasks. In this work, we\nintroduce a demonstration-free hierarchical planning approach capable of\ntackling intricate long-horizon tasks without necessitating any training. We\nemploy large language models (LLMs) to articulate a high-level, stage-by-stage\nplan corresponding to a specified task. For every individual stage, the LLM\nprovides both the tool's name and the Python code to craft intermediate subgoal\npoint clouds. With the tool and subgoal for a particular stage at our disposal,\nwe present a granular closed-loop model predictive control strategy. This\nleverages Differentiable Physics with Point-to-Point correspondence\n(DiffPhysics-P2P) loss in the earth mover distance (EMD) space, applied\niteratively. Experimental findings affirm that our technique surpasses multiple\nbenchmarks in dough manipulation, spanning both short and long horizons.\nRemarkably, our model demonstrates robust generalization capabilities to novel\nand previously unencountered complex tasks without any preliminary\ndemonstrations. We further substantiate our approach with experimental trials\non real-world robotic platforms. Our project page:\nhttps://qq456cvb.github.io/projects/donut."
                },
                "authors": [
                    {
                        "name": "Yang You"
                    },
                    {
                        "name": "Bokui Shen"
                    },
                    {
                        "name": "Congyue Deng"
                    },
                    {
                        "name": "Haoran Geng"
                    },
                    {
                        "name": "Songlin Wei"
                    },
                    {
                        "name": "He Wang"
                    },
                    {
                        "name": "Leonidas Guibas"
                    }
                ],
                "author_detail": {
                    "name": "Leonidas Guibas"
                },
                "author": "Leonidas Guibas",
                "arxiv_comment": "8 pages. IEEE Robotics and Automation Letters (RA-L). Preprint\n  Version. Accepted January, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.02787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.02787v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2403.13838v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2403.13838v2",
                "updated": "2025-02-02T03:55:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    55,
                    55,
                    6,
                    33,
                    0
                ],
                "published": "2024-03-14T03:24:14Z",
                "published_parsed": [
                    2024,
                    3,
                    14,
                    3,
                    24,
                    14,
                    3,
                    74,
                    0
                ],
                "title": "Circuit Transformer: A Transformer That Preserves Logical Equivalence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Circuit Transformer: A Transformer That Preserves Logical Equivalence"
                },
                "summary": "Implementing Boolean functions with circuits consisting of logic gates is\nfundamental in digital computer design. However, the implemented circuit must\nbe exactly equivalent, which hinders generative neural approaches on this task\ndue to their occasionally wrong predictions. In this study, we introduce a\ngenerative neural model, the \"Circuit Transformer\", which eliminates such wrong\npredictions and produces logic circuits strictly equivalent to given Boolean\nfunctions. The main idea is a carefully designed decoding mechanism that builds\na circuit step-by-step by generating tokens, which has beneficial \"cutoff\nproperties\" that block a candidate token once it invalidate equivalence. In\nsuch a way, the proposed model works similar to typical LLMs while logical\nequivalence is strictly preserved. A Markov decision process formulation is\nalso proposed for optimizing certain objectives of circuits. Experimentally, we\ntrained an 88-million-parameter Circuit Transformer to generate equivalent yet\nmore compact forms of input circuits, outperforming existing neural approaches\non both synthetic and real world benchmarks, without any violation of\nequivalence constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Implementing Boolean functions with circuits consisting of logic gates is\nfundamental in digital computer design. However, the implemented circuit must\nbe exactly equivalent, which hinders generative neural approaches on this task\ndue to their occasionally wrong predictions. In this study, we introduce a\ngenerative neural model, the \"Circuit Transformer\", which eliminates such wrong\npredictions and produces logic circuits strictly equivalent to given Boolean\nfunctions. The main idea is a carefully designed decoding mechanism that builds\na circuit step-by-step by generating tokens, which has beneficial \"cutoff\nproperties\" that block a candidate token once it invalidate equivalence. In\nsuch a way, the proposed model works similar to typical LLMs while logical\nequivalence is strictly preserved. A Markov decision process formulation is\nalso proposed for optimizing certain objectives of circuits. Experimentally, we\ntrained an 88-million-parameter Circuit Transformer to generate equivalent yet\nmore compact forms of input circuits, outperforming existing neural approaches\non both synthetic and real world benchmarks, without any violation of\nequivalence constraints."
                },
                "authors": [
                    {
                        "name": "Xihan Li"
                    },
                    {
                        "name": "Xing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Xing Zhang"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "arxiv_comment": "In ICLR 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2403.13838v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2403.13838v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13052v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13052v2",
                "updated": "2025-02-02T03:25:18Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    25,
                    18,
                    6,
                    33,
                    0
                ],
                "published": "2024-11-20T05:56:31Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    5,
                    56,
                    31,
                    2,
                    325,
                    0
                ],
                "title": "On-device Content-based Recommendation with Single-shot Embedding\n  Pruning: A Cooperative Game Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "On-device Content-based Recommendation with Single-shot Embedding\n  Pruning: A Cooperative Game Perspective"
                },
                "summary": "Content-based Recommender Systems (CRSs) play a crucial role in shaping user\nexperiences in e-commerce, online advertising, and personalized\nrecommendations. However, due to the vast amount of categorical features, the\nembedding tables used in CRS models pose a significant storage bottleneck for\nreal-world deployment, especially on resource-constrained devices. To address\nthis problem, various embedding pruning methods have been proposed, but most\nexisting ones require expensive retraining steps for each target parameter\nbudget, leading to enormous computation costs. In reality, this computation\ncost is a major hurdle in real-world applications with diverse storage\nrequirements, such as federated learning and streaming settings. In this paper,\nwe propose Shapley Value-guided Embedding Reduction (Shaver) as our response.\nWith Shaver, we view the problem from a cooperative game perspective, and\nquantify each embedding parameter's contribution with Shapley values to\nfacilitate contribution-based parameter pruning. To address the inherently high\ncomputation costs of Shapley values, we propose an efficient and unbiased\nmethod to estimate Shapley values of a CRS's embedding parameters. Moreover, in\nthe pruning stage, we put forward a field-aware codebook to mitigate the\ninformation loss in the traditional zero-out treatment. Through extensive\nexperiments on three real-world datasets, Shaver has demonstrated competitive\nperformance with lightweight recommendation models across various parameter\nbudgets. The source code is available at https://github.com/chenxing1999/shaver",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-based Recommender Systems (CRSs) play a crucial role in shaping user\nexperiences in e-commerce, online advertising, and personalized\nrecommendations. However, due to the vast amount of categorical features, the\nembedding tables used in CRS models pose a significant storage bottleneck for\nreal-world deployment, especially on resource-constrained devices. To address\nthis problem, various embedding pruning methods have been proposed, but most\nexisting ones require expensive retraining steps for each target parameter\nbudget, leading to enormous computation costs. In reality, this computation\ncost is a major hurdle in real-world applications with diverse storage\nrequirements, such as federated learning and streaming settings. In this paper,\nwe propose Shapley Value-guided Embedding Reduction (Shaver) as our response.\nWith Shaver, we view the problem from a cooperative game perspective, and\nquantify each embedding parameter's contribution with Shapley values to\nfacilitate contribution-based parameter pruning. To address the inherently high\ncomputation costs of Shapley values, we propose an efficient and unbiased\nmethod to estimate Shapley values of a CRS's embedding parameters. Moreover, in\nthe pruning stage, we put forward a field-aware codebook to mitigate the\ninformation loss in the traditional zero-out treatment. Through extensive\nexperiments on three real-world datasets, Shaver has demonstrated competitive\nperformance with lightweight recommendation models across various parameter\nbudgets. The source code is available at https://github.com/chenxing1999/shaver"
                },
                "authors": [
                    {
                        "name": "Hung Vinh Tran"
                    },
                    {
                        "name": "Tong Chen"
                    },
                    {
                        "name": "Guanhua Ye"
                    },
                    {
                        "name": "Quoc Viet Hung Nguyen"
                    },
                    {
                        "name": "Kai Zheng"
                    },
                    {
                        "name": "Hongzhi Yin"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Yin"
                },
                "author": "Hongzhi Yin",
                "arxiv_doi": "10.1145/3696410.3714921",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3696410.3714921",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13052v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13052v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "ACM Web Conference 2025 (WWW '25)",
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14484v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14484v2",
                "updated": "2025-02-02T03:05:02Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    5,
                    2,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-24T13:37:26Z",
                "published_parsed": [
                    2025,
                    1,
                    24,
                    13,
                    37,
                    26,
                    4,
                    24,
                    0
                ],
                "title": "$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with\n  High Hardware Compatibility",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "$SpikePack$: Enhanced Information Flow in Spiking Neural Networks with\n  High Hardware Compatibility"
                },
                "summary": "Spiking Neural Networks (SNNs) hold promise for energy-efficient,\nbiologically inspired computing. We identify substantial informatio loss during\nspike transmission, linked to temporal dependencies in traditional Leaky\nIntegrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN\nperformance. Existing SNN architectures also underutilize modern GPUs,\nconstrained by single-bit spike storage and isolated weight-spike operations\nthat restrict computational efficiency. We introduce ${SpikePack}$, a neuron\nmodel designed to reduce transmission loss while preserving essential features\nlike membrane potential reset and leaky integration. ${SpikePack}$ achieves\nconstant $\\mathcal{O}(1)$ time and space complexity, enabling efficient\nparallel processing on GPUs and also supporting serial inference on existing\nSNN hardware accelerators. Compatible with standard Artificial Neural Network\n(ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN\nconversion across various networks. Experimental results on tasks such as image\nclassification, detection, and segmentation show ${SpikePack}$ achieves\nsignificant gains in accuracy and efficiency for both directly trained and\nconverted SNNs over state-of-the-art models. Tests on FPGA-based platforms\nfurther confirm cross-platform flexibility, delivering high performance and\nenhanced sparsity. By enhancing information flow and rethinking SNN-ANN\nintegration, ${SpikePack}$ advances efficient SNN deployment across diverse\nhardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spiking Neural Networks (SNNs) hold promise for energy-efficient,\nbiologically inspired computing. We identify substantial informatio loss during\nspike transmission, linked to temporal dependencies in traditional Leaky\nIntegrate-and-Fire (LIF) neuron-a key factor potentially limiting SNN\nperformance. Existing SNN architectures also underutilize modern GPUs,\nconstrained by single-bit spike storage and isolated weight-spike operations\nthat restrict computational efficiency. We introduce ${SpikePack}$, a neuron\nmodel designed to reduce transmission loss while preserving essential features\nlike membrane potential reset and leaky integration. ${SpikePack}$ achieves\nconstant $\\mathcal{O}(1)$ time and space complexity, enabling efficient\nparallel processing on GPUs and also supporting serial inference on existing\nSNN hardware accelerators. Compatible with standard Artificial Neural Network\n(ANN) architectures, ${SpikePack}$ facilitates near-lossless ANN-to-SNN\nconversion across various networks. Experimental results on tasks such as image\nclassification, detection, and segmentation show ${SpikePack}$ achieves\nsignificant gains in accuracy and efficiency for both directly trained and\nconverted SNNs over state-of-the-art models. Tests on FPGA-based platforms\nfurther confirm cross-platform flexibility, delivering high performance and\nenhanced sparsity. By enhancing information flow and rethinking SNN-ANN\nintegration, ${SpikePack}$ advances efficient SNN deployment across diverse\nhardware platforms."
                },
                "authors": [
                    {
                        "name": "Guobin Shen"
                    },
                    {
                        "name": "Jindong Li"
                    },
                    {
                        "name": "Tenglong Li"
                    },
                    {
                        "name": "Dongcheng Zhao"
                    },
                    {
                        "name": "Yi Zeng"
                    }
                ],
                "author_detail": {
                    "name": "Yi Zeng"
                },
                "author": "Yi Zeng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14484v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14484v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16383v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16383v2",
                "updated": "2025-02-02T03:04:54Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    3,
                    4,
                    54,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-25T01:45:29Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    1,
                    45,
                    29,
                    5,
                    25,
                    0
                ],
                "title": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RotateKV: Accurate and Robust 2-Bit KV Cache Quantization for LLMs via\n  Outlier-Aware Adaptive Rotations"
                },
                "summary": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Key-Value (KV) cache facilitates efficient large language models (LLMs)\ninference by avoiding recomputation of past KVs. As the batch size and context\nlength increase, the oversized KV caches become a significant memory\nbottleneck, highlighting the need for efficient compression. Existing KV\nquantization rely on fine-grained quantization or the retention of a\nsignificant portion of high bit-widths caches, both of which compromise\ncompression ratio and often fail to maintain robustness at extremely low\naverage bit-widths. In this work, we explore the potential of rotation\ntechnique for 2-bit KV quantization and propose RotateKV, which achieves\naccurate and robust performance through the following innovations: (i)\nOutlier-Aware Rotation, which utilizes channel-reordering to adapt the\nrotations to varying channel-wise outlier distributions without sacrificing the\ncomputational efficiency of the fast Walsh-Hadamard transform (FWHT); (ii)\nPre-RoPE Grouped-Head Rotation, which mitigates the impact of rotary position\nembedding (RoPE) on proposed outlier-aware rotation and further smooths\noutliers across heads; (iii) Attention-Sink-Aware Quantization, which leverages\nthe massive activations to precisely identify and protect attention sinks.\nRotateKV achieves less than 0.3 perplexity (PPL) degradation with 2-bit\nquantization on WikiText-2 using LLaMA-2-13B, maintains strong CoT reasoning\nand long-context capabilities, with less than 1.7\\% degradation on GSM8K,\noutperforming existing methods even at lower average bit-widths. RotateKV also\nshowcases a 3.97x reduction in peak memory usage, supports 5.75x larger batch\nsizes, and achieves a 2.32x speedup in decoding stage."
                },
                "authors": [
                    {
                        "name": "Zunhai Su"
                    },
                    {
                        "name": "Zhe Chen"
                    },
                    {
                        "name": "Wang Shen"
                    },
                    {
                        "name": "Hanyu Wei"
                    },
                    {
                        "name": "Linge Li"
                    },
                    {
                        "name": "Huangqi Yu"
                    },
                    {
                        "name": "Kehong Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Kehong Yuan"
                },
                "author": "Kehong Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16383v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16383v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07288v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07288v2",
                "updated": "2025-02-02T00:43:45Z",
                "updated_parsed": [
                    2025,
                    2,
                    2,
                    0,
                    43,
                    45,
                    6,
                    33,
                    0
                ],
                "published": "2025-01-13T12:56:05Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    56,
                    5,
                    0,
                    13,
                    0
                ],
                "title": "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-Net: Democratizing LLMs-as-a-Service through Blockchain-based Expert\n  Networks"
                },
                "summary": "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The centralization of Large Language Models (LLMs) development has created\nsignificant barriers to AI advancement, limiting the democratization of these\npowerful technologies. This centralization, coupled with the scarcity of\nhigh-quality training data and mounting complexity of maintaining comprehensive\nexpertise across rapidly expanding knowledge domains, poses critical challenges\nto the continued growth of LLMs. While solutions like Retrieval-Augmented\nGeneration (RAG) offer potential remedies, maintaining up-to-date expert\nknowledge across diverse domains remains a significant challenge, particularly\ngiven the exponential growth of specialized information. This paper introduces\nLLMs Networks (LLM-Net), a blockchain-based framework that democratizes\nLLMs-as-a-Service through a decentralized network of specialized LLM providers.\nBy leveraging collective computational resources and distributed domain\nexpertise, LLM-Net incorporates fine-tuned expert models for various specific\ndomains, ensuring sustained knowledge growth while maintaining service quality\nthrough collaborative prompting mechanisms. The framework's robust design\nincludes blockchain technology for transparent transaction and performance\nvalidation, establishing an immutable record of service delivery. Our\nsimulation, built on top of state-of-the-art LLMs such as Claude 3.5 Sonnet,\nLlama 3.1, Grok-2, and GPT-4o, validates the effectiveness of the\nreputation-based mechanism in maintaining service quality by selecting\nhigh-performing respondents (LLM providers). Thereby it demonstrates the\npotential of LLM-Net to sustain AI advancement through the integration of\ndecentralized expertise and blockchain-based accountability."
                },
                "authors": [
                    {
                        "name": "Zan-Kai Chong"
                    },
                    {
                        "name": "Hiroyuki Ohsaki"
                    },
                    {
                        "name": "Bryan Ng"
                    }
                ],
                "author_detail": {
                    "name": "Bryan Ng"
                },
                "author": "Bryan Ng",
                "arxiv_comment": "8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07288v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07288v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06049v2",
                "updated": "2025-02-01T23:54:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    23,
                    54,
                    28,
                    5,
                    32,
                    0
                ],
                "published": "2024-02-06T03:24:27Z",
                "published_parsed": [
                    2024,
                    2,
                    6,
                    3,
                    24,
                    27,
                    1,
                    37,
                    0
                ],
                "title": "Limits of Large Language Models in Debating Humans",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Limits of Large Language Models in Debating Humans"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable promise in communicating\nwith humans. Their potential use as artificial partners with humans in\nsociological experiments involving conversation is an exciting prospect. But\nhow viable is it? Here, we rigorously test the limits of agents that debate\nusing LLMs in a preregistered study that runs multiple debate-based opinion\nconsensus games. Each game starts with six humans, six agents, or three humans\nand three agents. We found that agents can blend in and concentrate on a\ndebate's topic better than humans, improving the productivity of all players.\nYet, humans perceive agents as less convincing and confident than other humans,\nand several behavioral metrics of humans and agents we collected deviate\nmeasurably from each other. We observed that agents are already decent\ndebaters, but their behavior generates a pattern distinctly different from the\nhuman-generated data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable promise in communicating\nwith humans. Their potential use as artificial partners with humans in\nsociological experiments involving conversation is an exciting prospect. But\nhow viable is it? Here, we rigorously test the limits of agents that debate\nusing LLMs in a preregistered study that runs multiple debate-based opinion\nconsensus games. Each game starts with six humans, six agents, or three humans\nand three agents. We found that agents can blend in and concentrate on a\ndebate's topic better than humans, improving the productivity of all players.\nYet, humans perceive agents as less convincing and confident than other humans,\nand several behavioral metrics of humans and agents we collected deviate\nmeasurably from each other. We observed that agents are already decent\ndebaters, but their behavior generates a pattern distinctly different from the\nhuman-generated data."
                },
                "authors": [
                    {
                        "name": "James Flamino"
                    },
                    {
                        "name": "Mohammed Shahid Modi"
                    },
                    {
                        "name": "Boleslaw K. Szymanski"
                    },
                    {
                        "name": "Brendan Cross"
                    },
                    {
                        "name": "Colton Mikolajczyk"
                    }
                ],
                "author_detail": {
                    "name": "Colton Mikolajczyk"
                },
                "author": "Colton Mikolajczyk",
                "arxiv_comment": "23 pages, 4 figures, 3 tables, 42 pages of supplemental materials, 9\n  supplemental figures, 24 supplemental tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.14654v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.14654v3",
                "updated": "2025-02-01T22:39:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    22,
                    39,
                    4,
                    5,
                    32,
                    0
                ],
                "published": "2024-11-22T00:59:25Z",
                "published_parsed": [
                    2024,
                    11,
                    22,
                    0,
                    59,
                    25,
                    4,
                    327,
                    0
                ],
                "title": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparative Analysis of Pooling Mechanisms in LLMs: A Sentiment Analysis\n  Perspective"
                },
                "summary": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized natural language processing\n(NLP) by delivering state-of-the-art performance across a variety of tasks.\nAmong these, Transformer-based models like BERT and GPT rely on pooling layers\nto aggregate token-level embeddings into sentence-level representations. Common\npooling mechanisms such as Mean, Max, and Weighted Sum play a pivotal role in\nthis aggregation process. Despite their widespread use, the comparative\nperformance of these strategies on different LLM architectures remains\nunderexplored. To address this gap, this paper investigates the effects of\nthese pooling mechanisms on two prominent LLM families -- BERT and GPT, in the\ncontext of sentence-level sentiment analysis. Comprehensive experiments reveal\nthat each pooling mechanism exhibits unique strengths and weaknesses depending\non the task's specific requirements. Our findings underline the importance of\nselecting pooling methods tailored to the demands of particular applications,\nprompting a re-evaluation of common assumptions regarding pooling operations.\nBy offering actionable insights, this study contributes to the optimization of\nLLM-based models for downstream tasks."
                },
                "authors": [
                    {
                        "name": "Jinming Xing"
                    },
                    {
                        "name": "Dongwen Luo"
                    },
                    {
                        "name": "Chang Xue"
                    },
                    {
                        "name": "Ruilin Xing"
                    }
                ],
                "author_detail": {
                    "name": "Ruilin Xing"
                },
                "author": "Ruilin Xing",
                "arxiv_comment": "Accepted to ISMSI'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.14654v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.14654v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.00066v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.00066v3",
                "updated": "2025-02-01T21:56:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    21,
                    56,
                    34,
                    5,
                    32,
                    0
                ],
                "published": "2024-06-17T15:21:35Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    15,
                    21,
                    35,
                    0,
                    169,
                    0
                ],
                "title": "Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compress then Serve: Serving Thousands of LoRA Adapters with Little\n  Overhead"
                },
                "summary": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of compression when serving LoRAs. We propose a method for the joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are amenable\nto joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 1000 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fine-tuning large language models (LLMs) with low-rank adaptations (LoRAs)\nhas become common practice, often yielding numerous copies of the same LLM\ndiffering only in their LoRA updates. This paradigm presents challenges for\nsystems that serve real-time responses to queries that each involve a different\nLoRA. Prior works optimize the design of such systems but still require\ncontinuous loading and offloading of LoRAs, as it is infeasible to store\nthousands of LoRAs in GPU memory. To mitigate this issue, we investigate the\nefficacy of compression when serving LoRAs. We propose a method for the joint\ncompression of LoRAs into a shared basis paired with LoRA-specific scaling\nmatrices. We extend our algorithm to learn clusters of LoRAs that are amenable\nto joint compression, allowing it to scale gracefully to large LoRA\ncollections. Our experiments with up to 1000 LoRAs demonstrate that compressed\nLoRAs preserve performance while offering major throughput gains in realistic\nserving scenarios with over a thousand LoRAs, maintaining 80% of the throughput\nof serving a single LoRA."
                },
                "authors": [
                    {
                        "name": "Rickard Brüel-Gabrielsson"
                    },
                    {
                        "name": "Jiacheng Zhu"
                    },
                    {
                        "name": "Onkar Bhardwaj"
                    },
                    {
                        "name": "Leshem Choshen"
                    },
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Mikhail Yurochkin"
                    },
                    {
                        "name": "Justin Solomon"
                    }
                ],
                "author_detail": {
                    "name": "Justin Solomon"
                },
                "author": "Justin Solomon",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.00066v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.00066v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12869v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12869v3",
                "updated": "2025-02-01T19:08:49Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    19,
                    8,
                    49,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-14T01:57:25Z",
                "published_parsed": [
                    2024,
                    10,
                    14,
                    1,
                    57,
                    25,
                    0,
                    288,
                    0
                ],
                "title": "Language Model Preference Evaluation with Multiple Weak Evaluators",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Preference Evaluation with Multiple Weak Evaluators"
                },
                "summary": "Despite the remarkable success of Large Language Models (LLMs), evaluating\ntheir outputs' quality regarding *preference* remains a critical challenge.\nExisting works usually leverage an LLM as the judge for comparing LLMs' output\npairwisely, yet such model-based evaluator is *weak evaluator* due to\n*conflicting preference*, i.e., output A is better than B, B than C, but C than\nA, causing contradictory evaluation results. To address this, we introduce GED\n(Preference Graph Ensemble and Denoise), a novel approach that leverages\nmultiple model-based evaluators to construct preference graphs, and then\nensemble and denoise these graphs for better, non-contradictory evaluation\nresults. In particular, our method consists of two primary stages: aggregating\nevaluations into a unified graph and applying a denoising process to eliminate\ncyclic inconsistencies, ensuring a directed acyclic graph (DAG) structure. We\nprovide theoretical guarantees for our framework, demonstrating its efficacy in\nrecovering the ground truth preference structure. Extensive experiments on ten\nbenchmarks demonstrate GED's superiority in three applications: model ranking,\nresponse selection, and model alignment tasks. Notably, GED combines small LLM\nevaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to outperform stronger ones\n(e.g., Qwen2-72B), showcasing its effectiveness in enhancing evaluation\nreliability and improving model performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the remarkable success of Large Language Models (LLMs), evaluating\ntheir outputs' quality regarding *preference* remains a critical challenge.\nExisting works usually leverage an LLM as the judge for comparing LLMs' output\npairwisely, yet such model-based evaluator is *weak evaluator* due to\n*conflicting preference*, i.e., output A is better than B, B than C, but C than\nA, causing contradictory evaluation results. To address this, we introduce GED\n(Preference Graph Ensemble and Denoise), a novel approach that leverages\nmultiple model-based evaluators to construct preference graphs, and then\nensemble and denoise these graphs for better, non-contradictory evaluation\nresults. In particular, our method consists of two primary stages: aggregating\nevaluations into a unified graph and applying a denoising process to eliminate\ncyclic inconsistencies, ensuring a directed acyclic graph (DAG) structure. We\nprovide theoretical guarantees for our framework, demonstrating its efficacy in\nrecovering the ground truth preference structure. Extensive experiments on ten\nbenchmarks demonstrate GED's superiority in three applications: model ranking,\nresponse selection, and model alignment tasks. Notably, GED combines small LLM\nevaluators (e.g., Llama3-8B, Mistral-7B, Qwen2-7B) to outperform stronger ones\n(e.g., Qwen2-72B), showcasing its effectiveness in enhancing evaluation\nreliability and improving model performance."
                },
                "authors": [
                    {
                        "name": "Zhengyu Hu"
                    },
                    {
                        "name": "Jieyu Zhang"
                    },
                    {
                        "name": "Zhihan Xiong"
                    },
                    {
                        "name": "Alexander Ratner"
                    },
                    {
                        "name": "Hui Xiong"
                    },
                    {
                        "name": "Ranjay Krishna"
                    }
                ],
                "author_detail": {
                    "name": "Ranjay Krishna"
                },
                "author": "Ranjay Krishna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12869v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12869v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02625v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02625v2",
                "updated": "2025-02-01T18:58:20Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    18,
                    58,
                    20,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-05T18:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    5,
                    18,
                    41,
                    54,
                    6,
                    5,
                    0
                ],
                "title": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HALO: Hadamard-Assisted Lower-Precision Optimization for LLMs"
                },
                "summary": "Quantized training of Large Language Models (LLMs) remains an open challenge,\nas maintaining accuracy while performing all matrix multiplications in low\nprecision has proven difficult. This is particularly the case when fine-tuning\npre-trained models, which can have large weight and activation outlier values\nthat make lower-precision optimization difficult. To address this, we present\nHALO, a novel quantization-aware training approach for Transformers that\nenables accurate and efficient low-precision training by combining 1) strategic\nplacement of Hadamard rotations in both forward and backward passes, which\nmitigate outliers, 2) high-performance kernel support, and 3) FSDP integration\nfor low-precision communication. Our approach ensures that all large matrix\nmultiplications during the forward and backward passes are executed in lower\nprecision. Applied to LLAMA-family models, HALO achieves\nnear-full-precision-equivalent results during fine-tuning on various tasks,\nwhile delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX\n4090 GPUs. HALO efficiently supports both standard and parameterefficient\nfine-tuning (PEFT). Our results demonstrate the first practical approach to\nfully quantized LLM fine-tuning that maintains accuracy in 8-bit precision,\nwhile delivering performance benefits. Code is available at\n\\url{https://github.com/IST-DASLab/HALO}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantized training of Large Language Models (LLMs) remains an open challenge,\nas maintaining accuracy while performing all matrix multiplications in low\nprecision has proven difficult. This is particularly the case when fine-tuning\npre-trained models, which can have large weight and activation outlier values\nthat make lower-precision optimization difficult. To address this, we present\nHALO, a novel quantization-aware training approach for Transformers that\nenables accurate and efficient low-precision training by combining 1) strategic\nplacement of Hadamard rotations in both forward and backward passes, which\nmitigate outliers, 2) high-performance kernel support, and 3) FSDP integration\nfor low-precision communication. Our approach ensures that all large matrix\nmultiplications during the forward and backward passes are executed in lower\nprecision. Applied to LLAMA-family models, HALO achieves\nnear-full-precision-equivalent results during fine-tuning on various tasks,\nwhile delivering up to 1.41x end-to-end speedup for full fine-tuning on RTX\n4090 GPUs. HALO efficiently supports both standard and parameterefficient\nfine-tuning (PEFT). Our results demonstrate the first practical approach to\nfully quantized LLM fine-tuning that maintains accuracy in 8-bit precision,\nwhile delivering performance benefits. Code is available at\n\\url{https://github.com/IST-DASLab/HALO}."
                },
                "authors": [
                    {
                        "name": "Saleh Ashkboos"
                    },
                    {
                        "name": "Mahdi Nikdan"
                    },
                    {
                        "name": "Soroush Tabesh"
                    },
                    {
                        "name": "Roberto L. Castro"
                    },
                    {
                        "name": "Torsten Hoefler"
                    },
                    {
                        "name": "Dan Alistarh"
                    }
                ],
                "author_detail": {
                    "name": "Dan Alistarh"
                },
                "author": "Dan Alistarh",
                "arxiv_comment": "13 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02625v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02625v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07267v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07267v2",
                "updated": "2025-02-01T16:39:04Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    16,
                    39,
                    4,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-13T12:30:08Z",
                "published_parsed": [
                    2025,
                    1,
                    13,
                    12,
                    30,
                    8,
                    0,
                    13,
                    0
                ],
                "title": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming Role Classification in Scientific Teams Using LLMs and\n  Advanced Predictive Analytics"
                },
                "summary": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scientific team dynamics are critical in determining the nature and impact of\nresearch outputs. However, existing methods for classifying author roles based\non self-reports and clustering lack comprehensive contextual analysis of\ncontributions. Thus, we present a transformative approach to classifying author\nroles in scientific teams using advanced large language models (LLMs), which\noffers a more refined analysis compared to traditional clustering methods.\nSpecifically, we seek to complement and enhance these traditional methods by\nutilizing open source and proprietary LLMs, such as GPT-4, Llama3 70B, Llama2\n70B, and Mistral 7x8B, for role classification. Utilizing few-shot prompting,\nwe categorize author roles and demonstrate that GPT-4 outperforms other models\nacross multiple categories, surpassing traditional approaches such as XGBoost\nand BERT. Our methodology also includes building a predictive deep learning\nmodel using 10 features. By training this model on a dataset derived from the\nOpenAlex database, which provides detailed metadata on academic publications --\nsuch as author-publication history, author affiliation, research topics, and\ncitation counts -- we achieve an F1 score of 0.76, demonstrating robust\nclassification of author roles."
                },
                "authors": [
                    {
                        "name": "Wonduk Seo"
                    },
                    {
                        "name": "Yi Bu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Bu"
                },
                "author": "Yi Bu",
                "arxiv_comment": "16 pages, 5 figures, 3 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07267v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07267v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.14808v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.14808v2",
                "updated": "2025-02-01T15:14:44Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    15,
                    14,
                    44,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-15T16:32:27Z",
                "published_parsed": [
                    2025,
                    1,
                    15,
                    16,
                    32,
                    27,
                    2,
                    15,
                    0
                ],
                "title": "HyGen: Efficient LLM Serving via Elastic Online-Offline Request\n  Co-location",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyGen: Efficient LLM Serving via Elastic Online-Offline Request\n  Co-location"
                },
                "summary": "Large language models (LLMs) have facilitated a wide range of applications\nwith distinct service-level objectives (SLOs), from latency-sensitive online\ntasks like interactive chatbots to throughput-oriented offline workloads like\ndocument summarization. The existing deployment model, which dedicates machines\nto each workload, simplifies SLO management but often leads to poor resource\nutilization. This paper introduces HyGen, an interference-aware LLM serving\nsystem that enables efficient co-location of online and offline workloads while\npreserving latency requirements. HyGen incorporates two key innovations: (1)\nperformance control mechanisms, including a latency predictor to estimate batch\nexecution time and an SLO-aware profiler to quantify latency interference, and\n(2) SLO-aware offline scheduling policies that maximize serving throughput and\nprevent starvation, without compromising online serving latency. Our evaluation\non production workloads shows that HyGen achieves up to 3.87x overall\nthroughput and 5.84x offline throughput gains over online and hybrid serving\nbaselines, respectively, while strictly satisfying latency SLOs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have facilitated a wide range of applications\nwith distinct service-level objectives (SLOs), from latency-sensitive online\ntasks like interactive chatbots to throughput-oriented offline workloads like\ndocument summarization. The existing deployment model, which dedicates machines\nto each workload, simplifies SLO management but often leads to poor resource\nutilization. This paper introduces HyGen, an interference-aware LLM serving\nsystem that enables efficient co-location of online and offline workloads while\npreserving latency requirements. HyGen incorporates two key innovations: (1)\nperformance control mechanisms, including a latency predictor to estimate batch\nexecution time and an SLO-aware profiler to quantify latency interference, and\n(2) SLO-aware offline scheduling policies that maximize serving throughput and\nprevent starvation, without compromising online serving latency. Our evaluation\non production workloads shows that HyGen achieves up to 3.87x overall\nthroughput and 5.84x offline throughput gains over online and hybrid serving\nbaselines, respectively, while strictly satisfying latency SLOs."
                },
                "authors": [
                    {
                        "name": "Ting Sun"
                    },
                    {
                        "name": "Penghan Wang"
                    },
                    {
                        "name": "Fan Lai"
                    }
                ],
                "author_detail": {
                    "name": "Fan Lai"
                },
                "author": "Fan Lai",
                "arxiv_comment": "15 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.14808v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.14808v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.07062v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.07062v3",
                "updated": "2025-02-01T13:50:55Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    50,
                    55,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-09T17:03:49Z",
                "published_parsed": [
                    2024,
                    10,
                    9,
                    17,
                    3,
                    49,
                    2,
                    283,
                    0
                ],
                "title": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TinyEmo: Scaling down Emotional Reasoning via Metric Projection"
                },
                "summary": "This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems. We release code, models, and dataset at\nhttps://github.com/ggcr/TinyEmo",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces TinyEmo, a family of small multi-modal language models\nfor emotional reasoning and classification. Our approach features: (1) a\nsynthetic emotional instruct dataset for both pre-training and fine-tuning\nstages, (2) a Metric Projector that delegates classification from the language\nmodel allowing for more efficient training and inference, (3) a multi-modal\nlarge language model (MM-LLM) for emotional reasoning, and (4) a semi-automated\nframework for bias detection. TinyEmo is able to perform emotion classification\nand emotional reasoning, all while using substantially fewer parameters than\ncomparable models. This efficiency allows us to freely incorporate more diverse\nemotional datasets, enabling strong performance on classification tasks, with\nour smallest model (700M parameters) outperforming larger state-of-the-art\nmodels based on general-purpose MM-LLMs with over 7B parameters. Additionally,\nthe Metric Projector allows for interpretability and indirect bias detection in\nlarge models without additional training, offering an approach to understand\nand improve AI systems. We release code, models, and dataset at\nhttps://github.com/ggcr/TinyEmo"
                },
                "authors": [
                    {
                        "name": "Cristian Gutierrez"
                    }
                ],
                "author_detail": {
                    "name": "Cristian Gutierrez"
                },
                "author": "Cristian Gutierrez",
                "arxiv_comment": "Fix table format",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.07062v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.07062v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13865v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13865v2",
                "updated": "2025-02-01T13:05:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    13,
                    5,
                    17,
                    5,
                    32,
                    0
                ],
                "published": "2024-11-21T06:01:47Z",
                "published_parsed": [
                    2024,
                    11,
                    21,
                    6,
                    1,
                    47,
                    3,
                    326,
                    0
                ],
                "title": "Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for\n  Exploration and Exploitation in Recommender Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Breaking Information Cocoons: A Hyperbolic Graph-LLM Framework for\n  Exploration and Exploitation in Recommender Systems"
                },
                "summary": "Modern recommender systems often create information cocoons, restricting\nusers' exposure to diverse content. A key challenge lies in balancing content\nexploration and exploitation while allowing users to adjust their\nrecommendation preferences. Intuitively, this balance can be modeled as a\ntree-structured representation, where depth search facilitates exploitation and\nbreadth search enables exploration. However, existing approaches face two\nfundamental limitations: Euclidean methods struggle to capture hierarchical\nstructures, while hyperbolic methods, despite their superior hierarchical\nmodeling, lack semantic understanding of user and item profiles and fail to\nprovide a principled mechanism for balancing exploration and exploitation. To\naddress these challenges, we propose HERec, a hyperbolic graph-LLM framework\nthat effectively balances exploration and exploitation in recommender systems.\nOur framework introduces two key innovations: (1) a hierarchical-aware\ngraph-LLM mechanism that jointly aligns textual descriptions with user-item\ncollaborative information in hyperbolic space, and (2) a hierarchical\nrepresentation structure that enables user-adjustable exploration-exploitation\ntrade-offs. Extensive experiments demonstrate that HERec consistently\noutperforms both Euclidean and hyperbolic baselines, achieving up to 5.49%\nimprovement in utility metrics and 11.39% increase in diversity metrics,\neffectively mitigating information cocoons. We open-source our model\nimplementation at https://github.com/Martin-qyma/HERec.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern recommender systems often create information cocoons, restricting\nusers' exposure to diverse content. A key challenge lies in balancing content\nexploration and exploitation while allowing users to adjust their\nrecommendation preferences. Intuitively, this balance can be modeled as a\ntree-structured representation, where depth search facilitates exploitation and\nbreadth search enables exploration. However, existing approaches face two\nfundamental limitations: Euclidean methods struggle to capture hierarchical\nstructures, while hyperbolic methods, despite their superior hierarchical\nmodeling, lack semantic understanding of user and item profiles and fail to\nprovide a principled mechanism for balancing exploration and exploitation. To\naddress these challenges, we propose HERec, a hyperbolic graph-LLM framework\nthat effectively balances exploration and exploitation in recommender systems.\nOur framework introduces two key innovations: (1) a hierarchical-aware\ngraph-LLM mechanism that jointly aligns textual descriptions with user-item\ncollaborative information in hyperbolic space, and (2) a hierarchical\nrepresentation structure that enables user-adjustable exploration-exploitation\ntrade-offs. Extensive experiments demonstrate that HERec consistently\noutperforms both Euclidean and hyperbolic baselines, achieving up to 5.49%\nimprovement in utility metrics and 11.39% increase in diversity metrics,\neffectively mitigating information cocoons. We open-source our model\nimplementation at https://github.com/Martin-qyma/HERec."
                },
                "authors": [
                    {
                        "name": "Qiyao Ma"
                    },
                    {
                        "name": "Menglin Yang"
                    },
                    {
                        "name": "Mingxuan Ju"
                    },
                    {
                        "name": "Tong Zhao"
                    },
                    {
                        "name": "Neil Shah"
                    },
                    {
                        "name": "Rex Ying"
                    }
                ],
                "author_detail": {
                    "name": "Rex Ying"
                },
                "author": "Rex Ying",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13865v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13865v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.06331v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.06331v3",
                "updated": "2025-02-01T12:21:59Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    12,
                    21,
                    59,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-08T20:12:11Z",
                "published_parsed": [
                    2024,
                    10,
                    8,
                    20,
                    12,
                    11,
                    1,
                    282,
                    0
                ],
                "title": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locate-then-edit for Multi-hop Factual Recall under Knowledge Editing"
                },
                "summary": "The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve knowledge with implicit subject\ninformation from deeper MLP layers, unlike single-hop tasks, which rely on\nshallow layers. This distinction explains the poor performance of current\nmethods in multi-hop queries, as they primarily focus on editing shallow layers\nwith single-hop edit prompts, leaving deeper layers unchanged. To address this,\nwe propose IFMET, a novel locate-then-edit KE approach designed to edit both\nshallow and deep MLP layers. Beyond single-hop editing prompts, IFMET further\nincorporates multi-hop editing prompts to locate and modify knowledge across\ndifferent stages of reasoning. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\novercoming the limitations of previous locate-then-edit methods",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The locate-then-edit paradigm has shown significant promise for knowledge\nediting (KE) in Large Language Models (LLMs). While previous methods perform\nwell on single-hop fact recall tasks, they consistently struggle with multi-hop\nfactual recall tasks involving newly edited knowledge. In this paper,\nleveraging tools in mechanistic interpretability, we first identify that in\nmulti-hop tasks, LLMs tend to retrieve knowledge with implicit subject\ninformation from deeper MLP layers, unlike single-hop tasks, which rely on\nshallow layers. This distinction explains the poor performance of current\nmethods in multi-hop queries, as they primarily focus on editing shallow layers\nwith single-hop edit prompts, leaving deeper layers unchanged. To address this,\nwe propose IFMET, a novel locate-then-edit KE approach designed to edit both\nshallow and deep MLP layers. Beyond single-hop editing prompts, IFMET further\nincorporates multi-hop editing prompts to locate and modify knowledge across\ndifferent stages of reasoning. Experimental results demonstrate that IFMET\nsignificantly improves performance on multi-hop factual recall tasks,\novercoming the limitations of previous locate-then-edit methods"
                },
                "authors": [
                    {
                        "name": "Zhuoran Zhang"
                    },
                    {
                        "name": "Yongxiang Li"
                    },
                    {
                        "name": "Zijian Kan"
                    },
                    {
                        "name": "Keyuan Cheng"
                    },
                    {
                        "name": "Lijie Hu"
                    },
                    {
                        "name": "Di Wang"
                    }
                ],
                "author_detail": {
                    "name": "Di Wang"
                },
                "author": "Di Wang",
                "arxiv_comment": "24 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.06331v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.06331v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.18032v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.18032v3",
                "updated": "2025-02-01T11:36:30Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    11,
                    36,
                    30,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-23T17:02:59Z",
                "published_parsed": [
                    2024,
                    10,
                    23,
                    17,
                    2,
                    59,
                    2,
                    297,
                    0
                ],
                "title": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphTeam: Facilitating Large Language Model-based Graph Analysis via\n  Multi-Agent Collaboration"
                },
                "summary": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphs are widely used for modeling relational data in real-world scenarios,\nsuch as social networks and urban computing. Existing LLM-based graph analysis\napproaches either integrate graph neural networks (GNNs) for specific machine\nlearning tasks, limiting their transferability, or rely solely on LLMs'\ninternal reasoning ability, resulting in suboptimal performance. To address\nthese limitations, we take advantage of recent advances in LLM-based agents,\nwhich have shown capabilities of utilizing external knowledge or tools for\nproblem solving. By simulating human problem-solving strategies such as analogy\nand collaboration, we propose a multi-agent system based on LLMs named\nGraphTeam, for graph analysis. GraphTeam consists of five LLM-based agents from\nthree modules, and the agents with different specialities can collaborate with\neach other to address complex problems. Specifically, (1) input-output\nnormalization module: the question agent extracts and refines four key\narguments from the original question, facilitating the problem understanding,\nand the answer agent organizes the results to meet the output requirement; (2)\nexternal knowledge retrieval module: we first build a knowledge base consisting\nof relevant documentation and experience information, and then the search agent\nretrieves the most relevant entries for each question. (3) problem-solving\nmodule: given the retrieved information from search agent, the coding agent\nuses established algorithms via programming to generate solutions, and in case\nthe coding agent does not work, the reasoning agent will directly compute the\nresults without programming. Extensive experiments on six graph analysis\nbenchmarks demonstrate that GraphTeam achieves state-of-the-art performance\nwith an average 25.85% improvement over the best baseline in terms of accuracy.\nThe code and data are available at https://github.com/BUPT-GAMMA/GraphTeam."
                },
                "authors": [
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Qizhi Chu"
                    },
                    {
                        "name": "Yubin Chen"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Yaoqi Liu"
                    },
                    {
                        "name": "Zekai Yu"
                    },
                    {
                        "name": "Weize Chen"
                    },
                    {
                        "name": "Chen Qian"
                    },
                    {
                        "name": "Chuan Shi"
                    },
                    {
                        "name": "Cheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Cheng Yang"
                },
                "author": "Cheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.18032v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.18032v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.05858v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.05858v5",
                "updated": "2025-02-01T11:07:08Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    11,
                    7,
                    8,
                    5,
                    32,
                    0
                ],
                "published": "2023-10-09T16:52:48Z",
                "published_parsed": [
                    2023,
                    10,
                    9,
                    16,
                    52,
                    48,
                    0,
                    282,
                    0
                ],
                "title": "Distributional Soft Actor-Critic with Three Refinements",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distributional Soft Actor-Critic with Three Refinements"
                },
                "summary": "Reinforcement learning (RL) has shown remarkable success in solving complex\ndecision-making and control tasks. However, many model-free RL algorithms\nexperience performance degradation due to inaccurate value estimation,\nparticularly the overestimation of Q-values, which can lead to suboptimal\npolicies. To address this issue, we previously proposed the Distributional Soft\nActor-Critic (DSAC or DSACv1), an off-policy RL algorithm that enhances value\nestimation accuracy by learning a continuous Gaussian value distribution.\nDespite its effectiveness, DSACv1 faces challenges such as training instability\nand sensitivity to reward scaling, caused by high variance in critic gradients\ndue to return randomness. In this paper, we introduce three key refinements to\nDSACv1 to overcome these limitations and further improve Q-value estimation\naccuracy: expected value substitution, twin value distribution learning, and\nvariance-based critic gradient adjustment. The enhanced algorithm, termed DSAC\nwith Three refinements (DSAC-T or DSACv2), is systematically evaluated across a\ndiverse set of benchmark tasks. Without the need for task-specific\nhyperparameter tuning, DSAC-T consistently matches or outperforms leading\nmodel-free RL algorithms, including SAC, TD3, DDPG, TRPO, and PPO, in all\ntested environments. Additionally, DSAC-T ensures a stable learning process and\nmaintains robust performance across varying reward scales. Its effectiveness is\nfurther demonstrated through real-world application in controlling a wheeled\nrobot, highlighting its potential for deployment in practical robotic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement learning (RL) has shown remarkable success in solving complex\ndecision-making and control tasks. However, many model-free RL algorithms\nexperience performance degradation due to inaccurate value estimation,\nparticularly the overestimation of Q-values, which can lead to suboptimal\npolicies. To address this issue, we previously proposed the Distributional Soft\nActor-Critic (DSAC or DSACv1), an off-policy RL algorithm that enhances value\nestimation accuracy by learning a continuous Gaussian value distribution.\nDespite its effectiveness, DSACv1 faces challenges such as training instability\nand sensitivity to reward scaling, caused by high variance in critic gradients\ndue to return randomness. In this paper, we introduce three key refinements to\nDSACv1 to overcome these limitations and further improve Q-value estimation\naccuracy: expected value substitution, twin value distribution learning, and\nvariance-based critic gradient adjustment. The enhanced algorithm, termed DSAC\nwith Three refinements (DSAC-T or DSACv2), is systematically evaluated across a\ndiverse set of benchmark tasks. Without the need for task-specific\nhyperparameter tuning, DSAC-T consistently matches or outperforms leading\nmodel-free RL algorithms, including SAC, TD3, DDPG, TRPO, and PPO, in all\ntested environments. Additionally, DSAC-T ensures a stable learning process and\nmaintains robust performance across varying reward scales. Its effectiveness is\nfurther demonstrated through real-world application in controlling a wheeled\nrobot, highlighting its potential for deployment in practical robotic tasks."
                },
                "authors": [
                    {
                        "name": "Jingliang Duan"
                    },
                    {
                        "name": "Wenxuan Wang"
                    },
                    {
                        "name": "Liming Xiao"
                    },
                    {
                        "name": "Jiaxin Gao"
                    },
                    {
                        "name": "Shengbo Eben Li"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ya-Qin Zhang"
                    },
                    {
                        "name": "Bo Cheng"
                    },
                    {
                        "name": "Keqiang Li"
                    }
                ],
                "author_detail": {
                    "name": "Keqiang Li"
                },
                "author": "Keqiang Li",
                "arxiv_doi": "10.1109/TPAMI.2025.3537087",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TPAMI.2025.3537087",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.05858v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.05858v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "IEEE Transactions on Pattern Analysis and Machine Intelligence,\n  2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.08589v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.08589v2",
                "updated": "2025-02-01T10:30:17Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    10,
                    30,
                    17,
                    5,
                    32,
                    0
                ],
                "published": "2024-10-11T07:36:14Z",
                "published_parsed": [
                    2024,
                    10,
                    11,
                    7,
                    36,
                    14,
                    4,
                    285,
                    0
                ],
                "title": "Retraining-Free Merging of Sparse MoE via Hierarchical Clustering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retraining-Free Merging of Sparse MoE via Hierarchical Clustering"
                },
                "summary": "Sparse Mixture-of-Experts (SMoE) models represent a significant advancement\nin large language model (LLM) development through their efficient parameter\nutilization. These models achieve substantial performance improvements at\nreduced inference costs. However, the deployment of SMoE models faces\nconstraints from extensive memory requirements of expert components in\nresource-limited environments. To address these limitations, this paper\nintroduces Hierarchical Clustering for Sparsely activated Mixture of Experts\n(HC-SMoE), a task-agnostic expert merging framework for parameter reduction\nwithout retraining. HC-SMoE introduces a novel hierarchical clustering approach\nbased on expert outputs to ensure merging robustness independent of routing\ndecisions. The proposed output-based clustering method enables effective\ncapture of functional relationships between experts for large-scale\narchitectures. We provide theoretical analysis and comprehensive evaluations\nacross multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness\nin state-of-the-art models including Qwen and Mixtral. The experimental results\nvalidate HC-SMoE's superior performance and practical applicability for\nreal-world deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Mixture-of-Experts (SMoE) models represent a significant advancement\nin large language model (LLM) development through their efficient parameter\nutilization. These models achieve substantial performance improvements at\nreduced inference costs. However, the deployment of SMoE models faces\nconstraints from extensive memory requirements of expert components in\nresource-limited environments. To address these limitations, this paper\nintroduces Hierarchical Clustering for Sparsely activated Mixture of Experts\n(HC-SMoE), a task-agnostic expert merging framework for parameter reduction\nwithout retraining. HC-SMoE introduces a novel hierarchical clustering approach\nbased on expert outputs to ensure merging robustness independent of routing\ndecisions. The proposed output-based clustering method enables effective\ncapture of functional relationships between experts for large-scale\narchitectures. We provide theoretical analysis and comprehensive evaluations\nacross multiple zero-shot language tasks to demonstrate HC-SMoE's effectiveness\nin state-of-the-art models including Qwen and Mixtral. The experimental results\nvalidate HC-SMoE's superior performance and practical applicability for\nreal-world deployments."
                },
                "authors": [
                    {
                        "name": "I-Chun Chen"
                    },
                    {
                        "name": "Hsu-Shen Liu"
                    },
                    {
                        "name": "Wei-Fang Sun"
                    },
                    {
                        "name": "Chen-Hao Chao"
                    },
                    {
                        "name": "Yen-Chang Hsu"
                    },
                    {
                        "name": "Chun-Yi Lee"
                    }
                ],
                "author_detail": {
                    "name": "Chun-Yi Lee"
                },
                "author": "Chun-Yi Lee",
                "arxiv_comment": "Code: https://anonymous.4open.science/r/TAMP-11E2",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.08589v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.08589v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.17183v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.17183v2",
                "updated": "2025-02-01T10:18:11Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    10,
                    18,
                    11,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-25T12:26:44Z",
                "published_parsed": [
                    2025,
                    1,
                    25,
                    12,
                    26,
                    44,
                    5,
                    25,
                    0
                ],
                "title": "LLM Evaluation Based on Aerospace Manufacturing Expertise: Automated\n  Generation and Multi-Model Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Evaluation Based on Aerospace Manufacturing Expertise: Automated\n  Generation and Multi-Model Question Answering"
                },
                "summary": "Aerospace manufacturing demands exceptionally high precision in technical\nparameters. The remarkable performance of Large Language Models (LLMs), such as\nGPT-4 and QWen, in Natural Language Processing has sparked industry interest in\ntheir application to tasks including process design, material selection, and\ntool information retrieval. However, LLMs are prone to generating\n\"hallucinations\" in specialized domains, producing inaccurate or false\ninformation that poses significant risks to the quality of aerospace products\nand flight safety. This paper introduces a set of evaluation metrics tailored\nfor LLMs in aerospace manufacturing, aiming to assess their accuracy by\nanalyzing their performance in answering questions grounded in professional\nknowledge. Firstly, key information is extracted through in-depth textual\nanalysis of classic aerospace manufacturing textbooks and guidelines.\nSubsequently, utilizing LLM generation techniques, we meticulously construct\nmultiple-choice questions with multiple correct answers of varying difficulty.\nFollowing this, different LLM models are employed to answer these questions,\nand their accuracy is recorded. Experimental results demonstrate that the\ncapabilities of LLMs in aerospace professional knowledge are in urgent need of\nimprovement. This study provides a theoretical foundation and practical\nguidance for the application of LLMs in aerospace manufacturing, addressing a\ncritical gap in the field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aerospace manufacturing demands exceptionally high precision in technical\nparameters. The remarkable performance of Large Language Models (LLMs), such as\nGPT-4 and QWen, in Natural Language Processing has sparked industry interest in\ntheir application to tasks including process design, material selection, and\ntool information retrieval. However, LLMs are prone to generating\n\"hallucinations\" in specialized domains, producing inaccurate or false\ninformation that poses significant risks to the quality of aerospace products\nand flight safety. This paper introduces a set of evaluation metrics tailored\nfor LLMs in aerospace manufacturing, aiming to assess their accuracy by\nanalyzing their performance in answering questions grounded in professional\nknowledge. Firstly, key information is extracted through in-depth textual\nanalysis of classic aerospace manufacturing textbooks and guidelines.\nSubsequently, utilizing LLM generation techniques, we meticulously construct\nmultiple-choice questions with multiple correct answers of varying difficulty.\nFollowing this, different LLM models are employed to answer these questions,\nand their accuracy is recorded. Experimental results demonstrate that the\ncapabilities of LLMs in aerospace professional knowledge are in urgent need of\nimprovement. This study provides a theoretical foundation and practical\nguidance for the application of LLMs in aerospace manufacturing, addressing a\ncritical gap in the field."
                },
                "authors": [
                    {
                        "name": "Beiming Liu"
                    },
                    {
                        "name": "Zhizhuo Cui"
                    },
                    {
                        "name": "Siteng Hu"
                    },
                    {
                        "name": "Xiaohua Li"
                    },
                    {
                        "name": "Haifeng Lin"
                    },
                    {
                        "name": "Zhengxin Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhengxin Zhang"
                },
                "author": "Zhengxin Zhang",
                "arxiv_comment": "conference paper",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.17183v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.17183v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 90B30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; J.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07959v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07959v2",
                "updated": "2025-02-01T09:30:34Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    30,
                    34,
                    5,
                    32,
                    0
                ],
                "published": "2025-01-14T09:23:30Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    9,
                    23,
                    30,
                    1,
                    14,
                    0
                ],
                "title": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern\n  and Behavior Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-Instruct Few-Shot Jailbreaking: Decompose the Attack into Pattern\n  and Behavior Learning"
                },
                "summary": "Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. focus\non improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special\ntokens into the demos and employing demo-level random search, known as Improved\nFew-Shot Jailbreaking (I-FSJ). Nevertheless, we notice that this method may\nstill require a long context to jailbreak advanced models e.g. 32 shots of\ndemos for Meta-Llama-3-8B-Instruct (Llama-3) \\cite{llama3modelcard}. In this\npaper, we discuss the limitations of I-FSJ and propose Self-Instruct Few-Shot\nJailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search.\nThis framework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, several works have been conducted on jailbreaking Large Language\nModels (LLMs) with few-shot malicious demos. In particular, Zheng et al. focus\non improving the efficiency of Few-Shot Jailbreaking (FSJ) by injecting special\ntokens into the demos and employing demo-level random search, known as Improved\nFew-Shot Jailbreaking (I-FSJ). Nevertheless, we notice that this method may\nstill require a long context to jailbreak advanced models e.g. 32 shots of\ndemos for Meta-Llama-3-8B-Instruct (Llama-3) \\cite{llama3modelcard}. In this\npaper, we discuss the limitations of I-FSJ and propose Self-Instruct Few-Shot\nJailbreaking (Self-Instruct-FSJ) facilitated with the demo-level greedy search.\nThis framework decomposes the FSJ attack into pattern and behavior learning to\nexploit the model's vulnerabilities in a more generalized and efficient way. We\nconduct elaborate experiments to evaluate our method on common open-source\nmodels and compare it with baseline algorithms. Our code is available at\nhttps://github.com/iphosi/Self-Instruct-FSJ."
                },
                "authors": [
                    {
                        "name": "Jiaqi Hua"
                    },
                    {
                        "name": "Wanxu Wei"
                    }
                ],
                "author_detail": {
                    "name": "Wanxu Wei"
                },
                "author": "Wanxu Wei",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07959v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07959v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.06787v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.06787v3",
                "updated": "2025-02-01T09:17:16Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    9,
                    17,
                    16,
                    5,
                    32,
                    0
                ],
                "published": "2024-02-09T21:21:17Z",
                "published_parsed": [
                    2024,
                    2,
                    9,
                    21,
                    21,
                    17,
                    4,
                    40,
                    0
                ],
                "title": "ForestColl: Throughput-Optimal Collective Communications on\n  Heterogeneous Network Fabrics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ForestColl: Throughput-Optimal Collective Communications on\n  Heterogeneous Network Fabrics"
                },
                "summary": "As modern DNN models grow ever larger, collective communications between the\naccelerators (allreduce, etc.) emerge as a significant performance bottleneck.\nDesigning efficient communication schedules is challenging, given today's\nheterogeneous and diverse network fabrics. We present ForestColl, a tool that\ngenerates throughput-optimal schedules for any network topology. ForestColl\nconstructs broadcast/aggregation spanning trees as the communication schedule,\nachieving theoretical optimality. Its schedule generation runs in strongly\npolynomial time and is highly scalable. ForestColl supports any network\nfabrics, including both switching fabrics and direct accelerator connections.\nWe evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms.\nForestColl showed significant improvements over the vendors' own optimized\ncommunication libraries, RCCL and NCCL, across various settings and in LLM\ntraining. ForestColl also outperformed other state-of-the-art schedule\ngeneration techniques with both more efficient generated schedules and\nsubstantially faster schedule generation speed.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As modern DNN models grow ever larger, collective communications between the\naccelerators (allreduce, etc.) emerge as a significant performance bottleneck.\nDesigning efficient communication schedules is challenging, given today's\nheterogeneous and diverse network fabrics. We present ForestColl, a tool that\ngenerates throughput-optimal schedules for any network topology. ForestColl\nconstructs broadcast/aggregation spanning trees as the communication schedule,\nachieving theoretical optimality. Its schedule generation runs in strongly\npolynomial time and is highly scalable. ForestColl supports any network\nfabrics, including both switching fabrics and direct accelerator connections.\nWe evaluated ForestColl on multi-box AMD MI250 and NVIDIA DGX A100 platforms.\nForestColl showed significant improvements over the vendors' own optimized\ncommunication libraries, RCCL and NCCL, across various settings and in LLM\ntraining. ForestColl also outperformed other state-of-the-art schedule\ngeneration techniques with both more efficient generated schedules and\nsubstantially faster schedule generation speed."
                },
                "authors": [
                    {
                        "name": "Liangyu Zhao"
                    },
                    {
                        "name": "Saeed Maleki"
                    },
                    {
                        "name": "Ziyue Yang"
                    },
                    {
                        "name": "Hossein Pourreza"
                    },
                    {
                        "name": "Arvind Krishnamurthy"
                    }
                ],
                "author_detail": {
                    "name": "Arvind Krishnamurthy"
                },
                "author": "Arvind Krishnamurthy",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2305.18461",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.06787v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.06787v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.15594v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.15594v4",
                "updated": "2025-02-01T08:55:51Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    8,
                    55,
                    51,
                    5,
                    32,
                    0
                ],
                "published": "2024-11-23T16:03:35Z",
                "published_parsed": [
                    2024,
                    11,
                    23,
                    16,
                    3,
                    35,
                    5,
                    328,
                    0
                ],
                "title": "A Survey on LLM-as-a-Judge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on LLM-as-a-Judge"
                },
                "summary": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and consistent evaluation is crucial for decision-making across\nnumerous fields, yet it remains a challenging task due to inherent\nsubjectivity, variability, and scale. Large Language Models (LLMs) have\nachieved remarkable success across diverse domains, leading to the emergence of\n\"LLM-as-a-Judge,\" where LLMs are employed as evaluators for complex tasks. With\ntheir ability to process diverse data types and provide scalable,\ncost-effective, and consistent assessments, LLMs present a compelling\nalternative to traditional expert-driven evaluations. However, ensuring the\nreliability of LLM-as-a-Judge systems remains a significant challenge that\nrequires careful design and standardization. This paper provides a\ncomprehensive survey of LLM-as-a-Judge, addressing the core question: How can\nreliable LLM-as-a-Judge systems be built? We explore strategies to enhance\nreliability, including improving consistency, mitigating biases, and adapting\nto diverse assessment scenarios. Additionally, we propose methodologies for\nevaluating the reliability of LLM-as-a-Judge systems, supported by a novel\nbenchmark designed for this purpose. To advance the development and real-world\ndeployment of LLM-as-a-Judge systems, we also discussed practical applications,\nchallenges, and future directions. This survey serves as a foundational\nreference for researchers and practitioners in this rapidly evolving field."
                },
                "authors": [
                    {
                        "name": "Jiawei Gu"
                    },
                    {
                        "name": "Xuhui Jiang"
                    },
                    {
                        "name": "Zhichao Shi"
                    },
                    {
                        "name": "Hexiang Tan"
                    },
                    {
                        "name": "Xuehao Zhai"
                    },
                    {
                        "name": "Chengjin Xu"
                    },
                    {
                        "name": "Wei Li"
                    },
                    {
                        "name": "Yinghan Shen"
                    },
                    {
                        "name": "Shengjie Ma"
                    },
                    {
                        "name": "Honghao Liu"
                    },
                    {
                        "name": "Saizhuo Wang"
                    },
                    {
                        "name": "Kun Zhang"
                    },
                    {
                        "name": "Yuanzhuo Wang"
                    },
                    {
                        "name": "Wen Gao"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jian Guo"
                    }
                ],
                "author_detail": {
                    "name": "Jian Guo"
                },
                "author": "Jian Guo",
                "arxiv_comment": "Corrected typos & more discussion on reasoning models 33 pages, 9\n  figures. arXiv admin note: text overlap with arXiv:2310.05470 by other\n  authors",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.15594v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.15594v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.04531v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.04531v2",
                "updated": "2025-02-01T08:28:28Z",
                "updated_parsed": [
                    2025,
                    2,
                    1,
                    8,
                    28,
                    28,
                    5,
                    32,
                    0
                ],
                "published": "2024-06-06T22:07:50Z",
                "published_parsed": [
                    2024,
                    6,
                    6,
                    22,
                    7,
                    50,
                    3,
                    158,
                    0
                ],
                "title": "TESTEVAL: Benchmarking Large Language Models for Test Case Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TESTEVAL: Benchmarking Large Language Models for Test Case Generation"
                },
                "summary": "Testing plays a crucial role in the software development cycle, enabling the\ndetection of bugs, vulnerabilities, and other undesirable behaviors. To perform\nsoftware testing, testers need to write code snippets that execute the program\nunder test. Recently, researchers have recognized the potential of large\nlanguage models (LLMs) in software testing. However, there remains a lack of\nfair comparisons between different LLMs in terms of test case generation\ncapabilities.\n  In this paper, we propose TESTEVAL, a novel benchmark for test case\ngeneration with LLMs. We collect 210 Python programs from an online programming\nplatform, LeetCode, and design three different tasks: overall coverage,\ntargeted line/branch coverage, and targeted path coverage. We further evaluate\nsixteen popular LLMs, including both commercial and open-source ones, on\nTESTEVAL. We find that generating test cases to cover specific program\nlines/branches/paths is still challenging for current LLMs, indicating a lack\nof ability to comprehend program logic and execution paths. We have\nopen-sourced our dataset and benchmark pipelines at\nhttps://github.com/LLM4SoftwareTesting/TestEval.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing plays a crucial role in the software development cycle, enabling the\ndetection of bugs, vulnerabilities, and other undesirable behaviors. To perform\nsoftware testing, testers need to write code snippets that execute the program\nunder test. Recently, researchers have recognized the potential of large\nlanguage models (LLMs) in software testing. However, there remains a lack of\nfair comparisons between different LLMs in terms of test case generation\ncapabilities.\n  In this paper, we propose TESTEVAL, a novel benchmark for test case\ngeneration with LLMs. We collect 210 Python programs from an online programming\nplatform, LeetCode, and design three different tasks: overall coverage,\ntargeted line/branch coverage, and targeted path coverage. We further evaluate\nsixteen popular LLMs, including both commercial and open-source ones, on\nTESTEVAL. We find that generating test cases to cover specific program\nlines/branches/paths is still challenging for current LLMs, indicating a lack\nof ability to comprehend program logic and execution paths. We have\nopen-sourced our dataset and benchmark pipelines at\nhttps://github.com/LLM4SoftwareTesting/TestEval."
                },
                "authors": [
                    {
                        "name": "Wenhan Wang"
                    },
                    {
                        "name": "Chenyuan Yang"
                    },
                    {
                        "name": "Zhijie Wang"
                    },
                    {
                        "name": "Yuheng Huang"
                    },
                    {
                        "name": "Zhaoyang Chu"
                    },
                    {
                        "name": "Da Song"
                    },
                    {
                        "name": "Lingming Zhang"
                    },
                    {
                        "name": "An Ran Chen"
                    },
                    {
                        "name": "Lei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Lei Ma"
                },
                "author": "Lei Ma",
                "arxiv_comment": "In NAACL 2025 findings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.04531v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.04531v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]