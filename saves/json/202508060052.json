[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02280v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02280v1",
                "updated": "2025-08-04T10:51:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:51:20Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    51,
                    20,
                    0,
                    216,
                    0
                ],
                "title": "OnPair: Short Strings Compression for Fast Random Access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OnPair: Short Strings Compression for Fast Random Access"
                },
                "summary": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present OnPair, a dictionary-based compression algorithm designed to meet\nthe needs of in-memory database systems that require both high compression and\nfast random access. Existing methods either achieve strong compression ratios\nat significant computational and memory cost (e.g., BPE) or prioritize speed at\nthe expense of compression quality (e.g., FSST). OnPair bridges this gap by\nemploying a cache-friendly dictionary construction technique that incrementally\nmerges frequent adjacent substrings in a single sequential pass over a data\nsample. This enables fast, memory-efficient training without tracking global\npair positions, as required by traditional BPE. We also introduce OnPair16, a\nvariant that limits dictionary entries to 16 bytes, enabling faster parsing via\noptimized longest prefix matching. Both variants compress strings\nindependently, supporting fine-grained random access without block-level\noverhead. Experiments on real-world datasets show that OnPair and OnPair16\nachieve compression ratios comparable to BPE while significantly improving\ncompression speed and memory usage."
                },
                "authors": [
                    {
                        "name": "Francesco Gargiulo"
                    },
                    {
                        "name": "Rossano Venturini"
                    }
                ],
                "author_detail": {
                    "name": "Rossano Venturini"
                },
                "author": "Rossano Venturini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02280v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02280v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; E.4; H.3.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19906v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19906v2",
                "updated": "2025-08-04T08:19:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    8,
                    19,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-26T10:34:53Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    10,
                    34,
                    53,
                    5,
                    207,
                    0
                ],
                "title": "CaliDrop: KV Cache Compression with Calibration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaliDrop: KV Cache Compression with Calibration"
                },
                "summary": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) require substantial computational resources\nduring generation. While the Key-Value (KV) cache significantly accelerates\nthis process by storing attention intermediates, its memory footprint grows\nlinearly with sequence length, batch size, and model size, creating a\nbottleneck in long-context scenarios. Various KV cache compression techniques,\nincluding token eviction, quantization, and low-rank projection, have been\nproposed to mitigate this bottleneck, often complementing each other. This\npaper focuses on enhancing token eviction strategies. Token eviction leverages\nthe observation that the attention patterns are often sparse, allowing for the\nremoval of less critical KV entries to save memory. However, this reduction\nusually comes at the cost of notable accuracy degradation, particularly under\nhigh compression ratios. To address this issue, we propose \\textbf{CaliDrop}, a\nnovel strategy that enhances token eviction through calibration. Our\npreliminary experiments show that queries at nearby positions exhibit high\nsimilarity. Building on this observation, CaliDrop performs speculative\ncalibration on the discarded tokens to mitigate the accuracy loss caused by\ntoken eviction. Extensive experiments demonstrate that CaliDrop significantly\nimproves the accuracy of existing token eviction methods."
                },
                "authors": [
                    {
                        "name": "Yi Su"
                    },
                    {
                        "name": "Quantong Qiu"
                    },
                    {
                        "name": "Yuechi Zhou"
                    },
                    {
                        "name": "Juntao Li"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Min Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Min Zhang"
                },
                "author": "Min Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19906v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19906v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21492v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21492v2",
                "updated": "2025-08-04T04:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    4,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-29T04:21:11Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    4,
                    21,
                    11,
                    1,
                    210,
                    0
                ],
                "title": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Cache-Friendliness and Concurrency: A Locality-Optimized\n  In-Memory B-Skiplist"
                },
                "summary": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Skiplists are widely used for in-memory indexing in many key-value stores,\nsuch as RocksDB and LevelDB, due to their ease of implementation and simple\nconcurrency control mechanisms. However, traditional skiplists suffer from poor\ncache locality, as they store only a single element per node, leaving\nperformance on the table. Minimizing last-level cache misses is key to\nmaximizing in-memory index performance, making high cache locality essential.\nIn this paper, we present a practical concurrent B-skiplist that enhances cache\nlocality and performance while preserving the simplicity of traditional\nskiplist structures and concurrency control schemes. Our key contributions\ninclude a top-down, single-pass insertion algorithm for B-skiplists and a\ncorresponding simple and efficient top-down concurrency control scheme. On 128\nthreads, the proposed concurrent B-skiplist achieves between 2x-9x higher\nthroughput compared to state-of-the-art concurrent skiplist implementations,\nincluding Facebook's concurrent skiplist from Folly and the Java\nConcurrentSkipListMap. Furthermore, we find that the B-skiplist achieves\ncompetitive (0.9x-1.7x) throughput on point workloads compared to\nstate-of-the-art cache-optimized tree-based indices (e.g., Masstree). For a\nmore complete picture of the performance, we also measure the latency of\nskiplist and tree-based indices and find that the B-skiplist achieves between\n3.5x-103x lower 99% latency compared to other concurrent skiplists and between\n0.85x-64x lower 99% latency compared to tree-based indices on point workloads\nwith inserts."
                },
                "authors": [
                    {
                        "name": "Yicong Luo"
                    },
                    {
                        "name": "Senhe Hao"
                    },
                    {
                        "name": "Brian Wheatman"
                    },
                    {
                        "name": "Prashant Pandey"
                    },
                    {
                        "name": "Helen Xu"
                    }
                ],
                "author_detail": {
                    "name": "Helen Xu"
                },
                "author": "Helen Xu",
                "arxiv_doi": "10.1145/3754598.3754655",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754655",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.21492v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21492v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Original paper was accepted into ICPP 2025",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.02930v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.02930v4",
                "updated": "2025-08-04T02:47:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    47,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2024-07-03T09:02:05Z",
                "published_parsed": [
                    2024,
                    7,
                    3,
                    9,
                    2,
                    5,
                    2,
                    185,
                    0
                ],
                "title": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timely Requesting for Time-Critical Content Users in Decentralized\n  F-RANs"
                },
                "summary": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rising demand for high-rate and timely communications, fog radio\naccess networks (F-RANs) offer a promising solution. This work investigates age\nof information (AoI) performance in F-RANs, consisting of multiple content\nusers (CUs), enhanced remote radio heads (eRRHs), and content providers (CPs).\nTime-critical CUs need rapid content updates from CPs but cannot communicate\ndirectly with them; instead, eRRHs act as intermediaries. CUs decide whether to\nrequest content from a CP and which eRRH to send the request to, while eRRHs\ndecide whether to command CPs to update content or use cached content. We study\ntwo general classes of policies: (i) oblivious policies, where decision-making\nis independent of historical information, and (ii) non-oblivious policies,\nwhere decisions are influenced by historical information. First, we obtain\nclosed-form expressions for the average AoI of eRRHs under both policy types.\nDue to the complexity of calculating closed-form expressions for CUs, we then\nderive general upper bounds for their average AoI. Next, we identify optimal\npolicies for both types. Under both optimal policies, each CU requests content\nfrom each CP at an equal rate, consolidating all requests to a single eRRH when\ndemand is low or resources are limited, and distributing requests evenly among\neRRHs when demand is high and resources are ample. eRRHs command content from\neach CP at an equal rate under an optimal oblivious policy, while prioritize\nthe CP with the highest age under an optimal non-oblivious policy. Our\nnumerical results validate these theoretical findings."
                },
                "authors": [
                    {
                        "name": "Xingran Chen"
                    },
                    {
                        "name": "Kai Li"
                    },
                    {
                        "name": "Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Kun Yang"
                },
                "author": "Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.02930v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.02930v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.02252v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.02252v2",
                "updated": "2025-08-04T02:17:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    2,
                    17,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-03T08:29:27Z",
                "published_parsed": [
                    2024,
                    12,
                    3,
                    8,
                    29,
                    27,
                    1,
                    338,
                    0
                ],
                "title": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compressing KV Cache for Long-Context LLM Inference with Inter-Layer\n  Attention Similarity"
                },
                "summary": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid expansion of context window sizes in Large Language Models~(LLMs)\nhas enabled them to tackle increasingly complex tasks involving lengthy\ndocuments. However, this progress comes at the cost of a substantial increase\nin memory usage during inference, primarily due to the linear growth of the\nkey-value~(KV) cache. Existing KV cache compression methods often discard less\nrelevant tokens, which can lead to significant performance degradation when\ncritical information is lost. In this paper, we propose \\textsc{PoD}~(Proximal\ntokens over Distant tokens), a novel KV cache compression framework that\nallocates memory according to token importance, retaining less important tokens\nin a more compact, shared form rather than discarding them entirely. Our\napproach is motivated by two key observations: (1) proximal tokens -- those at\nthe beginning and end of the context -- are significantly more important for\nnext-token prediction, and (2) attention scores for distant tokens are highly\nredundant across consecutive layers. Leveraging these insights, \\textsc{PoD}\npreserves the full KV cache for proximal tokens, while for distant tokens, it\nshares key states across layers. Since attention scores are determined by both\nqueries and keys, sharing key states enables multiple layers to reuse a single\nset of keys for distant tokens, substantially reducing KV cache memory without\ndiscarding essential context. We further introduce a lightweight post-training\nadaptation to enable the model to adjust to this new attention-sharing\nstructure. Extensive experiments on both synthetic~(Needle in a Haystack) and\nreal-world long-context benchmarks demonstrate that \\textsc{PoD} reduces KV\ncache memory usage by up to 35\\% without compromising performance. Our method\nis orthogonal to existing token-selection-based techniques and can be combined\nwith them for further KV cache compression."
                },
                "authors": [
                    {
                        "name": "Da Ma"
                    },
                    {
                        "name": "Lu Chen"
                    },
                    {
                        "name": "Situo Zhang"
                    },
                    {
                        "name": "Yuxun Miao"
                    },
                    {
                        "name": "Su Zhu"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Hongshen Xu"
                    },
                    {
                        "name": "Hanqi Li"
                    },
                    {
                        "name": "Shuai Fan"
                    },
                    {
                        "name": "Lei Pan"
                    },
                    {
                        "name": "Kai Yu"
                    }
                ],
                "author_detail": {
                    "name": "Kai Yu"
                },
                "author": "Kai Yu",
                "arxiv_comment": "14 pages, 7 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.02252v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.02252v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01898v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01898v1",
                "updated": "2025-08-03T19:16:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T19:16:40Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    19,
                    16,
                    40,
                    6,
                    215,
                    0
                ],
                "title": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Wireless Video Caching Networks: A\n  Privacy-Preserving Two-Stage Solution"
                },
                "summary": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video caching can significantly improve delivery efficiency and enhance\nquality of video streaming, which constitutes the majority of wireless\ncommunication traffic. Due to limited cache size, caching strategies must be\ndesigned to adapt to and dynamic user demand in order to maximize system\nrevenue. The system revenue depends on the benefits of delivering the requested\nvideos and costs for (a) transporting the files to the users and (b) cache\nreplacement. Since the cache content at any point in time impacts the\nreplacement costs in the future, demand predictions over multiple cache\nplacement slots become an important prerequisite for efficient cache planning.\nMotivated by this, we introduce a novel two-stage privacy-preserving solution\nfor revenue optimization in wireless video caching networks. First, we train a\nTransformer using privacy-preserving federated learning (FL) to predict\nmulti-slot future demands. Given that prediction results are never entirely\naccurate, especially for longer horizons, we further combine global content\npopularity with per-user prediction results to estimate the content demand\ndistribution. Then, in the second stage, we leverage these estimation results\nto find caching strategies that maximize the long-term system revenue. This\nlatter problem takes on the form of a multi-stage knapsack problem, which we\nthen transform to a integer linear program. Our extensive simulation results\ndemonstrate that (i) our FL solution delivers nearly identical performance to\nthat of the ideal centralized solution and outperforms other existing caching\nmethods, and (ii) our novel revenue optimization approach provides deeper\nsystem performance insights than traditional cache hit ratio (CHR)-based\noptimization approaches."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Md-Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "arxiv_comment": "Under review for possible publication in the IEEE Transactions on\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01898v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01898v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01875v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01875v1",
                "updated": "2025-08-03T18:15:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "published": "2025-08-03T18:15:42Z",
                "published_parsed": [
                    2025,
                    8,
                    3,
                    18,
                    15,
                    42,
                    6,
                    215,
                    0
                ],
                "title": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamAgent: Towards Anticipatory Agents for Streaming Video\n  Understanding"
                },
                "summary": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time streaming video understanding in domains such as autonomous driving\nand intelligent surveillance poses challenges beyond conventional offline video\nprocessing, requiring continuous perception, proactive decision making, and\nresponsive interaction based on dynamically evolving visual content. However,\nexisting methods rely on alternating perception-reaction or asynchronous\ntriggers, lacking task-driven planning and future anticipation, which limits\ntheir real-time responsiveness and proactive decision making in evolving video\nstreams. To this end, we propose a StreamAgent that anticipates the temporal\nintervals and spatial regions expected to contain future task-relevant\ninformation to enable proactive and goal-driven responses. Specifically, we\nintegrate question semantics and historical observations through prompting the\nanticipatory agent to anticipate the temporal progression of key events, align\ncurrent observations with the expected future evidence, and subsequently adjust\nthe perception action (e.g., attending to task-relevant regions or continuously\ntracking in subsequent frames). To enable efficient inference, we design a\nstreaming KV-cache memory mechanism that constructs a hierarchical memory\nstructure for selective recall of relevant tokens, enabling efficient semantic\nretrieval while reducing the overhead of storing all tokens in the traditional\nKV-cache. Extensive experiments on streaming and long video understanding tasks\ndemonstrate that our method outperforms existing methods in response accuracy\nand real-time efficiency, highlighting its practical value for real-world\nstreaming scenarios."
                },
                "authors": [
                    {
                        "name": "Haolin Yang"
                    },
                    {
                        "name": "Feilong Tang"
                    },
                    {
                        "name": "Linxiao Zhao"
                    },
                    {
                        "name": "Xiang An"
                    },
                    {
                        "name": "Ming Hu"
                    },
                    {
                        "name": "Huifa Li"
                    },
                    {
                        "name": "Xinlin Zhuang"
                    },
                    {
                        "name": "Boqian Wang"
                    },
                    {
                        "name": "Yifan Lu"
                    },
                    {
                        "name": "Xiaofeng Zhang"
                    },
                    {
                        "name": "Abdalla Swikir"
                    },
                    {
                        "name": "Junjun He"
                    },
                    {
                        "name": "Zongyuan Ge"
                    },
                    {
                        "name": "Imran Razzak"
                    }
                ],
                "author_detail": {
                    "name": "Imran Razzak"
                },
                "author": "Imran Razzak",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01875v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01875v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.16607v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.16607v2",
                "updated": "2025-08-03T10:27:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    3,
                    10,
                    27,
                    19,
                    6,
                    215,
                    0
                ],
                "published": "2025-01-28T00:52:23Z",
                "published_parsed": [
                    2025,
                    1,
                    28,
                    0,
                    52,
                    23,
                    1,
                    28,
                    0
                ],
                "title": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MCTS-SQL: Light-Weight LLMs can Master the Text-to-SQL through Monte\n  Carlo Tree Search"
                },
                "summary": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Text-to-SQL is a fundamental yet challenging task in the NLP area, aiming at\ntranslating natural language questions into SQL queries. While recent advances\nin large language models have greatly improved performance, most existing\napproaches depend on models with tens of billions of parameters or costly APIs,\nlimiting their applicability in resource-constrained environments. For real\nworld, especially on edge devices, it is crucial for Text-to-SQL to ensure\ncost-effectiveness. Therefore, enabling the light-weight models for Text-to-SQL\nis of great practical significance. However, smaller LLMs often struggle with\ncomplicated user instruction, redundant schema linking or syntax correctness.\nTo address these challenges, we propose MCTS-SQL, a novel framework that uses\nMonte Carlo Tree Search to guide SQL generation through multi-step refinement.\nSince the light-weight models' weak performance of single-shot prediction, we\ngenerate better results through several trials with feedback. However, directly\napplying MCTS-based methods inevitably leads to significant time and\ncomputational overhead. Driven by this issue, we propose a token-level\nprefix-cache mechanism that stores prior information during iterations,\neffectively improved the execution speed. Experiments results on the SPIDER and\nBIRD benchmarks demonstrate the effectiveness of our approach. Using a small\nopen-source Qwen2.5-Coder-1.5B, our method outperforms ChatGPT-3.5. When\nleveraging a more powerful model Gemini 2.5 to explore the performance upper\nbound, we achieved results competitive with the SOTA. Our findings demonstrate\nthat even small models can be effectively deployed in practical Text-to-SQL\nsystems with the right strategy."
                },
                "authors": [
                    {
                        "name": "Shuozhi Yuan"
                    },
                    {
                        "name": "Limin Chen"
                    },
                    {
                        "name": "Miaomiao Yuan"
                    },
                    {
                        "name": "Jin Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jin Zhao"
                },
                "author": "Jin Zhao",
                "arxiv_comment": "15 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.16607v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.16607v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19718v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19718v2",
                "updated": "2025-08-02T23:59:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    23,
                    59,
                    11,
                    5,
                    214,
                    0
                ],
                "published": "2025-07-25T23:55:54Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    23,
                    55,
                    54,
                    4,
                    206,
                    0
                ],
                "title": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GSCache: Real-Time Radiance Caching for Volume Path Tracing using 3D\n  Gaussian Splatting"
                },
                "summary": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time path tracing is rapidly becoming the standard for rendering in\nentertainment and professional applications. In scientific visualization,\nvolume rendering plays a crucial role in helping researchers analyze and\ninterpret complex 3D data. Recently, photorealistic rendering techniques have\ngained popularity in scientific visualization, yet they face significant\nchallenges. One of the most prominent issues is slow rendering performance and\nhigh pixel variance caused by Monte Carlo integration. In this work, we\nintroduce a novel radiance caching approach for path-traced volume rendering.\nOur method leverages advances in volumetric scene representation and adapts 3D\nGaussian splatting to function as a multi-level, path-space radiance cache.\nThis cache is designed to be trainable on the fly, dynamically adapting to\nchanges in scene parameters such as lighting configurations and transfer\nfunctions. By incorporating our cache, we achieve less noisy, higher-quality\nimages without increasing rendering costs. To evaluate our approach, we compare\nit against a baseline path tracer that supports uniform sampling and next-event\nestimation and the state-of-the-art for neural radiance caching. Through both\nquantitative and qualitative analyses, we demonstrate that our path-space\nradiance cache is a robust solution that is easy to integrate and significantly\nenhances the rendering quality of volumetric visualization applications while\nmaintaining comparable computational efficiency."
                },
                "authors": [
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "Hamid Gadirov"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19718v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19718v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01488v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01488v1",
                "updated": "2025-08-02T21:00:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T21:00:55Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    21,
                    0,
                    55,
                    5,
                    214,
                    0
                ],
                "title": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PESTO: Real-Time Pitch Estimation with Self-supervised\n  Transposition-equivariant Objective"
                },
                "summary": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce PESTO, a self-supervised learning approach for\nsingle-pitch estimation using a Siamese architecture. Our model processes\nindividual frames of a Variable-$Q$ Transform (VQT) and predicts pitch\ndistributions. The neural network is designed to be equivariant to\ntranslations, notably thanks to a Toeplitz fully-connected layer. In addition,\nwe construct pitch-shifted pairs by translating and cropping the VQT frames and\ntrain our model with a novel class-based transposition-equivariant objective,\neliminating the need for annotated data. Thanks to this architecture and\ntraining objective, our model achieves remarkable performances while being very\nlightweight ($130$k parameters). Evaluations on music and speech datasets\n(MIR-1K, MDB-stem-synth, and PTDB) demonstrate that PESTO not only outperforms\nself-supervised baselines but also competes with supervised methods, exhibiting\nsuperior cross-dataset generalization. Finally, we enhance PESTO's practical\nutility by developing a streamable VQT implementation using cached\nconvolutions. Combined with our model's low latency (less than 10 ms) and\nminimal parameter count, this makes PESTO particularly suitable for real-time\napplications."
                },
                "authors": [
                    {
                        "name": "Alain Riou"
                    },
                    {
                        "name": "Bernardo Torres"
                    },
                    {
                        "name": "Ben Hayes"
                    },
                    {
                        "name": "Stefan Lattner"
                    },
                    {
                        "name": "Gatan Hadjeres"
                    },
                    {
                        "name": "Gal Richard"
                    },
                    {
                        "name": "Geoffroy Peeters"
                    }
                ],
                "author_detail": {
                    "name": "Geoffroy Peeters"
                },
                "author": "Geoffroy Peeters",
                "arxiv_comment": "Accepted to the Transactions of the International Society for Music\n  Information Retrieval",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01488v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01488v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01298v1",
                "updated": "2025-08-02T10:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T10:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    10,
                    12,
                    45,
                    5,
                    214,
                    0
                ],
                "title": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Improving performance of content-centric networks via decentralized\n  coded caching for multi-level popularity and access"
                },
                "summary": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Content-Centric Networking (CCN) offers a novel architectural paradigm that\nseeks to address the inherent limitations of the prevailing Internet Protocol\n(IP)-based networking model. In contrast to the host-centric communication\napproach of IP networks, CCN prioritizes content by enabling direct addressing\nand routing based on content identifiers. The potential performance\nimprovements of CCN can be further amplified through optimized management of\ncoded data storage and transmission strategies. Decentralized Coded Caching\n(DCC) emerges as a promising technique that harnesses the collective caching\npower of distributed network elements. By strategically pre-positioning\nfrequently accessed content closer to potential consumers during periods of low\nnetwork utilization, DCC has the potential to mitigate content transfer rates\nduring peak traffic periods. This paper proposes a series of fundamental\nmodifications to the CCN architecture by integrating DCC. The proposed\nframework incorporates differentiated coding strategies tailored to user access\nprivileges, thereby eliminating the overhead associated with queue-based\nsearching. Additionally, the framework facilitates recoding of uncoded data\nencountered along the content delivery path. These combined methodologies\ndemonstrably enhance network throughput, elevate cache hit ratios, and\nconsequently, reduce content delivery latency compared to conventional CCN\nimplementations."
                },
                "authors": [
                    {
                        "name": "Azadeh Sadat Miraftab"
                    },
                    {
                        "name": "Ahmadreza Montazerolghaem"
                    },
                    {
                        "name": "Behrad Mahboobi"
                    }
                ],
                "author_detail": {
                    "name": "Behrad Mahboobi"
                },
                "author": "Behrad Mahboobi",
                "arxiv_doi": "10.1007/s10586-025-05256-6",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1007/s10586-025-05256-6",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.01298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01261v1",
                "updated": "2025-08-02T08:33:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T08:33:30Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    8,
                    33,
                    30,
                    5,
                    214,
                    0
                ],
                "title": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unifying Mixture of Experts and Multi-Head Latent Attention for\n  Efficient Language Models"
                },
                "summary": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present MoE-MLA-RoPE, a novel architecture combination that combines\nMixture of Experts (MoE) with Multi-head Latent Attention (MLA) and Rotary\nPosition Embeddings (RoPE) for efficient language modeling. Our approach\naddresses the fundamental trade-off between model capacity and computational\nefficiency through three key innovations: (1) fine-grained expert routing with\n64 micro-experts and top-$k$ selection, enabling flexible specialization\nthrough 3.6 * 10^7 possible expert combinations; (2) shared expert isolation\nthat dedicates 2 always active experts for common patterns while routing to 6\nof 62 specialized experts; and (3) gradient-conflict-free load balancing that\nmaintains expert utilization without interfering with primary loss\noptimization.\n  Extensive experiments on models ranging from 17M to 202M parameters\ndemonstrate that MoE-MLA-RoPE with compression ratio r=d/2 achieves 68% KV\ncache memory reduction and 3.2x inference speedup while maintaining competitive\nperplexity (0.8% degradation). Compared to the parameters with 53.9M\nparameters, MoE-MLA-RoPE improves the validation loss by 6.9% over the vanilla\ntransformers while using 42% fewer active parameters per forward pass.\nFLOP-matched experiments reveal even larger gains: 11.1% improvement with 3.2x\ninference acceleration. Automated evaluation using GPT-4 as a judge confirms\nquality improvements in generation, with higher scores on coherence (8.1/10),\ncreativity (7.9/10) and grammatical correctness (8.2/10). Our results establish\nthat architectural novelty, not parameter scaling, defines the efficiency\nfrontier for resource-constrained language model deployment."
                },
                "authors": [
                    {
                        "name": "Sushant Mehta"
                    },
                    {
                        "name": "Raj Dandekar"
                    },
                    {
                        "name": "Rajat Dandekar"
                    },
                    {
                        "name": "Sreedath Panat"
                    }
                ],
                "author_detail": {
                    "name": "Sreedath Panat"
                },
                "author": "Sreedath Panat",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11972v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11972v2",
                "updated": "2025-08-02T06:50:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    50,
                    59,
                    5,
                    214,
                    0
                ],
                "published": "2025-03-15T02:48:27Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    2,
                    48,
                    27,
                    5,
                    74,
                    0
                ],
                "title": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MoDM: Efficient Serving for Image Generation via Mixture-of-Diffusion\n  Models"
                },
                "summary": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion-based text-to-image generation models trade latency for quality:\nsmall models are fast but generate lower-quality images, while large models\nproduce better images but are slow.\n  We present MoDM, a novel caching-based serving system for diffusion models\nthat dynamically balances latency and quality through a mixture of diffusion\nmodels. Unlike prior approaches that rely on model-specific internal features,\nMoDM caches final images, allowing seamless retrieval and reuse across multiple\ndiffusion model families.\n  This design enables adaptive serving by dynamically balancing latency and\nimage quality: using smaller models for cache-hit requests to reduce latency\nwhile reserving larger models for cache-miss requests to maintain quality.\nSmall model image quality is preserved using retrieved cached images.\n  We design a global monitor that optimally allocates GPU resources and\nbalances inference workload, ensuring high throughput while meeting\nservice-level objectives under varying request rates. Our evaluations show that\nMoDM significantly reduces average serving time by 2.5x while retaining image\nquality, making it a practical solution for scalable and resource-efficient\nmodel deployment."
                },
                "authors": [
                    {
                        "name": "Yuchen Xia"
                    },
                    {
                        "name": "Divyam Sharma"
                    },
                    {
                        "name": "Yichao Yuan"
                    },
                    {
                        "name": "Souvik Kundu"
                    },
                    {
                        "name": "Nishil Talati"
                    }
                ],
                "author_detail": {
                    "name": "Nishil Talati"
                },
                "author": "Nishil Talati",
                "arxiv_comment": "To appear in ASPLOS'26",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.11972v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11972v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01225v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01225v1",
                "updated": "2025-08-02T06:43:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "published": "2025-08-02T06:43:43Z",
                "published_parsed": [
                    2025,
                    8,
                    2,
                    6,
                    43,
                    43,
                    5,
                    214,
                    0
                ],
                "title": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Cache Enhanced Prototype Learning for Test-Time Generalization of\n  Vision-Language Models"
                },
                "summary": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In zero-shot setting, test-time adaptation adjusts pre-trained models using\nunlabeled data from the test phase to enhance performance on unknown test\ndistributions. Existing cache-enhanced TTA methods rely on a low-entropy\ncriterion to select samples for prototype construction, assuming intra-class\ncompactness. However, low-entropy samples may be unreliable under distribution\nshifts, and the resulting prototypes may not ensure compact intra-class\ndistributions. This study identifies a positive correlation between\ncache-enhanced performance and intra-class compactness. Based on this\nobservation, we propose a Multi-Cache enhanced Prototype-based Test-Time\nAdaptation (MCP) featuring three caches: an entropy cache for initializing\nprototype representations with low-entropy samples, an align cache for\nintegrating visual and textual information to achieve compact intra-class\ndistributions, and a negative cache for prediction calibration using\nhigh-entropy samples. We further developed MCP++, a framework incorporating\ncross-modal prototype alignment and residual learning, introducing prototype\nresidual fine-tuning. Comparative and ablation experiments across 15 downstream\ntasks demonstrate that the proposed method and framework achieve\nstate-of-the-art generalization performance."
                },
                "authors": [
                    {
                        "name": "Xinyu Chen"
                    },
                    {
                        "name": "Haotian Zhai"
                    },
                    {
                        "name": "Can Zhang"
                    },
                    {
                        "name": "Xiupeng Shi"
                    },
                    {
                        "name": "Ruirui Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruirui Li"
                },
                "author": "Ruirui Li",
                "arxiv_comment": "Accepted by ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01225v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01225v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.19849v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.19849v2",
                "updated": "2025-08-02T00:31:18Z",
                "updated_parsed": [
                    2025,
                    8,
                    2,
                    0,
                    31,
                    18,
                    5,
                    214,
                    0
                ],
                "published": "2025-05-26T11:35:04Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    11,
                    35,
                    4,
                    0,
                    146,
                    0
                ],
                "title": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HIT Model: A Hierarchical Interaction-Enhanced Two-Tower Model for\n  Pre-Ranking Systems"
                },
                "summary": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Online display advertising platforms rely on pre-ranking systems to\nefficiently filter and prioritize candidate ads from large corpora, balancing\nrelevance to users with strict computational constraints. The prevailing\ntwo-tower architecture, though highly efficient due to its decoupled design and\npre-caching, suffers from cross-domain interaction and coarse similarity\nmetrics, undermining its capacity to model complex user-ad relationships. In\nthis study, we propose the Hierarchical Interaction-Enhanced Two-Tower (HIT)\nmodel, a new architecture that augments the two-tower paradigm with two key\ncomponents: $\\textit{generators}$ that pre-generate holistic vectors\nincorporating coarse-grained user-ad interactions through a dual-generator\nframework with a cosine-similarity-based generation loss as the training\nobjective, and $\\textit{multi-head representers}$ that project embeddings into\nmultiple latent subspaces to capture fine-grained, multi-faceted user interests\nand multi-dimensional ad attributes. This design enhances modeling\neffectiveness without compromising inference efficiency. Extensive experiments\non public datasets and large-scale online A/B testing on Tencent's advertising\nplatform demonstrate that HIT significantly outperforms several baselines in\nrelevance metrics, yielding a $1.66\\%$ increase in Gross Merchandise Volume and\na $1.55\\%$ improvement in Return on Investment, alongside similar serving\nlatency to the vanilla two-tower models. The HIT model has been successfully\ndeployed in Tencent's online display advertising system, serving billions of\nimpressions daily. The code is available at\nhttps://github.com/HarveyYang123/HIT_model."
                },
                "authors": [
                    {
                        "name": "Haoqiang Yang"
                    },
                    {
                        "name": "Congde Yuan"
                    },
                    {
                        "name": "Kun Bai"
                    },
                    {
                        "name": "Mengzhuo Guo"
                    },
                    {
                        "name": "Wei Yang"
                    },
                    {
                        "name": "Chao Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Chao Zhou"
                },
                "author": "Chao Zhou",
                "arxiv_comment": "7 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.19849v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.19849v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.01051v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.01051v1",
                "updated": "2025-08-01T20:08:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T20:08:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    20,
                    8,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "QPP-RNG: A Conceptual Quantum System for True Randomness",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QPP-RNG: A Conceptual Quantum System for True Randomness"
                },
                "summary": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose and experimentally demonstrate the \\emph{Quasi-Superposition\nQuantum-inspired System (QSQS)} -- a conceptual quantum system for randomness\ngeneration built on measuring two conjugate observables of a permutation\nsorting process: the deterministic permutation count $n_p$ and the\nfundamentally non-deterministic sorting time $t$. By analogy with quantum\nsystems, these observables are linked by an uncertainty-like constraint:\nalgorithmic determinism ensures structural uniformity, while system-level\nfluctuations introduce irreducible unpredictability. We realize this framework\nconcretely as \\emph{QPP-RNG}, a system-embedded, software-based true random\nnumber generator (TRNG). In QPP-RNG, real-time measurements of sorting time $t$\n-- shaped by CPU pipeline jitter, cache latency, and OS scheduling --\ndynamically reseed the PRNG driving the permutation sequence. Crucially, QSQS\ntransforms initially right-skewed raw distributions of $n_p$ and $t$ into\nnearly uniform outputs after modulo reduction, thanks to internal degeneracies\nthat collapse many distinct states into the same output symbol. Empirical\nresults show that as the repetition factor $m$ increases, output entropy\nconverges toward theoretical maxima: Shannon and min-entropy values approach 8\nbits, chi-squared statistics stabilize near ideal uniformity, and bell curves\nvisually confirm the flattening from skewed to uniform distributions. Beyond\npractical implications, QSQS unifies deterministic algorithmic processes with\nnon-deterministic physical fluctuations, offering a physics-based perspective\nfor engineering true randomness in post-quantum cryptographic systems."
                },
                "authors": [
                    {
                        "name": "Randy Kuang"
                    }
                ],
                "author_detail": {
                    "name": "Randy Kuang"
                },
                "author": "Randy Kuang",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.01051v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.01051v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00647v1",
                "updated": "2025-08-01T14:05:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T14:05:44Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    14,
                    5,
                    44,
                    4,
                    213,
                    0
                ],
                "title": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Study of the HV power supply modules for the CUbesat Solar Polarimeter\n  (CUSP)"
                },
                "summary": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The CUbesat Solar Polarimeter (CUSP) project is a CubeSat mission orbiting\nthe Earth aimed to measure the linear polarization of solar flares in the hard\nX-ray band by means of a Compton scattering polarimeter. CUSP will allow to\nstudy the magnetic reconnection and particle acceleration in the flaring\nmagnetic structures of our star. CUSP is a project in the framework of the\nAlcor Program of the Italian Space Agency aimed to develop new CubeSat\nmissions. CUSP undergoing the Phase B started in December 2024 that will last\nfor 12 month. The Compton polarimeter of the CUSP payload performs coincidence\nmeasurements between plastic scintilaltors and GaGG(Ce) crystals to derive the\npolarization of X-rays. These sensors are readout by Multi Anode\nPhotomultiplier Tubes (MAPMTs) and Avalanche Photodiodes (APDs) respectively.\nBoth sensors need an HV power supply up to -1~kV (for the MAPMT) and +500~V\n(for the APD). We tested precision regulated High Voltage DC/DC Converters by\nHVM Technology Inc. with Sub-Miniature Case Size\n($0.85''\\times0.85''\\times0.60''$) of the SMHV series. These modules are\ncompact and suited for CubeSat missions."
                },
                "authors": [
                    {
                        "name": "Alessandro Lacerenza"
                    },
                    {
                        "name": "Alda Rubini"
                    },
                    {
                        "name": "Andrea Alimenti"
                    },
                    {
                        "name": "Sergio Fabiani"
                    },
                    {
                        "name": "Ettore Del Monte"
                    },
                    {
                        "name": "Riccardo Campana"
                    },
                    {
                        "name": "Mauro Centrone"
                    },
                    {
                        "name": "Enrico Costa"
                    },
                    {
                        "name": "Nicolas De Angelis"
                    },
                    {
                        "name": "Giovanni De Cesare"
                    },
                    {
                        "name": "Sergio Di Cosimo"
                    },
                    {
                        "name": "Giuseppe Di Persio"
                    },
                    {
                        "name": "Abhay Kumar"
                    },
                    {
                        "name": "Pasqualino Loffredo"
                    },
                    {
                        "name": "Giovanni Lombardi"
                    },
                    {
                        "name": "Gabriele Minervini"
                    },
                    {
                        "name": "Fabio Muleri"
                    },
                    {
                        "name": "Paolo Romano"
                    },
                    {
                        "name": "Emanuele Scalise"
                    },
                    {
                        "name": "Enrico Silva"
                    },
                    {
                        "name": "Paolo Soffitta"
                    },
                    {
                        "name": "Davide Albanesi"
                    },
                    {
                        "name": "Ilaria Baffo"
                    },
                    {
                        "name": "Daniele Brienza"
                    },
                    {
                        "name": "Valerio Campamaggiore"
                    },
                    {
                        "name": "Giovanni Cucinella"
                    },
                    {
                        "name": "Andrea Curatolo"
                    },
                    {
                        "name": "Giulia de Iulis"
                    },
                    {
                        "name": "Andrea Del Re"
                    },
                    {
                        "name": "Vito Di Bari"
                    },
                    {
                        "name": "Simone Di Filippo"
                    },
                    {
                        "name": "Immacolata Donnarumma"
                    },
                    {
                        "name": "Pierluigi Fanelli"
                    },
                    {
                        "name": "Nicolas Gagliardi"
                    },
                    {
                        "name": "Paolo Leonetti"
                    },
                    {
                        "name": "Matteo Merge"
                    },
                    {
                        "name": "Dario Modenini"
                    },
                    {
                        "name": "Andrea Negri"
                    },
                    {
                        "name": "Daniele Pecorella"
                    },
                    {
                        "name": "Massimo Perelli"
                    },
                    {
                        "name": "Alice Ponti"
                    },
                    {
                        "name": "Francesca Sbop"
                    },
                    {
                        "name": "Paolo Tortora"
                    },
                    {
                        "name": "Alessandro Turchi"
                    },
                    {
                        "name": "Valerio Vagelli"
                    },
                    {
                        "name": "Emanuele Zaccagnino"
                    },
                    {
                        "name": "Alessandro Zambardi"
                    },
                    {
                        "name": "Costantino Zazza"
                    }
                ],
                "author_detail": {
                    "name": "Costantino Zazza"
                },
                "author": "Costantino Zazza",
                "arxiv_comment": "6 pages, 2 figures, SPIE Optics+Photonics 2025 proceeding",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.space-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00629v1",
                "updated": "2025-08-01T13:40:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:40:52Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    40,
                    52,
                    4,
                    213,
                    0
                ],
                "title": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Energy-Aware CPU Orchestration in O-RAN: A dApp-Driven Lightweight\n  Approach"
                },
                "summary": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The transition toward softwarized Radio Access Networks (RANs), driven by the\nOpen RAN (O-RAN) paradigm, enables flexible, vendor-neutral deployments through\ndisaggregation and virtualization of base station functions. However, this\nshift introduces new challenges in managing CPU resources efficiently under\nstrict real-time constraints. In particular, the interplay between\nlatency-sensitive RAN workloads and general-purpose Operating System (OS)\nschedulers often leads to sub-optimal performance and unnecessary energy\nconsumption. This work proposes a lightweight, programmable distributed\napplication (dApp) deployed at the Distributed Unit (DU) level to dynamically\norchestrate CPU usage. The dApp operates in closed loop with the OS, leveraging\nthread-level telemetry like context switches, Instructions Per Cycle (IPC), and\ncache metrics, to adapt CPU thread affinity, core isolation, and frequency\nscaling in real time. Unlike existing solutions, it requires no access to\nproprietary RAN software, hardware-specific features, or kernel modifications.\nFully compliant with the O-RAN architecture and agnostic to the underlying RAN\nstack, the proposed solution introduces negligible overhead while improving\nenergy efficiency and CPU utilization. Experimental results using a\ncommercial-grade srsRAN deployment demonstrate consistent power savings without\ncompromising real-time processing performance, highlighting the potential of\nlow-latency dApps for fine-grained resource control in next-generation networks"
                },
                "authors": [
                    {
                        "name": "Francisco Crespo"
                    },
                    {
                        "name": "Javier Villegas"
                    },
                    {
                        "name": "Carlos Baena"
                    },
                    {
                        "name": "Eduardo Baena"
                    },
                    {
                        "name": "Sergio Fortes"
                    },
                    {
                        "name": "Raquel Barco"
                    }
                ],
                "author_detail": {
                    "name": "Raquel Barco"
                },
                "author": "Raquel Barco",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00616v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00616v1",
                "updated": "2025-08-01T13:25:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T13:25:28Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    13,
                    25,
                    28,
                    4,
                    213,
                    0
                ],
                "title": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Association and Phase Shifts Design for UAV-mounted Stacked\n  Intelligent Metasurfaces-assisted Communications"
                },
                "summary": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stacked intelligent metasurfaces (SIMs) have emerged as a promising\ntechnology for realizing wave-domain signal processing, while the fixed SIMs\nwill limit the communication performance of the system compared to the mobile\nSIMs. In this work, we consider a UAV-mounted SIMs (UAV-SIMs) assisted\ncommunication system, where UAVs as base stations (BSs) can cache the data\nprocessed by SIMs, and also as mobile vehicles flexibly deploy SIMs to enhance\nthe communication performance. To this end, we formulate a UAV-SIM-based joint\noptimization problem (USBJOP) to comprehensively consider the association\nbetween UAV-SIMs and users, the locations of UAV-SIMs, and the phase shifts of\nUAV-SIMs, aiming to maximize the network capacity. Due to the non-convexity and\nNP-hardness of USBJOP, we decompose it into three sub-optimization problems,\nwhich are the association between UAV-SIMs and users optimization problem\n(AUUOP), the UAV location optimization problem (ULOP), and the UAV-SIM phase\nshifts optimization problem (USPSOP). Then, these three sub-optimization\nproblems are solved by an alternating optimization (AO) strategy. Specifically,\nAUUOP and ULOP are transformed to a convex form and then solved by the CVX\ntool, while we employ a layer-by-layer iterative optimization method for\nUSPSOP. Simulation results verify the effectiveness of the proposed strategy\nunder different simulation setups."
                },
                "authors": [
                    {
                        "name": "Mingzhe Fan"
                    },
                    {
                        "name": "Geng Sun"
                    },
                    {
                        "name": "Hongyang Pan"
                    },
                    {
                        "name": "Jiacheng Wang"
                    },
                    {
                        "name": "Jiancheng An"
                    },
                    {
                        "name": "Hongyang Du"
                    },
                    {
                        "name": "Chau Yuen"
                    }
                ],
                "author_detail": {
                    "name": "Chau Yuen"
                },
                "author": "Chau Yuen",
                "arxiv_comment": "This papar has been submitted to the IEEE Global Communications\n  Conference. arXiv admin note: substantial text overlap with arXiv:2506.23488",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00616v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00616v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00412v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00412v1",
                "updated": "2025-08-01T08:10:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T08:10:54Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    8,
                    10,
                    54,
                    4,
                    213,
                    0
                ],
                "title": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sortblock: Similarity-Aware Feature Reuse for Diffusion Model"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable generative\ncapabilities, particularly benefiting from Transformer architectures that\nenhance visual and artistic fidelity. However, their inherently sequential\ndenoising process results in high inference latency, limiting their deployment\nin real-time scenarios. Existing training-free acceleration approaches\ntypically reuse intermediate features at fixed timesteps or layers, overlooking\nthe evolving semantic focus across denoising stages and Transformer blocks.To\naddress this, we propose Sortblock, a training-free inference acceleration\nframework that dynamically caches block-wise features based on their similarity\nacross adjacent timesteps. By ranking the evolution of residuals, Sortblock\nadaptively determines a recomputation ratio, selectively skipping redundant\ncomputations while preserving generation quality. Furthermore, we incorporate a\nlightweight linear prediction mechanism to reduce accumulated errors in skipped\nblocks.Extensive experiments across various tasks and DiT architectures\ndemonstrate that Sortblock achieves over 2$\\times$ inference speedup with\nminimal degradation in output quality, offering an effective and generalizable\nsolution for accelerating diffusion-based generative models."
                },
                "authors": [
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Zeyu Chen"
                    },
                    {
                        "name": "Yi Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yi Liu"
                },
                "author": "Yi Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00412v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00412v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00370v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00370v1",
                "updated": "2025-08-01T07:03:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "published": "2025-08-01T07:03:16Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    7,
                    3,
                    16,
                    4,
                    213,
                    0
                ],
                "title": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EdgeInfinite-Instruct: Bridging SFT-Based Optimization and NPU-Level\n  Efficiency for Edge Devices"
                },
                "summary": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deploying Transformer-based large language models (LLMs) on\nresource-constrained edge devices for long-sequence tasks remains challenging\ndue to the quadratic time complexity of self-attention and growing Key-Value\n(KV) cache demands. While existing KV cache optimizations improve memory\nefficiency, they often fail to reduce time to first token (TTFT) and may\ndegrade performance through token pruning. Alternative sequence modeling\narchitectures address some of these limitations, but typically require full\nretraining and lack infrastructure support. EdgeInfinite offers an efficient\nsolution by fine-tuning only a small subset of parameters, maintaining quality\nwhile reducing both computational and memory costs, including improved TTFT.\nHowever, its instruction-following ability is limited, and it lacks\nmobile-specific optimizations. To address these issues, we propose\nEdgeInfinite-Instruct, which introduces a Segmented Supervised Fine-Tuning\n(S-SFT) strategy tailored to long-sequence tasks such as summarization and\nquestion answering. We further optimized EdgeInfinite-Instruct for efficient\ndeployment on edge NPUs by employing fine-grained post-training quantization\n(PTQ) to reduce computational demands while maintaining accuracy, and by\nimplementing a fixed-shape computation graph that balances memory usage and\non-device efficiency through scenario-specific customization of input token and\ncache sizes. Experiments on long-context benchmarks and real-world mobile tasks\nshow that our approach improves domain-specific performance while maintaining\nefficiency on NPU-accelerated edge devices."
                },
                "authors": [
                    {
                        "name": "Jiyu Chen"
                    },
                    {
                        "name": "Poh Seng Lim"
                    },
                    {
                        "name": "Shuang Peng"
                    },
                    {
                        "name": "Daxiong Luo"
                    },
                    {
                        "name": "JungHau Foo"
                    },
                    {
                        "name": "Yap Deep"
                    },
                    {
                        "name": "Timothy Lee Jun Jie"
                    },
                    {
                        "name": "Kelvin Teh Kae Wen"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Danyu Feng"
                    },
                    {
                        "name": "Hao-Yun Chen"
                    },
                    {
                        "name": "Peng-Wen Chen"
                    },
                    {
                        "name": "Fangyuan Li"
                    },
                    {
                        "name": "Xiaoxin Chen"
                    },
                    {
                        "name": "Wong Wai Mun"
                    }
                ],
                "author_detail": {
                    "name": "Wong Wai Mun"
                },
                "author": "Wong Wai Mun",
                "arxiv_comment": "9 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00370v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00370v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23387v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23387v2",
                "updated": "2025-08-01T03:43:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    43,
                    24,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-31T10:02:26Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    10,
                    2,
                    26,
                    3,
                    212,
                    0
                ],
                "title": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SGEMM-cube: Emulating FP32 GEMM on Ascend NPUs Using FP16 Cube Units\n  with Precision Recovery"
                },
                "summary": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-precision matrix engines, such as FP16 cube, offer high throughput but\nlack support for full-precision computation. In this work, we propose\nSGEMM-cube, a high-performance algorithm for emulating FP32 general\nmatrix-matrix multiplication (GEMM) using only FP16 computation units on a\nrepresentative AI accelerator. The method decomposes each FP32 operand into two\nFP16 values and compensates for numerical errors through a tunable scaling\nstrategy. A detailed analysis of numerical errors, including underflow\nconditions and precision loss, guides the selection of scaling parameters to\npreserve up to 22 bits of mantissa accuracy. We further investigate the effect\nof computation order on accuracy and demonstrate that a term-wise accumulation\nscheme improves numerical stability over conventional FP32 GEMM in low-exponent\nregimes. Finally, a cache-aware blocking strategy and double-buffered pipeline\nare introduced to overlap memory transfers with computation, enabling\nSGEMM-cube to achieve up to 77\\% of the theoretical FP32-equivalent peak\nperformance on Ascend 910A NPU lacking native FP32 support. Extensive numerical\nexperiments confirm that our method not only recovers the accuracy of native\nFP32 GEMM but also exhibits superior numerical stability under certain\nconditions, due to its structured and error-aware computation order."
                },
                "authors": [
                    {
                        "name": "Weicheng Xue"
                    },
                    {
                        "name": "Baisong Xu"
                    },
                    {
                        "name": "Kai Yang"
                    },
                    {
                        "name": "Yongxiang Liu"
                    },
                    {
                        "name": "Dengdeng Fan"
                    },
                    {
                        "name": "Pengxiang Xu"
                    },
                    {
                        "name": "Yonghong Tian"
                    }
                ],
                "author_detail": {
                    "name": "Yonghong Tian"
                },
                "author": "Yonghong Tian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23387v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23387v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22746v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22746v2",
                "updated": "2025-08-01T03:37:42Z",
                "updated_parsed": [
                    2025,
                    8,
                    1,
                    3,
                    37,
                    42,
                    4,
                    213,
                    0
                ],
                "published": "2025-07-30T15:03:36Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    15,
                    3,
                    36,
                    2,
                    211,
                    0
                ],
                "title": "Next Tokens Denoising for Speech Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Tokens Denoising for Speech Synthesis"
                },
                "summary": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While diffusion and autoregressive (AR) models have significantly advanced\ngenerative modeling, they each present distinct limitations. AR models, which\nrely on causal attention, cannot exploit future context and suffer from slow\ngeneration speeds. Conversely, diffusion models struggle with key-value (KV)\ncaching. To overcome these challenges, we introduce Dragon-FM, a novel\ntext-to-speech (TTS) design that unifies AR and flow-matching. This model\nprocesses 48 kHz audio codec tokens in chunks at a compact rate of 12.5 tokens\nper second. This design enables AR modeling across chunks, ensuring global\ncoherence, while parallel flow-matching within chunks facilitates fast\niterative denoising. Thus, the model leverages KV-cache across chunks and\nutilizes bidirectional context within each chunk. Furthermore, it bridges\ncontinuous and discrete feature modeling, demonstrating that continuous AR\nflow-matching can predict discrete tokens with finite scalar quantizers. This\nefficient codec and fast chunk-autoregressive architecture also make the model\nhighly effective for generating long-form content, such as podcasts.\nExperiments on podcast datasets demonstrate its capability to efficiently\ngenerate high-quality zero-shot podcasts."
                },
                "authors": [
                    {
                        "name": "Yanqing Liu"
                    },
                    {
                        "name": "Ruiqing Xue"
                    },
                    {
                        "name": "Chong Zhang"
                    },
                    {
                        "name": "Yufei Liu"
                    },
                    {
                        "name": "Gang Wang"
                    },
                    {
                        "name": "Bohan Li"
                    },
                    {
                        "name": "Yao Qian"
                    },
                    {
                        "name": "Lei He"
                    },
                    {
                        "name": "Shujie Liu"
                    },
                    {
                        "name": "Sheng Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Sheng Zhao"
                },
                "author": "Sheng Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22746v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22746v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02659v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02659v2",
                "updated": "2025-07-31T21:00:28Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    21,
                    0,
                    28,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-03T14:20:41Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    14,
                    20,
                    41,
                    3,
                    184,
                    0
                ],
                "title": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OmniDraft: A Cross-vocabulary, Online Adaptive Drafter for On-device\n  Speculative Decoding"
                },
                "summary": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding generally dictates having a small, efficient draft model\nthat is either pretrained or distilled offline to a particular target model\nseries, for instance, Llama or Qwen models. However, within online deployment\nsettings, there are two major challenges: 1) usage of a target model that is\nincompatible with the draft model; 2) expectation of latency improvements over\nusage and time. In this work, we propose OmniDraft, a unified framework that\nenables a single draft model to operate with any target model and adapt\ndynamically to user data. We introduce an online n-gram cache with hybrid\ndistillation fine-tuning to address the cross-vocabulary mismatch across draft\nand target models; and further improve decoding speed by leveraging adaptive\ndrafting techniques. OmniDraft is particularly suitable for on-device LLM\napplications where model cost, efficiency and user customization are the major\npoints of contention. This further highlights the need to tackle the above\nchallenges and motivates the \\textit{``one drafter for all''} paradigm. We\nshowcase the proficiency of the OmniDraft framework by performing online\nlearning on math reasoning, coding and text generation tasks. Notably,\nOmniDraft enables a single Llama-68M model to pair with various target models\nincluding Vicuna-7B, Qwen2-7B and Llama3-8B models for speculative decoding;\nand additionally provides up to 1.5-2x speedup."
                },
                "authors": [
                    {
                        "name": "Ramchalam Kinattinkara Ramakrishnan"
                    },
                    {
                        "name": "Zhaocong Yuan"
                    },
                    {
                        "name": "Shaojie Zhuo"
                    },
                    {
                        "name": "Chen Feng"
                    },
                    {
                        "name": "Yicheng Lin"
                    },
                    {
                        "name": "Chenzheng Su"
                    },
                    {
                        "name": "Xiaopeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiaopeng Zhang"
                },
                "author": "Xiaopeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02659v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02659v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22701v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22701v2",
                "updated": "2025-07-31T16:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    16,
                    21,
                    3,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-30T14:10:16Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    14,
                    10,
                    16,
                    2,
                    211,
                    0
                ],
                "title": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAM: A Stability-Aware Cache Manager for Multi-Tenant Embedded Databases"
                },
                "summary": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The co-location of multiple database instances on resource constrained edge\nnodes creates significant cache contention, where traditional schemes are\ninefficient and unstable under dynamic workloads. To address this, we present\nSAM(a Stability-Aware Manager), an autonomic cache manager that establishes\ndecision stability as a first-class design principle. It achieves this through\nits core control policy, AURA(Autonomic Utility-balancing Resource Allocator),\nwhich resolves the classic exploitation-exploration dilemma by synthesizing two\northogonal factors: the H-factor, representing proven historical efficiency\n(exploitation), and the V-factor, for estimated marginal gain (exploration).\nThrough this practical synthesis and adaptive control, SAM achieves sustained\nhigh performance with strategic stability and robustness in volatile\nconditions.\n  Extensive experiments against 14 diverse baselines demonstrate SAM's\nsuperiority. It achieves top-tier throughput while being uniquely resilient to\ncomplex workload shifts and adversarial workloads like cache pollution.\nFurthermore, its decision latency is highly scalable, remaining nearly constant\nas the system grows to 120 databases. Crucially, SAM achieves superior decision\nstability -- maintaining consistent optimization directions despite noise,\navoiding performance oscillations while ensuring predictable Quality of\nService. These results prove that a principled, stability-aware design is\nessential for sustained high performance in real-world, large-scale systems."
                },
                "authors": [
                    {
                        "name": "Haoran Zhang"
                    },
                    {
                        "name": "Decheng Zuo"
                    },
                    {
                        "name": "Yu Yan"
                    },
                    {
                        "name": "Zhiyu Liang"
                    },
                    {
                        "name": "Hongzhi Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hongzhi Wang"
                },
                "author": "Hongzhi Wang",
                "arxiv_comment": "17 pages, 10 figures. An extended version of a paper under review at\n  the VLDB 2026 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22701v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22701v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.2.4; H.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23674v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23674v1",
                "updated": "2025-07-31T15:50:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T15:50:57Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    15,
                    50,
                    57,
                    3,
                    212,
                    0
                ],
                "title": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TweakLLM: A Routing Architecture for Dynamic Tailoring of Cached\n  Responses"
                },
                "summary": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) process millions of queries daily, making\nefficient response caching a compelling optimization for reducing cost and\nlatency. However, preserving relevance to user queries using this approach\nproves difficult due to the personalized nature of chatbot interactions and the\nlimited accuracy of semantic similarity search. To address this, we present\nTweakLLM, a novel routing architecture that employs a lightweight LLM to\ndynamically adapt cached responses to incoming prompts. Through comprehensive\nevaluation, including user studies with side-by-side comparisons, satisfaction\nvoting, as well as multi-agent LLM debates, we demonstrate that TweakLLM\nmaintains response quality comparable to frontier models while significantly\nimproving cache effectiveness. Our results across real-world datasets highlight\nTweakLLM as a scalable, resource-efficient caching solution for high-volume LLM\ndeployments without compromising user experience."
                },
                "authors": [
                    {
                        "name": "Muhammad Taha Cheema"
                    },
                    {
                        "name": "Abeer Aamir"
                    },
                    {
                        "name": "Khawaja Gul Muhammad"
                    },
                    {
                        "name": "Naveed Anwar Bhatti"
                    },
                    {
                        "name": "Ihsan Ayyub Qazi"
                    },
                    {
                        "name": "Zafar Ayyub Qazi"
                    }
                ],
                "author_detail": {
                    "name": "Zafar Ayyub Qazi"
                },
                "author": "Zafar Ayyub Qazi",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23674v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23674v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.21433v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.21433v2",
                "updated": "2025-07-31T07:53:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    53,
                    53,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-29T02:05:51Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    2,
                    5,
                    51,
                    1,
                    210,
                    0
                ],
                "title": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MemShare: Memory Efficient Inference for Large Reasoning Models through\n  KV Cache Reuse"
                },
                "summary": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Reasoning Models (LRMs) have achieved significant advances in\nmathematical reasoning and formal logic tasks. However, their tendency to\ngenerate lengthy chain-of-thought sequences leads to substantial memory\noverhead during inference. We observe that LRMs frequently produce highly\nsimilar intermediate reasoning steps, which correspond to similar KV cache\nstates across layers. Motivated by this observation, we propose MemShare, a\nnovel KV cache management approach that effectively reduces memory overhead.\nMemShare employs a collaborative filtering algorithm to efficiently identify\nreusable KV cache blocks and enables zero copy cache reuse to significantly\nreduce memory overhead, improve throughput while maintaining accuracy.\nExperimental results demonstrate that MemShare delivers up to 84.79\\%\nimprovement in throughput while maintaining better accuracy compared to\nexisting KV cache management methods."
                },
                "authors": [
                    {
                        "name": "Kaiwen Chen"
                    },
                    {
                        "name": "Xin Tan"
                    },
                    {
                        "name": "Minchen Yu"
                    },
                    {
                        "name": "Hong Xu"
                    }
                ],
                "author_detail": {
                    "name": "Hong Xu"
                },
                "author": "Hong Xu",
                "arxiv_comment": "11 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.21433v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.21433v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.01199v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.01199v2",
                "updated": "2025-07-31T07:35:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    35,
                    4,
                    3,
                    212,
                    0
                ],
                "published": "2025-03-03T05:52:02Z",
                "published_parsed": [
                    2025,
                    3,
                    3,
                    5,
                    52,
                    2,
                    0,
                    62,
                    0
                ],
                "title": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LiteGS: A High-performance Framework to Train 3DGS in Subminutes via\n  System and Algorithm Codesign"
                },
                "summary": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D Gaussian Splatting (3DGS) has emerged as promising alternative in 3D\nrepresentation. However, it still suffers from high training cost. This paper\nintroduces LiteGS, a high performance framework that systematically optimizes\nthe 3DGS training pipeline from multiple aspects. At the low-level computation\nlayer, we design a ``warp-based raster'' associated with two hardware-aware\noptimizations to significantly reduce gradient reduction overhead. At the\nmid-level data management layer, we introduce dynamic spatial sorting based on\nMorton coding to enable a performant ``Cluster-Cull-Compact'' pipeline and\nimprove data locality, therefore reducing cache misses. At the top-level\nalgorithm layer, we establish a new robust densification criterion based on the\nvariance of the opacity gradient, paired with a more stable opacity control\nmechanism, to achieve more precise parameter growth. Experimental results\ndemonstrate that LiteGS accelerates the original 3DGS training by up to 13.4x\nwith comparable or superior quality and surpasses the current SOTA in\nlightweight models by up to 1.4x speedup. For high-quality reconstruction\ntasks, LiteGS sets a new accuracy record and decreases the training time by an\norder of magnitude."
                },
                "authors": [
                    {
                        "name": "Kaimin Liao"
                    },
                    {
                        "name": "Hua Wang"
                    },
                    {
                        "name": "Zhi Chen"
                    },
                    {
                        "name": "Luchao Wang"
                    },
                    {
                        "name": "Yaohua Tang"
                    }
                ],
                "author_detail": {
                    "name": "Yaohua Tang"
                },
                "author": "Yaohua Tang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.01199v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.01199v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.23292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.23292v1",
                "updated": "2025-07-31T07:10:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "published": "2025-07-31T07:10:39Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    7,
                    10,
                    39,
                    3,
                    212,
                    0
                ],
                "title": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SequenceLayers: Sequence Processing and Streaming Neural Networks Made\n  Easy"
                },
                "summary": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a neural network layer API and library for sequence modeling,\ndesigned for easy creation of sequence models that can be executed both\nlayer-by-layer (e.g., teacher-forced training) and step-by-step (e.g.,\nautoregressive sampling). To achieve this, layers define an explicit\nrepresentation of their state over time (e.g., a Transformer KV cache, a\nconvolution buffer, an RNN hidden state), and a step method that evolves that\nstate, tested to give identical results to a stateless layer-wise invocation.\nThis and other aspects of the SequenceLayers contract enables complex models to\nbe immediately streamable, mitigates a wide range of common bugs arising in\nboth streaming and parallel sequence processing, and can be implemented in any\ndeep learning library. A composable and declarative API, along with a\ncomprehensive suite of layers and combinators, streamlines the construction of\nproduction-scale models from simple streamable components while preserving\nstrong correctness guarantees. Our current implementations of SequenceLayers\n(JAX, TensorFlow 2) are available at https://github.com/google/sequence-layers."
                },
                "authors": [
                    {
                        "name": "RJ Skerry-Ryan"
                    },
                    {
                        "name": "Julian Salazar"
                    },
                    {
                        "name": "Soroosh Mariooryad"
                    },
                    {
                        "name": "David Kao"
                    },
                    {
                        "name": "Daisy Stanton"
                    },
                    {
                        "name": "Eric Battenberg"
                    },
                    {
                        "name": "Matt Shannon"
                    },
                    {
                        "name": "Ron J. Weiss"
                    },
                    {
                        "name": "Robin Scheibler"
                    },
                    {
                        "name": "Jonas Rothfuss"
                    },
                    {
                        "name": "Tom Bagby"
                    }
                ],
                "author_detail": {
                    "name": "Tom Bagby"
                },
                "author": "Tom Bagby",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.23292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.23292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.07966v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.07966v3",
                "updated": "2025-07-30T16:55:33Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    55,
                    33,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-10T17:47:40Z",
                "published_parsed": [
                    2025,
                    7,
                    10,
                    17,
                    47,
                    40,
                    3,
                    191,
                    0
                ],
                "title": "Scaling RL to Long Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling RL to Long Videos"
                },
                "summary": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce a full-stack framework that scales up reasoning in\nvision-language models (VLMs) to long videos, leveraging reinforcement\nlearning. We address the unique challenges of long video reasoning by\nintegrating three critical components: (1) a large-scale dataset,\nLongVideo-Reason, comprising 104K long video QA pairs with high-quality\nreasoning annotations across diverse domains such as sports, games, and vlogs;\n(2) a two-stage training pipeline that extends VLMs with chain-of-thought\nsupervised fine-tuning (CoT-SFT) and reinforcement learning (RL); and (3) a\ntraining infrastructure for long video RL, named Multi-modal Reinforcement\nSequence Parallelism (MR-SP), which incorporates sequence parallelism and a\nvLLM-based engine tailored for long video, using cached video embeddings for\nefficient rollout and prefilling. In our experiments, LongVILA-R1-7B achieves\nstrong performance on video benchmarks, reaching 65.1% and 71.1% accuracy on\nVideoMME without and with subtitles, respectively, and consistently\noutperforming LongVILA-7B across multiple benchmarks. Moreover, LongVILA-R1-7B\nsupports processing up to 8,192 video frames per video, and configurable FPS\nsettings. Notably, our MR-SP system achieves up to 2.1x speedup on long video\nRL training. In addition, we release our training system for public\navailability that supports RL training on various modalities (video, text, and\naudio), various models (VILA and Qwen series), and even image and video\ngeneration models. On a single A100 node (8 GPUs), it supports RL training on\nhour-long videos (e.g., 3,600 frames)."
                },
                "authors": [
                    {
                        "name": "Yukang Chen"
                    },
                    {
                        "name": "Wei Huang"
                    },
                    {
                        "name": "Baifeng Shi"
                    },
                    {
                        "name": "Qinghao Hu"
                    },
                    {
                        "name": "Hanrong Ye"
                    },
                    {
                        "name": "Ligeng Zhu"
                    },
                    {
                        "name": "Zhijian Liu"
                    },
                    {
                        "name": "Pavlo Molchanov"
                    },
                    {
                        "name": "Jan Kautz"
                    },
                    {
                        "name": "Xiaojuan Qi"
                    },
                    {
                        "name": "Sifei Liu"
                    },
                    {
                        "name": "Hongxu Yin"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "Code at https://github.com/NVlabs/Long-RL and model at\n  https://huggingface.co/Efficient-Large-Model/LongVILA-R1-7B",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.07966v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.07966v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22801v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22801v1",
                "updated": "2025-07-30T16:04:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T16:04:01Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    16,
                    4,
                    1,
                    2,
                    211,
                    0
                ],
                "title": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DSPE: Profit Maximization in Edge-Cloud Storage System using Dynamic\n  Space Partitioning with Erasure Code"
                },
                "summary": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Edge Storage Systems have emerged as a critical enabler of low latency data\naccess in modern cloud networks by bringing storage and computation closer to\nend users. However, the limited storage capacity of edge servers poses\nsignificant challenges in handling high volume and latency sensitive data\naccess requests, particularly under dynamic workloads. In this work, we propose\na profit driven framework that integrates three key mechanisms which are\ncollaborative caching, erasure coding, and elastic storage partitioning. Unlike\ntraditional replication, erasure coding enables space efficient redundancy,\nallowing data to be reconstructed from any subset of K out of K plus M coded\nblocks. We dynamically partition each edge server s storage into private and\npublic regions. The private region is further subdivided among access points\nbased on their incoming request rates, enabling adaptive control over data\nlocality and ownership. We design a data placement and replacement policy that\ndetermines how and where to store or evict coded data blocks to maximize data\naccess within deadlines. While the private region serves requests from local\nAPs, the public region handles cooperative storage requests from neighboring\nservers. Our proposed Dynamic Space Partitioning and Elastic caching strategy\nis evaluated on both synthetic and real world traces from Netflix and Spotify.\nExperimental results show that our method improves overall system profitability\nby approximately 5 to 8% compared to state of the art approaches under varied\nworkload conditions."
                },
                "authors": [
                    {
                        "name": "Shubhradeep Roy"
                    },
                    {
                        "name": "Suvarthi Sarkar"
                    },
                    {
                        "name": "Vivek Verma"
                    },
                    {
                        "name": "Aryabartta Sahu"
                    }
                ],
                "author_detail": {
                    "name": "Aryabartta Sahu"
                },
                "author": "Aryabartta Sahu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22801v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22801v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.22636v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.22636v1",
                "updated": "2025-07-30T12:55:55Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-30T12:55:55Z",
                "published_parsed": [
                    2025,
                    7,
                    30,
                    12,
                    55,
                    55,
                    2,
                    211,
                    0
                ],
                "title": "All-gluon amplitudes with off-shell recursion in multiplet bases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-gluon amplitudes with off-shell recursion in multiplet bases"
                },
                "summary": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficient computation of color-summed QCD amplitudes at high parton\nmultiplicities remains a central challenge for precision collider predictions.\nExisting approaches using trace, color-flow, or adjoint bases suffer from\nnon-orthogonality, which complicates the color algebra and scales poorly with\nmultiplicity. In this work, we present an off-shell recursive framework for\ncomputing all-gluon tree-level amplitudes directly in orthogonal multiplet\nbases. Utilizing Wigner $6j$ coefficients, we construct an algorithm that\nbuilds multiplet-projected off-shell currents from lower-point currents. By\noptimizing the recursion through partial summation and caching, we find that\nthe computational complexity of calculating $n$-gluon color-summed squared\namplitudes scales as $\\mathcal{O}(17^n)$. This demonstrates the potential\ncompetitiveness of multiplet bases for high-multiplicity processes."
                },
                "authors": [
                    {
                        "name": "Oskar Bolinder"
                    },
                    {
                        "name": "Rikkert Frederix"
                    },
                    {
                        "name": "Malin Sjodahl"
                    }
                ],
                "author_detail": {
                    "name": "Malin Sjodahl"
                },
                "author": "Malin Sjodahl",
                "arxiv_comment": "15 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.22636v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.22636v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20984v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20984v2",
                "updated": "2025-07-30T06:29:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    6,
                    29,
                    40,
                    2,
                    211,
                    0
                ],
                "published": "2025-07-28T16:45:14Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    45,
                    14,
                    0,
                    209,
                    0
                ],
                "title": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SmallThinker: A Family of Efficient Large Language Models Natively\n  Trained for Local Deployment"
                },
                "summary": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While frontier large language models (LLMs) continue to push capability\nboundaries, their deployment remains confined to GPU-powered cloud\ninfrastructure. We challenge this paradigm with SmallThinker, a family of LLMs\nnatively designed - not adapted - for the unique constraints of local devices:\nweak computational power, limited memory, and slow storage. Unlike traditional\napproaches that mainly compress existing models built for clouds, we architect\nSmallThinker from the ground up to thrive within these limitations. Our\ninnovation lies in a deployment-aware architecture that transforms constraints\ninto design principles. First, We introduce a two-level sparse structure\ncombining fine-grained Mixture-of-Experts (MoE) with sparse feed-forward\nnetworks, drastically reducing computational demands without sacrificing model\ncapacity. Second, to conquer the I/O bottleneck of slow storage, we design a\npre-attention router that enables our co-designed inference engine to prefetch\nexpert parameters from storage while computing attention, effectively hiding\nstorage latency that would otherwise cripple on-device inference. Third, for\nmemory efficiency, we utilize NoPE-RoPE hybrid sparse attention mechanism to\nslash KV cache requirements. We release SmallThinker-4B-A0.6B and\nSmallThinker-21B-A3B, which achieve state-of-the-art performance scores and\neven outperform larger LLMs. Remarkably, our co-designed system mostly\neliminates the need for expensive GPU hardware: with Q4_0 quantization, both\nmodels exceed 20 tokens/s on ordinary consumer CPUs, while consuming only 1GB\nand 8GB of memory respectively. SmallThinker is publicly available at\nhf.co/PowerInfer/SmallThinker-4BA0.6B-Instruct and\nhf.co/PowerInfer/SmallThinker-21BA3B-Instruct."
                },
                "authors": [
                    {
                        "name": "Yixin Song"
                    },
                    {
                        "name": "Zhenliang Xue"
                    },
                    {
                        "name": "Dongliang Wei"
                    },
                    {
                        "name": "Feiyang Chen"
                    },
                    {
                        "name": "Jianxiang Gao"
                    },
                    {
                        "name": "Junchen Liu"
                    },
                    {
                        "name": "Hangyu Liang"
                    },
                    {
                        "name": "Guangshuo Qin"
                    },
                    {
                        "name": "Chengrong Tian"
                    },
                    {
                        "name": "Bo Wen"
                    },
                    {
                        "name": "Longyu Zhao"
                    },
                    {
                        "name": "Xinrui Zheng"
                    },
                    {
                        "name": "Zeyu Mi"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20984v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20984v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.19442v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.19442v3",
                "updated": "2025-07-30T05:24:46Z",
                "updated_parsed": [
                    2025,
                    7,
                    30,
                    5,
                    24,
                    46,
                    2,
                    211,
                    0
                ],
                "published": "2024-12-27T04:17:57Z",
                "published_parsed": [
                    2024,
                    12,
                    27,
                    4,
                    17,
                    57,
                    4,
                    362,
                    0
                ],
                "title": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Large Language Model Acceleration based on KV Cache\n  Management"
                },
                "summary": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have revolutionized a wide range of domains such\nas natural language processing, computer vision, and multi-modal tasks due to\ntheir ability to comprehend context and perform logical reasoning. However, the\ncomputational and memory demands of LLMs, particularly during inference, pose\nsignificant challenges when scaling them to real-world, long-context, and\nreal-time applications. Key-Value (KV) cache management has emerged as a\ncritical optimization technique for accelerating LLM inference by reducing\nredundant computations and improving memory utilization. This survey provides a\ncomprehensive overview of KV cache management strategies for LLM acceleration,\ncategorizing them into token-level, model-level, and system-level\noptimizations. Token-level strategies include KV cache selection, budget\nallocation, merging, quantization, and low-rank decomposition, while\nmodel-level optimizations focus on architectural innovations and attention\nmechanisms to enhance KV reuse. System-level approaches address memory\nmanagement, scheduling, and hardware-aware designs to improve efficiency across\ndiverse computing environments. Additionally, the survey provides an overview\nof both text and multimodal datasets and benchmarks used to evaluate these\nstrategies. By presenting detailed taxonomies and comparative analyses, this\nwork aims to offer useful insights for researchers and practitioners to support\nthe development of efficient and scalable KV cache management techniques,\ncontributing to the practical deployment of LLMs in real-world applications.\nThe curated paper list for KV cache management is in:\n\\href{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}{https://github.com/TreeAI-Lab/Awesome-KV-Cache-Management}."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Tianhao Tang"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Nicole Hu"
                    },
                    {
                        "name": "Wei Dong"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    }
                ],
                "author_detail": {
                    "name": "Lei Chen"
                },
                "author": "Lei Chen",
                "arxiv_comment": "Accepted to TMLR 2025. The revised version incorporates more papers\n  and has been further polished",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.19442v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.19442v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00904v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00904v1",
                "updated": "2025-07-29T03:08:31Z",
                "updated_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "published": "2025-07-29T03:08:31Z",
                "published_parsed": [
                    2025,
                    7,
                    29,
                    3,
                    8,
                    31,
                    1,
                    210,
                    0
                ],
                "title": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting LLM Inference Performance via Hardware-Agnostic Analytical\n  Modeling"
                },
                "summary": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have been increasingly deployed as local agents\non personal devices with CPUs, NPUs and integrated GPUs. However, forecasting\ninference performance on devices with such heterogeneity remains challenging\ndue to the dynamic compute and memory demands. Existing approaches rely on GPU\nbenchmarking or machine learning-based latency predictors, which are often\nhardware-specific and lack generalizability. To this end, we introduce LIFE, a\nlightweight and modular analytical framework that is comprised of modular\nanalytical model of operators, configurable to characterize LLM inference\nworkloads in a hardware and dataset-agnostic manner. LIFE characterizes the\ninfluence of software and model optimizations, such as quantization, KV cache\ncompression, LoRA adapters, chunked prefill, different attentions, and operator\nfusion, on performance metrics such as time-to-first-token (TTFT),\ntime-per-output-token (TPOT) and tokens-per-second (TPS). LIFE enables\nperformance forecasting using only hardware specifications, such as TOPS and\nmemory bandwidth, without requiring extensive dataset benchmarking. We validate\nLIFE's forecasting with inference on AMD Ryzen CPUs, NPUs, iGPUs and NVIDIA\nV100 GPUs, with Llama2-7B variants, demonstrating the utility of LIFE in\nforecasting LLM performance through lens of system efficiency to enable\nefficient LLM deployment across different hardware platforms."
                },
                "authors": [
                    {
                        "name": "Rajeev Patwari"
                    },
                    {
                        "name": "Ashish Sirasao"
                    },
                    {
                        "name": "Devleena Das"
                    }
                ],
                "author_detail": {
                    "name": "Devleena Das"
                },
                "author": "Devleena Das",
                "arxiv_comment": "10 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00904v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00904v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.24358v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.24358v2",
                "updated": "2025-07-28T20:44:23Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    20,
                    44,
                    23,
                    0,
                    209,
                    0
                ],
                "published": "2025-03-31T17:37:32Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    17,
                    37,
                    32,
                    0,
                    90,
                    0
                ],
                "title": "SQuat: Subspace-orthogonal KV Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SQuat: Subspace-orthogonal KV Cache Quantization"
                },
                "summary": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The key-value (KV) cache accelerates LLMs decoding by storing KV tensors from\npreviously generated tokens. It reduces redundant computation at the cost of\nincreased memory usage. To mitigate this overhead, existing approaches compress\nKV tensors into lower-bit representations; however, quantization errors can\naccumulate as more tokens are generated, potentially resulting in undesired\noutputs. In this paper, we introduce SQuat (Subspace-orthogonal KV cache\nquantization). It first constructs a subspace spanned by query tensors to\ncapture the most critical task-related information. During key tensor\nquantization, it enforces that the difference between the (de)quantized and\noriginal keys remains orthogonal to this subspace, minimizing the impact of\nquantization errors on the attention mechanism's outputs. SQuat requires no\nmodel fine-tuning, no additional calibration dataset for offline learning, and\nis grounded in a theoretical framework we develop. Through numerical\nexperiments, we show that our method reduces peak memory by 2.17 to 2.82,\nimproves throughput by 2.45 to 3.60, and achieves more favorable benchmark\nscores than existing KV cache quantization algorithms."
                },
                "authors": [
                    {
                        "name": "Hao Wang"
                    },
                    {
                        "name": "Ligong Han"
                    },
                    {
                        "name": "Kai Xu"
                    },
                    {
                        "name": "Akash Srivastava"
                    }
                ],
                "author_detail": {
                    "name": "Akash Srivastava"
                },
                "author": "Akash Srivastava",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.24358v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.24358v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2311.13349v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2311.13349v3",
                "updated": "2025-07-28T14:11:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    14,
                    11,
                    53,
                    0,
                    209,
                    0
                ],
                "published": "2023-11-22T12:34:51Z",
                "published_parsed": [
                    2023,
                    11,
                    22,
                    12,
                    34,
                    51,
                    2,
                    326,
                    0
                ],
                "title": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REDS: Resource-Efficient Deep Subnetworks for Dynamic Resource\n  Constraints"
                },
                "summary": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Deep learning models deployed on edge devices frequently encounter resource\nvariability, which arises from fluctuating energy levels, timing constraints,\nor prioritization of other critical tasks within the system. State-of-the-art\nmachine learning pipelines generate resource-agnostic models that are not\ncapable to adapt at runtime. In this work, we introduce Resource-Efficient Deep\nSubnetworks (REDS) to tackle model adaptation to variable resources. In\ncontrast to the state-of-the-art, REDS leverages structured sparsity\nconstructively by exploiting permutation invariance of neurons, which allows\nfor hardware-specific optimizations. Specifically, REDS achieves computational\nefficiency by (1) skipping sequential computational blocks identified by a\nnovel iterative knapsack optimizer, and (2) taking advantage of data cache by\nre-arranging the order of operations in REDS computational graph. REDS supports\nconventional deep networks frequently deployed on the edge and provides\ncomputational benefits even for small and simple networks. We evaluate REDS on\neight benchmark architectures trained on the Visual Wake Words, Google Speech\nCommands, Fashion-MNIST, CIFAR-10 and ImageNet-1K datasets, and test on four\noff-the-shelf mobile and embedded hardware platforms. We provide a theoretical\nresult and empirical evidence demonstrating REDS' outstanding performance in\nterms of submodels' test set accuracy, and demonstrate an adaptation time in\nresponse to dynamic resource constraints of under 40$\\mu$s, utilizing a\nfully-connected network on Arduino Nano 33 BLE."
                },
                "authors": [
                    {
                        "name": "Francesco Corti"
                    },
                    {
                        "name": "Balz Maag"
                    },
                    {
                        "name": "Joachim Schauer"
                    },
                    {
                        "name": "Ulrich Pferschy"
                    },
                    {
                        "name": "Olga Saukh"
                    }
                ],
                "author_detail": {
                    "name": "Olga Saukh"
                },
                "author": "Olga Saukh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2311.13349v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2311.13349v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20677v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20677v1",
                "updated": "2025-07-28T09:59:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T09:59:22Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    9,
                    59,
                    22,
                    0,
                    209,
                    0
                ],
                "title": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Circuit Caches and Compressors for Low Latency, High Throughput\n  Computing"
                },
                "summary": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Utility-scale quantum programs contain operations on the order of $>10^{15}$\nwhich must be prepared and piped from a classical co-processor to the control\nunit of the quantum device. The latency of this process significantly increases\nwith the size of the program: existing high-level classical representations of\nquantum programs are typically memory intensive and do not na\\\"ively\nefficiently scale to the degree required to execute utility-scale programs in\nreal-time. To combat this limitation, we propose the utilization of high-level\nquantum circuit caches and compressors. The first save on the time associated\nwith repetitive tasks and sub-circuits, and the latter are useful for\nrepresenting the programs/circuits in memory-efficient formats. We present\nnumerical evidence that caches and compressors can offer five orders of\nmagnitude lower latencies during the automatic transpilation of extremely large\nquantum circuits."
                },
                "authors": [
                    {
                        "name": "Ioana Moflic"
                    },
                    {
                        "name": "Alan Robertson"
                    },
                    {
                        "name": "Simon J. Devitt"
                    },
                    {
                        "name": "Alexandru Paler"
                    }
                ],
                "author_detail": {
                    "name": "Alexandru Paler"
                },
                "author": "Alexandru Paler",
                "arxiv_comment": "accepted at Q-CORE workshop of the QCE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20677v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20677v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20613v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20613v1",
                "updated": "2025-07-28T08:27:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "published": "2025-07-28T08:27:40Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    8,
                    27,
                    40,
                    0,
                    209,
                    0
                ],
                "title": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache\n  Compression"
                },
                "summary": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large multimodal models (LMMs) have advanced significantly by integrating\nvisual encoders with extensive language models, enabling robust reasoning\ncapabilities. However, compressing LMMs for deployment on edge devices remains\na critical challenge. In this work, we propose an adaptive search algorithm\nthat optimizes sparsity and KV cache compression to enhance LMM efficiency.\nUtilizing the Tree-structured Parzen Estimator, our method dynamically adjusts\npruning ratios and KV cache quantization bandwidth across different LMM layers,\nusing model performance as the optimization objective. This approach uniquely\ncombines pruning with key-value cache quantization and incorporates a fast\npruning technique that eliminates the need for additional fine-tuning or weight\nadjustments, achieving efficient compression without compromising accuracy.\nComprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and\n13B, demonstrate our method superiority over state-of-the-art techniques such\nas SparseGPT and Wanda across various compression levels. Notably, our\nframework automatic allocation of KV cache compression resources sets a new\nstandard in LMM optimization, delivering memory efficiency without sacrificing\nmuch performance."
                },
                "authors": [
                    {
                        "name": "Te Zhang"
                    },
                    {
                        "name": "Yuheng Li"
                    },
                    {
                        "name": "Junxiang Wang"
                    },
                    {
                        "name": "Lujun Li"
                    }
                ],
                "author_detail": {
                    "name": "Lujun Li"
                },
                "author": "Lujun Li",
                "arxiv_comment": "6 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20613v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20613v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01760v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01760v2",
                "updated": "2025-07-28T04:25:58Z",
                "updated_parsed": [
                    2025,
                    7,
                    28,
                    4,
                    25,
                    58,
                    0,
                    209,
                    0
                ],
                "published": "2024-10-02T17:14:47Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    17,
                    14,
                    47,
                    2,
                    276,
                    0
                ],
                "title": "Learning-Augmented Online Caching: New Upper Bounds",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning-Augmented Online Caching: New Upper Bounds"
                },
                "summary": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the problem of learning-augmented online caching in the scenario\nwhen each request is accompanied by a prediction of the next occurrence of the\nrequested page. We improve currently known bounds on the competitive ratio of\nthe BlindOracle algorithm, which evicts a page predicted to be requested last.\nWe also prove a lower bound on the competitive ratio of any randomized\nalgorithm and show that a combination of the BlindOracle with the Marker\nalgorithm achieves a competitive ratio that is optimal up to some constant."
                },
                "authors": [
                    {
                        "name": "Daniel Skachkov"
                    },
                    {
                        "name": "Denis Ponomaryov"
                    },
                    {
                        "name": "Yuri Dorn"
                    },
                    {
                        "name": "Alexander Demin"
                    }
                ],
                "author_detail": {
                    "name": "Alexander Demin"
                },
                "author": "Alexander Demin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01760v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01760v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08161v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08161v2",
                "updated": "2025-07-27T09:58:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    9,
                    58,
                    25,
                    6,
                    208,
                    0
                ],
                "published": "2025-06-09T19:13:16Z",
                "published_parsed": [
                    2025,
                    6,
                    9,
                    19,
                    13,
                    16,
                    0,
                    160,
                    0
                ],
                "title": "GATE: Geometry-Aware Trained Encoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATE: Geometry-Aware Trained Encoding"
                },
                "summary": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The encoding of input parameters is one of the fundamental building blocks of\nneural network algorithms. Its goal is to map the input data to a\nhigher-dimensional space, typically supported by trained feature vectors. The\nmapping is crucial for the efficiency and approximation quality of neural\nnetworks. We propose a novel geometry-aware encoding called GATE that stores\nfeature vectors on the surface of triangular meshes. Our encoding is suitable\nfor neural rendering-related algorithms, for example, neural radiance caching.\nIt also avoids limitations of previous hash-based encoding schemes, such as\nhash collisions, selection of resolution versus scene size, and divergent\nmemory access. Our approach decouples feature vector density from geometry\ndensity using mesh colors, while allowing for finer control over neural network\ntraining and adaptive level-of-detail."
                },
                "authors": [
                    {
                        "name": "Jakub Bokansk"
                    },
                    {
                        "name": "Daniel Meister"
                    },
                    {
                        "name": "Carsten Benthin"
                    }
                ],
                "author_detail": {
                    "name": "Carsten Benthin"
                },
                "author": "Carsten Benthin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08161v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08161v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20173v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20173v1",
                "updated": "2025-07-27T08:25:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T08:25:08Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    8,
                    25,
                    8,
                    6,
                    208,
                    0
                ],
                "title": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-Performance Parallel Optimization of the Fish School Behaviour on\n  the Setonix Platform Using OpenMP"
                },
                "summary": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an in-depth investigation into the high-performance\nparallel optimization of the Fish School Behaviour (FSB) algorithm on the\nSetonix supercomputing platform using the OpenMP framework. Given the\nincreasing demand for enhanced computational capabilities for complex,\nlarge-scale calculations across diverse domains, there's an imperative need for\noptimized parallel algorithms and computing structures. The FSB algorithm,\ninspired by nature's social behavior patterns, provides an ideal platform for\nparallelization due to its iterative and computationally intensive nature. This\nstudy leverages the capabilities of the Setonix platform and the OpenMP\nframework to analyze various aspects of multi-threading, such as thread counts,\nscheduling strategies, and OpenMP constructs, aiming to discern patterns and\nstrategies that can elevate program performance. Experiments were designed to\nrigorously test different configurations, and our results not only offer\ninsights for parallel optimization of FSB on Setonix but also provide valuable\nreferences for other parallel computational research using OpenMP. Looking\nforward, other factors, such as cache behavior and thread scheduling strategies\nat micro and macro levels, hold potential for further exploration and\noptimization."
                },
                "authors": [
                    {
                        "name": "Haitian Wang"
                    },
                    {
                        "name": "Long Qin"
                    }
                ],
                "author_detail": {
                    "name": "Long Qin"
                },
                "author": "Long Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20173v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20173v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20116v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20116v1",
                "updated": "2025-07-27T03:45:07Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "published": "2025-07-27T03:45:07Z",
                "published_parsed": [
                    2025,
                    7,
                    27,
                    3,
                    45,
                    7,
                    6,
                    208,
                    0
                ],
                "title": "Accelerating Containerized Service Delivery at the Network Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Containerized Service Delivery at the Network Edge"
                },
                "summary": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient container image distribution is crucial for enabling machine\nlearning inference at the network edge, where resource limitations and dynamic\nnetwork conditions create significant challenges. In this paper, we present\nPeerSync, a decentralized P2P-based system designed to optimize image\ndistribution in edge environments. PeerSync employs a popularity- and\nnetwork-aware download engine that dynamically adapts to content popularity and\nreal-time network conditions using a sliding window mechanism. PeerSync further\nintegrates automated tracker election for rapid peer discovery and dynamic\ncache management for efficient storage utilization. We implement PeerSync with\n8000+ lines of Rust code and test its performance extensively on both physical\nedge devices and Docker-based emulations. Experimental results show that\nPeerSync delivers a remarkable speed increase of 2.72$\\times$, 1.79$\\times$,\nand 1.28$\\times$ compared to the Baseline, Dragonfly, and Kraken, respectively,\nwhile significantly reducing peak cross-network traffic by 90.72\\% under\ncongested and varying network conditions."
                },
                "authors": [
                    {
                        "name": "Yinuo Deng"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Dongjing Wang"
                    },
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Wenzhuo Qian"
                    },
                    {
                        "name": "Jianwei Yin"
                    },
                    {
                        "name": "Schahram Dustdar"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20116v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20116v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.18300v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.18300v3",
                "updated": "2025-07-27T00:40:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    27,
                    0,
                    40,
                    47,
                    6,
                    208,
                    0
                ],
                "published": "2025-05-23T18:46:10Z",
                "published_parsed": [
                    2025,
                    5,
                    23,
                    18,
                    46,
                    10,
                    4,
                    143,
                    0
                ],
                "title": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Self-Repellent Kernels: History-Driven Target Towards Efficient\n  Nonlinear MCMC on General Graphs"
                },
                "summary": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a history-driven target (HDT) framework in Markov Chain Monte\nCarlo (MCMC) to improve any random walk algorithm on discrete state spaces,\nsuch as general undirected graphs, for efficient sampling from target\ndistribution $\\boldsymbol{\\mu}$. With broad applications in network science and\ndistributed optimization, recent innovations like the self-repellent random\nwalk (SRRW) achieve near-zero variance by prioritizing under-sampled states\nthrough transition kernel modifications based on past visit frequencies.\nHowever, SRRW's reliance on explicit computation of transition probabilities\nfor all neighbors at each step introduces substantial computational overhead,\nwhile its strict dependence on time-reversible Markov chains excludes advanced\nnon-reversible MCMC methods. To overcome these limitations, instead of direct\nmodification of transition kernel, HDT introduces a history-dependent target\ndistribution $\\boldsymbol{\\pi}[\\mathbf{x}]$ to replace the original target\n$\\boldsymbol{\\mu}$ in any graph sampler, where $\\mathbf{x}$ represents the\nempirical measure of past visits. This design preserves lightweight\nimplementation by requiring only local information between the current and\nproposed states and achieves compatibility with both reversible and\nnon-reversible MCMC samplers, while retaining unbiased samples with target\ndistribution $\\boldsymbol{\\mu}$ and near-zero variance performance. Extensive\nexperiments in graph sampling demonstrate consistent performance gains, and a\nmemory-efficient Least Recently Used (LRU) cache ensures scalability to large\ngeneral graphs."
                },
                "authors": [
                    {
                        "name": "Jie Hu"
                    },
                    {
                        "name": "Yi-Ting Ma"
                    },
                    {
                        "name": "Do Young Eun"
                    }
                ],
                "author_detail": {
                    "name": "Do Young Eun"
                },
                "author": "Do Young Eun",
                "arxiv_comment": "Accepted at ICML 2025 (Oral)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.18300v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.18300v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20030v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20030v1",
                "updated": "2025-07-26T18:20:25Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T18:20:25Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    18,
                    20,
                    25,
                    5,
                    207,
                    0
                ],
                "title": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache\n  Compression"
                },
                "summary": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The efficacy of Large Language Models (LLMs) in long-context tasks is often\nhampered by the substantial memory footprint and computational demands of the\nKey-Value (KV) cache. Current compression strategies, including token eviction\nand learned projections, frequently lead to biased representations -- either by\noveremphasizing recent/high-attention tokens or by repeatedly degrading\ninformation from earlier context -- and may require costly model retraining. We\npresent FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,\ntraining-free KV cache compression framework that ensures unbiased information\nretention. FAEDKV operates by transforming the KV cache into the frequency\ndomain using a proposed Infinite-Window Fourier Transform (IWDFT). This\napproach allows for the equalized contribution of all tokens to the compressed\nrepresentation, effectively preserving both early and recent contextual\ninformation. A preliminary frequency ablation study identifies critical\nspectral components for layer-wise, targeted compression. Experiments on\nLongBench benchmark demonstrate FAEDKV's superiority over existing methods by\nup to 22\\%. In addition, our method shows superior, position-agnostic retrieval\naccuracy on the Needle-In-A-Haystack task compared to compression based\napproaches."
                },
                "authors": [
                    {
                        "name": "Runchao Li"
                    },
                    {
                        "name": "Yao Fu"
                    },
                    {
                        "name": "Mu Sheng"
                    },
                    {
                        "name": "Xianxuan Long"
                    },
                    {
                        "name": "Haotian Yu"
                    },
                    {
                        "name": "Pan Li"
                    }
                ],
                "author_detail": {
                    "name": "Pan Li"
                },
                "author": "Pan Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20030v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20030v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03140v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03140v2",
                "updated": "2025-07-26T15:25:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    25,
                    22,
                    5,
                    207,
                    0
                ],
                "published": "2025-04-04T03:30:15Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    3,
                    30,
                    15,
                    4,
                    94,
                    0
                ],
                "title": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Model Reveals What to Cache: Profiling-Based Feature Reuse for Video\n  Diffusion Models"
                },
                "summary": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in diffusion models have demonstrated remarkable capabilities\nin video generation. However, the computational intensity remains a significant\nchallenge for practical applications. While feature caching has been proposed\nto reduce the computational burden of diffusion models, existing methods\ntypically overlook the heterogeneous significance of individual blocks,\nresulting in suboptimal reuse and degraded output quality. To this end, we\naddress this gap by introducing ProfilingDiT, a novel adaptive caching strategy\nthat explicitly disentangles foreground and background-focused blocks. Through\na systematic analysis of attention distributions in diffusion models, we reveal\na key observation: 1) Most layers exhibit a consistent preference for either\nforeground or background regions. 2) Predicted noise shows low inter-step\nsimilarity initially, which stabilizes as denoising progresses. This finding\ninspires us to formulate a selective caching strategy that preserves full\ncomputation for dynamic foreground elements while efficiently caching static\nbackground features. Our approach substantially reduces computational overhead\nwhile preserving visual fidelity. Extensive experiments demonstrate that our\nframework achieves significant acceleration (e.g., 2.01 times speedup for\nWan2.1) while maintaining visual fidelity across comprehensive quality metrics,\nestablishing a viable method for efficient video generation."
                },
                "authors": [
                    {
                        "name": "Xuran Ma"
                    },
                    {
                        "name": "Yexin Liu"
                    },
                    {
                        "name": "Yaofu Liu"
                    },
                    {
                        "name": "Xianfeng Wu"
                    },
                    {
                        "name": "Mingzhe Zheng"
                    },
                    {
                        "name": "Zihao Wang"
                    },
                    {
                        "name": "Ser-Nam Lim"
                    },
                    {
                        "name": "Harry Yang"
                    }
                ],
                "author_detail": {
                    "name": "Harry Yang"
                },
                "author": "Harry Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03140v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03140v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.09720v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.09720v3",
                "updated": "2025-07-26T15:13:56Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    15,
                    13,
                    56,
                    5,
                    207,
                    0
                ],
                "published": "2025-02-13T19:11:40Z",
                "published_parsed": [
                    2025,
                    2,
                    13,
                    19,
                    11,
                    40,
                    3,
                    44,
                    0
                ],
                "title": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NestQuant: Nested Lattice Quantization for Matrix Products and LLMs"
                },
                "summary": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Post-training quantization (PTQ) has emerged as a critical technique for\nefficient deployment of large language models (LLMs). This work proposes\nNestQuant, a novel PTQ scheme for weights and activations that is based on\nself-similar nested lattices. Recent works have mathematically shown such\nquantizers to be information-theoretically optimal for low-precision matrix\nmultiplication. We implement a practical low-complexity version of NestQuant\nbased on Gosset lattice, making it a drop-in quantizer for any matrix\nmultiplication step (e.g., in self-attention, MLP etc). For example, NestQuant\nquantizes weights, KV-cache, and activations of Llama-3-8B to 4 bits, achieving\nperplexity of 6.6 on wikitext2. This represents more than 55% reduction in\nperplexity gap with respect to unquantized model (perplexity of 6.14) compared\nto state-of-the-art Metas SpinQuant (perplexity 7.3), OstQuant (7.3) and QuaRot\n(8.2). Comparisons on bigger models (up to 70B) and on various LLM evaluation\nbenchmarks confirm uniform superiority of NestQuant."
                },
                "authors": [
                    {
                        "name": "Semyon Savkin"
                    },
                    {
                        "name": "Eitan Porat"
                    },
                    {
                        "name": "Or Ordentlich"
                    },
                    {
                        "name": "Yury Polyanskiy"
                    }
                ],
                "author_detail": {
                    "name": "Yury Polyanskiy"
                },
                "author": "Yury Polyanskiy",
                "arxiv_comment": "23 pages; Accepted at the 42nd International Conference on Machine\n  Learning (ICML 2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.09720v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.09720v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.20677v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.20677v2",
                "updated": "2025-07-26T13:33:06Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    13,
                    33,
                    6,
                    5,
                    207,
                    0
                ],
                "published": "2024-12-30T03:05:45Z",
                "published_parsed": [
                    2024,
                    12,
                    30,
                    3,
                    5,
                    45,
                    0,
                    365,
                    0
                ],
                "title": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Align Attention Heads Before Merging Them: An Effective Way for\n  Converting MHA to GQA"
                },
                "summary": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated exceptional performance across\ndiverse natural language processing tasks. However, as the model size and the\ninput sequence's length increase, the linearly increasing key-value (KV) cache\nsignificantly degrades inference throughput. Therefore, grouped-query attention\n(GQA), as an alternative to multi-head attention (MHA), has been widely\nintroduced into LLMs. In this work, we propose a cost-effective method for\nconverting MHA into GQA with any compression ratio of KV heads. The key point\nof our method lies in the application of Procrustes analysis to the attention\nheads, which enhances the similarity among attention heads while preserving\ncomputational invariance, thereby improving the model's post-training\nperformance. Subsequently, we employ $\\mathit{L_0}$ regularization to prune\nredundant parameters. The model after pruning can be adapted to the standard\nGQA framework. Experimental results show that our strategy can compress up to\n87.5\\% KV heads of LLaMA2-7B model and 75\\% KV heads of Sheared-LLaMA-1.3B with\nacceptable performance degradation. Our code is released at\nhttps://github.com/fpcsong/mha2gqa."
                },
                "authors": [
                    {
                        "name": "Qingyun Jin"
                    },
                    {
                        "name": "Xiaohui Song"
                    },
                    {
                        "name": "Feng Zhou"
                    },
                    {
                        "name": "Zengchang Qin"
                    }
                ],
                "author_detail": {
                    "name": "Zengchang Qin"
                },
                "author": "Zengchang Qin",
                "arxiv_comment": "13 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.20677v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.20677v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19823v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19823v1",
                "updated": "2025-07-26T06:43:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "published": "2025-07-26T06:43:14Z",
                "published_parsed": [
                    2025,
                    7,
                    26,
                    6,
                    43,
                    14,
                    5,
                    207,
                    0
                ],
                "title": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HCAttention: Extreme KV Cache Compression via Heterogeneous Attention\n  Computing for LLMs"
                },
                "summary": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Processing long-context inputs with large language models presents a\nsignificant challenge due to the enormous memory requirements of the Key-Value\n(KV) cache during inference. Existing KV cache compression methods exhibit\nnoticeable performance degradation when memory is reduced by more than 85%.\nAdditionally, strategies that leverage GPU-CPU collaboration for approximate\nattention remain underexplored in this setting. We propose HCAttention, a\nheterogeneous attention computation framework that integrates key quantization,\nvalue offloading, and dynamic KV eviction to enable efficient inference under\nextreme memory constraints. The method is compatible with existing transformer\narchitectures and does not require model fine-tuning. Experimental results on\nthe LongBench benchmark demonstrate that our approach preserves the accuracy of\nfull-attention model while shrinking the KV cache memory footprint to 25% of\nits original size. Remarkably, it stays competitive with only 12.5% of the\ncache, setting a new state-of-the-art in LLM KV cache compression. To the best\nof our knowledge, HCAttention is the first to extend the Llama-3-8B model to\nprocess 4 million tokens on a single A100 GPU with 80GB memory."
                },
                "authors": [
                    {
                        "name": "Dongquan Yang"
                    },
                    {
                        "name": "Yifan Yang"
                    },
                    {
                        "name": "Xiaotian Yu"
                    },
                    {
                        "name": "Xianbiao Qi"
                    },
                    {
                        "name": "Rong Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Rong Xiao"
                },
                "author": "Rong Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19823v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19823v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19427v1",
                "updated": "2025-07-25T16:53:13Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T16:53:13Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    16,
                    53,
                    13,
                    4,
                    206,
                    0
                ],
                "title": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Step-3 is Large yet Affordable: Model-system Co-design for\n  Cost-effective Decoding"
                },
                "summary": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) face low hardware efficiency during decoding,\nespecially for long-context reasoning tasks. This paper introduces Step-3, a\n321B-parameter VLM with hardware-aware model-system co-design optimized for\nminimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel\nMulti-Matrix Factorization Attention (MFA) mechanism that significantly reduces\nboth KV cache size and computation while maintaining high attention\nexpressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed\ninference system that decouples attention and Feed-Forward Network (FFN) layers\ninto specialized subsystems. This co-design achieves unprecedented cost\nefficiency: Step-3 significantly reduces theoretical decoding costs compared\nwith models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at\nlonger context. Step-3 achieves low cost while activating 38B parameters per\ntoken (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that\nhardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are\ncritical to cost-effectiveness. We perform a head-to-head comparison with\nDeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs\nachieves a decoding throughput of up to 4,039 tokens per second per GPU under\n50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324\nin the same setup and sets a new Pareto frontier for LLM decoding."
                },
                "authors": [
                    {
                        "name": "StepFun"
                    },
                    {
                        "name": ":"
                    },
                    {
                        "name": "Bin Wang"
                    },
                    {
                        "name": "Bojun Wang"
                    },
                    {
                        "name": "Changyi Wan"
                    },
                    {
                        "name": "Guanzhe Huang"
                    },
                    {
                        "name": "Hanpeng Hu"
                    },
                    {
                        "name": "Haonan Jia"
                    },
                    {
                        "name": "Hao Nie"
                    },
                    {
                        "name": "Mingliang Li"
                    },
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Siyu Chen"
                    },
                    {
                        "name": "Song Yuan"
                    },
                    {
                        "name": "Wuxun Xie"
                    },
                    {
                        "name": "Xiaoniu Song"
                    },
                    {
                        "name": "Xing Chen"
                    },
                    {
                        "name": "Xingping Yang"
                    },
                    {
                        "name": "Xuelin Zhang"
                    },
                    {
                        "name": "Yanbo Yu"
                    },
                    {
                        "name": "Yaoyu Wang"
                    },
                    {
                        "name": "Yibo Zhu"
                    },
                    {
                        "name": "Yimin Jiang"
                    },
                    {
                        "name": "Yu Zhou"
                    },
                    {
                        "name": "Yuanwei Lu"
                    },
                    {
                        "name": "Houyi Li"
                    },
                    {
                        "name": "Jingcheng Hu"
                    },
                    {
                        "name": "Ka Man Lo"
                    },
                    {
                        "name": "Ailin Huang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Bo Li"
                    },
                    {
                        "name": "Boyu Chen"
                    },
                    {
                        "name": "Changxin Miao"
                    },
                    {
                        "name": "Chang Lou"
                    },
                    {
                        "name": "Chen Hu"
                    },
                    {
                        "name": "Chen Xu"
                    },
                    {
                        "name": "Chenfeng Yu"
                    },
                    {
                        "name": "Chengyuan Yao"
                    },
                    {
                        "name": "Daokuan Lv"
                    },
                    {
                        "name": "Dapeng Shi"
                    },
                    {
                        "name": "Deshan Sun"
                    },
                    {
                        "name": "Ding Huang"
                    },
                    {
                        "name": "Dingyuan Hu"
                    },
                    {
                        "name": "Dongqing Pang"
                    },
                    {
                        "name": "Enle Liu"
                    },
                    {
                        "name": "Fajie Zhang"
                    },
                    {
                        "name": "Fanqi Wan"
                    },
                    {
                        "name": "Gulin Yan"
                    },
                    {
                        "name": "Han Zhang"
                    },
                    {
                        "name": "Han Zhou"
                    },
                    {
                        "name": "Hanghao Wu"
                    },
                    {
                        "name": "Hangyu Guo"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Hanshan Zhang"
                    },
                    {
                        "name": "Hao Wu"
                    },
                    {
                        "name": "Haocheng Zhang"
                    },
                    {
                        "name": "Haolong Yan"
                    },
                    {
                        "name": "Haoran Lv"
                    },
                    {
                        "name": "Haoran Wei"
                    },
                    {
                        "name": "Hebin Zhou"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Heng Wang"
                    },
                    {
                        "name": "Hongxin Li"
                    },
                    {
                        "name": "Hongyu Zhou"
                    },
                    {
                        "name": "Hongyuan Wang"
                    },
                    {
                        "name": "Huiyong Guo"
                    },
                    {
                        "name": "Jia Wang"
                    },
                    {
                        "name": "Jiahao Gong"
                    },
                    {
                        "name": "Jialing Xie"
                    },
                    {
                        "name": "Jian Zhou"
                    },
                    {
                        "name": "Jianjian Sun"
                    },
                    {
                        "name": "Jiaoren Wu"
                    },
                    {
                        "name": "Jiaran Zhang"
                    },
                    {
                        "name": "Jiayu Liu"
                    },
                    {
                        "name": "Jie Cheng"
                    },
                    {
                        "name": "Jie Luo"
                    },
                    {
                        "name": "Jie Yan"
                    },
                    {
                        "name": "Jie Yang"
                    },
                    {
                        "name": "Jieyi Hou"
                    },
                    {
                        "name": "Jinguang Zhang"
                    },
                    {
                        "name": "Jinlan Cao"
                    },
                    {
                        "name": "Jisheng Yin"
                    },
                    {
                        "name": "Junfeng Liu"
                    },
                    {
                        "name": "Junhao Huang"
                    },
                    {
                        "name": "Junzhe Lin"
                    },
                    {
                        "name": "Kaijun Tan"
                    },
                    {
                        "name": "Kaixiang Li"
                    },
                    {
                        "name": "Kang An"
                    },
                    {
                        "name": "Kangheng Lin"
                    },
                    {
                        "name": "Kenkun Liu"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Liang Zhao"
                    },
                    {
                        "name": "Liangyu Chen"
                    },
                    {
                        "name": "Lieyu Shi"
                    },
                    {
                        "name": "Liguo Tan"
                    },
                    {
                        "name": "Lin Lin"
                    },
                    {
                        "name": "Lin Zhang"
                    },
                    {
                        "name": "Lina Chen"
                    },
                    {
                        "name": "Liwen Huang"
                    },
                    {
                        "name": "Liying Shi"
                    },
                    {
                        "name": "Longlong Gu"
                    },
                    {
                        "name": "Mei Chen"
                    },
                    {
                        "name": "Mengqiang Ren"
                    },
                    {
                        "name": "Ming Li"
                    },
                    {
                        "name": "Mingzhe Chen"
                    },
                    {
                        "name": "Na Wang"
                    },
                    {
                        "name": "Nan Wu"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Qian Zhao"
                    },
                    {
                        "name": "Qiang Zhang"
                    },
                    {
                        "name": "Qianni Liu"
                    },
                    {
                        "name": "Qiaohui Chen"
                    },
                    {
                        "name": "Qiling Wu"
                    },
                    {
                        "name": "Qinglin He"
                    },
                    {
                        "name": "Qinyuan Tan"
                    },
                    {
                        "name": "Qiufeng Wang"
                    },
                    {
                        "name": "Qiuping Wu"
                    },
                    {
                        "name": "Qiuyan Liang"
                    },
                    {
                        "name": "Quan Sun"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Ruihang Miao"
                    },
                    {
                        "name": "Ruosi Wan"
                    },
                    {
                        "name": "Ruyan Guo"
                    },
                    {
                        "name": "Shangwu Zhong"
                    },
                    {
                        "name": "Shaoliang Pang"
                    },
                    {
                        "name": "Shengjie Fan"
                    },
                    {
                        "name": "Shijie Shang"
                    },
                    {
                        "name": "Shilei Jiang"
                    },
                    {
                        "name": "Shiliang Yang"
                    },
                    {
                        "name": "Shiming Hao"
                    },
                    {
                        "name": "Shuli Gao"
                    },
                    {
                        "name": "Siming Huang"
                    },
                    {
                        "name": "Siqi Liu"
                    },
                    {
                        "name": "Tiancheng Cao"
                    },
                    {
                        "name": "Tianhao Cheng"
                    },
                    {
                        "name": "Tianhao Peng"
                    },
                    {
                        "name": "Wang You"
                    },
                    {
                        "name": "Wei Ji"
                    },
                    {
                        "name": "Wen Sun"
                    },
                    {
                        "name": "Wenjin Deng"
                    },
                    {
                        "name": "Wenqing He"
                    },
                    {
                        "name": "Wenzhen Zheng"
                    },
                    {
                        "name": "Xi Chen"
                    },
                    {
                        "name": "Xiangwen Kong"
                    },
                    {
                        "name": "Xianzhen Luo"
                    },
                    {
                        "name": "Xiaobo Yang"
                    },
                    {
                        "name": "Xiaojia Liu"
                    },
                    {
                        "name": "Xiaoxiao Ren"
                    },
                    {
                        "name": "Xin Han"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Xin Wu"
                    },
                    {
                        "name": "Xu Zhao"
                    },
                    {
                        "name": "Yanan Wei"
                    },
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Yangguang Li"
                    },
                    {
                        "name": "Yangshijie Xu"
                    },
                    {
                        "name": "Yanming Xu"
                    },
                    {
                        "name": "Yaqiang Shi"
                    },
                    {
                        "name": "Yeqing Shen"
                    },
                    {
                        "name": "Yi Yang"
                    },
                    {
                        "name": "Yifei Yang"
                    },
                    {
                        "name": "Yifeng Gong"
                    },
                    {
                        "name": "Yihan Chen"
                    },
                    {
                        "name": "Yijing Yang"
                    },
                    {
                        "name": "Yinmin Zhang"
                    },
                    {
                        "name": "Yizhuang Zhou"
                    },
                    {
                        "name": "Yuanhao Ding"
                    },
                    {
                        "name": "Yuantao Fan"
                    },
                    {
                        "name": "Yuanzhen Yang"
                    },
                    {
                        "name": "Yuchu Luo"
                    },
                    {
                        "name": "Yue Peng"
                    },
                    {
                        "name": "Yufan Lu"
                    },
                    {
                        "name": "Yuhang Deng"
                    },
                    {
                        "name": "Yuhe Yin"
                    },
                    {
                        "name": "Yujie Liu"
                    },
                    {
                        "name": "Yukun Chen"
                    },
                    {
                        "name": "Yuling Zhao"
                    },
                    {
                        "name": "Yun Mou"
                    },
                    {
                        "name": "Yunlong Li"
                    },
                    {
                        "name": "Yunzhou Ju"
                    },
                    {
                        "name": "Yusheng Li"
                    },
                    {
                        "name": "Yuxiang Yang"
                    },
                    {
                        "name": "Yuxiang Zhang"
                    },
                    {
                        "name": "Yuyang Chen"
                    },
                    {
                        "name": "Zejia Weng"
                    },
                    {
                        "name": "Zhe Xie"
                    },
                    {
                        "name": "Zheng Ge"
                    },
                    {
                        "name": "Zheng Gong"
                    },
                    {
                        "name": "Zhenyi Lu"
                    },
                    {
                        "name": "Zhewei Huang"
                    },
                    {
                        "name": "Zhichao Chang"
                    },
                    {
                        "name": "Zhiguo Huang"
                    },
                    {
                        "name": "Zhirui Wang"
                    },
                    {
                        "name": "Zidong Yang"
                    },
                    {
                        "name": "Zili Wang"
                    },
                    {
                        "name": "Ziqi Wang"
                    },
                    {
                        "name": "Zixin Zhang"
                    },
                    {
                        "name": "Binxing Jiao"
                    },
                    {
                        "name": "Daxin Jiang"
                    },
                    {
                        "name": "Heung-Yeung Shum"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiangyu Zhang"
                },
                "author": "Xiangyu Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.19367v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.19367v1",
                "updated": "2025-07-25T15:17:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "published": "2025-07-25T15:17:29Z",
                "published_parsed": [
                    2025,
                    7,
                    25,
                    15,
                    17,
                    29,
                    4,
                    206,
                    0
                ],
                "title": "Empowering IoT Firmware Secure Update with Customization Rights",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empowering IoT Firmware Secure Update with Customization Rights"
                },
                "summary": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Firmware updates remain the primary line of defense for IoT devices; however,\nthe update channel itself has become a well-established attack vector. Existing\ndefenses mainly focus on securing monolithic firmware images, leaving\nmodule-level customization -a growing user demand-largely unprotected and\ninsufficiently explored. To address this gap, we conduct a pilot study on the\nupdate workflows of 200 Linux-based IoT devices across 23 vendors, uncovering\nfive previously undocumented vulnerabilities caused by customization practices.\nA broader analysis of update-related CVEs from 2020 to 2024 reveals that over\nhalf originate from customization-induced issues. These findings highlight a\ncritical yet underexamined reality: as customization increases, so does the\nattack surface, while current defenses fail to keep pace. We propose IMUP\n(Integrity-Centric Modular Update Platform), the first framework to address two\nkey challenges: constructing a trustworthy cross-module integrity chain and\nscaling update performance under mass customization. IMUP combines three\ntechniques: per-module chameleon hashing for integrity, server-side\nproof-of-work offloading to reduce device overhead, and server-side caching to\nreuse module combinations, minimizing rebuild costs. Security analysis shows\nthat even when 95 percent of secret keys are exposed, forging a valid image\nincurs over 300 times the cost of the legitimate server. Experiments on\nheterogeneous IoT devices demonstrate that IMUP reduces server-side generation\ntime by 2.9 times and device downtime by 5.9 times compared to a\npackage-manager baseline."
                },
                "authors": [
                    {
                        "name": "Weihao Chen"
                    },
                    {
                        "name": "Yansong Gao"
                    },
                    {
                        "name": "Boyu Kuang"
                    },
                    {
                        "name": "Jin B. Hong"
                    },
                    {
                        "name": "Yuqing Zhang"
                    },
                    {
                        "name": "Anmin Fu"
                    }
                ],
                "author_detail": {
                    "name": "Anmin Fu"
                },
                "author": "Anmin Fu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.19367v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.19367v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.01802v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.01802v2",
                "updated": "2025-07-24T19:44:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    19,
                    44,
                    36,
                    3,
                    205,
                    0
                ],
                "published": "2025-02-03T20:30:25Z",
                "published_parsed": [
                    2025,
                    2,
                    3,
                    20,
                    30,
                    25,
                    0,
                    34,
                    0
                ],
                "title": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "General kinetic ion induced electron emission model for metallic walls\n  applied to biased Z-pinch electrodes"
                },
                "summary": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A kinetic ion induced electron emission (IIEE) model for general applications\nis developed to obtain the emitted electron energy spectrum for a distribution\nof ion impacts on a metallic surface. We assume an ionization cascade mechanism\nand use empirical models for the ion and electron stopping powers. The emission\nspectrum and the secondary electron yield (SEY) are validated for a variety of\nmaterials. The IIEE model is used to study the effect of IIEE on the\nplasma-material interactions of Z-pinch electrodes. Un-magnetized\nBoltzmann-Poisson simulations are performed for a Z-pinch plasma doubly bounded\nby two biased copper electrodes with and without IIEE at bias potentials from 0\nto 9 kV. At the anode, the SEY decreases from 0 to 1 kV, but then increases at\nhigher bias potentials. At the cathode, the SEY is much larger due to higher\nenergy ion bombardment and grows with bias potential. As the bias potential\nincreases, the emitted cathode electrons are accelerated to higher energies\ninto the domain collisionally heating the plasma. Above 1 kV, the heating is\nstrong enough to increase the plasma potential. Despite SEY greater than 1,\nonly a classical sheath forms as opposed to a space-charge limited or inverse\nsheath due to the emitted electron flux not reaching the space charge current\nsaturation limits. Furthermore, the current in the emissionless cases saturates\nto a value lower than experiment. With IIEE, the current does not saturate and\ncontinues to increase with the 4 kV case matching most closely with experiment."
                },
                "authors": [
                    {
                        "name": "Chirag R. Skolar"
                    },
                    {
                        "name": "Kolter Bradshaw"
                    },
                    {
                        "name": "Manaure Francisquez"
                    },
                    {
                        "name": "Lucio Murillo"
                    },
                    {
                        "name": "Vignesh Krishna Kumar"
                    },
                    {
                        "name": "Bhuvana Srinivasan"
                    }
                ],
                "author_detail": {
                    "name": "Bhuvana Srinivasan"
                },
                "author": "Bhuvana Srinivasan",
                "arxiv_comment": "21 pages, 14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.01802v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.01802v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16870v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16870v2",
                "updated": "2025-07-24T17:30:12Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    17,
                    30,
                    12,
                    3,
                    205,
                    0
                ],
                "published": "2025-03-21T05:58:18Z",
                "published_parsed": [
                    2025,
                    3,
                    21,
                    5,
                    58,
                    18,
                    4,
                    80,
                    0
                ],
                "title": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Logit Sampling: Accelerating Knowledge Distillation in LLMs"
                },
                "summary": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Knowledge distillation can be a cost-effective technique to distill knowledge\nin Large Language Models, if the teacher output logits can be pre-computed and\ncached. However, successfully applying this to pre-training remains largely\nunexplored. In this work, we prove that naive approaches for sparse knowledge\ndistillation such as caching Top-K probabilities, while intuitive, provide\nbiased estimates of teacher probability distribution to the student, resulting\nin suboptimal performance and calibration. We propose an\nimportance-sampling-based method `Random Sampling Knowledge Distillation',\nwhich provides unbiased estimates, preserves the gradient in expectation, and\nrequires storing significantly sparser logits. Our method enables faster\ntraining of student models with marginal overhead (<10%) compared to\ncross-entropy based training, while maintaining competitive performance\ncompared to full distillation, across a range of model sizes from 300M to 3B."
                },
                "authors": [
                    {
                        "name": "Anshumann"
                    },
                    {
                        "name": "Mohd Abbas Zaidi"
                    },
                    {
                        "name": "Akhil Kedia"
                    },
                    {
                        "name": "Jinwoo Ahn"
                    },
                    {
                        "name": "Taehwak Kwon"
                    },
                    {
                        "name": "Kangwook Lee"
                    },
                    {
                        "name": "Haejun Lee"
                    },
                    {
                        "name": "Joohyung Lee"
                    }
                ],
                "author_detail": {
                    "name": "Joohyung Lee"
                },
                "author": "Joohyung Lee",
                "arxiv_comment": "Accepted as Oral paper at ACL 2025. Source code is available at\n  https://github.com/akhilkedia/RandomSamplingKD . Anshumann, Mohd Abbas Zaidi\n  and Akhil Kedia have Equal Contribution",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.16870v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16870v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04704v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04704v2",
                "updated": "2025-07-24T16:25:51Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    16,
                    25,
                    51,
                    3,
                    205,
                    0
                ],
                "published": "2025-04-07T03:22:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    3,
                    22,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LagKV: Lag-Relative Information of the KV Cache Tells Which Tokens Are\n  Important"
                },
                "summary": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing size of the Key-Value (KV) cache during the Large Language\nModels long-context inference is the main obstacle for its balance between the\ndeployment cost and task accuracy. To reduce the KV cache size in such\nscenarios, most previous efforts leveraged on the attention weight to evict\nnon-critical cache tokens. But there is a trade-off in those methods, they\nusually require major modification of the inference infrastructure and\nsignificant computation overhead. Based on the fact that the Large Language\nmodels are autoregressive models, we propose LagKV, a KV compression strategy\nonly relying on straight forward comparison among KV themselves. It is a\ntotally attention free method which offers easy integration to the main stream\ninference platform and comparable performance comparing to other complicated KV\ncompression methods. Results on RULER benchmark show that, our approach\noutperforms SnapKV and StreamingLLM in different compression ratios. Especially\nin the 64-digit passkey retrieval task, our method outperforms the attention\nweight based method $H_2O$ over $50\\%$ with same compression ratios. Our code\nis available at https://github.com/AI-Lab-China-Merchants-Bank/LagKV."
                },
                "authors": [
                    {
                        "name": "Manlai Liang"
                    },
                    {
                        "name": "JiaMing Zhang"
                    },
                    {
                        "name": "Xiong Li"
                    },
                    {
                        "name": "Jinlong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jinlong Li"
                },
                "author": "Jinlong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04704v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04704v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18446v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18446v1",
                "updated": "2025-07-24T14:30:48Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T14:30:48Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    14,
                    30,
                    48,
                    3,
                    205,
                    0
                ],
                "title": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming Sortformer: Speaker Cache-Based Online Speaker Diarization\n  with Arrival-Time Ordering"
                },
                "summary": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a streaming extension for the Sortformer speaker\ndiarization framework, whose key property is the arrival-time ordering of\noutput speakers. The proposed approach employs an Arrival-Order Speaker Cache\n(AOSC) to store frame-level acoustic embeddings of previously observed\nspeakers. Unlike conventional speaker-tracing buffers, AOSC orders embeddings\nby speaker index corresponding to their arrival time order, and is dynamically\nupdated by selecting frames with the highest scores based on the model's past\npredictions. Notably, the number of stored embeddings per speaker is determined\ndynamically by the update mechanism, ensuring efficient cache utilization and\nprecise speaker tracking. Experiments on benchmark datasets confirm the\neffectiveness and flexibility of our approach, even in low-latency setups.\nThese results establish Streaming Sortformer as a robust solution for real-time\nmulti-speaker tracking and a foundation for streaming multi-talker speech\nprocessing."
                },
                "authors": [
                    {
                        "name": "Ivan Medennikov"
                    },
                    {
                        "name": "Taejin Park"
                    },
                    {
                        "name": "Weiqing Wang"
                    },
                    {
                        "name": "He Huang"
                    },
                    {
                        "name": "Kunal Dhawan"
                    },
                    {
                        "name": "Jinhan Wang"
                    },
                    {
                        "name": "Jagadeesh Balam"
                    },
                    {
                        "name": "Boris Ginsburg"
                    }
                ],
                "author_detail": {
                    "name": "Boris Ginsburg"
                },
                "author": "Boris Ginsburg",
                "arxiv_comment": "Accepted to Interspeech 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18446v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18446v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18028v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18028v1",
                "updated": "2025-07-24T02:00:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "published": "2025-07-24T02:00:09Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    2,
                    0,
                    9,
                    3,
                    205,
                    0
                ],
                "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural\n  KV Database"
                },
                "summary": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficiently editing knowledge stored in large language models (LLMs) enables\nmodel updates without large-scale training. One possible solution is\nLocate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number\nof facts. However, such editing may compromise the general abilities of LLMs\nand even result in forgetting edited facts when scaling up to thousands of\nedits. In this paper, we model existing linear L\\&E methods as querying a\nKey-Value (KV) database. From this perspective, we then propose NeuralDB, an\nediting framework that explicitly represents the edited facts as a neural KV\ndatabase equipped with a non-linear gated retrieval module, % In particular,\nour gated module only operates when inference involves the edited facts,\neffectively preserving the general abilities of LLMs. Comprehensive experiments\ninvolving the editing of 10,000 facts were conducted on the ZsRE and\nCounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results\ndemonstrate that NeuralDB not only excels in editing efficacy, generalization,\nspecificity, fluency, and consistency, but also preserves overall performance\nacross six representative text understanding and generation tasks. Further\nexperiments indicate that NeuralDB maintains its effectiveness even when scaled\nto 100,000 facts (\\textbf{50x} more than in prior work)."
                },
                "authors": [
                    {
                        "name": "Weizhi Fei"
                    },
                    {
                        "name": "Hao Shi"
                    },
                    {
                        "name": "Jing Xu"
                    },
                    {
                        "name": "Jingchen Peng"
                    },
                    {
                        "name": "Jiazheng Li"
                    },
                    {
                        "name": "Jingzhao Zhang"
                    },
                    {
                        "name": "Bo Bai"
                    },
                    {
                        "name": "Wei Han"
                    },
                    {
                        "name": "Zhenyuan Chen"
                    },
                    {
                        "name": "Xueyan Niu"
                    }
                ],
                "author_detail": {
                    "name": "Xueyan Niu"
                },
                "author": "Xueyan Niu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18028v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18028v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17744v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17744v1",
                "updated": "2025-07-23T17:57:09Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T17:57:09Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    17,
                    57,
                    9,
                    2,
                    204,
                    0
                ],
                "title": "Yume: An Interactive World Generation Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume: An Interactive World Generation Model"
                },
                "summary": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Yume aims to use images, text, or videos to create an interactive, realistic,\nand dynamic world, which allows exploration and control using peripheral\ndevices or neural signals. In this report, we present a preview version of\n\\method, which creates a dynamic world from an input image and allows\nexploration of the world using keyboard actions. To achieve this high-fidelity\nand interactive video world generation, we introduce a well-designed framework,\nwhich consists of four main components, including camera motion quantization,\nvideo generation architecture, advanced sampler, and model acceleration. First,\nwe quantize camera motions for stable training and user-friendly interaction\nusing keyboard inputs. Then, we introduce the Masked Video Diffusion\nTransformer~(MVDT) with a memory module for infinite video generation in an\nautoregressive manner. After that, training-free Anti-Artifact Mechanism (AAM)\nand Time Travel Sampling based on Stochastic Differential Equations (TTS-SDE)\nare introduced to the sampler for better visual quality and more precise\ncontrol. Moreover, we investigate model acceleration by synergistic\noptimization of adversarial distillation and caching mechanisms. We use the\nhigh-quality world exploration dataset \\sekai to train \\method, and it achieves\nremarkable results in diverse scenes and applications. All data, codebase, and\nmodel weights are available on https://github.com/stdstu12/YUME. Yume will\nupdate monthly to achieve its original goal. Project page:\nhttps://stdstu12.github.io/YUME-Project/."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Mao"
                    },
                    {
                        "name": "Shaoheng Lin"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "Chuanhao Li"
                    },
                    {
                        "name": "Wenshuo Peng"
                    },
                    {
                        "name": "Tong He"
                    },
                    {
                        "name": "Jiangmiao Pang"
                    },
                    {
                        "name": "Mingmin Chi"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Kaipeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Kaipeng Zhang"
                },
                "author": "Kaipeng Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17744v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17744v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17647v1",
                "updated": "2025-07-23T16:09:10Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T16:09:10Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    9,
                    10,
                    2,
                    204,
                    0
                ],
                "title": "SHINE: A Scalable HNSW Index in Disaggregated Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SHINE: A Scalable HNSW Index in Disaggregated Memory"
                },
                "summary": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximate nearest neighbor (ANN) search is a fundamental problem in\ncomputer science for which in-memory graph-based methods, such as Hierarchical\nNavigable Small World (HNSW), perform exceptionally well. To scale beyond\nbillions of high-dimensional vectors, the index must be distributed. The\ndisaggregated memory architecture physically separates compute and memory into\ntwo distinct hardware units and has become popular in modern data centers. Both\nunits are connected via RDMA networks that allow compute nodes to directly\naccess remote memory and perform all the computations, posing unique challenges\nfor disaggregated indexes.\n  In this work, we propose a scalable HNSW index for ANN search in\ndisaggregated memory. In contrast to existing distributed approaches, which\npartition the graph at the cost of accuracy, our method builds a\ngraph-preserving index that reaches the same accuracy as a single-machine HNSW.\nContinuously fetching high-dimensional vector data from remote memory leads to\nsevere network bandwidth limitations, which we overcome by employing an\nefficient caching mechanism. Since answering a single query involves processing\nnumerous unique graph nodes, caching alone is not sufficient to achieve high\nscalability. We logically combine the caches of the compute nodes to increase\nthe overall cache effectiveness and confirm the efficiency and scalability of\nour method in our evaluation."
                },
                "authors": [
                    {
                        "name": "Manuel Widmoser"
                    },
                    {
                        "name": "Daniel Kocher"
                    },
                    {
                        "name": "Nikolaus Augsten"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaus Augsten"
                },
                "author": "Nikolaus Augsten",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16242v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16242v2",
                "updated": "2025-07-23T15:59:38Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    15,
                    59,
                    38,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T05:26:28Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    26,
                    28,
                    1,
                    203,
                    0
                ],
                "title": "Toward a Lightweight and Robust Design for Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a Lightweight and Robust Design for Caching"
                },
                "summary": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The online caching problem aims to minimize cache misses when serving a\nsequence of requests under a limited cache size. While naive learning-augmented\ncaching algorithms achieve ideal $1$-consistency, they lack robustness\nguarantees. Existing robustification methods either sacrifice $1$-consistency\nor introduce significant computational overhead. In this paper, we introduce\nGuard, a lightweight robustification framework that enhances the robustness of\na broad class of learning-augmented caching algorithms to $2H_k + 2$, while\npreserving their $1$-consistency. Guard achieves the current best-known\ntrade-off between consistency and robustness, with only $O(1)$ additional\nper-request overhead, thereby maintaining the original time complexity of the\nbase algorithm. Extensive experiments across multiple real-world datasets and\nprediction models validate the effectiveness of Guard in practice."
                },
                "authors": [
                    {
                        "name": "Peng Chen"
                    },
                    {
                        "name": "Hailiang Zhao"
                    },
                    {
                        "name": "Jiaji Zhang"
                    },
                    {
                        "name": "Xueyan Tang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Shuiguang Deng"
                    }
                ],
                "author_detail": {
                    "name": "Shuiguang Deng"
                },
                "author": "Shuiguang Deng",
                "arxiv_comment": "preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16242v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16242v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17554v1",
                "updated": "2025-07-23T14:43:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T14:43:22Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    14,
                    43,
                    22,
                    2,
                    204,
                    0
                ],
                "title": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An h-space Based Adversarial Attack for Protection Against Few-shot\n  Personalization"
                },
                "summary": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The versatility of diffusion models in generating customized images from few\nsamples raises significant privacy concerns, particularly regarding\nunauthorized modifications of private content. This concerning issue has\nrenewed the efforts in developing protection mechanisms based on adversarial\nattacks, which generate effective perturbations to poison diffusion models. Our\nwork is motivated by the observation that these models exhibit a high degree of\nabstraction within their semantic latent space (`h-space'), which encodes\ncritical high-level features for generating coherent and meaningful content. In\nthis paper, we propose a novel anti-customization approach, called HAAD\n(h-space based Adversarial Attack for Diffusion models), that leverages\nadversarial attacks to craft perturbations based on the h-space that can\nefficiently degrade the image generation process. Building upon HAAD, we\nfurther introduce a more efficient variant, HAAD-KV, that constructs\nperturbations solely based on the KV parameters of the h-space. This strategy\noffers a stronger protection, that is computationally less expensive. Despite\ntheir simplicity, our methods outperform state-of-the-art adversarial attacks,\nhighlighting their effectiveness."
                },
                "authors": [
                    {
                        "name": "Xide Xu"
                    },
                    {
                        "name": "Sandesh Kamath"
                    },
                    {
                        "name": "Muhammad Atif Butt"
                    },
                    {
                        "name": "Bogdan Raducanu"
                    }
                ],
                "author_detail": {
                    "name": "Bogdan Raducanu"
                },
                "author": "Bogdan Raducanu",
                "arxiv_comment": "32 pages, 15 figures. Accepted by ACM Multimedia 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.23956v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.23956v3",
                "updated": "2025-07-23T11:42:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    42,
                    3,
                    2,
                    204,
                    0
                ],
                "published": "2025-03-31T11:13:18Z",
                "published_parsed": [
                    2025,
                    3,
                    31,
                    11,
                    13,
                    18,
                    0,
                    90,
                    0
                ],
                "title": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirCache: Activating Inter-modal Relevancy KV Cache Compression for\n  Efficient Large Vision-Language Model Inference"
                },
                "summary": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Visual Language Models (LVLMs) have gained\nsignificant attention due to their remarkable reasoning capabilities and\nproficiency in generalization. However, processing a large number of visual\ntokens and generating long-context outputs impose substantial computational\noverhead, leading to excessive demands for key-value (KV) cache. To address\nthis critical bottleneck, we propose AirCache, a novel KV cache compression\nmethod aimed at accelerating LVLMs inference. This work systematically\ninvestigates the correlations between visual and textual tokens within the\nattention mechanisms of LVLMs. Our empirical analysis reveals considerable\nredundancy in cached visual tokens, wherein strategically eliminating these\ntokens preserves model performance while significantly accelerating context\ngeneration. Inspired by these findings, we introduce an elite observation\nwindow for assessing the importance of visual components in the KV cache,\nfocusing on stable inter-modal relevancy modeling with enhanced\nmulti-perspective consistency. Additionally, we develop an adaptive layer-wise\nbudget allocation strategy that capitalizes on the strength and skewness of\ntoken importance distribution, showcasing superior efficiency compared to\nuniform allocation. Comprehensive evaluations across multiple LVLMs and\nbenchmarks demonstrate that our method achieves comparable performance to the\nfull cache while retaining only 10% of visual KV cache, thereby reducing\ndecoding latency by 29% to 66% across various batch size and prompt length of\ninputs. Notably, as cache retention rates decrease, our method exhibits\nincreasing performance advantages over existing approaches."
                },
                "authors": [
                    {
                        "name": "Kai Huang"
                    },
                    {
                        "name": "Hao Zou"
                    },
                    {
                        "name": "Bochen Wang"
                    },
                    {
                        "name": "Ye Xi"
                    },
                    {
                        "name": "Zhen Xie"
                    },
                    {
                        "name": "Hao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Hao Wang"
                },
                "author": "Hao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.23956v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.23956v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17411v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17411v1",
                "updated": "2025-07-23T11:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-23T11:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    11,
                    12,
                    8,
                    2,
                    204,
                    0
                ],
                "title": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multiprocessor Scheduling with Memory Constraints: Fundamental\n  Properties and Finding Optimal Solutions"
                },
                "summary": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the problem of scheduling a general computational DAG on multiple\nprocessors in a 2-level memory hierarchy. This setting is a natural\ngeneralization of several prominent models in the literature, and it\nsimultaneously captures workload balancing, communication, and data movement\ndue to cache size limitations. We first analyze the fundamental properties of\nthis problem from a theoretical perspective, such as its computational\ncomplexity. We also prove that optimizing parallelization and memory management\nseparately, as done in many applications, can result in a solution that is a\nlinear factor away from the optimum.\n  On the algorithmic side, we discuss a natural technique to represent and\nsolve the problem as an Integer Linear Program (ILP). We develop a holistic\nscheduling algorithm based on this approach, and we experimentally study its\nperformance and properties on a small benchmark of computational tasks. Our\nresults confirm that the ILP-based method can indeed find considerably better\nsolutions than a baseline which combines classical scheduling algorithms and\nmemory management policies."
                },
                "authors": [
                    {
                        "name": "Pl Andrs Papp"
                    },
                    {
                        "name": "Toni Bhnlein"
                    },
                    {
                        "name": "A. N. Yzelman"
                    }
                ],
                "author_detail": {
                    "name": "A. N. Yzelman"
                },
                "author": "A. N. Yzelman",
                "arxiv_doi": "10.1145/3754598.3754676",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3754598.3754676",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.17411v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17411v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the 54th International Conference on Parallel Processing\n  (ICPP 2025)",
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B35, 90C10, 68Q10, 68W10",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "C.1.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13373v2",
                "updated": "2025-07-23T10:10:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    10,
                    10,
                    53,
                    2,
                    204,
                    0
                ],
                "published": "2024-11-20T14:52:36Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    14,
                    52,
                    36,
                    2,
                    325,
                    0
                ],
                "title": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upgrade of the Diagnostic Neutral Beam Injector for the RFX-mod2\n  experiment"
                },
                "summary": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diagnostic Neutral Beam Injectors (DNBI), through the combined use of Charge\nExchange Recombination Spectroscopy (CHERS) and Motional Stark effect\ndiagnostics (MSE), are a well-known tool to access important information about\nmagnetically confined plasmas, such as radial profiles of ion temperature, ion\nflow, impurity content and intensity and direction of the magnetic field. For\nthis purpose, a DNBI was installed and operated in the RFX-mod experiment,\nwhich was designed to confine plasma mainly through the Reversed Field Pinch\nconfiguration. The DNBI, designed and built by the Budker Institute of Nuclear\nPhysics (BINP), was based on a source of positive hydrogen ions, accelerated to\n50 keV and for a maximum ion current of 5 A. The beam could be modulated and\nthe maximum overall duration was 50 ms. With the upgrade of RFX-mod to the\npresent RFX-mod2 machine, the DNBI is being renovated to solve several power\nunits faults and improve the overall reliability of the system. The 50 kV power\nsupply is being improved, as well as the power supplies in the high voltage\ndeck and its insulation transformer. Magnetic field survival tests were\nperformed on the toroidal-core-based DC-DC converters that should power the\nelectronic boards in a reliable way. The control system, originally based on\nCAMAC technology, was redesigned to be fully replaced. This contribution\nreviews the technical criticalities emerged in the DNBI check-up and the new\nsolutions adopted to make the DNBI operative and more reliable."
                },
                "authors": [
                    {
                        "name": "Marco Barbisan"
                    },
                    {
                        "name": "Marco Boldrin"
                    },
                    {
                        "name": "Luca Cinnirella"
                    },
                    {
                        "name": "Bruno Laterza"
                    },
                    {
                        "name": "Alberto Maistrello"
                    },
                    {
                        "name": "Lionello Marrelli"
                    },
                    {
                        "name": "Federico Molon"
                    },
                    {
                        "name": "Simone Peruzzo"
                    },
                    {
                        "name": "Cesare Taliercio"
                    },
                    {
                        "name": "Marco Valisa"
                    },
                    {
                        "name": "Enrico Zampiva"
                    }
                ],
                "author_detail": {
                    "name": "Enrico Zampiva"
                },
                "author": "Enrico Zampiva",
                "arxiv_doi": "10.1016/j.fusengdes.2025.115320",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.fusengdes.2025.115320",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2411.13373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "8 pages (excl. highlights), 8 figures. Contribution to the 33rd\n  Symposium on Fusion Technology (SOFT), 22-27 September 2024. This is the\n  accepted manuscript for the \"Fusion Engineering and Design\" journal",
                "arxiv_journal_ref": "Fusion Engineering and Design, Volume 220, November 2025, 115320",
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16391v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16391v2",
                "updated": "2025-07-23T09:31:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    9,
                    31,
                    1,
                    2,
                    204,
                    0
                ],
                "published": "2025-07-22T09:35:59Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    9,
                    35,
                    59,
                    1,
                    203,
                    0
                ],
                "title": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ironman: Accelerating Oblivious Transfer Extension for\n  Privacy-Preserving AI with Near-Memory Processing"
                },
                "summary": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the wide application of machine learning (ML), privacy concerns arise\nwith user data as they may contain sensitive information. Privacy-preserving ML\n(PPML) based on cryptographic primitives has emerged as a promising solution in\nwhich an ML model is directly computed on the encrypted data to provide a\nformal privacy guarantee. However, PPML frameworks heavily rely on the\noblivious transfer (OT) primitive to compute nonlinear functions. OT mainly\ninvolves the computation of single-point correlated OT (SPCOT) and learning\nparity with noise (LPN) operations. As OT is still computed extensively on\ngeneral-purpose CPUs, it becomes the latency bottleneck of modern PPML\nframeworks.\n  In this paper, we propose a novel OT accelerator, dubbed Ironman, to\nsignificantly increase the efficiency of OT and the overall PPML framework. We\nobserve that SPCOT is computation-bounded, and thus propose a hardware-friendly\nSPCOT algorithm with a customized accelerator to improve SPCOT computation\nthroughput. In contrast, LPN is memory-bandwidth-bounded due to irregular\nmemory access patterns. Hence, we further leverage the near-memory processing\n(NMP) architecture equipped with memory-side cache and index sorting to improve\neffective memory bandwidth. With extensive experiments, we demonstrate Ironman\nachieves a 39.2-237.4 times improvement in OT throughput across different NMP\nconfigurations compared to the full-thread CPU implementation. For different\nPPML frameworks, Ironman demonstrates a 2.1-3.4 times reduction in end-to-end\nlatency for both CNN and Transformer models."
                },
                "authors": [
                    {
                        "name": "Chenqi Lin"
                    },
                    {
                        "name": "Kang Yang"
                    },
                    {
                        "name": "Tianshi Xu"
                    },
                    {
                        "name": "Ling Liang"
                    },
                    {
                        "name": "Yufei Wang"
                    },
                    {
                        "name": "Zhaohui Chen"
                    },
                    {
                        "name": "Runsheng Wang"
                    },
                    {
                        "name": "Mingyu Gao"
                    },
                    {
                        "name": "Meng Li"
                    }
                ],
                "author_detail": {
                    "name": "Meng Li"
                },
                "author": "Meng Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16391v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16391v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.02634v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.02634v4",
                "updated": "2025-07-23T08:07:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    8,
                    7,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-03T08:51:38Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    8,
                    51,
                    38,
                    1,
                    154,
                    0
                ],
                "title": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVCache Cache in the Wild: Characterizing and Optimizing KVCache Cache\n  at a Large Cloud Provider"
                },
                "summary": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serving large language models (LLMs) is important for cloud providers, and\ncaching intermediate results (KV\\$) after processing each request substantially\nimproves serving throughput and latency. However, there is limited\nunderstanding of how LLM serving benefits from KV\\$ caching, where system\ndesign decisions like cache eviction policies are highly workload-dependent. In\nthis paper, we present the first systematic characterization of the KV\\$\nworkload patterns from one of the leading LLM service providers. We draw\nobservations that were not covered by previous studies focusing on synthetic\nworkloads, including: KV\\$ reuses are skewed across requests, where reuses\nbetween single-turn requests are equally important as multi-turn requests; the\nreuse time and probability are diverse considering all requests, but for a\nspecific request category, the pattern tends to be predictable; and the overall\ncache size required for an ideal cache hit ratio is moderate. Based on the\ncharacterization, we further propose a workload-aware cache eviction policy\nthat improves the serving performance under real-world traces, especially with\nlimited cache capacity."
                },
                "authors": [
                    {
                        "name": "Jiahao Wang"
                    },
                    {
                        "name": "Jinbo Han"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Sijie Shen"
                    },
                    {
                        "name": "Dingyan Zhang"
                    },
                    {
                        "name": "Chenguang Fang"
                    },
                    {
                        "name": "Rong Chen"
                    },
                    {
                        "name": "Wenyuan Yu"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "Accepted by USENIX ATC'25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.02634v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.02634v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.17286v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.17286v2",
                "updated": "2025-07-23T05:57:32Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    5,
                    57,
                    32,
                    2,
                    204,
                    0
                ],
                "published": "2025-06-15T07:19:33Z",
                "published_parsed": [
                    2025,
                    6,
                    15,
                    7,
                    19,
                    33,
                    6,
                    166,
                    0
                ],
                "title": "GTA: Grouped-head latenT Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GTA: Grouped-head latenT Attention"
                },
                "summary": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Attention mechanisms underpin the success of large language models (LLMs),\nyet their substantial computational and memory overhead poses challenges for\noptimizing efficiency and performance. A critical bottleneck arises as KV cache\nand attention computations scale rapidly with text length, challenging\ndeployment on hardware with limited computational and memory resources. We\nobserve that attention mechanisms exhibit substantial redundancy, since the KV\ncache can be significantly compressed and attention maps across heads display\nhigh similarity, revealing that much of the computation and storage is\nunnecessary. Leveraging these insights, we propose \\textbf{G}rouped-Head\nLaten\\textbf{T} \\textbf{A}ttention (GTA), a novel attention mechanism that\nreduces memory usage and computational complexity while maintaining\nperformance. GTA comprises two components: (1) a shared attention map mechanism\nthat reuses attention scores across multiple heads, decreasing the key cache\nsize; and (2) a nonlinear value decoder with learned projections that\ncompresses the value cache into a latent space, further cutting memory needs.\nGTA cuts attention computation FLOPs by up to \\emph{62.5\\%} versus\nGrouped-Query Attention and shrink the KV cache by up to \\emph{70\\%}, all while\navoiding the extra overhead of Multi-Head Latent Attention to improve LLM\ndeployment efficiency. Consequently, GTA models achieve a \\emph{2x} increase in\nend-to-end inference speed, with prefill benefiting from reduced computational\ncost and decoding benefiting from the smaller cache footprint."
                },
                "authors": [
                    {
                        "name": "Luoyang Sun"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jiwen Jiang"
                    },
                    {
                        "name": "Xinjian Wu"
                    },
                    {
                        "name": "Haifeng Zhang"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Lionel Ni"
                    },
                    {
                        "name": "Jun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Wang"
                },
                "author": "Jun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.17286v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.17286v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.11046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.11046v2",
                "updated": "2025-07-23T01:42:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    23,
                    1,
                    42,
                    19,
                    2,
                    204,
                    0
                ],
                "published": "2025-02-16T09:08:36Z",
                "published_parsed": [
                    2025,
                    2,
                    16,
                    9,
                    8,
                    36,
                    6,
                    47,
                    0
                ],
                "title": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enabling Efficient Transaction Processing on CXL-Based Memory Sharing"
                },
                "summary": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transaction processing systems are the crux for modern data-center\napplications, yet current multi-node systems are slow due to network overheads.\nThis paper advocates for Compute Express Link (CXL) as a network alternative,\nwhich enables low-latency and cache-coherent shared memory accesses. However,\ndirectly adopting standard CXL primitives leads to performance degradation due\nto the high cost of maintaining cross-node cache coherence. To address the CXL\nchallenges, this paper introduces CtXnL, a software-hardware co-designed system\nthat implements a novel hybrid coherence primitive tailored to the loosely\ncoherent nature of transactional data. The core innovation of CtXnL is\nempowering transaction system developers with the ability to selectively\nachieve data coherence. Our evaluations on OLTP workloads demonstrate that\nCtXnL enhances performance, outperforming current network-based systems and\nachieves with up to 2.08x greater throughput than vanilla CXL memory sharing\narchitectures across universal transaction processing policies."
                },
                "authors": [
                    {
                        "name": "Zhao Wang"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Cong Li"
                    },
                    {
                        "name": "Dimin Niu"
                    },
                    {
                        "name": "Tianchan Guan"
                    },
                    {
                        "name": "Zhaoyang Du"
                    },
                    {
                        "name": "Xingda Wei"
                    },
                    {
                        "name": "Guangyu Sun"
                    }
                ],
                "author_detail": {
                    "name": "Guangyu Sun"
                },
                "author": "Guangyu Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.11046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.11046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17033v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17033v1",
                "updated": "2025-07-22T21:41:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:41:43Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    41,
                    43,
                    1,
                    203,
                    0
                ],
                "title": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GATEBLEED: Exploiting On-Core Accelerator Power Gating for High\n  Performance & Stealthy Attacks on AI"
                },
                "summary": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As power consumption from AI training and inference continues to increase, AI\naccelerators are being integrated directly into the CPU. Intel's Advanced\nMatrix Extensions (AMX) is one such example, debuting on the 4th generation\nIntel Xeon Scalable CPU. We discover a timing side and covert channel,\nGATEBLEED, caused by the aggressive power gating utilized to keep the CPU\nwithin operating limits. We show that the GATEBLEED side channel is a threat to\nAI privacy as many ML models such as transformers and CNNs make critical\ncomputationally-heavy decisions based on private values like confidence\nthresholds and routing logits. Timing delays from selective powering down of\nAMX components mean that each matrix multiplication is a potential leakage\npoint when executed on the AMX accelerator. Our research identifies over a\ndozen potential gadgets across popular ML libraries (HuggingFace, PyTorch,\nTensorFlow, etc.), revealing that they can leak sensitive and private\ninformation. GATEBLEED poses a risk for local and remote timing inference, even\nunder previous protective measures. GATEBLEED can be used as a high\nperformance, stealthy remote covert channel and a generic magnifier for timing\ntransmission channels, capable of bypassing traditional cache defenses to leak\narbitrary memory addresses and evading state of the art microarchitectural\nattack detectors under realistic network conditions and system configurations\nin which previous attacks fail. We implement an end-to-end microarchitectural\ninference attack on a transformer model optimized with Intel AMX, achieving a\nmembership inference accuracy of 81% and a precision of 0.89. In a CNN-based or\ntransformer-based mixture-of-experts model optimized with Intel AMX, we leak\nexpert choice with 100% accuracy."
                },
                "authors": [
                    {
                        "name": "Joshua Kalyanapu"
                    },
                    {
                        "name": "Farshad Dizani"
                    },
                    {
                        "name": "Darsh Asher"
                    },
                    {
                        "name": "Azam Ghanbari"
                    },
                    {
                        "name": "Rosario Cammarota"
                    },
                    {
                        "name": "Aydin Aysu"
                    },
                    {
                        "name": "Samira Mirbagher Ajorpaz"
                    }
                ],
                "author_detail": {
                    "name": "Samira Mirbagher Ajorpaz"
                },
                "author": "Samira Mirbagher Ajorpaz",
                "arxiv_comment": "Accepted at MICRO 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17033v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17033v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17029v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17029v1",
                "updated": "2025-07-22T21:33:30Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T21:33:30Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    21,
                    33,
                    30,
                    1,
                    203,
                    0
                ],
                "title": "StreamME: Simplify 3D Gaussian Avatar within Live Stream",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StreamME: Simplify 3D Gaussian Avatar within Live Stream"
                },
                "summary": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose StreamME, a method focuses on fast 3D avatar reconstruction. The\nStreamME synchronously records and reconstructs a head avatar from live video\nstreams without any pre-cached data, enabling seamless integration of the\nreconstructed appearance into downstream applications. This exceptionally fast\ntraining strategy, which we refer to as on-the-fly training, is central to our\napproach. Our method is built upon 3D Gaussian Splatting (3DGS), eliminating\nthe reliance on MLPs in deformable 3DGS and relying solely on geometry, which\nsignificantly improves the adaptation speed to facial expression. To further\nensure high efficiency in on-the-fly training, we introduced a simplification\nstrategy based on primary points, which distributes the point clouds more\nsparsely across the facial surface, optimizing points number while maintaining\nrendering quality. Leveraging the on-the-fly training capabilities, our method\nprotects the facial privacy and reduces communication bandwidth in VR system or\nonline conference. Additionally, it can be directly applied to downstream\napplication such as animation, toonify, and relighting. Please refer to our\nproject page for more details: https://songluchuan.github.io/StreamME/."
                },
                "authors": [
                    {
                        "name": "Luchuan Song"
                    },
                    {
                        "name": "Yang Zhou"
                    },
                    {
                        "name": "Zhan Xu"
                    },
                    {
                        "name": "Yi Zhou"
                    },
                    {
                        "name": "Deepali Aneja"
                    },
                    {
                        "name": "Chenliang Xu"
                    }
                ],
                "author_detail": {
                    "name": "Chenliang Xu"
                },
                "author": "Chenliang Xu",
                "arxiv_comment": "12 pages, 15 Figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17029v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17029v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16933v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16933v1",
                "updated": "2025-07-22T18:17:53Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T18:17:53Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    18,
                    17,
                    53,
                    1,
                    203,
                    0
                ],
                "title": "SiLQ: Simple Large Language Model Quantization-Aware Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SiLQ: Simple Large Language Model Quantization-Aware Training"
                },
                "summary": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models can be quantized to reduce inference time latency,\nmodel size, and energy consumption, thereby delivering a better user experience\nat lower cost. A challenge exists to deliver quantized models with minimal loss\nof accuracy in reasonable time, and in particular to do so without requiring\nmechanisms incompatible with specialized inference accelerators. Here, we\ndemonstrate a simple, end-to-end quantization-aware training approach that,\nwith an increase in total model training budget of less than 0.1%, outperforms\nthe leading published quantization methods by large margins on several modern\nbenchmarks, with both base and instruct model variants. The approach easily\ngeneralizes across different model architectures, can be applied to\nactivations, cache, and weights, and requires the introduction of no additional\noperations to the model other than the quantization itself."
                },
                "authors": [
                    {
                        "name": "Steven K. Esser"
                    },
                    {
                        "name": "Jeffrey L. McKinstry"
                    },
                    {
                        "name": "Deepika Bablani"
                    },
                    {
                        "name": "Rathinakumar Appuswamy"
                    },
                    {
                        "name": "Dharmendra S. Modha"
                    }
                ],
                "author_detail": {
                    "name": "Dharmendra S. Modha"
                },
                "author": "Dharmendra S. Modha",
                "arxiv_comment": "12 pages, 3 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16933v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16933v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16784v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16784v1",
                "updated": "2025-07-22T17:30:04Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:30:04Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    30,
                    4,
                    1,
                    203,
                    0
                ],
                "title": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Context Limits: Subconscious Threads for Long-Horizon Reasoning"
                },
                "summary": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To break the context limits of large language models (LLMs) that bottleneck\nreasoning accuracy and efficiency, we propose the Thread Inference Model (TIM),\na family of LLMs trained for recursive and decompositional problem solving, and\nTIMRUN, an inference runtime enabling long-horizon structured reasoning beyond\ncontext limits. Together, TIM hosted on TIMRUN supports virtually unlimited\nworking memory and multi-hop tool calls within a single language model\ninference, overcoming output limits, positional-embedding constraints, and\nGPU-memory bottlenecks. Performance is achieved by modeling natural language as\nreasoning trees measured by both length and depth instead of linear sequences.\nThe reasoning trees consist of tasks with thoughts, recursive subtasks, and\nconclusions based on the concept we proposed in Schroeder et al, 2025. During\ngeneration, we maintain a working memory that retains only the key-value states\nof the most relevant context tokens, selected by a rule-based subtask-pruning\nmechanism, enabling reuse of positional embeddings and GPU memory pages\nthroughout reasoning. Experimental results show that our system sustains high\ninference throughput, even when manipulating up to 90% of the KV cache in GPU\nmemory. It also delivers accurate reasoning on mathematical tasks and handles\ninformation retrieval challenges that require long-horizon reasoning and\nmulti-hop tool use."
                },
                "authors": [
                    {
                        "name": "Hongyin Luo"
                    },
                    {
                        "name": "Nathaniel Morgan"
                    },
                    {
                        "name": "Tina Li"
                    },
                    {
                        "name": "Derek Zhao"
                    },
                    {
                        "name": "Ai Vy Ngo"
                    },
                    {
                        "name": "Philip Schroeder"
                    },
                    {
                        "name": "Lijie Yang"
                    },
                    {
                        "name": "Assaf Ben-Kish"
                    },
                    {
                        "name": "Jack O'Brien"
                    },
                    {
                        "name": "James Glass"
                    }
                ],
                "author_detail": {
                    "name": "James Glass"
                },
                "author": "James Glass",
                "arxiv_comment": "Research preview",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16784v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16784v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16768v1",
                "updated": "2025-07-22T17:13:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T17:13:47Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    17,
                    13,
                    47,
                    1,
                    203,
                    0
                ],
                "title": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WGRAMMAR: Leverage Prior Knowledge to Accelerate Structured Decoding"
                },
                "summary": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Structured decoding enables large language models (LLMs) to generate outputs\nin formats required by downstream systems, such as HTML or JSON. However,\nexisting methods suffer from efficiency bottlenecks due to grammar compilation,\nstate tracking, and mask creation. We observe that many real-world tasks embed\nstrong prior knowledge about output structure. Leveraging this, we propose a\ndecomposition of constraints into static and dynamic components -- precompiling\nstatic structures offline and instantiating dynamic arguments at runtime using\ngrammar snippets. Instead of relying on pushdown automata, we employ a\ncompositional set of operators to model regular formats, achieving lower\ntransition latency. We introduce wgrammar, a lightweight decoding engine that\nintegrates domain-aware simplification, constraint decomposition, and mask\ncaching, achieving up to 250x speedup over existing systems. wgrammar's source\ncode is publicly available at https://github.com/wrran/wgrammar."
                },
                "authors": [
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Hao Ren"
                    },
                    {
                        "name": "Gang Chen"
                    },
                    {
                        "name": "Fanchao Qi"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2212.10131v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2212.10131v3",
                "updated": "2025-07-22T16:49:24Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    16,
                    49,
                    24,
                    1,
                    203,
                    0
                ],
                "published": "2022-12-20T09:58:39Z",
                "published_parsed": [
                    2022,
                    12,
                    20,
                    9,
                    58,
                    39,
                    1,
                    354,
                    0
                ],
                "title": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hydra: Virtualized Multi-Language Runtime for High-Density Serverless\n  Platforms"
                },
                "summary": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Serverless is an attractive computing model that offers seamless scalability\nand elasticity; it takes the infrastructure management burden away from users\nand enables a pay-as-you-use billing model. As a result, serverless is becoming\nincreasingly popular to support highly elastic and bursty workloads. However,\nexisting platforms are supported by bloated virtualization stacks, which,\ncombined with bursty and irregular invocations, lead to high memory and latency\noverheads.\n  To reduce the virtualization stack bloat, we propose Hydra, a virtualized\nmulti-language runtime and platform capable of hosting multiple sandboxes\nrunning concurrently. To fully leverage Hydra's virtualized runtime, we revisit\nthe existing serverless platform design to make it colocation-aware across\nowners and functions, and to feature a caching layer of pre-allocated Hydra\ninstances that can be used by different functions written in different\nlanguages to reduce cold starts. We also propose a snapshotting mechanism to\ncheckpoint and restore individual sandboxes.\n  By consolidating multiple serverless function invocations through Hydra, we\nimprove the overall function density (ops/GB-sec) by 2.41x on average compared\nto OpenWhisk runtimes, the state-of-the-art single-language runtimes used in\nmost serverless platforms, and by 1.43x on average compared to Knative runtimes\nsupporting invocation colocation within the same function. When reproducing the\nAzure Functions trace, our serverless platform operating Hydra instances\nreduces the overall memory footprint by 21.3-43.9% compared to operating\nOpenWhisk instances and by 14.5-30% compared to operating Knative instances.\nHydra eliminates cold starts thanks to the pool of pre-warmed runtime\ninstances, reducing p99 latency by 45.3-375.5x compared to OpenWhisk and by\n1.9-51.4x compared to Knative."
                },
                "authors": [
                    {
                        "name": "Serhii Ivanenko"
                    },
                    {
                        "name": "Vasyl Lanko"
                    },
                    {
                        "name": "Rudi Horn"
                    },
                    {
                        "name": "Vojin Jovanovic"
                    },
                    {
                        "name": "Rodrigo Bruno"
                    }
                ],
                "author_detail": {
                    "name": "Rodrigo Bruno"
                },
                "author": "Rodrigo Bruno",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2212.10131v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2212.10131v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16243v1",
                "updated": "2025-07-22T05:34:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T05:34:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    5,
                    34,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Genus Zero Kashiwara-Vergne Solutions from Braids",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Genus Zero Kashiwara-Vergne Solutions from Braids"
                },
                "summary": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Using the language of moperads-monoids in the category of right modules over\nan operad-we reinterpret the Alekseev-Enriquez-Torossian construction of\nKashiwara-Vergne (KV) solutions from associators. We show that any isomorphism\nbetween the moperad of parenthesized braids with a frozen strand and the\nmoperad of chord diagrams gives rise to a family of genus zero KV solutions\noperadically generated by a single classical KV solution. We show that the\nGrothendieck-Teichm\\\"uller module groups act on the latter, intertwining the\nactions of the KV symmetry groups. In the other direction, we show that any\nsymmetric KV solution gives rise to a morphism from the moperad of\nparenthesized braids with a frozen strand to the moperad of tangential\nautomorphisms of free Lie algebras. This morphism factors through the moperad\nof chord diagrams if and only if the associated KV associator is a Drinfeld\nassociator."
                },
                "authors": [
                    {
                        "name": "Zsuzsanna Dancso"
                    },
                    {
                        "name": "Iva Halacheva"
                    },
                    {
                        "name": "Guillaume Laplante-Anfossi"
                    },
                    {
                        "name": "Marcy Robertson"
                    },
                    {
                        "name": "Chandan Singh"
                    }
                ],
                "author_detail": {
                    "name": "Chandan Singh"
                },
                "author": "Chandan Singh",
                "arxiv_comment": "comments welcome!",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.AT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.AT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.CT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.QA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "18M60, 17B, 55",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.16217v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.16217v1",
                "updated": "2025-07-22T04:21:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "published": "2025-07-22T04:21:03Z",
                "published_parsed": [
                    2025,
                    7,
                    22,
                    4,
                    21,
                    3,
                    1,
                    203,
                    0
                ],
                "title": "Towards Compute-Optimal Many-Shot In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Compute-Optimal Many-Shot In-Context Learning"
                },
                "summary": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Long-context large language models (LLMs) are able to process inputs\ncontaining up to several million tokens. In the scope of in-context learning\n(ICL), this translates into using hundreds/thousands of demonstrations in the\ninput prompt, enabling many-shot ICL. In practice, a fixed set of\ndemonstrations is often selected at random in many-shot settings due to (1)\nhigh inference costs, (2) the benefits of caching and reusing computations, and\n(3) the similar performance offered by this strategy compared to others when\nscaled. In this work, we propose two straightforward strategies for\ndemonstration selection in many-shot ICL that improve performance with minimal\ncomputational overhead. Our first method combines a small number of\ndemonstrations, selected based on their similarity to each test sample, with a\ndisproportionately larger set of random demonstrations that are cached. The\nsecond strategy improves the first by replacing random demonstrations with\nthose selected using centroids derived from test sample representations via\nk-means clustering. Our experiments with Gemini Pro and Flash across several\ndatasets indicate that our strategies consistently outperform random selection\nand surpass or match the most performant selection approach while supporting\ncaching and reducing inference cost by up to an order of magnitude. We also\nshow that adjusting the proportion of demonstrations selected based on\ndifferent criteria can balance performance and inference cost in many-shot ICL."
                },
                "authors": [
                    {
                        "name": "Shahriar Golchin"
                    },
                    {
                        "name": "Yanfei Chen"
                    },
                    {
                        "name": "Rujun Han"
                    },
                    {
                        "name": "Manan Gandhi"
                    },
                    {
                        "name": "Tianli Yu"
                    },
                    {
                        "name": "Swaroop Mishra"
                    },
                    {
                        "name": "Mihai Surdeanu"
                    },
                    {
                        "name": "Rishabh Agarwal"
                    },
                    {
                        "name": "Chen-Yu Lee"
                    },
                    {
                        "name": "Tomas Pfister"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Pfister"
                },
                "author": "Tomas Pfister",
                "arxiv_comment": "Final version; accepted at COLM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.16217v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.16217v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10789v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10789v2",
                "updated": "2025-07-21T19:31:37Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    31,
                    37,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T20:38:09Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    20,
                    38,
                    9,
                    0,
                    195,
                    0
                ],
                "title": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dissecting the NVIDIA Blackwell Architecture with Microbenchmarks"
                },
                "summary": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid development in scientific research provides a need for more compute\npower, which is partly being solved by GPUs. This paper presents a\nmicroarchitectural analysis of the modern NVIDIA Blackwell architecture by\nstudying GPU performance\n  features with thought through microbenchmarks. We unveil key subsystems,\nincluding the memory hierarchy, SM execution\n  pipelines, and the SM sub-core units, including the 5th generation tensor\ncores supporting FP4 and FP6 precisions.\n  To understand the different key features of the NVIDIA GPU, we study latency,\nthroughput, cache behavior, and scheduling\n  details, revealing subtle tuning metrics in the design of Blackwell. To\ndevelop a comprehensive analysis, we compare the\n  Blackwell architecture with the previous Hopper architecture by using the\nGeForce RTX 5080 and H100 PCIe, respectively. We\n  evaluate and compare results, presenting both generational improvements and\nperformance regressions. Additionally, we\n  investigate the role of power efficiency and energy consumption under varied\nworkloads. Our findings provide actionable insights\n  for application developers, compiler writers, and performance engineers to\noptimize workloads on Blackwell-based platforms,\n  and contribute new data to the growing research on GPU architectures."
                },
                "authors": [
                    {
                        "name": "Aaron Jarmusch"
                    },
                    {
                        "name": "Nathan Graddon"
                    },
                    {
                        "name": "Sunita Chandrasekaran"
                    }
                ],
                "author_detail": {
                    "name": "Sunita Chandrasekaran"
                },
                "author": "Sunita Chandrasekaran",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10789v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10789v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.14003v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.14003v2",
                "updated": "2025-07-21T19:05:01Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    19,
                    5,
                    1,
                    0,
                    202,
                    0
                ],
                "published": "2024-10-17T20:11:34Z",
                "published_parsed": [
                    2024,
                    10,
                    17,
                    20,
                    11,
                    34,
                    3,
                    291,
                    0
                ],
                "title": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Per-Bank Bandwidth Regulation of Shared Last-Level Cache for Real-Time\n  Systems"
                },
                "summary": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern commercial-off-the-shelf (COTS) multicore processors have advanced\nmemory hierarchies that enhance memory-level parallelism (MLP), which is\ncrucial for high performance. To support high MLP, shared last-level caches\n(LLCs) are divided into multiple banks, allowing parallel access. However,\nuneven distribution of cache requests from the cores, especially when requests\nfrom multiple cores are concentrated on a single bank, can result in\nsignificant contention affecting all cores that access the cache. Such cache\nbank contention can even be maliciously induced -- known as cache bank-aware\ndenial-of-service (DoS) attacks -- in order to jeopardize the system's timing\npredictability.\n  In this paper, we propose a per-bank bandwidth regulation approach for\nmulti-banked shared LLC based multicore real-time systems. By regulating\nbandwidth on a per-bank basis, the approach aims to prevent unnecessary\nthrottling of cache accesses to non-contended banks, thus improving overall\nperformance (throughput) without compromising isolation benefits of throttling.\nWe implement our approach on a RISC-V system-on-chip (SoC) platform using\nFireSim and evaluate extensively using both synthetic and real-world workloads.\nOur evaluation results show that the proposed per-bank regulation approach\neffectively protects real-time tasks from co-running cache bank-aware DoS\nattacks, and offers up to a 3.66$\\times$ performance improvement for the\nthrottled benign best-effort tasks compared to prior bank-oblivious bandwidth\nthrottling approaches."
                },
                "authors": [
                    {
                        "name": "Connor Sullivan"
                    },
                    {
                        "name": "Alex Manley"
                    },
                    {
                        "name": "Mohammad Alian"
                    },
                    {
                        "name": "Heechul Yun"
                    }
                ],
                "author_detail": {
                    "name": "Heechul Yun"
                },
                "author": "Heechul Yun",
                "arxiv_doi": "10.1109/RTSS62706.2024.00036",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/RTSS62706.2024.00036",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.14003v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.14003v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Update to Fig. 11: The previous version used mismatched cache\n  capacities between the 2-bank and 4-bank configurations in the simulation\n  setup. This has been corrected to ensure both configurations have equal total\n  cache capacity. As a result, the specific numerical results in Fig. 11 have\n  changed. However, the overall trend shown in Fig. 11 and key findings of the\n  paper remain consistent",
                "arxiv_journal_ref": "IEEE Real-Time Systems Symposium (RTSS), 2024, pp. 336-348",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18974v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18974v3",
                "updated": "2025-07-21T14:50:41Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    14,
                    50,
                    41,
                    0,
                    202,
                    0
                ],
                "published": "2025-03-22T06:14:33Z",
                "published_parsed": [
                    2025,
                    3,
                    22,
                    6,
                    14,
                    33,
                    5,
                    81,
                    0
                ],
                "title": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient Frequency-Based Approach for Maximal Square Detection in\n  Binary Matrices"
                },
                "summary": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Detecting maximal square submatrices of ones in binary matrices is a\nfundamental problem with applications in computer vision and pattern\nrecognition. While the standard dynamic programming (DP) solution achieves\noptimal asymptotic complexity, its practical performance suffers from repeated\nminimum operations and inefficient memory access patterns that degrade cache\nutilization. To address these limitations, we introduce a novel frequency-based\nalgorithm that employs a greedy approach to track the columnar continuity of\nones through an adaptive frequency array and a dynamic thresholding mechanism.\nExtensive benchmarking demonstrates that the frequency-based algorithm achieves\nfaster performance than the standard DP in 100% of test cases with an average\nspeedup of 3.32x, a maximum speedup of 4.60x, and a minimum speedup of 2.31x\nacross matrices up to 5000x5000 with densities from 0.1 to 0.9. The algorithm's\naverage speedup exceeds 2.5x for all densities and rises to over 3.5x for\ndensities of 0.7 and higher across all matrix sizes. These results demonstrate\nthat the frequency-based approach is a superior alternative to standard DP and\nopens new possibilities for efficient matrix analysis in performance-critical\napplications."
                },
                "authors": [
                    {
                        "name": "Swastik Bhandari"
                    }
                ],
                "author_detail": {
                    "name": "Swastik Bhandari"
                },
                "author": "Swastik Bhandari",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.18974v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18974v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10524v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10524v2",
                "updated": "2025-07-21T07:45:14Z",
                "updated_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    45,
                    14,
                    0,
                    202,
                    0
                ],
                "published": "2025-07-14T17:49:00Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    17,
                    49,
                    0,
                    0,
                    195,
                    0
                ],
                "title": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Recursions: Learning Dynamic Recursive Depths for Adaptive\n  Token-Level Computation"
                },
                "summary": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Scaling language models unlocks impressive capabilities, but the accompanying\ncomputational and memory demands make both training and deployment expensive.\nExisting efficiency efforts typically target either parameter sharing or\nadaptive computation, leaving open the question of how to attain both\nsimultaneously. We introduce Mixture-of-Recursions (MoR), a unified framework\nthat combines the two axes of efficiency inside a single Recursive Transformer.\nMoR reuses a shared stack of layers across recursion steps to achieve parameter\nefficiency, while lightweight routers enable adaptive token-level thinking by\ndynamically assigning different recursion depths to individual tokens. This\nallows MoR to focus quadratic attention computation only among tokens still\nactive at a given recursion depth, further improving memory access efficiency\nby selectively caching only their key-value pairs. Beyond these core\nmechanisms, we also propose a KV sharing variant that reuses KV pairs from the\nfirst recursion, specifically designed to decrease prefill latency and memory\nfootprint. Across model scales ranging from 135M to 1.7B parameters, MoR forms\na new Pareto frontier: at equal training FLOPs and smaller model sizes, it\nsignificantly lowers validation perplexity and improves few-shot accuracy,\nwhile delivering higher throughput compared with vanilla and existing recursive\nbaselines. These gains demonstrate that MoR is an effective path towards\nlarge-model quality without incurring large-model cost."
                },
                "authors": [
                    {
                        "name": "Sangmin Bae"
                    },
                    {
                        "name": "Yujin Kim"
                    },
                    {
                        "name": "Reza Bayat"
                    },
                    {
                        "name": "Sungnyun Kim"
                    },
                    {
                        "name": "Jiyoun Ha"
                    },
                    {
                        "name": "Tal Schuster"
                    },
                    {
                        "name": "Adam Fisch"
                    },
                    {
                        "name": "Hrayr Harutyunyan"
                    },
                    {
                        "name": "Ziwei Ji"
                    },
                    {
                        "name": "Aaron Courville"
                    },
                    {
                        "name": "Se-Young Yun"
                    }
                ],
                "author_detail": {
                    "name": "Se-Young Yun"
                },
                "author": "Se-Young Yun",
                "arxiv_comment": "36 pages, 9 figures, 14 tables, codes at\n  https://github.com/raymin0223/mixture_of_recursions",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.10524v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10524v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09025v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09025v2",
                "updated": "2025-07-20T03:49:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    20,
                    3,
                    49,
                    3,
                    6,
                    201,
                    0
                ],
                "published": "2025-07-11T21:19:18Z",
                "published_parsed": [
                    2025,
                    7,
                    11,
                    21,
                    19,
                    18,
                    4,
                    192,
                    0
                ],
                "title": "Lizard: An Efficient Linearization Framework for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Lizard: An Efficient Linearization Framework for Large Language Models"
                },
                "summary": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose Lizard, a linearization framework that transforms pretrained\nTransformer-based Large Language Models (LLMs) into flexible, subquadratic\narchitectures for infinite-context generation. Transformer-based LLMs face\nsignificant memory and computational bottlenecks as context lengths increase,\ndue to the quadratic complexity of softmax attention and the growing key-value\n(KV) cache. Lizard addresses these limitations by introducing a subquadratic\nattention mechanism that closely approximates softmax attention while\npreserving the output quality. Unlike previous linearization methods, which are\noften limited by fixed model structures and therefore exclude gating\nmechanisms, Lizard incorporates a gating module inspired by recent\nstate-of-the-art linear models. This enables adaptive memory control, supports\nconstant-memory inference, offers strong length generalization, and allows more\nflexible model design. Lizard combines gated linear attention for global\ncontext compression with sliding window attention enhanced by meta memory,\nforming a hybrid mechanism that captures both long-range dependencies and\nfine-grained local interactions. Moreover, we introduce a hardware-aware\nalgorithm that accelerates the training speed of our models. Extensive\nexperiments show that Lizard achieves near-lossless recovery of the teacher\nmodel's performance across standard language modeling tasks, while\nsignificantly outperforming previous linearization methods. On the 5-shot MMLU\nbenchmark, Lizard improves over prior models by 18 points and shows significant\nimprovements on associative recall tasks."
                },
                "authors": [
                    {
                        "name": "Chien Van Nguyen"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Hanieh Deilamsalehy"
                    },
                    {
                        "name": "Puneet Mathur"
                    },
                    {
                        "name": "Viet Dac Lai"
                    },
                    {
                        "name": "Haoliang Wang"
                    },
                    {
                        "name": "Jayakumar Subramanian"
                    },
                    {
                        "name": "Ryan A. Rossi"
                    },
                    {
                        "name": "Trung Bui"
                    },
                    {
                        "name": "Nikos Vlassis"
                    },
                    {
                        "name": "Franck Dernoncourt"
                    },
                    {
                        "name": "Thien Huu Nguyen"
                    }
                ],
                "author_detail": {
                    "name": "Thien Huu Nguyen"
                },
                "author": "Thien Huu Nguyen",
                "arxiv_comment": "15 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09025v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09025v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.11092v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.11092v2",
                "updated": "2025-07-19T17:46:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    46,
                    19,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-05T19:47:22Z",
                "published_parsed": [
                    2025,
                    6,
                    5,
                    19,
                    47,
                    22,
                    3,
                    156,
                    0
                ],
                "title": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Context Tuning for Retrieval-Augmented Generation: Enhancing\n  Multi-Turn Planning and Tool Adaptation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has significantly advanced large\nlanguage models (LLMs) by grounding their outputs in external tools and\nknowledge sources. However, existing RAG systems are typically constrained to\nstatic, single-turn interactions with fixed toolsets, making them ill-suited\nfor dynamic domains such as healthcare and smart homes, where user intent,\navailable tools, and contextual factors evolve over time. We present Dynamic\nContext Tuning (DCT), a lightweight framework that extends RAG to support\nmulti-turn dialogue and evolving tool environments without requiring\nretraining. DCT integrates an attention-based context cache to track relevant\npast information, LoRA-based retrieval to dynamically select domain-specific\ntools, and efficient context compression to maintain inputs within LLM context\nlimits. Experiments on both synthetic and real-world benchmarks show that DCT\nimproves plan accuracy by 14% and reduces hallucinations by 37%, while matching\nGPT-4 performance at significantly lower cost. Furthermore, DCT generalizes to\npreviously unseen tools, enabling scalable and adaptable AI assistants across a\nwide range of dynamic environments."
                },
                "authors": [
                    {
                        "name": "Jubin Abhishek Soni"
                    },
                    {
                        "name": "Amit Anand"
                    },
                    {
                        "name": "Rajesh Kumar Pandey"
                    },
                    {
                        "name": "Aniket Abhishek Soni"
                    }
                ],
                "author_detail": {
                    "name": "Aniket Abhishek Soni"
                },
                "author": "Aniket Abhishek Soni",
                "arxiv_comment": "We are withdrawing the submission in order to thoroughly revise the\n  work",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.11092v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.11092v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17772v1",
                "updated": "2025-07-19T17:02:15Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T17:02:15Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    17,
                    2,
                    15,
                    5,
                    200,
                    0
                ],
                "title": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caching Techniques for Reducing the Communication Cost of Federated\n  Learning in IoT Environments"
                },
                "summary": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) allows multiple distributed devices to jointly train\na shared model without centralizing data, but communication cost remains a\nmajor bottleneck, especially in resource-constrained environments. This paper\nintroduces caching strategies - FIFO, LRU, and Priority-Based - to reduce\nunnecessary model update transmissions. By selectively forwarding significant\nupdates, our approach lowers bandwidth usage while maintaining model accuracy.\nExperiments on CIFAR-10 and medical datasets show reduced communication with\nminimal accuracy loss. Results confirm that intelligent caching improves\nscalability, memory efficiency, and supports reliable FL in edge IoT networks,\nmaking it practical for deployment in smart cities, healthcare, and other\nlatency-sensitive applications."
                },
                "authors": [
                    {
                        "name": "Ahmad Alhonainy"
                    },
                    {
                        "name": "Praveen Rao"
                    }
                ],
                "author_detail": {
                    "name": "Praveen Rao"
                },
                "arxiv_affiliation": "University of Missouri, USA",
                "author": "Praveen Rao",
                "arxiv_comment": "Journal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.16002v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.16002v3",
                "updated": "2025-07-19T07:41:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    7,
                    41,
                    3,
                    5,
                    200,
                    0
                ],
                "published": "2025-02-21T23:34:29Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    23,
                    34,
                    29,
                    4,
                    52,
                    0
                ],
                "title": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KVLink: Accelerating Large Language Models via Efficient KV Cache Reuse"
                },
                "summary": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We describe KVLink, an approach for efficient key-value (KV) cache reuse in\nlarge language models (LLMs). In many LLM applications, different inputs can\nshare overlapping context, such as the same retrieved document appearing in\nmultiple queries. However, the LLMs still need to encode the entire context for\neach query, leading to redundant computation. In this paper, we investigate a\nnew strategy to eliminate such inefficiency, where the KV cache of each\ndocument is precomputed independently. During inference, the KV caches of\nretrieved documents are concatenated, allowing the model to reuse cached\nrepresentations instead of recomputing them. To mitigate the performance\ndegradation when using KV caches computed independently for each document,\nKVLink introduces two key techniques: adjusting positional embeddings of the KV\ncache at inference to match the global position after concatenation, and using\ntrainable special tokens to restore self-attention across independently encoded\ndocuments. Experiments across 7 datasets demonstrate that KVLink improves\nquestion answering accuracy by an average of 4% over state-of-the-art methods.\nFurthermore, by leveraging precomputed KV caches, our approach reduces\ntime-to-first-token by up to 96% compared to standard LLM inference, making it\na scalable and efficient solution for context reuse. Additionally, KVLink can\nbe combined with KV cache compression to further save cache loading and storage\noverhead while outperforming the baselines."
                },
                "authors": [
                    {
                        "name": "Jingbo Yang"
                    },
                    {
                        "name": "Bairu Hou"
                    },
                    {
                        "name": "Wei Wei"
                    },
                    {
                        "name": "Yujia Bao"
                    },
                    {
                        "name": "Shiyu Chang"
                    }
                ],
                "author_detail": {
                    "name": "Shiyu Chang"
                },
                "author": "Shiyu Chang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.16002v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.16002v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08373v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08373v2",
                "updated": "2025-07-19T03:40:40Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    3,
                    40,
                    40,
                    5,
                    200,
                    0
                ],
                "published": "2025-06-10T02:37:46Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    2,
                    37,
                    46,
                    1,
                    161,
                    0
                ],
                "title": "Draft-based Approximate Inference for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Draft-based Approximate Inference for LLMs"
                },
                "summary": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing inference for long-context Large Language Models (LLMs) is\nincreasingly important due to the quadratic compute and linear memory\ncomplexity of Transformers. Existing approximation methods, such as key-value\n(KV) cache dropping, sparse attention, and prompt compression, typically rely\non rough predictions of token or KV pair importance. We propose a novel\nframework for approximate LLM inference that leverages small draft models to\nmore accurately predict the importance of tokens and KV pairs. Specifically, we\nintroduce two instantiations of our proposed framework: (i) SpecKV, the first\nmethod that leverages a draft output to accurately assess the importance of\neach KV pair for more effective KV cache dropping, and (ii) SpecPC, which uses\nthe draft model's attention activations to identify and discard unimportant\nprompt tokens. We motivate our methods with theoretical and empirical analyses,\nand show a strong correlation between the attention patterns of draft and\ntarget models. Extensive experiments on long-context benchmarks show that our\nmethods consistently achieve higher accuracy than existing baselines, while\npreserving the same improvements in memory usage, latency, and throughput. Our\ncode is available at https://github.com/furiosa-ai/draft-based-approx-llm."
                },
                "authors": [
                    {
                        "name": "Kevin Galim"
                    },
                    {
                        "name": "Ethan Ewer"
                    },
                    {
                        "name": "Wonjun Kang"
                    },
                    {
                        "name": "Minjae Lee"
                    },
                    {
                        "name": "Hyung Il Koo"
                    },
                    {
                        "name": "Kangwook Lee"
                    }
                ],
                "author_detail": {
                    "name": "Kangwook Lee"
                },
                "author": "Kangwook Lee",
                "arxiv_comment": "Added discussion and comparison with SpecPrefill",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08373v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08373v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.17771v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.17771v1",
                "updated": "2025-07-19T00:57:54Z",
                "updated_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "published": "2025-07-19T00:57:54Z",
                "published_parsed": [
                    2025,
                    7,
                    19,
                    0,
                    57,
                    54,
                    5,
                    200,
                    0
                ],
                "title": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Vector Integration in Embedded RISC-V SoCs for End to End CNN\n  Inference Acceleration"
                },
                "summary": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of heterogeneity and domain-specific architectures targeting\ndeep learning inference show great potential for enabling the deployment of\nmodern CNNs on resource-constrained embedded platforms. A significant\ndevelopment is the diversification of custom hardware solely targeting the most\nexpensive parts of CNNs. DLAs (deep learning accelerators) and NPUs (neural\nprocessing units), among others, can overcome the approaching limits of\ntraditional silicon scaling and provide a solution to the power/performance\ntradeoff within embedded SoCs. Efficient DSA utilization requires proper system\nintegration and a compilation/execution model for balanced execution in these\nheterogeneous architectures. There is a critical need for proper system\nintegration and an efficient compilation/execution model for balanced execution\nin these heterogeneous architectures. This work highlights the hardware\nintegration challenges for efficiently placing these units within the memory\nhierarchy and correct proximity to other execution blocks. We experimentally\nverify performance bottlenecks in CNN execution and pre/post-processing at\nruntime, where previous attention has generally been given to accelerator\nspeedup alone. This work takes advantage of the ratification of the RISC-V\nVector 1.0 extension and demonstrates its potential as a flexible target within\na well-suited cache hierarchy scheme to reduce pre-processing bottlenecks and\nCPU fallback processes. Our results show up to a 9x speedup of image\npre-processing and YOLOv3 fallback layer execution by up to 3x compared to CPU.\nWe demonstrate RVV-1.0 in exposing a flexible programming model that can enable\na balanced computation and memory footprint on accelerator-rich embedded SoCs\nsupporting modern deep-learning dataflows while consuming less power than\ntraditional parallel execution platforms."
                },
                "authors": [
                    {
                        "name": "Dmitri Lyalikov"
                    }
                ],
                "author_detail": {
                    "name": "Dmitri Lyalikov"
                },
                "author": "Dmitri Lyalikov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.17771v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.17771v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13961v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13961v1",
                "updated": "2025-07-18T14:24:29Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T14:24:29Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    14,
                    24,
                    29,
                    4,
                    199,
                    0
                ],
                "title": "Secretive Hotplug Coded Caching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Secretive Hotplug Coded Caching"
                },
                "summary": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider a coded caching model called \\textit{hotplug coded\ncaching}, in which some users are offline during the delivery phase. The\nconcept of Hotplug Placement Delivery Arrays (HpPDAs) for hotplug coded caching\nsystems has been introduced in the literature, and two classes of HpPDAs are\nknown. In this paper, we consider a secrecy constraint in hotplug coded caching\nsetup, where users should not learn anything about any file from their cache\ncontent, and active users should not gain any information about files other\nthan their demanded file from either their cache content or the server\ntransmissions. We propose two secretive schemes for the two classes of HpPDAs\nand compare them with a baseline scheme, which is a secretive scheme using PDAs\nfor the classical coded caching setup and can be trivially adapted for the\nhotplug coded caching setup. We numerically show that our schemes outperform\nthe baseline scheme in certain memory regions."
                },
                "authors": [
                    {
                        "name": "Mallikharjuna Chinnapadamala"
                    },
                    {
                        "name": "Charul Rajput"
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "11 pages and 2 figures. arXiv admin note: text overlap with\n  arXiv:2404.06433",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13961v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13961v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v2",
                "updated": "2025-07-18T13:29:47Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    13,
                    29,
                    47,
                    4,
                    199,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "arxiv_journal_ref": "Proceedings of the Nineteenth ACM Conference on Recommender\n  Systems (RecSys '25), September 22--26, 2025, Prague, Czech Republic",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13681v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13681v1",
                "updated": "2025-07-18T06:12:08Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "published": "2025-07-18T06:12:08Z",
                "published_parsed": [
                    2025,
                    7,
                    18,
                    6,
                    12,
                    8,
                    4,
                    199,
                    0
                ],
                "title": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LoopServe: An Adaptive Dual-phase LLM Inference Acceleration System for\n  Multi-Turn Dialogues"
                },
                "summary": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-turn dialogues are essential in many real-world applications of large\nlanguage models, such as chatbots and virtual assistants. As conversation\nhistories become longer, existing large language models face increasing\ncomputational and memory challenges, which hinder their ability to provide\nefficient and responsive interactions. Most current acceleration methods either\ncompress the context or optimize key value caching, but they often rely on\nfixed or position-based heuristics that do not adapt well to the dynamic and\nunpredictable patterns found in actual multi-turn conversations. In this paper,\nwe present LoopServe, an adaptive dual-phase inference acceleration framework\nfor large language models in multi-turn dialogues. LoopServe introduces two\nmain innovations. First, it performs online sparsification during the\nprefilling phase by dynamically selecting the most important parts of the\nattention matrix for each new input. Second, it uses progressive key value\ncompression during decoding by adaptively maintaining a relevant and efficient\ncache based on the most recently generated output tokens. We also propose a\n\\href{https://huggingface.co/datasets/TreeAILab/Multi-turn_Long-context_Benchmark_for_LLMs}{new\nbenchmark} with eleven multi-turn datasets that reflect realistic query\npositions and conversational dependencies. Extensive experiments demonstrate\nthat LoopServe consistently achieves superior effectiveness compared to\nexisting baselines and significantly accelerates LLM inference across a wide\nrange of long-context dialogue tasks."
                },
                "authors": [
                    {
                        "name": "Haoyang Li"
                    },
                    {
                        "name": "Zhanchao Xu"
                    },
                    {
                        "name": "Yiming Li"
                    },
                    {
                        "name": "Xuejia Chen"
                    },
                    {
                        "name": "Darian Li"
                    },
                    {
                        "name": "Anxin Tian"
                    },
                    {
                        "name": "Qingfa Xiao"
                    },
                    {
                        "name": "Cheng Deng"
                    },
                    {
                        "name": "Jun Wang"
                    },
                    {
                        "name": "Qing Li"
                    },
                    {
                        "name": "Lei Chen"
                    },
                    {
                        "name": "Mingxuan Yuan"
                    }
                ],
                "author_detail": {
                    "name": "Mingxuan Yuan"
                },
                "author": "Mingxuan Yuan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13681v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13681v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v3",
                "updated": "2025-07-18T01:49:36Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    49,
                    36,
                    4,
                    199,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the \\textbf{E}rror-\\textbf{O}ptimized\n\\textbf{C}ache (\\textbf{EOC}). This method introduces three key improvements:\n\\textbf{(1)} Prior knowledge extraction: Extract and process the caching\ndifferences; \\textbf{(2)} A judgment method for cache optimization: Determine\nwhether certain caching steps need to be optimized; \\textbf{(3)} Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of \\textbf{75}\\%,\n\\textbf{50}\\%, and \\textbf{25}\\%, and the training-based model\nLearning-to-cache has a caching level of \\textbf{22}\\%. Specifically, the FID\nvalues change from 30.454 to 21.690 (\\textbf{28.8}\\%), from 6.857 to 5.821\n(\\textbf{15.1}\\%), from 3.870 to 3.692 (\\textbf{4.6}\\%), and from 3.539 to\n3.451 (\\textbf{2.5}\\%) respectively. Code is available at\nhttps://github.com/qiujx0520/EOC_MM2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ACM MM 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05156v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05156v2",
                "updated": "2025-07-18T01:36:03Z",
                "updated_parsed": [
                    2025,
                    7,
                    18,
                    1,
                    36,
                    3,
                    4,
                    199,
                    0
                ],
                "published": "2025-03-07T05:31:47Z",
                "published_parsed": [
                    2025,
                    3,
                    7,
                    5,
                    31,
                    47,
                    4,
                    66,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Gradient-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Gradient-Optimized Cache"
                },
                "summary": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Feature caching has emerged as an effective strategy to accelerate diffusion\ntransformer (DiT) sampling through temporal feature reuse. It is a challenging\nproblem since (1) Progressive error accumulation from cached blocks\nsignificantly degrades generation quality, particularly when over 50\\% of\nblocks are cached; (2) Current error compensation approaches neglect dynamic\nperturbation patterns during the caching process, leading to suboptimal error\ncorrection. To solve these problems, we propose the Gradient-Optimized Cache\n(GOC) with two key innovations: (1) Cached Gradient Propagation: A gradient\nqueue dynamically computes the gradient differences between cached and\nrecomputed features. These gradients are weighted and propagated to subsequent\nsteps, directly compensating for the approximation errors introduced by\ncaching. (2) Inflection-Aware Optimization: Through statistical analysis of\nfeature variation patterns, we identify critical inflection points where the\ndenoising trajectory changes direction. By aligning gradient updates with these\ndetected phases, we prevent conflicting gradient directions during error\ncorrection. Extensive evaluations on ImageNet demonstrate GOC's superior\ntrade-off between efficiency and quality. With 50\\% cached blocks, GOC achieves\nIS 216.28 (26.3\\% higher) and FID 3.907 (43\\% lower) compared to baseline DiT,\nwhile maintaining identical computational costs. These improvements persist\nacross various cache ratios, demonstrating robust adaptability to different\nacceleration requirements. Code is available at\nhttps://github.com/qiujx0520/GOC_ICCV2025.git."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Kezhou Chen"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "arxiv_journal_ref": "ICCV 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05156v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05156v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.13575v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.13575v1",
                "updated": "2025-07-17T23:37:19Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-17T23:37:19Z",
                "published_parsed": [
                    2025,
                    7,
                    17,
                    23,
                    37,
                    19,
                    3,
                    198,
                    0
                ],
                "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Apple Intelligence Foundation Language Models: Tech Report 2025"
                },
                "summary": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce two multilingual, multimodal foundation language models that\npower Apple Intelligence features across Apple devices and services: i a\n3B-parameter on-device model optimized for Apple silicon through architectural\ninnovations such as KV-cache sharing and 2-bit quantization-aware training; and\nii a scalable server model built on a novel Parallel-Track Mixture-of-Experts\nPT-MoE transformer that combines track parallelism, mixture-of-experts sparse\ncomputation, and interleaved global-local attention to deliver high quality\nwith competitive cost on Apple's Private Cloud Compute platform. Both models\nare trained on large-scale multilingual and multimodal datasets sourced via\nresponsible web crawling, licensed corpora, and high-quality synthetic data,\nthen further refined with supervised fine-tuning and reinforcement learning on\na new asynchronous platform. The resulting models support several additional\nlanguages while understanding images and executing tool calls. In public\nbenchmarks and human evaluations, both the server model and the on-device model\nmatch or surpass comparably sized open baselines.\n  A new Swift-centric Foundation Models framework exposes guided generation,\nconstrained tool calling, and LoRA adapter fine-tuning, allowing developers to\nintegrate these capabilities with a few lines of code. The latest advancements\nin Apple Intelligence models are grounded in our Responsible AI approach with\nsafeguards like content filtering and locale-specific evaluation, as well as\nour commitment to protecting our users' privacy with innovations like Private\nCloud Compute."
                },
                "authors": [
                    {
                        "name": "Hanzhi Zhou"
                    },
                    {
                        "name": "Erik Hornberger"
                    },
                    {
                        "name": "Pengsheng Guo"
                    },
                    {
                        "name": "Xiyou Zhou"
                    },
                    {
                        "name": "Saiwen Wang"
                    },
                    {
                        "name": "Xin Wang"
                    },
                    {
                        "name": "Yifei He"
                    },
                    {
                        "name": "Xuankai Chang"
                    },
                    {
                        "name": "Rene Rauch"
                    },
                    {
                        "name": "Louis D'hauwe"
                    },
                    {
                        "name": "John Peebles"
                    },
                    {
                        "name": "Alec Doane"
                    },
                    {
                        "name": "Kohen Chia"
                    },
                    {
                        "name": "Jenna Thibodeau"
                    },
                    {
                        "name": "Zi-Yi Dou"
                    },
                    {
                        "name": "Yuanyang Zhang"
                    },
                    {
                        "name": "Ruoming Pang"
                    },
                    {
                        "name": "Reed Li"
                    },
                    {
                        "name": "Zhifeng Chen"
                    },
                    {
                        "name": "Jeremy Warner"
                    },
                    {
                        "name": "Zhaoyang Xu"
                    },
                    {
                        "name": "Sophy Lee"
                    },
                    {
                        "name": "David Mizrahi"
                    },
                    {
                        "name": "Ramsey Tantawi"
                    },
                    {
                        "name": "Chris Chaney"
                    },
                    {
                        "name": "Kelsey Peterson"
                    },
                    {
                        "name": "Jun Qin"
                    },
                    {
                        "name": "Alex Dombrowski"
                    },
                    {
                        "name": "Mira Chiang"
                    },
                    {
                        "name": "Aiswarya Raghavan"
                    },
                    {
                        "name": "Gerard Casamayor"
                    },
                    {
                        "name": "Qibin Chen"
                    },
                    {
                        "name": "Aonan Zhang"
                    },
                    {
                        "name": "Nathalie Tran"
                    },
                    {
                        "name": "Jianyu Wang"
                    },
                    {
                        "name": "Hang Su"
                    },
                    {
                        "name": "Thomas Voice"
                    },
                    {
                        "name": "Alessandro Pappalardo"
                    },
                    {
                        "name": "Brycen Wershing"
                    },
                    {
                        "name": "Prasanth Yadla"
                    },
                    {
                        "name": "Rui Li"
                    },
                    {
                        "name": "Priyal Chhatrapati"
                    },
                    {
                        "name": "Ismael Fernandez"
                    },
                    {
                        "name": "Yusuf Goren"
                    },
                    {
                        "name": "Xin Zheng"
                    },
                    {
                        "name": "Forrest Huang"
                    },
                    {
                        "name": "Tao Lei"
                    },
                    {
                        "name": "Eray Yildiz"
                    },
                    {
                        "name": "Alper Kokmen"
                    },
                    {
                        "name": "Gokul Santhanam"
                    },
                    {
                        "name": "Areeba Kamal"
                    },
                    {
                        "name": "Kaan Elgin"
                    },
                    {
                        "name": "Dian Ang Yap"
                    },
                    {
                        "name": "Jeremy Liu"
                    },
                    {
                        "name": "Peter Gray"
                    },
                    {
                        "name": "Howard Xing"
                    },
                    {
                        "name": "Kieran Liu"
                    },
                    {
                        "name": "Matteo Ronchi"
                    },
                    {
                        "name": "Moritz Schwarzer-Becker"
                    },
                    {
                        "name": "Yun Zhu"
                    },
                    {
                        "name": "Mandana Saebi"
                    },
                    {
                        "name": "Jeremy Snow"
                    },
                    {
                        "name": "David Griffiths"
                    },
                    {
                        "name": "Guillaume Tartavel"
                    },
                    {
                        "name": "Erin Feldman"
                    },
                    {
                        "name": "Simon Lehnerer"
                    },
                    {
                        "name": "Fernando Bermdez-Medina"
                    },
                    {
                        "name": "Hans Han"
                    },
                    {
                        "name": "Joe Zhou"
                    },
                    {
                        "name": "Xiaoyi Ren"
                    },
                    {
                        "name": "Sujeeth Reddy"
                    },
                    {
                        "name": "Zirui Wang"
                    },
                    {
                        "name": "Tom Gunter"
                    },
                    {
                        "name": "Albert Antony"
                    },
                    {
                        "name": "Yuanzhi Li"
                    },
                    {
                        "name": "John Dennison"
                    },
                    {
                        "name": "Tony Sun"
                    },
                    {
                        "name": "Yena Han"
                    },
                    {
                        "name": "Yi Qin"
                    },
                    {
                        "name": "Sam Davarnia"
                    },
                    {
                        "name": "Jeffrey Bigham"
                    },
                    {
                        "name": "Wayne Shan"
                    },
                    {
                        "name": "Hannah Gillis Coleman"
                    },
                    {
                        "name": "Guillaume Klein"
                    },
                    {
                        "name": "Peng Liu"
                    },
                    {
                        "name": "Muyang Yu"
                    },
                    {
                        "name": "Jack Cackler"
                    },
                    {
                        "name": "Yuan Gao"
                    },
                    {
                        "name": "Crystal Xiao"
                    },
                    {
                        "name": "Binazir Karimzadeh"
                    },
                    {
                        "name": "Zhengdong Zhang"
                    },
                    {
                        "name": "Felix Bai"
                    },
                    {
                        "name": "Albin Madappally Jose"
                    },
                    {
                        "name": "Feng Nan"
                    },
                    {
                        "name": "Nazir Kamaldin"
                    },
                    {
                        "name": "Dong Yin"
                    },
                    {
                        "name": "Hans Hao"
                    },
                    {
                        "name": "Yanchao Sun"
                    },
                    {
                        "name": "Yi Hua"
                    },
                    {
                        "name": "Charles Maalouf"
                    },
                    {
                        "name": "Alex Guillen Garcia"
                    },
                    {
                        "name": "Guoli Yin"
                    },
                    {
                        "name": "Lezhi Li"
                    },
                    {
                        "name": "Mohana Prasad Sathya Moorthy"
                    },
                    {
                        "name": "Hongbin Gao"
                    },
                    {
                        "name": "Jay Tang"
                    },
                    {
                        "name": "Joanna Arreaza-Taylor"
                    },
                    {
                        "name": "Faye Lao"
                    },
                    {
                        "name": "Carina Peng"
                    },
                    {
                        "name": "Josh Shaffer"
                    },
                    {
                        "name": "Dan Masi"
                    },
                    {
                        "name": "Sushma Rao"
                    },
                    {
                        "name": "Tommi Vehvilainen"
                    },
                    {
                        "name": "Senyu Tong"
                    },
                    {
                        "name": "Dongcai Shen"
                    },
                    {
                        "name": "Yang Zhao"
                    },
                    {
                        "name": "Chris Bartels"
                    },
                    {
                        "name": "Peter Fu"
                    },
                    {
                        "name": "Qingqing Cao"
                    },
                    {
                        "name": "Christopher Neubauer"
                    },
                    {
                        "name": "Ethan Li"
                    },
                    {
                        "name": "Mingfei Gao"
                    },
                    {
                        "name": "Rebecca Callahan"
                    },
                    {
                        "name": "Richard Wei"
                    },
                    {
                        "name": "Patrick Dong"
                    },
                    {
                        "name": "Alex Braunstein"
                    },
                    {
                        "name": "Sachin Ravi"
                    },
                    {
                        "name": "Adolfo Lopez Mendez"
                    },
                    {
                        "name": "Kaiwei Huang"
                    },
                    {
                        "name": "Kun Duan"
                    },
                    {
                        "name": "Haoshuo Huang"
                    },
                    {
                        "name": "Rui Qian"
                    },
                    {
                        "name": "Stefano Ligas"
                    },
                    {
                        "name": "Jordan Huffaker"
                    },
                    {
                        "name": "Dongxu Li"
                    },
                    {
                        "name": "Bailin Wang"
                    },
                    {
                        "name": "Nanzhu Wang"
                    },
                    {
                        "name": "Anuva Agarwal"
                    },
                    {
                        "name": "Tait Madsen"
                    },
                    {
                        "name": "Josh Newnham"
                    },
                    {
                        "name": "Abhishek Sharma"
                    },
                    {
                        "name": "Zhile Ren"
                    },
                    {
                        "name": "Deepak Gopinath"
                    },
                    {
                        "name": "Erik Daxberger"
                    },
                    {
                        "name": "Saptarshi Guha"
                    },
                    {
                        "name": "Oron Levy"
                    },
                    {
                        "name": "Jing Lu"
                    },
                    {
                        "name": "Nan Dun"
                    },
                    {
                        "name": "Marc Kirchner"
                    },
                    {
                        "name": "Yinfei Yang"
                    },
                    {
                        "name": "Manjot Bilkhu"
                    },
                    {
                        "name": "Dave Nelson"
                    },
                    {
                        "name": "Anthony Spalvieri-Kruse"
                    },
                    {
                        "name": "Juan Lao Tebar"
                    },
                    {
                        "name": "Yang Xu"
                    },
                    {
                        "name": "Phani Mutyala"
                    },
                    {
                        "name": "Gabriel Jacoby-Cooper"
                    },
                    {
                        "name": "Yingbo Wang"
                    },
                    {
                        "name": "Karla Vega"
                    },
                    {
                        "name": "Vishaal Mahtani"
                    },
                    {
                        "name": "Darren Botten"
                    },
                    {
                        "name": "Eric Wang"
                    },
                    {
                        "name": "Hanli Li"
                    },
                    {
                        "name": "Matthias Paulik"
                    },
                    {
                        "name": "Haoran Yan"
                    },
                    {
                        "name": "Navid Shiee"
                    },
                    {
                        "name": "Yihao Qian"
                    },
                    {
                        "name": "Bugu Wu"
                    },
                    {
                        "name": "Qi Zhu"
                    },
                    {
                        "name": "Ob Adaranijo"
                    },
                    {
                        "name": "Bhuwan Dhingra"
                    },
                    {
                        "name": "Zhe Gan"
                    },
                    {
                        "name": "Nicholas Seidl"
                    },
                    {
                        "name": "Grace Duanmu"
                    },
                    {
                        "name": "Rong Situ"
                    },
                    {
                        "name": "Yiping Ma"
                    },
                    {
                        "name": "Yin Xia"
                    },
                    {
                        "name": "David Riazati"
                    },
                    {
                        "name": "Vasileios Saveris"
                    },
                    {
                        "name": "Anh Nguyen"
                    },
                    {
                        "name": "Michael"
                    },
                    {
                        "name": "Lee"
                    },
                    {
                        "name": "Patrick Sonnenberg"
                    },
                    {
                        "name": "Chinguun Erdenebileg"
                    },
                    {
                        "name": "Yanghao Li"
                    },
                    {
                        "name": "Vivian Ma"
                    },
                    {
                        "name": "James Chou"
                    },
                    {
                        "name": "Isha Garg"
                    },
                    {
                        "name": "Mark Lee"
                    },
                    {
                        "name": "Keen You"
                    },
                    {
                        "name": "Yuhong Li"
                    },
                    {
                        "name": "Ransen Niu"
                    },
                    {
                        "name": "Nandhitha Raghuram"
                    },
                    {
                        "name": "Pulkit Agrawal"
                    },
                    {
                        "name": "Henry Mason"
                    },
                    {
                        "name": "Sumeet Singh"
                    },
                    {
                        "name": "Keyu He"
                    },
                    {
                        "name": "Hong-You Chen"
                    },
                    {
                        "name": "Lucas Guibert"
                    },
                    {
                        "name": "Shiyu Li"
                    },
                    {
                        "name": "Varsha Paidi"
                    },
                    {
                        "name": "Narendran Raghavan"
                    },
                    {
                        "name": "Mingze Xu"
                    },
                    {
                        "name": "Yuli Yang"
                    },
                    {
                        "name": "Sergiu Sima"
                    },
                    {
                        "name": "Irina Belousova"
                    },
                    {
                        "name": "Sprite Chu"
                    },
                    {
                        "name": "Afshin Dehghan"
                    },
                    {
                        "name": "Philipp Dufter"
                    },
                    {
                        "name": "David Haldimann"
                    },
                    {
                        "name": "Zhen Yang"
                    },
                    {
                        "name": "Margit Bowler"
                    },
                    {
                        "name": "Chang Liu"
                    },
                    {
                        "name": "Ying-Chang Cheng"
                    },
                    {
                        "name": "Vivek Rathod"
                    },
                    {
                        "name": "Syd Evans"
                    },
                    {
                        "name": "Wilson Tsao"
                    },
                    {
                        "name": "Dustin Withers"
                    },
                    {
                        "name": "Haitian Sun"
                    },
                    {
                        "name": "Biyao Wang"
                    },
                    {
                        "name": "Peter Grasch"
                    },
                    {
                        "name": "Walker Cheng"
                    },
                    {
                        "name": "Yihao Feng"
                    },
                    {
                        "name": "Vivek Kumar"
                    },
                    {
                        "name": "Frank Chu"
                    },
                    {
                        "name": "Victoria MnchJuan Haladjian"
                    },
                    {
                        "name": "Doug Kang"
                    },
                    {
                        "name": "Jiarui Lu"
                    },
                    {
                        "name": "Ciro Sannino"
                    },
                    {
                        "name": "Max Lam"
                    },
                    {
                        "name": "Floris Weers"
                    },
                    {
                        "name": "Bowen Pan"
                    },
                    {
                        "name": "Kenneth Jung"
                    },
                    {
                        "name": "Dhaval Doshi"
                    },
                    {
                        "name": "Fangping Shi"
                    },
                    {
                        "name": "Olli Saarikivi"
                    },
                    {
                        "name": "Alp Aygar"
                    },
                    {
                        "name": "Josh Elman"
                    },
                    {
                        "name": "Cheng Leong"
                    },
                    {
                        "name": "Eshan Verma"
                    },
                    {
                        "name": "Matthew Lei"
                    },
                    {
                        "name": "Jeff Nichols"
                    },
                    {
                        "name": "Jiulong Shan"
                    },
                    {
                        "name": "Donald Zhang"
                    },
                    {
                        "name": "Lawrence Zhou"
                    },
                    {
                        "name": "Stephen Murphy"
                    },
                    {
                        "name": "Xianzhi Du"
                    },
                    {
                        "name": "Chang Lan"
                    },
                    {
                        "name": "Ankur Jain"
                    },
                    {
                        "name": "Elmira Amirloo"
                    },
                    {
                        "name": "Marcin Eichner"
                    },
                    {
                        "name": "Naomy Sabo"
                    },
                    {
                        "name": "Anupama Mann Anupama"
                    },
                    {
                        "name": "David Qiu"
                    },
                    {
                        "name": "Zhao Meng"
                    },
                    {
                        "name": "Michael FitzMaurice"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Simon Yeung"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Marco Zuliani"
                    },
                    {
                        "name": "Andrew Hansen"
                    },
                    {
                        "name": "Yang Lu"
                    },
                    {
                        "name": "Brent Ramerth"
                    },
                    {
                        "name": "Ziyi Zhong"
                    },
                    {
                        "name": "Parsa Mazaheri"
                    },
                    {
                        "name": "Matthew Hopkins"
                    },
                    {
                        "name": "Mengyu Li"
                    },
                    {
                        "name": "Simon Wang"
                    },
                    {
                        "name": "David Chen"
                    },
                    {
                        "name": "Farzin Rasteh"
                    },
                    {
                        "name": "Chong Wang"
                    },
                    {
                        "name": "Josh Gardner"
                    },
                    {
                        "name": "Asaf Liberman"
                    },
                    {
                        "name": "Haoxuan You"
                    },
                    {
                        "name": "Andrew Walkingshaw"
                    },
                    {
                        "name": "Xingyu Zhou"
                    },
                    {
                        "name": "Jinhao Lei"
                    },
                    {
                        "name": "Yan Meng"
                    },
                    {
                        "name": "Quentin Keunebroek"
                    },
                    {
                        "name": "Sam Wiseman"
                    },
                    {
                        "name": "Anders Boesen Lindbo Larsen"
                    },
                    {
                        "name": "Yi Zhang"
                    },
                    {
                        "name": "Zaid Ahmed"
                    },
                    {
                        "name": "Haiming Gang"
                    },
                    {
                        "name": "Aaron Franklin"
                    },
                    {
                        "name": "Kelvin Zou"
                    },
                    {
                        "name": "Guillaume Seguin"
                    },
                    {
                        "name": "Jonathan Janke"
                    },
                    {
                        "name": "Rachel Burger"
                    },
                    {
                        "name": "Co Giang"
                    },
                    {
                        "name": "Cheng Shen"
                    },
                    {
                        "name": "Jen Liu"
                    },
                    {
                        "name": "Sanskruti Shah"
                    },
                    {
                        "name": "Xiang Kong"
                    },
                    {
                        "name": "Yiran Fei"
                    },
                    {
                        "name": "TJ Collins"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Zhiyun Lu"
                    },
                    {
                        "name": "Michael Booker"
                    },
                    {
                        "name": "Qin Ba"
                    },
                    {
                        "name": "Yasutaka Tanaka"
                    },
                    {
                        "name": "Andres Romero Mier Y Teran"
                    },
                    {
                        "name": "Federico Scozzafava"
                    },
                    {
                        "name": "Regan Poston"
                    },
                    {
                        "name": "Jane Li"
                    },
                    {
                        "name": "Eduardo Jimenez"
                    },
                    {
                        "name": "Bas Straathof"
                    },
                    {
                        "name": "Karanjeet Singh"
                    },
                    {
                        "name": "Lindsay Hislop"
                    },
                    {
                        "name": "Rajat Arora"
                    },
                    {
                        "name": "Deepa Seshadri"
                    },
                    {
                        "name": "Boyue Li"
                    },
                    {
                        "name": "Colorado Reed"
                    },
                    {
                        "name": "Zhen Li"
                    },
                    {
                        "name": "TJ Lu"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Kaelen Haag"
                    },
                    {
                        "name": "Nicholas Lusskin"
                    },
                    {
                        "name": "Raunak Sinha"
                    },
                    {
                        "name": "Rahul Nair"
                    },
                    {
                        "name": "Eldon Schoop"
                    },
                    {
                        "name": "Mary Beth Kery"
                    },
                    {
                        "name": "Mehrdad Farajtbar"
                    },
                    {
                        "name": "Brenda Yang"
                    },
                    {
                        "name": "George Horrell"
                    },
                    {
                        "name": "Shiwen Zhao"
                    },
                    {
                        "name": "Dhruti Shah"
                    },
                    {
                        "name": "Cha Chen"
                    },
                    {
                        "name": "Bowen Zhang"
                    },
                    {
                        "name": "Chang Gao"
                    },
                    {
                        "name": "Devi Krishna"
                    },
                    {
                        "name": "Jennifer Mallalieu"
                    },
                    {
                        "name": "Javier Movellan"
                    },
                    {
                        "name": "Di Feng"
                    },
                    {
                        "name": "Emily Zhang"
                    },
                    {
                        "name": "Sam Xu"
                    },
                    {
                        "name": "Junting Pan"
                    },
                    {
                        "name": "Dominik Moritz"
                    },
                    {
                        "name": "Suma Jayaram"
                    },
                    {
                        "name": "Kevin Smith"
                    },
                    {
                        "name": "Dongseong Hwang"
                    },
                    {
                        "name": "Daniel Parilla"
                    },
                    {
                        "name": "Jiaming Hu"
                    },
                    {
                        "name": "You-Cyuan Jhang"
                    },
                    {
                        "name": "Emad Soroush"
                    },
                    {
                        "name": "Fred Hohman"
                    },
                    {
                        "name": "Nan Du"
                    },
                    {
                        "name": "Emma Wang"
                    },
                    {
                        "name": "Sam Dodge"
                    },
                    {
                        "name": "Pragnya Sridhar"
                    },
                    {
                        "name": "Joris Pelemans"
                    },
                    {
                        "name": "Wei Fang"
                    },
                    {
                        "name": "Nina Wenzel"
                    },
                    {
                        "name": "Joseph Yitan Cheng"
                    },
                    {
                        "name": "Hadas Kotek"
                    },
                    {
                        "name": "Chung-Cheng Chiu"
                    },
                    {
                        "name": "Meng Cao"
                    },
                    {
                        "name": "Haijing Fu"
                    },
                    {
                        "name": "Ruixuan Hou"
                    },
                    {
                        "name": "Ke Ye"
                    },
                    {
                        "name": "Diane Zhu"
                    },
                    {
                        "name": "Nikhil Bhendawade"
                    },
                    {
                        "name": "Joseph Astrauskas"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Sai Aitharaju"
                    },
                    {
                        "name": "Wentao Wu"
                    },
                    {
                        "name": "Artsiom Peshko"
                    },
                    {
                        "name": "Hyunjik Kim"
                    },
                    {
                        "name": "Nilesh Shahdadpuri"
                    },
                    {
                        "name": "Andy De Wang"
                    },
                    {
                        "name": "Qi Shan"
                    },
                    {
                        "name": "Piotr Maj"
                    },
                    {
                        "name": "Raul Rea Menacho"
                    },
                    {
                        "name": "Justin Lazarow"
                    },
                    {
                        "name": "Eric Liang Yang"
                    },
                    {
                        "name": "Arsalan Farooq"
                    },
                    {
                        "name": "Donghan Yu"
                    },
                    {
                        "name": "David Gera"
                    },
                    {
                        "name": "Minsik Cho"
                    },
                    {
                        "name": "Kavya Nerella"
                    },
                    {
                        "name": "Yongqiang Wang"
                    },
                    {
                        "name": "Tao Jia"
                    },
                    {
                        "name": "John Park"
                    },
                    {
                        "name": "Jeff Lai"
                    },
                    {
                        "name": "Haotian Zhang"
                    },
                    {
                        "name": "Futang Peng"
                    },
                    {
                        "name": "Daniele Molinari"
                    },
                    {
                        "name": "Aparna Rajamani"
                    },
                    {
                        "name": "Tyler Johnson"
                    },
                    {
                        "name": "Lauren Gardiner"
                    },
                    {
                        "name": "Chao Jia"
                    },
                    {
                        "name": "Violet Yao"
                    },
                    {
                        "name": "Wojciech Kryscinski"
                    },
                    {
                        "name": "Xiujun Li"
                    },
                    {
                        "name": "Shang-Chen Wu"
                    }
                ],
                "author_detail": {
                    "name": "Shang-Chen Wu"
                },
                "arxiv_affiliation": "Taoyi",
                "author": "Shang-Chen Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.13575v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.13575v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04018v2",
                "updated": "2025-07-17T13:44:39Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    13,
                    44,
                    39,
                    3,
                    198,
                    0
                ],
                "published": "2025-02-06T12:19:34Z",
                "published_parsed": [
                    2025,
                    2,
                    6,
                    12,
                    19,
                    34,
                    3,
                    37,
                    0
                ],
                "title": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PINT: Physics-Informed Neural Time Series Models with Applications to\n  Long-term Inference on WeatherBench 2m-Temperature Data"
                },
                "summary": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces PINT (Physics-Informed Neural Time Series Models), a\nframework that integrates physical constraints into neural time series models\nto improve their ability to capture complex dynamics. We apply PINT to the ERA5\nWeatherBench dataset, focusing on long-term forecasting of 2m-temperature data.\nPINT incorporates the Simple Harmonic Oscillator Equation as a physics-informed\nprior, embedding its periodic dynamics into RNN, LSTM, and GRU architectures.\nThis equation's analytical solutions (sine and cosine functions) facilitate\nrigorous evaluation of the benefits of incorporating physics-informed\nconstraints. By benchmarking against a linear regression baseline derived from\nits exact solutions, we quantify the impact of embedding physical principles in\ndata-driven models. Unlike traditional time series models that rely on future\nobservations, PINT is designed for practical forecasting. Using only the first\n90 days of observed data, it iteratively predicts the next two years,\naddressing challenges posed by limited real-time updates. Experiments on the\nWeatherBench dataset demonstrate PINT's ability to generalize, capture periodic\ntrends, and align with physical principles. This study highlights the potential\nof physics-informed neural models in bridging machine learning and\ninterpretable climate applications.\n  Our models and datasets are publicly available on GitHub:\nhttps://github.com/KV-Park."
                },
                "authors": [
                    {
                        "name": "Keonvin Park"
                    },
                    {
                        "name": "Jisu Kim"
                    },
                    {
                        "name": "Jaemin Seo"
                    }
                ],
                "author_detail": {
                    "name": "Jaemin Seo"
                },
                "author": "Jaemin Seo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.00929v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.00929v4",
                "updated": "2025-07-17T09:55:43Z",
                "updated_parsed": [
                    2025,
                    7,
                    17,
                    9,
                    55,
                    43,
                    3,
                    198,
                    0
                ],
                "published": "2025-07-01T16:36:23Z",
                "published_parsed": [
                    2025,
                    7,
                    1,
                    16,
                    36,
                    23,
                    1,
                    182,
                    0
                ],
                "title": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating nano- and micrometer-scale energy deposition models for\n  mechanistic prediction of radiation-induced DNA damage and cell survival"
                },
                "summary": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present an integrated modeling framework that combines the Generalized\nStochastic Microdosimetric Model (GSM2), used to predict cell survival\nfractions, with MINAS-TIRITH, a fast and efficient Geant4 DNA-based tool for\nsimulating radiation-induced DNA damage in cell populations. This approach\nenables the generation of spatially and structurally resolved double-strand\nbreak (DSB) distributions, capturing key features such as damage complexity and\nchromosome specificity. A novel application of the DBSCAN clustering algorithm\nis introduced to group DSBs at the micrometer scale. This allows the\nidentification of physical aggregates of DNA damage and their association with\nsubnuclear domains, providing a direct link to the cell survival probability as\npredicted by \\gsm.\n  The model was validated using experimental data from HUVEC cells irradiated\nwith 220 kV X-rays and H460 cells exposed to protons over a wide linear energy\ntransfer (LET) range, from approximately 4 keV/{\\mu}m to over 20 keV/{\\mu}m.\nResults show excellent agreement between simulations and experimental survival\nprobabilities, making this one of the first consistent multi-scale models to\nbridge nanodosimetric and microdosimetric representations of radiation with\nbiological outcomes such as cell survival.\n  By incorporating the inherent stochastic nature of radiation-matter\ninteractions, this framework effectively connects the physical properties of\nthe radiation field to the biological response at the cellular level. Its\naccuracy across various radiation types and energies supports its potential for\nuse in biologically optimized radiotherapy."
                },
                "authors": [
                    {
                        "name": "Giulio Bordieri"
                    },
                    {
                        "name": "Marta Missiaggia"
                    },
                    {
                        "name": "Gianluca Lattanzi"
                    },
                    {
                        "name": "Carmen Villagrasa"
                    },
                    {
                        "name": "Yann Perrot"
                    },
                    {
                        "name": "Francesco G. Cordoni"
                    }
                ],
                "author_detail": {
                    "name": "Francesco G. Cordoni"
                },
                "author": "Francesco G. Cordoni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.00929v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.00929v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.bio-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.bio-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11953v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11953v1",
                "updated": "2025-07-16T06:39:11Z",
                "updated_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "published": "2025-07-16T06:39:11Z",
                "published_parsed": [
                    2025,
                    7,
                    16,
                    6,
                    39,
                    11,
                    2,
                    197,
                    0
                ],
                "title": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "IAM: Efficient Inference through Attention Mapping between\n  Different-scale LLMs"
                },
                "summary": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLMs encounter significant challenges in resource consumption nowadays,\nespecially with long contexts. Despite extensive efforts dedicate to enhancing\ninference efficiency, these methods primarily exploit internal sparsity within\nthe models, without leveraging external information for optimization. We\nidentify the high similarity of attention matrices across different-scale LLMs,\nwhich offers a novel perspective for optimization. We first conduct a\ncomprehensive analysis of how to measure similarity, how to select mapping\nLayers and whether mapping is consistency. Based on these insights, we\nintroduce the IAM framework, which achieves dual benefits of accelerated\nattention computation and reduced KV cache usage by performing attention\nmapping between small and large LLMs. Our experimental results demonstrate that\nIAM can accelerate prefill by 15% and reduce KV cache usage by 22.1% without\nappreciably sacrificing performance. Experiments on different series of models\nshow the generalizability of IAM. Importantly, it is also orthogonal to many\nexisting KV cache optimization methods, making it a versatile addition to the\ncurrent toolkit for enhancing LLM efficiency."
                },
                "authors": [
                    {
                        "name": "Yi Zhao"
                    },
                    {
                        "name": "Zuchao Li"
                    },
                    {
                        "name": "Hai Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Hai Zhao"
                },
                "author": "Hai Zhao",
                "arxiv_comment": "ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11953v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11953v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11539v1",
                "updated": "2025-07-15T17:59:57Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:59:57Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    59,
                    57,
                    1,
                    196,
                    0
                ],
                "title": "Streaming 4D Visual Geometry Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Streaming 4D Visual Geometry Transformer"
                },
                "summary": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Perceiving and reconstructing 4D spatial-temporal geometry from videos is a\nfundamental yet challenging computer vision task. To facilitate interactive and\nreal-time applications, we propose a streaming 4D visual geometry transformer\nthat shares a similar philosophy with autoregressive large language models. We\nexplore a simple and efficient design and employ a causal transformer\narchitecture to process the input sequence in an online manner. We use temporal\ncausal attention and cache the historical keys and values as implicit memory to\nenable efficient streaming long-term 4D reconstruction. This design can handle\nreal-time 4D reconstruction by incrementally integrating historical information\nwhile maintaining high-quality spatial consistency. For efficient training, we\npropose to distill knowledge from the dense bidirectional visual geometry\ngrounded transformer (VGGT) to our causal model. For inference, our model\nsupports the migration of optimized efficient attention operator (e.g.,\nFlashAttention) from the field of large language models. Extensive experiments\non various 4D geometry perception benchmarks demonstrate that our model\nincreases the inference speed in online scenarios while maintaining competitive\nperformance, paving the way for scalable and interactive 4D vision systems.\nCode is available at: https://github.com/wzzheng/StreamVGGT."
                },
                "authors": [
                    {
                        "name": "Dong Zhuo"
                    },
                    {
                        "name": "Wenzhao Zheng"
                    },
                    {
                        "name": "Jiahe Guo"
                    },
                    {
                        "name": "Yuqi Wu"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Jiwen Lu"
                    }
                ],
                "author_detail": {
                    "name": "Jiwen Lu"
                },
                "author": "Jiwen Lu",
                "arxiv_comment": "Code is available at: https://github.com/wzzheng/StreamVGGT",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.11507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.11507v1",
                "updated": "2025-07-15T17:23:22Z",
                "updated_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "published": "2025-07-15T17:23:22Z",
                "published_parsed": [
                    2025,
                    7,
                    15,
                    17,
                    23,
                    22,
                    1,
                    196,
                    0
                ],
                "title": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIRAGE: KV Cache Optimization through Parameter Remapping for\n  Multi-tenant LLM Serving"
                },
                "summary": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KV cache accelerates LLM inference by avoiding redundant computation, at the\nexpense of memory. To support larger KV caches, prior work extends GPU memory\nwith CPU memory via CPU-offloading. This involves swapping KV cache between GPU\nand CPU memory. However, because the cache updates dynamically, such swapping\nincurs high CPU memory traffic. We make a key observation that model parameters\nremain constant during runtime, unlike the dynamically updated KV cache.\nBuilding on this, we introduce MIRAGE, which avoids KV cache swapping by\nremapping, and thereby repurposing, the memory allocated to model parameters\nfor KV cache. This parameter remapping is especially beneficial in multi-tenant\nenvironments, where the memory used for the parameters of the inactive models\ncan be more aggressively reclaimed. Exploiting the high CPU-GPU bandwidth\noffered by the modern hardware, such as the NVIDIA Grace Hopper Superchip, we\nshow that MIRAGE significantly outperforms state-of-the-art solutions,\nachieving a reduction of 44.8%-82.5% in tail time-between-token latency,\n20.7%-99.3% in tail time-to-first-token latency, and 6.6%-86.7% higher\nthroughput compared to vLLM."
                },
                "authors": [
                    {
                        "name": "Ruihao Li"
                    },
                    {
                        "name": "Shagnik Pal"
                    },
                    {
                        "name": "Vineeth Narayan Pullu"
                    },
                    {
                        "name": "Prasoon Sinha"
                    },
                    {
                        "name": "Jeeho Ryoo"
                    },
                    {
                        "name": "Lizy K. John"
                    },
                    {
                        "name": "Neeraja J. Yadwadkar"
                    }
                ],
                "author_detail": {
                    "name": "Neeraja J. Yadwadkar"
                },
                "author": "Neeraja J. Yadwadkar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.11507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.11507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.OS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.02668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02668v1",
                "updated": "2025-08-04T17:58:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    58,
                    22,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:58:22Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    58,
                    22,
                    0,
                    216,
                    0
                ],
                "title": "LOST: Low-rank and Sparse Pre-training for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOST: Low-rank and Sparse Pre-training for Large Language Models"
                },
                "summary": "While large language models (LLMs) have achieved remarkable performance\nacross a wide range of tasks, their massive scale incurs prohibitive\ncomputational and memory costs for pre-training from scratch. Recent studies\nhave investigated the use of low-rank parameterization as a means of reducing\nmodel size and training cost. In this context, sparsity is often employed as a\ncomplementary technique to recover important information lost in low-rank\ncompression by capturing salient features in the residual space. However,\nexisting approaches typically combine low-rank and sparse components in a\nsimplistic or ad hoc manner, often resulting in undesirable performance\ndegradation compared to full-rank training. In this paper, we propose\n\\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for\nLLMs, a novel method that ingeniously integrates low-rank and sparse structures\nto enable effective training of LLMs from scratch under strict efficiency\nconstraints. LOST applies singular value decomposition to weight matrices,\npreserving the dominant low-rank components, while allocating the remaining\nsingular values to construct channel-wise sparse components to complement the\nexpressiveness of low-rank training. We evaluate LOST on LLM pretraining\nranging from 60M to 7B parameters. Our experiments show that LOST achieves\ncompetitive or superior performance compared to full-rank models, while\nsignificantly reducing both memory and compute overhead. Moreover, Code is\navailable at\n\\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST\nRepo}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have achieved remarkable performance\nacross a wide range of tasks, their massive scale incurs prohibitive\ncomputational and memory costs for pre-training from scratch. Recent studies\nhave investigated the use of low-rank parameterization as a means of reducing\nmodel size and training cost. In this context, sparsity is often employed as a\ncomplementary technique to recover important information lost in low-rank\ncompression by capturing salient features in the residual space. However,\nexisting approaches typically combine low-rank and sparse components in a\nsimplistic or ad hoc manner, often resulting in undesirable performance\ndegradation compared to full-rank training. In this paper, we propose\n\\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for\nLLMs, a novel method that ingeniously integrates low-rank and sparse structures\nto enable effective training of LLMs from scratch under strict efficiency\nconstraints. LOST applies singular value decomposition to weight matrices,\npreserving the dominant low-rank components, while allocating the remaining\nsingular values to construct channel-wise sparse components to complement the\nexpressiveness of low-rank training. We evaluate LOST on LLM pretraining\nranging from 60M to 7B parameters. Our experiments show that LOST achieves\ncompetitive or superior performance compared to full-rank models, while\nsignificantly reducing both memory and compute overhead. Moreover, Code is\navailable at\n\\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST\nRepo}"
                },
                "authors": [
                    {
                        "name": "Jiaxi Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jinjin Xu"
                    },
                    {
                        "name": "Liwu Xu"
                    },
                    {
                        "name": "Tianjin Huang"
                    },
                    {
                        "name": "Wenwu Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Xilu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xilu Wang"
                },
                "author": "Xilu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02666v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02666v1",
                "updated": "2025-08-04T17:55:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    55,
                    49,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:55:49Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    55,
                    49,
                    0,
                    216,
                    0
                ],
                "title": "Testing Dark Matter with Generative Models for Extragalactic Stellar\n  Streams",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Testing Dark Matter with Generative Models for Extragalactic Stellar\n  Streams"
                },
                "summary": "Upcoming ground and space-based surveys are poised to illuminate low surface\nbrightness tidal features, providing a new observable connection to dark matter\nphysics. From imaging of tidal debris, the morphology of stellar streams can be\nused to infer the geometry of dark matter halos. In this paper, we develop a\ngenerative approach, X-Stream, which translates stream imaging into constraints\non the radial density profile of dark matter halos--from the inner region out\nto the virial radius. Using the GPU-accelerated code streamsculptor, we\ngenerate thousands of stream realizations in trial gravitational potentials and\napply nested sampling with a custom objective function to explore viable\nregions of parameter space. We find that multiple stellar streams can be used\nto constrain the entire radial density profile of a halo, including both its\ninner and outer density slopes. These constraints provide a test for\nalternatives to cold dark matter, such as self-interacting dark matter, which\npredicts cored density profiles. From cosmological simulations, the outer\ndensity slope is expected to correlate with merger histories though remains\nunderexplored observationally. With ongoing and upcoming missions such as\nEuclid, the Rubin Observatory, ARRAKIHS, and the Nancy Grace Roman Space\nTelescope, X-Stream will enable detailed mapping of dark matter for thousands\nof galaxies across a wide range of redshifts and halo masses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Upcoming ground and space-based surveys are poised to illuminate low surface\nbrightness tidal features, providing a new observable connection to dark matter\nphysics. From imaging of tidal debris, the morphology of stellar streams can be\nused to infer the geometry of dark matter halos. In this paper, we develop a\ngenerative approach, X-Stream, which translates stream imaging into constraints\non the radial density profile of dark matter halos--from the inner region out\nto the virial radius. Using the GPU-accelerated code streamsculptor, we\ngenerate thousands of stream realizations in trial gravitational potentials and\napply nested sampling with a custom objective function to explore viable\nregions of parameter space. We find that multiple stellar streams can be used\nto constrain the entire radial density profile of a halo, including both its\ninner and outer density slopes. These constraints provide a test for\nalternatives to cold dark matter, such as self-interacting dark matter, which\npredicts cored density profiles. From cosmological simulations, the outer\ndensity slope is expected to correlate with merger histories though remains\nunderexplored observationally. With ongoing and upcoming missions such as\nEuclid, the Rubin Observatory, ARRAKIHS, and the Nancy Grace Roman Space\nTelescope, X-Stream will enable detailed mapping of dark matter for thousands\nof galaxies across a wide range of redshifts and halo masses."
                },
                "authors": [
                    {
                        "name": "Jacob Nibauer"
                    },
                    {
                        "name": "Sarah Pearson"
                    }
                ],
                "author_detail": {
                    "name": "Sarah Pearson"
                },
                "author": "Sarah Pearson",
                "arxiv_comment": "24 pages, 12 figures, 2 tables. Submitted to AAS Journals. Comments\n  welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02666v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02666v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.GA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02647v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02647v1",
                "updated": "2025-08-04T17:38:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    38,
                    12,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:38:12Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    38,
                    12,
                    0,
                    216,
                    0
                ],
                "title": "Optimal Adjustment and Combination of Independent Discrete $p$-Values",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimal Adjustment and Combination of Independent Discrete $p$-Values"
                },
                "summary": "Combining p-values from multiple independent tests is a fundamental task in\nstatistical inference, but presents unique challenges when the p-values are\ndiscrete. We extend a recent optimal transport-based framework for combining\ndiscrete p-values, which constructs a continuous surrogate distribution by\nminimizing the Wasserstein distance between the transformed discrete null and\nits continuous analogue. We provide a unified approach for several classical\ncombination methods, including Fisher's, Pearson's, George's, Stouffer's, and\nEdgington's statistics. Our theoretical analysis and extensive simulations show\nthat accurate Type I error control is achieved when the variance of the\nadjusted discrete statistic closely matches that of the continuous case. We\nfurther demonstrate that, when the likelihood ratio test is a monotonic\nfunction of a combination statistic, the proposed approximation achieves power\ncomparable to the uniformly most powerful (UMP) test. The methodology is\nillustrated with a genetic association study of rare variants using\ncase-control data, and is implemented in the R package DPComb.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Combining p-values from multiple independent tests is a fundamental task in\nstatistical inference, but presents unique challenges when the p-values are\ndiscrete. We extend a recent optimal transport-based framework for combining\ndiscrete p-values, which constructs a continuous surrogate distribution by\nminimizing the Wasserstein distance between the transformed discrete null and\nits continuous analogue. We provide a unified approach for several classical\ncombination methods, including Fisher's, Pearson's, George's, Stouffer's, and\nEdgington's statistics. Our theoretical analysis and extensive simulations show\nthat accurate Type I error control is achieved when the variance of the\nadjusted discrete statistic closely matches that of the continuous case. We\nfurther demonstrate that, when the likelihood ratio test is a monotonic\nfunction of a combination statistic, the proposed approximation achieves power\ncomparable to the uniformly most powerful (UMP) test. The methodology is\nillustrated with a genetic association study of rare variants using\ncase-control data, and is implemented in the R package DPComb."
                },
                "authors": [
                    {
                        "name": "Gonzalo Contador"
                    },
                    {
                        "name": "Zheyang Wu"
                    }
                ],
                "author_detail": {
                    "name": "Zheyang Wu"
                },
                "author": "Zheyang Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02647v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02647v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2402.01121v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2402.01121v2",
                "updated": "2025-08-04T17:23:50Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    23,
                    50,
                    0,
                    216,
                    0
                ],
                "published": "2024-02-02T03:41:38Z",
                "published_parsed": [
                    2024,
                    2,
                    2,
                    3,
                    41,
                    38,
                    4,
                    33,
                    0
                ],
                "title": "Causal Estimation and Inference in Nonlinear Mendelian Randomization\n  Studies",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal Estimation and Inference in Nonlinear Mendelian Randomization\n  Studies"
                },
                "summary": "Mendelian randomization (MR) is widely used to uncover causal relationships\nin the presence of unmeasured confounders. However, most existing MR methods\npresuppose linear causality, risking bias when the true relationships are\nnonlinear, which is a common empirical scenario. In this paper, we compared two\nprevalent instrumental variable techniques (the two-stage prediction method and\nthe control function method) under both linear and nonlinear settings, and\naddressed key issues such as horizontal pleiotropy and violations of classical\nassumptions in control function method. Most notably, we proposed a flexible\nsemiparametric approach that estimates the causal function without a priori\nspecification, reducing the risk of model misspecification, and extended our\nmethods to binary outcomes, broadening its applicability. For all approaches,\nwe provided estimators, standard errors, and test statistics, to facilitate\nrobust causal inference. Extensive numerical simulations demonstrated that our\nproposed methods exhibited both accuracy and robustness across diverse\nscenarios. Applying our methods to UK Biobank data uncovered significant\nnonlinear causal effects missed by linear MR approaches. We offer an R package\nimplementation for broader and more convenient use.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mendelian randomization (MR) is widely used to uncover causal relationships\nin the presence of unmeasured confounders. However, most existing MR methods\npresuppose linear causality, risking bias when the true relationships are\nnonlinear, which is a common empirical scenario. In this paper, we compared two\nprevalent instrumental variable techniques (the two-stage prediction method and\nthe control function method) under both linear and nonlinear settings, and\naddressed key issues such as horizontal pleiotropy and violations of classical\nassumptions in control function method. Most notably, we proposed a flexible\nsemiparametric approach that estimates the causal function without a priori\nspecification, reducing the risk of model misspecification, and extended our\nmethods to binary outcomes, broadening its applicability. For all approaches,\nwe provided estimators, standard errors, and test statistics, to facilitate\nrobust causal inference. Extensive numerical simulations demonstrated that our\nproposed methods exhibited both accuracy and robustness across diverse\nscenarios. Applying our methods to UK Biobank data uncovered significant\nnonlinear causal effects missed by linear MR approaches. We offer an R package\nimplementation for broader and more convenient use."
                },
                "authors": [
                    {
                        "name": "Xinpei Wang"
                    },
                    {
                        "name": "Tao Huang"
                    },
                    {
                        "name": "Jinzhu Jia"
                    }
                ],
                "author_detail": {
                    "name": "Jinzhu Jia"
                },
                "author": "Jinzhu Jia",
                "arxiv_comment": "25 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2402.01121v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2402.01121v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02635v1",
                "updated": "2025-08-04T17:22:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    22,
                    8,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:22:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    22,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "Test Set Quality in Multilingual LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Set Quality in Multilingual LLM Evaluation"
                },
                "summary": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues."
                },
                "authors": [
                    {
                        "name": "Kranti Chalamalasetti"
                    },
                    {
                        "name": "Gabriel Bernier-Colborne"
                    },
                    {
                        "name": "Yvan Gauthier"
                    },
                    {
                        "name": "Sowmya Vajjala"
                    }
                ],
                "author_detail": {
                    "name": "Sowmya Vajjala"
                },
                "author": "Sowmya Vajjala",
                "arxiv_comment": "Accepted at the 1st Workshop on Multilingual Data Quality Signals,\n  COLM 2025, Short paper. 10 pages in total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02629v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02629v1",
                "updated": "2025-08-04T17:18:14Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    18,
                    14,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:18:14Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    18,
                    14,
                    0,
                    216,
                    0
                ],
                "title": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HyCodePolicy: Hybrid Language Controllers for Multimodal Monitoring and\n  Decision in Embodied Agents"
                },
                "summary": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in multimodal large language models (MLLMs) have enabled\nricher perceptual grounding for code policy generation in embodied agents.\nHowever, most existing systems lack effective mechanisms to adaptively monitor\npolicy execution and repair codes during task completion. In this work, we\nintroduce HyCodePolicy, a hybrid language-based control framework that\nsystematically integrates code synthesis, geometric grounding, perceptual\nmonitoring, and iterative repair into a closed-loop programming cycle for\nembodied agents. Technically, given a natural language instruction, our system\nfirst decomposes it into subgoals and generates an initial executable program\ngrounded in object-centric geometric primitives. The program is then executed\nin simulation, while a vision-language model (VLM) observes selected\ncheckpoints to detect and localize execution failures and infer failure\nreasons. By fusing structured execution traces capturing program-level events\nwith VLM-based perceptual feedback, HyCodePolicy infers failure causes and\nrepairs programs. This hybrid dual feedback mechanism enables self-correcting\nprogram synthesis with minimal human supervision. Our results demonstrate that\nHyCodePolicy significantly improves the robustness and sample efficiency of\nrobot manipulation policies, offering a scalable strategy for integrating\nmultimodal reasoning into autonomous decision-making pipelines."
                },
                "authors": [
                    {
                        "name": "Yibin Liu"
                    },
                    {
                        "name": "Zhixuan Liang"
                    },
                    {
                        "name": "Zanxin Chen"
                    },
                    {
                        "name": "Tianxing Chen"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Wanxi Dong"
                    },
                    {
                        "name": "Congsheng Xu"
                    },
                    {
                        "name": "Zhaoming Han"
                    },
                    {
                        "name": "Yusen Qin"
                    },
                    {
                        "name": "Yao Mu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Mu"
                },
                "author": "Yao Mu",
                "arxiv_comment": "Accepted to ICCV 2025 Workshop on Multi-Modal Reasoning for Agentic\n  Intelligence",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02629v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02629v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.06565v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.06565v2",
                "updated": "2025-08-04T17:11:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    11,
                    22,
                    0,
                    216,
                    0
                ],
                "published": "2024-09-10T14:57:42Z",
                "published_parsed": [
                    2024,
                    9,
                    10,
                    14,
                    57,
                    42,
                    1,
                    254,
                    0
                ],
                "title": "Functional limit theorems and parameter inference for multiscale\n  stochastic models of enzyme kinetics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Functional limit theorems and parameter inference for multiscale\n  stochastic models of enzyme kinetics"
                },
                "summary": "We study a class of Stochastic Differential Equations (SDEs) with jumps\nmodeling multistage Michaelis--Menten enzyme kinetics, in which a substrate is\nsequentially transformed into a product via a cascade of intermediate\ncomplexes. These networks are typically high dimensional and exhibit multiscale\nbehavior with strong coupling between different components, posing substantial\nanalytical and computational challenges. In particular, the problem of\nstatistical inference of reaction rates is significantly difficult, and becomes\neven more intricate when direct observations of system states are unavailable\nand only a random sample of product formation times is observed. We address\nthis in two stages. First, in a suitable scaling regime consistent with the\nQuasi-Steady State Approximation (QSSA), we rigorously establish two asymptotic\nresults: (i) a stochastic averaging principle yielding a reduced model for the\nproduct--substrate dynamics; and (ii) a Functional Central Limit Theorem (FCLT)\ncharacterizing the associated fluctuations. Guided by the reduced-order\ndynamics, we next construct a novel Interacting Particle System (IPS) that\napproximates the product-substrate process at the particle level. This IPS\nplays a pivotal role in the inference methodology; in particular, we establish\na propagation of chaos result that mathematically justifies an approximate\nproduct-form likelihood based solely on a random sample of product formation\ntimes, without requiring access to the system states. Numerical examples are\npresented to demonstrate the accuracy and applicability of the theoretical\nresults.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study a class of Stochastic Differential Equations (SDEs) with jumps\nmodeling multistage Michaelis--Menten enzyme kinetics, in which a substrate is\nsequentially transformed into a product via a cascade of intermediate\ncomplexes. These networks are typically high dimensional and exhibit multiscale\nbehavior with strong coupling between different components, posing substantial\nanalytical and computational challenges. In particular, the problem of\nstatistical inference of reaction rates is significantly difficult, and becomes\neven more intricate when direct observations of system states are unavailable\nand only a random sample of product formation times is observed. We address\nthis in two stages. First, in a suitable scaling regime consistent with the\nQuasi-Steady State Approximation (QSSA), we rigorously establish two asymptotic\nresults: (i) a stochastic averaging principle yielding a reduced model for the\nproduct--substrate dynamics; and (ii) a Functional Central Limit Theorem (FCLT)\ncharacterizing the associated fluctuations. Guided by the reduced-order\ndynamics, we next construct a novel Interacting Particle System (IPS) that\napproximates the product-substrate process at the particle level. This IPS\nplays a pivotal role in the inference methodology; in particular, we establish\na propagation of chaos result that mathematically justifies an approximate\nproduct-form likelihood based solely on a random sample of product formation\ntimes, without requiring access to the system states. Numerical examples are\npresented to demonstrate the accuracy and applicability of the theoretical\nresults."
                },
                "authors": [
                    {
                        "name": "Arnab Ganguly"
                    },
                    {
                        "name": "Wasiur R. KhudaBukhsh"
                    }
                ],
                "author_detail": {
                    "name": "Wasiur R. KhudaBukhsh"
                },
                "author": "Wasiur R. KhudaBukhsh",
                "arxiv_comment": "Typos fixed",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.06565v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.06565v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.PR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.PR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.FA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "60F17, 60F05, 62F99, 62M99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02622v1",
                "updated": "2025-08-04T17:10:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    10,
                    8,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:10:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    10,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction"
                },
                "summary": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research."
                },
                "authors": [
                    {
                        "name": "Enrico De Santis"
                    },
                    {
                        "name": "Antonello Rizzi"
                    }
                ],
                "author_detail": {
                    "name": "Antonello Rizzi"
                },
                "author": "Antonello Rizzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02618v1",
                "updated": "2025-08-04T17:06:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    6,
                    23,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:06:23Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    6,
                    23,
                    0,
                    216,
                    0
                ],
                "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via\n  Interaction Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Attention Hacking in Preference-Based Reward Modeling via\n  Interaction Distillation"
                },
                "summary": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM."
                },
                "authors": [
                    {
                        "name": "Jianxiang Zang"
                    },
                    {
                        "name": "Meiling Ning"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02611v1",
                "updated": "2025-08-04T17:01:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    1,
                    10,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:01:10Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    1,
                    10,
                    0,
                    216,
                    0
                ],
                "title": "Meta-RAG on Large Codebases Using Code Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-RAG on Large Codebases Using Code Summarization"
                },
                "summary": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Vali Tawosia"
                    },
                    {
                        "name": "Salwa Alamir"
                    },
                    {
                        "name": "Xiaomo Liu"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02602v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02602v1",
                "updated": "2025-08-04T16:56:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    56,
                    11,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:56:11Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    56,
                    11,
                    0,
                    216,
                    0
                ],
                "title": "Trustworthy scientific inference for inverse problems with generative\n  models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Trustworthy scientific inference for inverse problems with generative\n  models"
                },
                "summary": "Generative artificial intelligence (AI) excels at producing complex data\nstructures (text, images, videos) by learning patterns from training examples.\nAcross scientific disciplines, researchers are now applying generative models\nto ``inverse problems'' to infer hidden parameters from observed data. While\nthese methods can handle intractable models and large-scale studies, they can\nalso produce biased or overconfident conclusions. We present a solution with\nFrequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes\nAI-generated probability distributions into confidence regions that\nconsistently include true parameters with the expected probability, while\nachieving minimum size when training and target data align. We demonstrate\nFreB's effectiveness by tackling diverse case studies in the physical sciences:\nidentifying unknown sources under dataset shift, reconciling competing\ntheoretical models, and mitigating selection bias and systematics in\nobservational studies. By providing validity guarantees with interpretable\ndiagnostics, FreB enables trustworthy scientific inference across fields where\ndirect likelihood evaluation remains impossible or prohibitively expensive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative artificial intelligence (AI) excels at producing complex data\nstructures (text, images, videos) by learning patterns from training examples.\nAcross scientific disciplines, researchers are now applying generative models\nto ``inverse problems'' to infer hidden parameters from observed data. While\nthese methods can handle intractable models and large-scale studies, they can\nalso produce biased or overconfident conclusions. We present a solution with\nFrequentist-Bayes (FreB), a mathematically rigorous protocol that reshapes\nAI-generated probability distributions into confidence regions that\nconsistently include true parameters with the expected probability, while\nachieving minimum size when training and target data align. We demonstrate\nFreB's effectiveness by tackling diverse case studies in the physical sciences:\nidentifying unknown sources under dataset shift, reconciling competing\ntheoretical models, and mitigating selection bias and systematics in\nobservational studies. By providing validity guarantees with interpretable\ndiagnostics, FreB enables trustworthy scientific inference across fields where\ndirect likelihood evaluation remains impossible or prohibitively expensive."
                },
                "authors": [
                    {
                        "name": "James Carzon"
                    },
                    {
                        "name": "Luca Masserano"
                    },
                    {
                        "name": "Joshua D. Ingram"
                    },
                    {
                        "name": "Alex Shen"
                    },
                    {
                        "name": "Antonio Carlos Herling Ribeiro Junior"
                    },
                    {
                        "name": "Tommaso Dorigo"
                    },
                    {
                        "name": "Michele Doro"
                    },
                    {
                        "name": "Joshua S. Speagle"
                    },
                    {
                        "name": "Rafael Izbicki"
                    },
                    {
                        "name": "Ann B. Lee"
                    }
                ],
                "author_detail": {
                    "name": "Ann B. Lee"
                },
                "author": "Ann B. Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02602v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02602v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02601v1",
                "updated": "2025-08-04T16:55:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    55,
                    2,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:55:02Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    55,
                    2,
                    0,
                    216,
                    0
                ],
                "title": "StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis\n  in Low-Data Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis\n  in Low-Data Regimes"
                },
                "summary": "The application of machine learning on tabular data in specialized domains is\nseverely limited by data scarcity. While generative models offer a solution,\ntraditional methods falter in low-data regimes, and recent Large Language\nModels (LLMs) often ignore the explicit dependency structure of tabular data,\nleading to low-fidelity synthetics. To address these limitations, we introduce\nStructSynth, a novel framework that integrates the generative power of LLMs\nwith robust structural control. StructSynth employs a two-stage architecture.\nFirst, it performs explicit structure discovery to learn a Directed Acyclic\nGraph (DAG) from the available data. Second, this learned structure serves as a\nhigh-fidelity blueprint to steer the LLM's generation process, forcing it to\nadhere to the learned feature dependencies and thereby ensuring the generated\ndata respects the underlying structure by design. Our extensive experiments\ndemonstrate that StructSynth produces synthetic data with significantly higher\nstructural integrity and downstream utility than state-of-the-art methods. It\nproves especially effective in challenging low-data scenarios, successfully\nnavigating the trade-off between privacy preservation and statistical fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of machine learning on tabular data in specialized domains is\nseverely limited by data scarcity. While generative models offer a solution,\ntraditional methods falter in low-data regimes, and recent Large Language\nModels (LLMs) often ignore the explicit dependency structure of tabular data,\nleading to low-fidelity synthetics. To address these limitations, we introduce\nStructSynth, a novel framework that integrates the generative power of LLMs\nwith robust structural control. StructSynth employs a two-stage architecture.\nFirst, it performs explicit structure discovery to learn a Directed Acyclic\nGraph (DAG) from the available data. Second, this learned structure serves as a\nhigh-fidelity blueprint to steer the LLM's generation process, forcing it to\nadhere to the learned feature dependencies and thereby ensuring the generated\ndata respects the underlying structure by design. Our extensive experiments\ndemonstrate that StructSynth produces synthetic data with significantly higher\nstructural integrity and downstream utility than state-of-the-art methods. It\nproves especially effective in challenging low-data scenarios, successfully\nnavigating the trade-off between privacy preservation and statistical fidelity."
                },
                "authors": [
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Yujia Zheng"
                    },
                    {
                        "name": "Yongqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongqi Zhang"
                },
                "author": "Yongqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22967v2",
                "updated": "2025-08-04T16:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    54,
                    44,
                    0,
                    216,
                    0
                ],
                "published": "2025-06-28T17:57:58Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    17,
                    57,
                    58,
                    5,
                    179,
                    0
                ],
                "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment"
                },
                "summary": "We address the task of zero-shot video classification for extremely\nfine-grained actions (e.g., Windmill Dunk in basketball), where no video\nexamples or temporal annotations are available for unseen classes. While\nimage-language models (e.g., CLIP, SigLIP) show strong open-set recognition,\nthey lack temporal modeling needed for video understanding. We propose\nActAlign, a truly zero-shot, training-free method that formulates video\nclassification as a sequence alignment problem, preserving the generalization\nstrength of pretrained image-language models. For each class, a large language\nmodel (LLM) generates an ordered sequence of sub-actions, which we align with\nvideo frames using Dynamic Time Warping (DTW) in a shared embedding space.\nWithout any video-text supervision or fine-tuning, ActAlign achieves 30.5%\naccuracy on ActionAtlas--the most diverse benchmark of fine-grained actions\nacross multiple sports--where human performance is only 61.6%. ActAlign\noutperforms billion-parameter video-language models while using 8x fewer\nparameters. Our approach is model-agnostic and domain-general, demonstrating\nthat structured language priors combined with classical alignment methods can\nunlock the open-set recognition potential of image-language models for\nfine-grained video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the task of zero-shot video classification for extremely\nfine-grained actions (e.g., Windmill Dunk in basketball), where no video\nexamples or temporal annotations are available for unseen classes. While\nimage-language models (e.g., CLIP, SigLIP) show strong open-set recognition,\nthey lack temporal modeling needed for video understanding. We propose\nActAlign, a truly zero-shot, training-free method that formulates video\nclassification as a sequence alignment problem, preserving the generalization\nstrength of pretrained image-language models. For each class, a large language\nmodel (LLM) generates an ordered sequence of sub-actions, which we align with\nvideo frames using Dynamic Time Warping (DTW) in a shared embedding space.\nWithout any video-text supervision or fine-tuning, ActAlign achieves 30.5%\naccuracy on ActionAtlas--the most diverse benchmark of fine-grained actions\nacross multiple sports--where human performance is only 61.6%. ActAlign\noutperforms billion-parameter video-language models while using 8x fewer\nparameters. Our approach is model-agnostic and domain-general, demonstrating\nthat structured language priors combined with classical alignment methods can\nunlock the open-set recognition potential of image-language models for\nfine-grained video understanding."
                },
                "authors": [
                    {
                        "name": "Amir Aghdam"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Bjrn Ommer"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn Ommer"
                },
                "author": "Bjrn Ommer",
                "arxiv_comment": "Preprint manuscript - Project page:\n  https://amir-aghdam.github.io/act-align/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02591v1",
                "updated": "2025-08-04T16:46:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    46,
                    15,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:46:15Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    46,
                    15,
                    0,
                    216,
                    0
                ],
                "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks"
                },
                "summary": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks."
                },
                "authors": [
                    {
                        "name": "Omri Uzan"
                    },
                    {
                        "name": "Yuval Pinter"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Pinter"
                },
                "author": "Yuval Pinter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2310.07543v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2310.07543v4",
                "updated": "2025-08-04T16:43:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    43,
                    53,
                    0,
                    216,
                    0
                ],
                "published": "2023-10-11T14:47:51Z",
                "published_parsed": [
                    2023,
                    10,
                    11,
                    14,
                    47,
                    51,
                    2,
                    284,
                    0
                ],
                "title": "Ordinal Characterization of Similarity Judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ordinal Characterization of Similarity Judgments"
                },
                "summary": "Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing judgments of similarity within a perceptual or semantic\ndomain, and making inferences about the underlying structure of this domain\nfrom these judgments, has an increasingly important role in cognitive and\nsystems neuroscience. We present a new framework for this purpose that makes\nlimited assumptions about how perceptual distances are converted into\nsimilarity judgments. The approach starts from a dataset of empirical judgments\nof relative similarities: the fraction of times that a subject chooses one of\ntwo comparison stimuli to be more similar to a reference stimulus. These\nempirical judgments provide Bayesian estimates of underling choice\nprobabilities. From these estimates, we derive indices that characterize the\nset of judgments in three ways: compatibility with a symmetric dis-similarity,\ncompatibility with an ultrametric space, and compatibility with an additive\ntree. Each of the indices is derived from rank-order relationships among the\nchoice probabilities that, as we show, are necessary and sufficient for local\nconsistency with the three respective characteristics. We illustrate this\napproach with simulations and example psychophysical datasets of dis-similarity\njudgments in several visual domains and provide code that implements the\nanalyses at https://github.com/jvlab/simrank."
                },
                "authors": [
                    {
                        "name": "Jonathan D. Victor"
                    },
                    {
                        "name": "Guillermo Aguilar"
                    },
                    {
                        "name": "Suniyya A. Waraich"
                    }
                ],
                "author_detail": {
                    "name": "Suniyya A. Waraich"
                },
                "author": "Suniyya A. Waraich",
                "arxiv_doi": "10.46298/mna.12457",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.46298/mna.12457",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2310.07543v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2310.07543v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "64 pages, 16 figures + 7 supplementary figures. Final version\n  accepted for publication in Mathematical Neuroscience and Applications",
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "91E30 (Primary) 62P15, 92-08, 92-10, 51-08 (Secondary)",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02585v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02585v1",
                "updated": "2025-08-04T16:40:33Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    40,
                    33,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:40:33Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    40,
                    33,
                    0,
                    216,
                    0
                ],
                "title": "Variational Bernstein-von Mises theorem with increasing parameter\n  dimension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bernstein-von Mises theorem with increasing parameter\n  dimension"
                },
                "summary": "Variational Bayes (VB) provides a computationally efficient alternative to\nMarkov Chain Monte Carlo, especially for high-dimensional and large-scale\ninference. However, existing theory on VB primarily focuses on\nfixed-dimensional settings or specific models. To address this limitation, this\npaper develops a finite-sample theory for VB in a broad class of parametric\nmodels with latent variables. We establish theoretical properties of the VB\nposterior, including a non-asymptotic variational Bernstein--von Mises theorem.\nFurthermore, we derive consistency and asymptotic normality of the VB\nestimator. An application to multivariate Gaussian mixture models is presented\nfor illustration.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Variational Bayes (VB) provides a computationally efficient alternative to\nMarkov Chain Monte Carlo, especially for high-dimensional and large-scale\ninference. However, existing theory on VB primarily focuses on\nfixed-dimensional settings or specific models. To address this limitation, this\npaper develops a finite-sample theory for VB in a broad class of parametric\nmodels with latent variables. We establish theoretical properties of the VB\nposterior, including a non-asymptotic variational Bernstein--von Mises theorem.\nFurthermore, we derive consistency and asymptotic normality of the VB\nestimator. An application to multivariate Gaussian mixture models is presented\nfor illustration."
                },
                "authors": [
                    {
                        "name": "Jiawei Yan"
                    },
                    {
                        "name": "Peirong Xu"
                    },
                    {
                        "name": "Tao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Tao Wang"
                },
                "author": "Tao Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02585v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02585v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.ST",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.ST",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.TH",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02584v1",
                "updated": "2025-08-04T16:40:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    40,
                    2,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:40:02Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    40,
                    2,
                    0,
                    216,
                    0
                ],
                "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language\n  Models for Justifiable Claim Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MArgE: Meshing Argumentative Evidence from Multiple Large Language\n  Models for Justifiable Claim Verification"
                },
                "summary": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs."
                },
                "authors": [
                    {
                        "name": "Ming Pok Ng"
                    },
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02583v1",
                "updated": "2025-08-04T16:39:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    39,
                    24,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:39:24Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    39,
                    24,
                    0,
                    216,
                    0
                ],
                "title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone."
                },
                "authors": [
                    {
                        "name": "Lei Zan"
                    },
                    {
                        "name": "Keli Zhang"
                    },
                    {
                        "name": "Ruichu Cai"
                    },
                    {
                        "name": "Lujia Pan"
                    }
                ],
                "author_detail": {
                    "name": "Lujia Pan"
                },
                "author": "Lujia Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07927v3",
                "updated": "2025-08-04T16:37:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    37,
                    0,
                    0,
                    216,
                    0
                ],
                "published": "2025-01-14T08:30:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    30,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "Gandalf the Red: Adaptive Security for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gandalf the Red: Adaptive Security for LLMs"
                },
                "summary": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications."
                },
                "authors": [
                    {
                        "name": "Niklas Pfister"
                    },
                    {
                        "name": "Vclav Volhejn"
                    },
                    {
                        "name": "Manuel Knott"
                    },
                    {
                        "name": "Santiago Arias"
                    },
                    {
                        "name": "Julia Baziska"
                    },
                    {
                        "name": "Mykhailo Bichurin"
                    },
                    {
                        "name": "Alan Commike"
                    },
                    {
                        "name": "Janet Darling"
                    },
                    {
                        "name": "Peter Dienes"
                    },
                    {
                        "name": "Matthew Fiedler"
                    },
                    {
                        "name": "David Haber"
                    },
                    {
                        "name": "Matthias Kraft"
                    },
                    {
                        "name": "Marco Lancini"
                    },
                    {
                        "name": "Max Mathys"
                    },
                    {
                        "name": "Damin Pascual-Ortiz"
                    },
                    {
                        "name": "Jakub Podolak"
                    },
                    {
                        "name": "Adri Romero-Lpez"
                    },
                    {
                        "name": "Kyriacos Shiarlis"
                    },
                    {
                        "name": "Andreas Signer"
                    },
                    {
                        "name": "Zsolt Terek"
                    },
                    {
                        "name": "Athanasios Theocharis"
                    },
                    {
                        "name": "Daniel Timbrell"
                    },
                    {
                        "name": "Samuel Trautwein"
                    },
                    {
                        "name": "Samuel Watts"
                    },
                    {
                        "name": "Yun-Han Wu"
                    },
                    {
                        "name": "Mateo Rojas-Carulla"
                    }
                ],
                "author_detail": {
                    "name": "Mateo Rojas-Carulla"
                },
                "author": "Mateo Rojas-Carulla",
                "arxiv_comment": "Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02573v1",
                "updated": "2025-08-04T16:27:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    27,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:27:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    27,
                    56,
                    0,
                    216,
                    0
                ],
                "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guess or Recall? Training CNNs to Classify and Localize Memorization in\n  LLMs"
                },
                "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization."
                },
                "authors": [
                    {
                        "name": "Jrmie Dentan"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02571v1",
                "updated": "2025-08-04T16:26:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    26,
                    17,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:26:17Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    26,
                    17,
                    0,
                    216,
                    0
                ],
                "title": "ASINT: Learning AS-to-Organization Mapping from Internet Metadata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASINT: Learning AS-to-Organization Mapping from Internet Metadata"
                },
                "summary": "Accurately mapping Autonomous Systems (ASNs) to their owning or operating\norganizations underpins Internet measurement research and security\napplications. Yet existing approaches commonly rely solely on WHOIS or\nPeeringDB, missing important relationships (e.g., cross-regional aliases,\nparent-child ownership) and failing to unify organizations scattered across\ndifferent RIR identifiers. We introduce ASINT, an end-to-end pipeline that\nfuses bulk registry data with unstructured Web sources, then employs\nretrieval-augmented generation (RAG) to guide large language model (LLM)\ninference. Through a multi-stage procedure, ASINT merges ASNs into\n\"organization families,\" capturing nuanced ties beyond the scope of simpler\nheuristics.\n  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both\nAS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g.,\noperator aliases, rebrands) that other datasets overlook. Moreover, our refined\nmappings enhance multiple security and measurement tasks: ASINT exposes 27.5%\nmore intra-organizational RPKI misconfigurations, cuts false-positive hijack\nalarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.\n  Finally, ASINT supports periodic updates and cost-sensitive LLM selection,\ndemonstrating that broader Web evidence can provide a more accurate, evolving\nview of the Internet's organizational structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately mapping Autonomous Systems (ASNs) to their owning or operating\norganizations underpins Internet measurement research and security\napplications. Yet existing approaches commonly rely solely on WHOIS or\nPeeringDB, missing important relationships (e.g., cross-regional aliases,\nparent-child ownership) and failing to unify organizations scattered across\ndifferent RIR identifiers. We introduce ASINT, an end-to-end pipeline that\nfuses bulk registry data with unstructured Web sources, then employs\nretrieval-augmented generation (RAG) to guide large language model (LLM)\ninference. Through a multi-stage procedure, ASINT merges ASNs into\n\"organization families,\" capturing nuanced ties beyond the scope of simpler\nheuristics.\n  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both\nAS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g.,\noperator aliases, rebrands) that other datasets overlook. Moreover, our refined\nmappings enhance multiple security and measurement tasks: ASINT exposes 27.5%\nmore intra-organizational RPKI misconfigurations, cuts false-positive hijack\nalarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.\n  Finally, ASINT supports periodic updates and cost-sensitive LLM selection,\ndemonstrating that broader Web evidence can provide a more accurate, evolving\nview of the Internet's organizational structure."
                },
                "authors": [
                    {
                        "name": "Yongzhe Xu"
                    },
                    {
                        "name": "Weitong Li"
                    },
                    {
                        "name": "Eeshan Umrani"
                    },
                    {
                        "name": "Taejoong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Taejoong Chung"
                },
                "author": "Taejoong Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02550v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02550v1",
                "updated": "2025-08-04T16:01:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    1,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:01:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    1,
                    56,
                    0,
                    216,
                    0
                ],
                "title": "Stakeholder Perspectives on Humanistic Implementation of Computer\n  Perception in Healthcare: A Qualitative Study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Stakeholder Perspectives on Humanistic Implementation of Computer\n  Perception in Healthcare: A Qualitative Study"
                },
                "summary": "Computer perception (CP) technologies (digital phenotyping, affective\ncomputing and related passive sensing approaches) offer unprecedented\nopportunities to personalize healthcare, but provoke concerns about privacy,\nbias and the erosion of empathic, relationship-centered practice. A\ncomprehensive understanding of perceived risks, benefits, and implementation\nchallenges from those who design, deploy and experience these tools in\nreal-world settings remains elusive. This study provides the first\nevidence-based account of key stakeholder perspectives on the relational,\ntechnical, and governance challenges raised by the integration of CP\ntechnologies into patient care. We conducted in-depth, semi-structured\ninterviews with 102 stakeholders: adolescent patients and their caregivers,\nfrontline clinicians, technology developers, and ethics, legal, policy or\nphilosophy scholars. Transcripts underwent thematic analysis by a\nmultidisciplinary team; reliability was enhanced through double coding and\nconsensus adjudication. Stakeholders articulated seven interlocking concern\ndomains: (1) trustworthiness and data integrity; (2) patient-specific\nrelevance; (3) utility and workflow integration; (4) regulation and governance;\n(5) privacy and data protection; (6) direct and indirect patient harms; and (7)\nphilosophical critiques of reductionism. To operationalize humanistic\nsafeguards, we propose \"personalized roadmaps\": co-designed plans that\npredetermine which metrics will be monitored, how and when feedback is shared,\nthresholds for clinical action, and procedures for reconciling discrepancies\nbetween algorithmic inferences and lived experience. By translating these\ninsights into personalized roadmaps, we offer a practical framework for\ndevelopers, clinicians and policymakers seeking to harness continuous\nbehavioral data while preserving the humanistic core of care.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computer perception (CP) technologies (digital phenotyping, affective\ncomputing and related passive sensing approaches) offer unprecedented\nopportunities to personalize healthcare, but provoke concerns about privacy,\nbias and the erosion of empathic, relationship-centered practice. A\ncomprehensive understanding of perceived risks, benefits, and implementation\nchallenges from those who design, deploy and experience these tools in\nreal-world settings remains elusive. This study provides the first\nevidence-based account of key stakeholder perspectives on the relational,\ntechnical, and governance challenges raised by the integration of CP\ntechnologies into patient care. We conducted in-depth, semi-structured\ninterviews with 102 stakeholders: adolescent patients and their caregivers,\nfrontline clinicians, technology developers, and ethics, legal, policy or\nphilosophy scholars. Transcripts underwent thematic analysis by a\nmultidisciplinary team; reliability was enhanced through double coding and\nconsensus adjudication. Stakeholders articulated seven interlocking concern\ndomains: (1) trustworthiness and data integrity; (2) patient-specific\nrelevance; (3) utility and workflow integration; (4) regulation and governance;\n(5) privacy and data protection; (6) direct and indirect patient harms; and (7)\nphilosophical critiques of reductionism. To operationalize humanistic\nsafeguards, we propose \"personalized roadmaps\": co-designed plans that\npredetermine which metrics will be monitored, how and when feedback is shared,\nthresholds for clinical action, and procedures for reconciling discrepancies\nbetween algorithmic inferences and lived experience. By translating these\ninsights into personalized roadmaps, we offer a practical framework for\ndevelopers, clinicians and policymakers seeking to harness continuous\nbehavioral data while preserving the humanistic core of care."
                },
                "authors": [
                    {
                        "name": "Kristin M. Kostick-Quenet"
                    },
                    {
                        "name": "Meghan E. Hurley"
                    },
                    {
                        "name": "Syed Ayaz"
                    },
                    {
                        "name": "John Herrington"
                    },
                    {
                        "name": "Casey Zampella"
                    },
                    {
                        "name": "Julia Parish-Morris"
                    },
                    {
                        "name": "Birkan Tun"
                    },
                    {
                        "name": "Gabriel Lzaro-Muoz"
                    },
                    {
                        "name": "J. S. Blumenthal-Barby"
                    },
                    {
                        "name": "Eric A. Storch"
                    }
                ],
                "author_detail": {
                    "name": "Eric A. Storch"
                },
                "arxiv_affiliation": "Baylor College of Medicine, Houston, TX, 77030, USA",
                "author": "Eric A. Storch",
                "arxiv_comment": "65 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02550v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02550v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18182v2",
                "updated": "2025-08-04T15:53:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    53,
                    52,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-24T08:28:17Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    28,
                    17,
                    3,
                    205,
                    0
                ],
                "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations."
                },
                "authors": [
                    {
                        "name": "Wonjun Jeong"
                    },
                    {
                        "name": "Dongseok Kim"
                    },
                    {
                        "name": "Taegkeun Whangbo"
                    }
                ],
                "author_detail": {
                    "name": "Taegkeun Whangbo"
                },
                "author": "Taegkeun Whangbo",
                "arxiv_comment": "Comments: 34 pages, 1 figure. v2: All \"Consequence.\" statements in\n  the Theoretical Analysis section relabeled as \"Corollary.\"; duplicated values\n  in Table 20 (previously identical to Table 15) corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00705v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00705v2",
                "updated": "2025-08-04T15:52:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    52,
                    12,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-01T15:20:24Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    15,
                    20,
                    24,
                    4,
                    213,
                    0
                ],
                "title": "An Online Data Analysis Framework for Accelerator-Based Physics\n  Experiments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Online Data Analysis Framework for Accelerator-Based Physics\n  Experiments"
                },
                "summary": "A robust and flexible architecture capable of providing real-time analysis on\ndiagnostic data is of crucial importance to physics experiments. In this paper,\nwe present such an online framework, used in June 2025 as part of the HRMT-68\nexperiment, performed at the HiRadMat facility at CERN, using the Super Proton\nSynchrotron (SPS) beam line. HRMT-68 was a fixed-target laboratory astrophysics\nexperiment aiming to identify plasma instabilities generated by a relativistic\nelectron-positron beam during traversal of an argon plasma. This framework was\nessential for experimental data acquisition and analysis, and can be adapted\nfor a broad range of experiments with a variety of experimental diagnostics.\nThe framework's modular and customizable design enabled us to rapidly observe\nand extract emergent features from a diverse range of diagnostic data.\nSimultaneously, it allowed for both the introduction of new diagnostic devices\nand the modification of our analysis as features of interest were identified.\nAs a result, we were able to effectively diagnose equipment malfunction, and\ninfer the beam's response to varying bunch duration, beam intensity, and the\nplasma state without resorting to offline analysis, at which time adjustment or\nimprovement would have been impossible. We present the features of this agile\nframework, whose codebase we have made publicly available, which can be adapted\nfor future experiments with minimal modification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A robust and flexible architecture capable of providing real-time analysis on\ndiagnostic data is of crucial importance to physics experiments. In this paper,\nwe present such an online framework, used in June 2025 as part of the HRMT-68\nexperiment, performed at the HiRadMat facility at CERN, using the Super Proton\nSynchrotron (SPS) beam line. HRMT-68 was a fixed-target laboratory astrophysics\nexperiment aiming to identify plasma instabilities generated by a relativistic\nelectron-positron beam during traversal of an argon plasma. This framework was\nessential for experimental data acquisition and analysis, and can be adapted\nfor a broad range of experiments with a variety of experimental diagnostics.\nThe framework's modular and customizable design enabled us to rapidly observe\nand extract emergent features from a diverse range of diagnostic data.\nSimultaneously, it allowed for both the introduction of new diagnostic devices\nand the modification of our analysis as features of interest were identified.\nAs a result, we were able to effectively diagnose equipment malfunction, and\ninfer the beam's response to varying bunch duration, beam intensity, and the\nplasma state without resorting to offline analysis, at which time adjustment or\nimprovement would have been impossible. We present the features of this agile\nframework, whose codebase we have made publicly available, which can be adapted\nfor future experiments with minimal modification."
                },
                "authors": [
                    {
                        "name": "Hayden Ramm"
                    },
                    {
                        "name": "Pascal Simon"
                    },
                    {
                        "name": "Paraskevi Alexaki"
                    },
                    {
                        "name": "Christopher Arran"
                    },
                    {
                        "name": "Robert Bingham"
                    },
                    {
                        "name": "Alice Goillot"
                    },
                    {
                        "name": "Jon Tomas Gudmundsson"
                    },
                    {
                        "name": "Jonathan Halliday"
                    },
                    {
                        "name": "Bryn Lloyd"
                    },
                    {
                        "name": "Eva Los"
                    },
                    {
                        "name": "Vasiliki Stergiou"
                    },
                    {
                        "name": "Sifei Zhang"
                    },
                    {
                        "name": "Gianluca Gregori"
                    },
                    {
                        "name": "Nikolaos Charitonidis"
                    }
                ],
                "author_detail": {
                    "name": "Nikolaos Charitonidis"
                },
                "author": "Nikolaos Charitonidis",
                "arxiv_comment": "23 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00705v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00705v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.acc-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.acc-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03336v2",
                "updated": "2025-08-04T15:48:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    48,
                    55,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-04T06:49:02Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    2,
                    4,
                    185,
                    0
                ],
                "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky"
                },
                "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents."
                },
                "authors": [
                    {
                        "name": "Ashutosh Hathidara"
                    },
                    {
                        "name": "Julien Yu"
                    },
                    {
                        "name": "Sebastian Schreiber"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Schreiber"
                },
                "author": "Sebastian Schreiber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18661v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18661v3",
                "updated": "2025-08-04T15:39:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    39,
                    40,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-23T16:58:44Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    58,
                    44,
                    2,
                    204,
                    0
                ],
                "title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by\n  Reinforcement Learning from Visual Map Feed Back",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by\n  Reinforcement Learning from Visual Map Feed Back"
                },
                "summary": "Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches."
                },
                "authors": [
                    {
                        "name": "Ruixing Zhang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Tongyu Zhu"
                    },
                    {
                        "name": "Leilei Sun"
                    },
                    {
                        "name": "Weifeng Lv"
                    }
                ],
                "author_detail": {
                    "name": "Weifeng Lv"
                },
                "author": "Weifeng Lv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18661v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18661v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02529v1",
                "updated": "2025-08-04T15:38:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    38,
                    13,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:38:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    38,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking"
                },
                "summary": "Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community."
                },
                "authors": [
                    {
                        "name": "Peihan Li"
                    },
                    {
                        "name": "Jiazhen Liu"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Gaurav S. Sukhatme"
                    },
                    {
                        "name": "Vijay Kumar"
                    },
                    {
                        "name": "Lifeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Zhou"
                },
                "author": "Lifeng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02528v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02528v1",
                "updated": "2025-08-04T15:36:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    36,
                    58,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:36:58Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    36,
                    58,
                    0,
                    216,
                    0
                ],
                "title": "From Pixels to Pathology: Restoration Diffusion for\n  Diagnostic-Consistent Virtual IHC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Pixels to Pathology: Restoration Diffusion for\n  Diagnostic-Consistent Virtual IHC"
                },
                "summary": "Hematoxylin and eosin (H&E) staining is the clinical standard for assessing\ntissue morphology, but it lacks molecular-level diagnostic information. In\ncontrast, immunohistochemistry (IHC) provides crucial insights into biomarker\nexpression, such as HER2 status for breast cancer grading, but remains costly\nand time-consuming, limiting its use in time-sensitive clinical workflows. To\naddress this gap, virtual staining from H&E to IHC has emerged as a promising\nalternative, yet faces two core challenges: (1) Lack of fair evaluation of\nsynthetic images against misaligned IHC ground truths, and (2) preserving\nstructural integrity and biological variability during translation. To this\nend, we present an end-to-end framework encompassing both generation and\nevaluation in this work. We introduce Star-Diff, a structure-aware staining\nrestoration diffusion model that reformulates virtual staining as an image\nrestoration task. By combining residual and noise-based generation pathways,\nStar-Diff maintains tissue structure while modeling realistic biomarker\nvariability. To evaluate the diagnostic consistency of the generated IHC\npatches, we propose the Semantic Fidelity Score (SFS), a\nclinical-grading-task-driven metric that quantifies class-wise semantic\ndegradation based on biomarker classification accuracy. Unlike pixel-level\nmetrics such as SSIM and PSNR, SFS remains robust under spatial misalignment\nand classifier uncertainty. Experiments on the BCI dataset demonstrate that\nStar-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity\nand diagnostic relevance. With rapid inference and strong clinical alignment,it\npresents a practical solution for applications such as intraoperative virtual\nIHC synthesis.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hematoxylin and eosin (H&E) staining is the clinical standard for assessing\ntissue morphology, but it lacks molecular-level diagnostic information. In\ncontrast, immunohistochemistry (IHC) provides crucial insights into biomarker\nexpression, such as HER2 status for breast cancer grading, but remains costly\nand time-consuming, limiting its use in time-sensitive clinical workflows. To\naddress this gap, virtual staining from H&E to IHC has emerged as a promising\nalternative, yet faces two core challenges: (1) Lack of fair evaluation of\nsynthetic images against misaligned IHC ground truths, and (2) preserving\nstructural integrity and biological variability during translation. To this\nend, we present an end-to-end framework encompassing both generation and\nevaluation in this work. We introduce Star-Diff, a structure-aware staining\nrestoration diffusion model that reformulates virtual staining as an image\nrestoration task. By combining residual and noise-based generation pathways,\nStar-Diff maintains tissue structure while modeling realistic biomarker\nvariability. To evaluate the diagnostic consistency of the generated IHC\npatches, we propose the Semantic Fidelity Score (SFS), a\nclinical-grading-task-driven metric that quantifies class-wise semantic\ndegradation based on biomarker classification accuracy. Unlike pixel-level\nmetrics such as SSIM and PSNR, SFS remains robust under spatial misalignment\nand classifier uncertainty. Experiments on the BCI dataset demonstrate that\nStar-Diff achieves state-of-the-art (SOTA) performance in both visual fidelity\nand diagnostic relevance. With rapid inference and strong clinical alignment,it\npresents a practical solution for applications such as intraoperative virtual\nIHC synthesis."
                },
                "authors": [
                    {
                        "name": "Jingsong Liu"
                    },
                    {
                        "name": "Xiaofeng Deng"
                    },
                    {
                        "name": "Han Li"
                    },
                    {
                        "name": "Azar Kazemi"
                    },
                    {
                        "name": "Christian Grashei"
                    },
                    {
                        "name": "Gesa Wilkens"
                    },
                    {
                        "name": "Xin You"
                    },
                    {
                        "name": "Tanja Groll"
                    },
                    {
                        "name": "Nassir Navab"
                    },
                    {
                        "name": "Carolin Mogler"
                    },
                    {
                        "name": "Peter J. Schffler"
                    }
                ],
                "author_detail": {
                    "name": "Peter J. Schffler"
                },
                "author": "Peter J. Schffler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02528v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02528v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.IV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02525v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02525v1",
                "updated": "2025-08-04T15:35:36Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    35,
                    36,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:35:36Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    35,
                    36,
                    0,
                    216,
                    0
                ],
                "title": "Accurate and Interpretable Postmenstrual Age Prediction via Multimodal\n  Large Language Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and Interpretable Postmenstrual Age Prediction via Multimodal\n  Large Language Model"
                },
                "summary": "Accurate estimation of postmenstrual age (PMA) at scan is crucial for\nassessing neonatal development and health. While deep learning models have\nachieved high accuracy in predicting PMA from brain MRI, they often function as\nblack boxes, offering limited transparency and interpretability in clinical\ndecision support. In this work, we address the dual challenge of accuracy and\ninterpretability by adapting a multimodal large language model (MLLM) to\nperform both precise PMA prediction and clinically relevant explanation\ngeneration. We introduce a parameter-efficient fine-tuning (PEFT) strategy\nusing instruction tuning and Low-Rank Adaptation (LoRA) applied to the\nQwen2.5-VL-7B model. The model is trained on four 2D cortical surface\nprojection maps derived from neonatal MRI scans. By employing distinct prompts\nfor training and inference, our approach enables the MLLM to handle a\nregression task during training and generate clinically relevant explanations\nduring inference. The fine-tuned model achieves a low prediction error with a\n95 percent confidence interval of 0.78 to 1.52 weeks, while producing\ninterpretable outputs grounded in developmental features, marking a significant\nstep toward transparent and trustworthy AI systems in perinatal neuroscience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate estimation of postmenstrual age (PMA) at scan is crucial for\nassessing neonatal development and health. While deep learning models have\nachieved high accuracy in predicting PMA from brain MRI, they often function as\nblack boxes, offering limited transparency and interpretability in clinical\ndecision support. In this work, we address the dual challenge of accuracy and\ninterpretability by adapting a multimodal large language model (MLLM) to\nperform both precise PMA prediction and clinically relevant explanation\ngeneration. We introduce a parameter-efficient fine-tuning (PEFT) strategy\nusing instruction tuning and Low-Rank Adaptation (LoRA) applied to the\nQwen2.5-VL-7B model. The model is trained on four 2D cortical surface\nprojection maps derived from neonatal MRI scans. By employing distinct prompts\nfor training and inference, our approach enables the MLLM to handle a\nregression task during training and generate clinically relevant explanations\nduring inference. The fine-tuned model achieves a low prediction error with a\n95 percent confidence interval of 0.78 to 1.52 weeks, while producing\ninterpretable outputs grounded in developmental features, marking a significant\nstep toward transparent and trustworthy AI systems in perinatal neuroscience."
                },
                "authors": [
                    {
                        "name": "Qifan Chen"
                    },
                    {
                        "name": "Jin Cui"
                    },
                    {
                        "name": "Cindy Duan"
                    },
                    {
                        "name": "Yushuo Han"
                    },
                    {
                        "name": "Yifei Shi"
                    }
                ],
                "author_detail": {
                    "name": "Yifei Shi"
                },
                "author": "Yifei Shi",
                "arxiv_comment": "Submitted to the NeurIPS 2025 Workshop GenAI4Health. Conference\n  website: https://aihealth.ischool.utexas.edu/GenAI4HealthNeurips2025/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02525v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02525v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02524v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02524v1",
                "updated": "2025-08-04T15:35:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    35,
                    8,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:35:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    35,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "Causality and Interpretability for Electrical Distribution System faults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causality and Interpretability for Electrical Distribution System faults"
                },
                "summary": "Causal analysis helps us understand variables that are responsible for system\nfailures. This improves fault detection and makes system more reliable. In this\nwork, we present a new method that combines causal inference with machine\nlearning to classify faults in electrical distribution systems (EDS) using\ngraph-based models. We first build causal graphs using transfer entropy (TE).\nEach fault case is represented as a graph, where the nodes are features such as\nvoltage and current, and the edges demonstrate how these features influence\neach other. Then, the graphs are classified using machine learning and\nGraphSAGE where the model learns from both the node values and the structure of\nthe graph to predict the type of fault. To make the predictions understandable,\nwe further developed an integrated approach using GNNExplainer and Captums\nIntegrated Gradients to highlight the nodes (features) that influences the most\non the final prediction. This gives us clear insights into the possible causes\nof the fault. Our experiments show high accuracy: 99.44% on the EDS fault\ndataset, which is better than state of art models. By combining causal graphs\nwith machine learning, our method not only predicts faults accurately but also\nhelps understand their root causes. This makes it a strong and practical tool\nfor improving system reliability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Causal analysis helps us understand variables that are responsible for system\nfailures. This improves fault detection and makes system more reliable. In this\nwork, we present a new method that combines causal inference with machine\nlearning to classify faults in electrical distribution systems (EDS) using\ngraph-based models. We first build causal graphs using transfer entropy (TE).\nEach fault case is represented as a graph, where the nodes are features such as\nvoltage and current, and the edges demonstrate how these features influence\neach other. Then, the graphs are classified using machine learning and\nGraphSAGE where the model learns from both the node values and the structure of\nthe graph to predict the type of fault. To make the predictions understandable,\nwe further developed an integrated approach using GNNExplainer and Captums\nIntegrated Gradients to highlight the nodes (features) that influences the most\non the final prediction. This gives us clear insights into the possible causes\nof the fault. Our experiments show high accuracy: 99.44% on the EDS fault\ndataset, which is better than state of art models. By combining causal graphs\nwith machine learning, our method not only predicts faults accurately but also\nhelps understand their root causes. This makes it a strong and practical tool\nfor improving system reliability."
                },
                "authors": [
                    {
                        "name": "Karthik Peddi"
                    },
                    {
                        "name": "Sai Ram Aditya Parisineni"
                    },
                    {
                        "name": "Hemanth Macharla"
                    },
                    {
                        "name": "Mayukha Pal"
                    }
                ],
                "author_detail": {
                    "name": "Mayukha Pal"
                },
                "author": "Mayukha Pal",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02524v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02524v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02523v1",
                "updated": "2025-08-04T15:34:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    34,
                    25,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:34:25Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    34,
                    25,
                    0,
                    216,
                    0
                ],
                "title": "Transportation Cyber Incident Awareness through Generative AI-Based\n  Incident Analysis and Retrieval-Augmented Question-Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transportation Cyber Incident Awareness through Generative AI-Based\n  Incident Analysis and Retrieval-Augmented Question-Answering Systems"
                },
                "summary": "Technological advancements have revolutionized numerous industries, including\ntransportation. While digitalization, automation, and connectivity have\nenhanced safety and efficiency, they have also introduced new vulnerabilities.\nWith 95% of data breaches attributed to human error, promoting cybersecurity\nawareness in transportation is increasingly critical. Despite numerous\ncyberattacks on transportation systems worldwide, comprehensive and centralized\nrecords of these incidents remain scarce. To address this gap and enhance cyber\nawareness, this paper presents a large language model (LLM) based approach to\nextract and organize transportation related cyber incidents from publicly\navailable datasets. A key contribution of this work is the use of generative AI\nto transform unstructured, heterogeneous cyber incident data into structured\nformats. Incidents were sourced from the Center for Strategic & International\nStudies (CSIS) List of Significant Cyber Incidents, the University of Maryland\nCyber Events Database (UMCED), the European Repository of Cyber Incidents\n(EuRepoC), the Maritime Cyber Attack Database (MCAD), and the U.S. DOT\nTransportation Cybersecurity and Resiliency (TraCR) Examples of Cyber Attacks\nin Transportation (2018 to 2022). These were classified by a fine tuned LLM\ninto five transportation modes: aviation, maritime, rail, road, and multimodal,\nforming a transportation specific cyber incident database. Another key\ncontribution of this work is the development of a Retrieval Augmented\nGeneration question answering system, designed to enhance accessibility and\npractical use by enabling users to query the curated database for specific\ndetails on transportation related cyber incidents. By leveraging LLMs for both\ndata extraction and user interaction, this study contributes a novel,\naccessible tool for improving cybersecurity awareness in the transportation\nsector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technological advancements have revolutionized numerous industries, including\ntransportation. While digitalization, automation, and connectivity have\nenhanced safety and efficiency, they have also introduced new vulnerabilities.\nWith 95% of data breaches attributed to human error, promoting cybersecurity\nawareness in transportation is increasingly critical. Despite numerous\ncyberattacks on transportation systems worldwide, comprehensive and centralized\nrecords of these incidents remain scarce. To address this gap and enhance cyber\nawareness, this paper presents a large language model (LLM) based approach to\nextract and organize transportation related cyber incidents from publicly\navailable datasets. A key contribution of this work is the use of generative AI\nto transform unstructured, heterogeneous cyber incident data into structured\nformats. Incidents were sourced from the Center for Strategic & International\nStudies (CSIS) List of Significant Cyber Incidents, the University of Maryland\nCyber Events Database (UMCED), the European Repository of Cyber Incidents\n(EuRepoC), the Maritime Cyber Attack Database (MCAD), and the U.S. DOT\nTransportation Cybersecurity and Resiliency (TraCR) Examples of Cyber Attacks\nin Transportation (2018 to 2022). These were classified by a fine tuned LLM\ninto five transportation modes: aviation, maritime, rail, road, and multimodal,\nforming a transportation specific cyber incident database. Another key\ncontribution of this work is the development of a Retrieval Augmented\nGeneration question answering system, designed to enhance accessibility and\npractical use by enabling users to query the curated database for specific\ndetails on transportation related cyber incidents. By leveraging LLMs for both\ndata extraction and user interaction, this study contributes a novel,\naccessible tool for improving cybersecurity awareness in the transportation\nsector."
                },
                "authors": [
                    {
                        "name": "Ostonya Thomas"
                    },
                    {
                        "name": "Muhaimin Bin Munir"
                    },
                    {
                        "name": "Jean-Michel Tine"
                    },
                    {
                        "name": "Mizanur Rahman"
                    },
                    {
                        "name": "Yuchen Cai"
                    },
                    {
                        "name": "Khandakar Ashrafi Akbar"
                    },
                    {
                        "name": "Md Nahiyan Uddin"
                    },
                    {
                        "name": "Latifur Khan"
                    },
                    {
                        "name": "Trayce Hockstad"
                    },
                    {
                        "name": "Mashrur Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mashrur Chowdhury"
                },
                "author": "Mashrur Chowdhury",
                "arxiv_comment": "This paper has been submitted to the Transportation Research Board\n  (TRB) for consideration for presentation at the 2026 Annual Meeting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02522v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02522v1",
                "updated": "2025-08-04T15:32:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    32,
                    19,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:32:19Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    32,
                    19,
                    0,
                    216,
                    0
                ],
                "title": "Modelling Stochastic Inflow Patterns to a Reservoir with a Hidden\n  Phase-Type Markov Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modelling Stochastic Inflow Patterns to a Reservoir with a Hidden\n  Phase-Type Markov Model"
                },
                "summary": "This paper presents a novel methodology for modelling precipitation patterns\nin a specific geographical region using Hidden Markov Models (HMMs). Departing\nfrom conventional HMMs, where the hidden state process is assumed to be\nMarkovian, we introduce non-Markovian behaviour by incorporating phase-type\ndistributions to model state durations. The primary objective is to capture the\nalternating sequences of dry and wet periods that characterize the local\nclimate, providing deeper insight into its temporal structure. Building on this\nfoundation, we extend the model to represent reservoir inflow patterns, which\nare then used to explain the observed water storage levels via a Moran model.\nThe dataset includes historical rainfall and inflow records, where the latter\nis influenced by latent conditions governed by the hidden states. Direct\nmodelling based solely on observed rainfall is insufficient due to the\ncomplexity of the system, hence the use of HMMs to infer these unobserved\ndynamics. This approach facilitates more accurate characterization of the\nunderlying climatic processes and enables forecasting of future inflows based\non historical data, supporting improved water resource management in the\nregion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a novel methodology for modelling precipitation patterns\nin a specific geographical region using Hidden Markov Models (HMMs). Departing\nfrom conventional HMMs, where the hidden state process is assumed to be\nMarkovian, we introduce non-Markovian behaviour by incorporating phase-type\ndistributions to model state durations. The primary objective is to capture the\nalternating sequences of dry and wet periods that characterize the local\nclimate, providing deeper insight into its temporal structure. Building on this\nfoundation, we extend the model to represent reservoir inflow patterns, which\nare then used to explain the observed water storage levels via a Moran model.\nThe dataset includes historical rainfall and inflow records, where the latter\nis influenced by latent conditions governed by the hidden states. Direct\nmodelling based solely on observed rainfall is insufficient due to the\ncomplexity of the system, hence the use of HMMs to infer these unobserved\ndynamics. This approach facilitates more accurate characterization of the\nunderlying climatic processes and enables forecasting of future inflows based\non historical data, supporting improved water resource management in the\nregion."
                },
                "authors": [
                    {
                        "name": "M. L. Gamiz"
                    },
                    {
                        "name": "D. Montoro"
                    },
                    {
                        "name": "M. C Segovia-Garcia"
                    }
                ],
                "author_detail": {
                    "name": "M. C Segovia-Garcia"
                },
                "author": "M. C Segovia-Garcia",
                "arxiv_comment": "30 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02522v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02522v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "90B25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "G.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02520v2",
                "updated": "2025-08-05T13:45:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    13,
                    45,
                    27,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T15:30:57Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    30,
                    57,
                    0,
                    216,
                    0
                ],
                "title": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384"
                },
                "summary": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs."
                },
                "authors": [
                    {
                        "name": "Ao Xiao"
                    },
                    {
                        "name": "Bangzheng He"
                    },
                    {
                        "name": "Baoquan Zhang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Bingji Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Boyi Hou"
                    },
                    {
                        "name": "Chan Yang"
                    },
                    {
                        "name": "Changhong Liu"
                    },
                    {
                        "name": "Cheng Cui"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Cong Feng"
                    },
                    {
                        "name": "Daohui Wang"
                    },
                    {
                        "name": "Dayun Lin"
                    },
                    {
                        "name": "Duo Zhao"
                    },
                    {
                        "name": "Fengshao Zou"
                    },
                    {
                        "name": "Fu Wang"
                    },
                    {
                        "name": "Gangqiang Zhang"
                    },
                    {
                        "name": "Gengyuan Dan"
                    },
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Guodong Guan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Hao Huang"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Hengrui Ma"
                    },
                    {
                        "name": "Hengtao Fan"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Jie Meng"
                    },
                    {
                        "name": "Jinhan Xin"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Juwei Chen"
                    },
                    {
                        "name": "Lan Yu"
                    },
                    {
                        "name": "Lanxin Miao"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Linan Jing"
                    },
                    {
                        "name": "Lu Zhou"
                    },
                    {
                        "name": "Meina Han"
                    },
                    {
                        "name": "Mingkun Deng"
                    },
                    {
                        "name": "Mingyu Deng"
                    },
                    {
                        "name": "Naitian Deng"
                    },
                    {
                        "name": "Nizhong Lin"
                    },
                    {
                        "name": "Peihan Zhao"
                    },
                    {
                        "name": "Peng Pan"
                    },
                    {
                        "name": "Pengfei Shen"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Qingyi Zhang"
                    },
                    {
                        "name": "Qunchao Fu"
                    },
                    {
                        "name": "Ren Guo"
                    },
                    {
                        "name": "Ruimin Gao"
                    },
                    {
                        "name": "Shaochun Li"
                    },
                    {
                        "name": "Sheng Long"
                    },
                    {
                        "name": "Shentian Li"
                    },
                    {
                        "name": "Shining Wan"
                    },
                    {
                        "name": "Shuai Shen"
                    },
                    {
                        "name": "Shuangfu Zeng"
                    },
                    {
                        "name": "Shuming Jing"
                    },
                    {
                        "name": "Siqi Yang"
                    },
                    {
                        "name": "Song Zhang"
                    },
                    {
                        "name": "Tao Xu"
                    },
                    {
                        "name": "Tianlin Du"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Wanxu Wu"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Weinan Tong"
                    },
                    {
                        "name": "Weiwei Chen"
                    },
                    {
                        "name": "Wen Peng"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Wenquan Yang"
                    },
                    {
                        "name": "Wenxin Liang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Xiaoli Zhou"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yalong Shan"
                    },
                    {
                        "name": "Yang Gan"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Yi Deng"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Yingfei Zheng"
                    },
                    {
                        "name": "Yiyun Zheng"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Yong Gao"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Yuanjin Gong"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yuetao Chen"
                    },
                    {
                        "name": "Yukun Zhu"
                    },
                    {
                        "name": "Yulong He"
                    },
                    {
                        "name": "Yusu Zhao"
                    },
                    {
                        "name": "Yuyan Wu"
                    },
                    {
                        "name": "Zenan Zhang"
                    },
                    {
                        "name": "Zhaojin Zhuo"
                    },
                    {
                        "name": "Zhaoyang Ji"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhenhua Yang"
                    },
                    {
                        "name": "Zhenli Sheng"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Zhigang Ji"
                    },
                    {
                        "name": "Zhihao Ren"
                    },
                    {
                        "name": "Zhipeng Bian"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Zhiyu Dong"
                    },
                    {
                        "name": "Zhonghua Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Zhuoming Shen"
                    },
                    {
                        "name": "Zhuwei Peng"
                    },
                    {
                        "name": "Zi Ye"
                    },
                    {
                        "name": "Zihao Xiang"
                    },
                    {
                        "name": "Zimin Fu"
                    },
                    {
                        "name": "Zixuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zixuan Zhang"
                },
                "author": "Zixuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02518v1",
                "updated": "2025-08-04T15:25:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    25,
                    48,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:25:48Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    25,
                    48,
                    0,
                    216,
                    0
                ],
                "title": "AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via\n  Multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via\n  Multi-modal LLMs"
                },
                "summary": "Despite advances in analog design automation, analog front-end design still\nheavily depends on expert intuition and iterative simulations, underscoring\ncritical gaps in fully automated optimization for performance-critical\napplications. Recently, the rapid development of Large Language Models (LLMs)\nhas brought new promise to analog design automation. However, existing work\nremains in its early stages, and holistic joint optimization for practical\nend-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a\nunified multimodal LLM-based framework that integrates generative capabilities\nand optimization techniques to jointly explore circuit topologies and optimize\ndevice sizing, automatically generating performance-specific, fully sized\nschematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning\nLLMs on high-quality synthesized circuit data and introduces a multimodal\ndiagnosis and repair workflow based on functional specifications and waveform\nimages. By leveraging LLMs to interpret generated circuit netlists,\nAnalogCoder-Pro automates the extraction of critical design parameters and the\nformulation of parameter spaces, establishing an end-to-end workflow for\nsimultaneous topology generation and device sizing optimization. Extensive\nexperiments demonstrate that these orthogonal approaches significantly improve\nthe success rate of analog circuit design and enhance circuit performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in analog design automation, analog front-end design still\nheavily depends on expert intuition and iterative simulations, underscoring\ncritical gaps in fully automated optimization for performance-critical\napplications. Recently, the rapid development of Large Language Models (LLMs)\nhas brought new promise to analog design automation. However, existing work\nremains in its early stages, and holistic joint optimization for practical\nend-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a\nunified multimodal LLM-based framework that integrates generative capabilities\nand optimization techniques to jointly explore circuit topologies and optimize\ndevice sizing, automatically generating performance-specific, fully sized\nschematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning\nLLMs on high-quality synthesized circuit data and introduces a multimodal\ndiagnosis and repair workflow based on functional specifications and waveform\nimages. By leveraging LLMs to interpret generated circuit netlists,\nAnalogCoder-Pro automates the extraction of critical design parameters and the\nformulation of parameter spaces, establishing an end-to-end workflow for\nsimultaneous topology generation and device sizing optimization. Extensive\nexperiments demonstrate that these orthogonal approaches significantly improve\nthe success rate of analog circuit design and enhance circuit performance."
                },
                "authors": [
                    {
                        "name": "Yao Lai"
                    },
                    {
                        "name": "Souradip Poddar"
                    },
                    {
                        "name": "Sungyoung Lee"
                    },
                    {
                        "name": "Guojin Chen"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "David Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "David Z. Pan"
                },
                "author": "David Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02515v1",
                "updated": "2025-08-04T15:19:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    19,
                    22,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:19:22Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    19,
                    22,
                    0,
                    216,
                    0
                ],
                "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese\n  Songci with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoeTone: A Framework for Constrained Generation of Structured Chinese\n  Songci with LLMs"
                },
                "summary": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts."
                },
                "authors": [
                    {
                        "name": "Zhan Qu"
                    },
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael Frber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Frber"
                },
                "author": "Michael Frber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02513v1",
                "updated": "2025-08-04T15:18:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    18,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:18:41Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    18,
                    41,
                    0,
                    216,
                    0
                ],
                "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Arithmetic: Language Models Solve Math Digit by Digit"
                },
                "summary": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks."
                },
                "authors": [
                    {
                        "name": "Tanja Baeumel"
                    },
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Yusser al Ghussin"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02511v1",
                "updated": "2025-08-04T15:17:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    17,
                    13,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:17:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    17,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "Test-time Prompt Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Prompt Intervention"
                },
                "summary": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning."
                },
                "authors": [
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Mz Dai"
                    },
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Mingyu Zheng"
                    },
                    {
                        "name": "Minghui Chen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "23 pages, 16 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02509v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02509v1",
                "updated": "2025-08-04T15:16:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    16,
                    39,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:16:39Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    16,
                    39,
                    0,
                    216,
                    0
                ],
                "title": "Quantitative and Predictive Folding Models from Limited Single-Molecule\n  Data Using Simulation-Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantitative and Predictive Folding Models from Limited Single-Molecule\n  Data Using Simulation-Based Inference"
                },
                "summary": "The study of biomolecular folding has been greatly advanced by\nsingle-molecule force spectroscopy (SMFS), which enables the observation of the\ndynamics of individual molecules. However, extracting quantitative models of\nfundamental properties such as folding landscapes from SNFS data is very\nchallenging due to instrumental noise, linker artifacts, and the inherent\nstochasticity of the process, often requiring extensive datasets and complex\ncalibration experiments. Here, we introduce a framework based on\nsimulation-based inference (SBI) that overcomes these limitations by\nintegrating physics-based modeling with deep learning. We apply this framework\nto analyze constant-force measurements of a DNA hairpin. From a single, short\nexperimental trajectory of only two seconds, we successfully reconstruct the\nhairpin's free energy landscape and folding dynamics, obtaining results that\nare in close agreement with established deconvolution methods that require\napproximately 100 times more data. Furthermore, the Bayesian nature of this\napproach robustly quantifies uncertainties for inferred parameter values,\nincluding the free-energy profile, diffusion coefficients, and linker\nstiffness, without needing independent measurements of instrumental properties.\nThe inferred model is predictive, generating simulated trajectories that\nquantitatively reproduce the thermodynamic and kinetic properties of the\nexperimental data. This work establishes SBI as a highly efficient and powerful\ntool for analyzing single-molecule experiments. The ability to derive\nstatistically robust models from minimal datasets is crucial for investigating\ncomplex biomolecular systems where extensive data collection is impractical or\nimpossible. Consequently, our SBI framework enables the rigorous quantitative\nanalysis of previously intractable biomolecular systems, paving the way for\nnovel applications of SMFS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The study of biomolecular folding has been greatly advanced by\nsingle-molecule force spectroscopy (SMFS), which enables the observation of the\ndynamics of individual molecules. However, extracting quantitative models of\nfundamental properties such as folding landscapes from SNFS data is very\nchallenging due to instrumental noise, linker artifacts, and the inherent\nstochasticity of the process, often requiring extensive datasets and complex\ncalibration experiments. Here, we introduce a framework based on\nsimulation-based inference (SBI) that overcomes these limitations by\nintegrating physics-based modeling with deep learning. We apply this framework\nto analyze constant-force measurements of a DNA hairpin. From a single, short\nexperimental trajectory of only two seconds, we successfully reconstruct the\nhairpin's free energy landscape and folding dynamics, obtaining results that\nare in close agreement with established deconvolution methods that require\napproximately 100 times more data. Furthermore, the Bayesian nature of this\napproach robustly quantifies uncertainties for inferred parameter values,\nincluding the free-energy profile, diffusion coefficients, and linker\nstiffness, without needing independent measurements of instrumental properties.\nThe inferred model is predictive, generating simulated trajectories that\nquantitatively reproduce the thermodynamic and kinetic properties of the\nexperimental data. This work establishes SBI as a highly efficient and powerful\ntool for analyzing single-molecule experiments. The ability to derive\nstatistically robust models from minimal datasets is crucial for investigating\ncomplex biomolecular systems where extensive data collection is impractical or\nimpossible. Consequently, our SBI framework enables the rigorous quantitative\nanalysis of previously intractable biomolecular systems, paving the way for\nnovel applications of SMFS."
                },
                "authors": [
                    {
                        "name": "Lars Dingeldein"
                    },
                    {
                        "name": "Aaron Lyons"
                    },
                    {
                        "name": "Pilar Cossio"
                    },
                    {
                        "name": "Michael Woodside"
                    },
                    {
                        "name": "Roberto Covino"
                    }
                ],
                "author_detail": {
                    "name": "Roberto Covino"
                },
                "author": "Roberto Covino",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02509v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02509v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.chem-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.chem-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12172v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12172v3",
                "updated": "2025-08-04T15:15:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    15,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-03-15T15:29:05Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    15,
                    29,
                    5,
                    5,
                    74,
                    0
                ],
                "title": "SEAL: Semantic Aware Image Watermarking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SEAL: Semantic Aware Image Watermarking"
                },
                "summary": "Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative models have rapidly evolved to generate realistic outputs.\nHowever, their synthetic outputs increasingly challenge the clear distinction\nbetween natural and AI-generated content, necessitating robust watermarking\ntechniques. Watermarks are typically expected to preserve the integrity of the\ntarget image, withstand removal attempts, and prevent unauthorized replication\nonto unrelated images. To address this need, recent methods embed persistent\nwatermarks into images produced by diffusion models using the initial noise.\nYet, to do so, they either distort the distribution of generated images or rely\non searching through a long dictionary of used keys for detection.\n  In this paper, we propose a novel watermarking method that embeds semantic\ninformation about the generated image directly into the watermark, enabling a\ndistortion-free watermark that can be verified without requiring a database of\nkey patterns. Instead, the key pattern can be inferred from the semantic\nembedding of the image using locality-sensitive hashing. Furthermore,\nconditioning the watermark detection on the original image content improves\nrobustness against forgery attacks. To demonstrate that, we consider two\nlargely overlooked attack strategies: (i) an attacker extracting the initial\nnoise and generating a novel image with the same pattern; (ii) an attacker\ninserting an unrelated (potentially harmful) object into a watermarked image,\npossibly while preserving the watermark. We empirically validate our method's\nincreased robustness to these attacks. Taken together, our results suggest that\ncontent-aware watermarks can mitigate risks arising from image-generative\nmodels."
                },
                "authors": [
                    {
                        "name": "Kasra Arabi"
                    },
                    {
                        "name": "R. Teal Witter"
                    },
                    {
                        "name": "Chinmay Hegde"
                    },
                    {
                        "name": "Niv Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Niv Cohen"
                },
                "author": "Niv Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12172v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12172v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02507v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02507v1",
                "updated": "2025-08-04T15:14:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    14,
                    47,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:14:47Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    14,
                    47,
                    0,
                    216,
                    0
                ],
                "title": "Rethinking Transparent Object Grasping: Depth Completion with Monocular\n  Depth Estimation and Instance Mask",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Transparent Object Grasping: Depth Completion with Monocular\n  Depth Estimation and Instance Mask"
                },
                "summary": "Due to the optical properties, transparent objects often lead depth cameras\nto generate incomplete or invalid depth data, which in turn reduces the\naccuracy and reliability of robotic grasping. Existing approaches typically\ninput the RGB-D image directly into the network to output the complete depth,\nexpecting the model to implicitly infer the reliability of depth values.\nHowever, while effective in training datasets, such methods often fail to\ngeneralize to real-world scenarios, where complex light interactions lead to\nhighly variable distributions of valid and invalid depth data. To address this,\nwe propose ReMake, a novel depth completion framework guided by an instance\nmask and monocular depth estimation. By explicitly distinguishing transparent\nregions from non-transparent ones, the mask enables the model to concentrate on\nlearning accurate depth estimation in these areas from RGB-D input during\ntraining. This targeted supervision reduces reliance on implicit reasoning and\nimproves generalization to real-world scenarios. Additionally, monocular depth\nestimation provides depth context between the transparent object and its\nsurroundings, enhancing depth prediction accuracy. Extensive experiments show\nthat our method outperforms existing approaches on both benchmark datasets and\nreal-world scenarios, demonstrating superior accuracy and generalization\ncapability. Code and videos are available at\nhttps://chengyaofeng.github.io/ReMake.github.io/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to the optical properties, transparent objects often lead depth cameras\nto generate incomplete or invalid depth data, which in turn reduces the\naccuracy and reliability of robotic grasping. Existing approaches typically\ninput the RGB-D image directly into the network to output the complete depth,\nexpecting the model to implicitly infer the reliability of depth values.\nHowever, while effective in training datasets, such methods often fail to\ngeneralize to real-world scenarios, where complex light interactions lead to\nhighly variable distributions of valid and invalid depth data. To address this,\nwe propose ReMake, a novel depth completion framework guided by an instance\nmask and monocular depth estimation. By explicitly distinguishing transparent\nregions from non-transparent ones, the mask enables the model to concentrate on\nlearning accurate depth estimation in these areas from RGB-D input during\ntraining. This targeted supervision reduces reliance on implicit reasoning and\nimproves generalization to real-world scenarios. Additionally, monocular depth\nestimation provides depth context between the transparent object and its\nsurroundings, enhancing depth prediction accuracy. Extensive experiments show\nthat our method outperforms existing approaches on both benchmark datasets and\nreal-world scenarios, demonstrating superior accuracy and generalization\ncapability. Code and videos are available at\nhttps://chengyaofeng.github.io/ReMake.github.io/."
                },
                "authors": [
                    {
                        "name": "Yaofeng Cheng"
                    },
                    {
                        "name": "Xinkai Gao"
                    },
                    {
                        "name": "Sen Zhang"
                    },
                    {
                        "name": "Chao Zeng"
                    },
                    {
                        "name": "Fusheng Zha"
                    },
                    {
                        "name": "Lining Sun"
                    },
                    {
                        "name": "Chenguang Yang"
                    }
                ],
                "author_detail": {
                    "name": "Chenguang Yang"
                },
                "author": "Chenguang Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02507v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02507v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02506v1",
                "updated": "2025-08-04T15:14:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    14,
                    9,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:14:09Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    14,
                    9,
                    0,
                    216,
                    0
                ],
                "title": "Decomposed Reasoning with Reinforcement Learning for Relevance\n  Assessment in UGC Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposed Reasoning with Reinforcement Learning for Relevance\n  Assessment in UGC Platforms"
                },
                "summary": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments."
                },
                "authors": [
                    {
                        "name": "Xiaowei Yuan"
                    },
                    {
                        "name": "Lei Jin"
                    },
                    {
                        "name": "Haoxin Zhang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Ziyang Huang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02505v1",
                "updated": "2025-08-04T15:13:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    13,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:13:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    13,
                    56,
                    0,
                    216,
                    0
                ],
                "title": "Would you let a humanoid play storytelling with your child? A usability\n  study on LLM-powered narrative Humanoid-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Would you let a humanoid play storytelling with your child? A usability\n  study on LLM-powered narrative Humanoid-Robot Interaction"
                },
                "summary": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role."
                },
                "authors": [
                    {
                        "name": "Maria Lombardi"
                    },
                    {
                        "name": "Carmela Calabrese"
                    },
                    {
                        "name": "Davide Ghiglino"
                    },
                    {
                        "name": "Caterina Foglino"
                    },
                    {
                        "name": "Davide De Tommaso"
                    },
                    {
                        "name": "Giulia Da Lisca"
                    },
                    {
                        "name": "Lorenzo Natale"
                    },
                    {
                        "name": "Agnieszka Wykowska"
                    }
                ],
                "author_detail": {
                    "name": "Agnieszka Wykowska"
                },
                "author": "Agnieszka Wykowska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02503v1",
                "updated": "2025-08-04T15:11:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    11,
                    51,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:11:51Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    11,
                    51,
                    0,
                    216,
                    0
                ],
                "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical\n  Modeling"
                },
                "summary": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems."
                },
                "authors": [
                    {
                        "name": "Maxime Bouscary"
                    },
                    {
                        "name": "Saurabh Amin"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Amin"
                },
                "author": "Saurabh Amin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03165v2",
                "updated": "2025-08-04T15:10:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    10,
                    59,
                    0,
                    216,
                    0
                ],
                "published": "2025-04-04T04:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    43,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Kaiming Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xuanyu Lei"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02502v1",
                "updated": "2025-08-04T15:10:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    10,
                    44,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:10:44Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    10,
                    44,
                    0,
                    216,
                    0
                ],
                "title": "From Monolingual to Bilingual: Investigating Language Conditioning in\n  Large Language Models for Psycholinguistic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Monolingual to Bilingual: Investigating Language Conditioning in\n  Large Language Models for Psycholinguistic Tasks"
                },
                "summary": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition."
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Zhan Qu"
                    },
                    {
                        "name": "Mario Tawfelis"
                    },
                    {
                        "name": "Michael Frber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Frber"
                },
                "author": "Michael Frber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02497v1",
                "updated": "2025-08-04T15:07:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    7,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:07:35Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    7,
                    35,
                    0,
                    216,
                    0
                ],
                "title": "Bridging Language Gaps in Open-Source Documentation with\n  Large-Language-Model Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Language Gaps in Open-Source Documentation with\n  Large-Language-Model Translation"
                },
                "summary": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation."
                },
                "authors": [
                    {
                        "name": "Elijah Kayode Adejumo"
                    },
                    {
                        "name": "Brittany Johnson"
                    },
                    {
                        "name": "Mariam Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Mariam Guizani"
                },
                "author": "Mariam Guizani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02490v1",
                "updated": "2025-08-04T15:01:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    1,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:01:41Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    1,
                    41,
                    0,
                    216,
                    0
                ],
                "title": "PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic\n  Evaluation of Large Models in Prognostics and Health Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic\n  Evaluation of Large Models in Prognostics and Health Management"
                },
                "summary": "With the rapid advancement of generative artificial intelligence, large\nlanguage models (LLMs) are increasingly adopted in industrial domains, offering\nnew opportunities for Prognostics and Health Management (PHM). These models\nhelp address challenges such as high development costs, long deployment cycles,\nand limited generalizability. However, despite the growing synergy between PHM\nand LLMs, existing evaluation methodologies often fall short in structural\ncompleteness, dimensional comprehensiveness, and evaluation granularity. This\nhampers the in-depth integration of LLMs into the PHM domain. To address these\nlimitations, this study proposes PHM-Bench, a novel three-dimensional\nevaluation framework for PHM-oriented large models. Grounded in the triadic\nstructure of fundamental capability, core task, and entire lifecycle, PHM-Bench\nis tailored to the unique demands of PHM system engineering. It defines\nmulti-level evaluation metrics spanning knowledge comprehension, algorithmic\ngeneration, and task optimization. These metrics align with typical PHM tasks,\nincluding condition monitoring, fault diagnosis, RUL prediction, and\nmaintenance decision-making. Utilizing both curated case sets and publicly\navailable industrial datasets, our study enables multi-dimensional evaluation\nof general-purpose and domain-specific models across diverse PHM tasks.\nPHM-Bench establishes a methodological foundation for large-scale assessment of\nLLMs in PHM and offers a critical benchmark to guide the transition from\ngeneral-purpose to PHM-specialized models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of generative artificial intelligence, large\nlanguage models (LLMs) are increasingly adopted in industrial domains, offering\nnew opportunities for Prognostics and Health Management (PHM). These models\nhelp address challenges such as high development costs, long deployment cycles,\nand limited generalizability. However, despite the growing synergy between PHM\nand LLMs, existing evaluation methodologies often fall short in structural\ncompleteness, dimensional comprehensiveness, and evaluation granularity. This\nhampers the in-depth integration of LLMs into the PHM domain. To address these\nlimitations, this study proposes PHM-Bench, a novel three-dimensional\nevaluation framework for PHM-oriented large models. Grounded in the triadic\nstructure of fundamental capability, core task, and entire lifecycle, PHM-Bench\nis tailored to the unique demands of PHM system engineering. It defines\nmulti-level evaluation metrics spanning knowledge comprehension, algorithmic\ngeneration, and task optimization. These metrics align with typical PHM tasks,\nincluding condition monitoring, fault diagnosis, RUL prediction, and\nmaintenance decision-making. Utilizing both curated case sets and publicly\navailable industrial datasets, our study enables multi-dimensional evaluation\nof general-purpose and domain-specific models across diverse PHM tasks.\nPHM-Bench establishes a methodological foundation for large-scale assessment of\nLLMs in PHM and offers a critical benchmark to guide the transition from\ngeneral-purpose to PHM-specialized models."
                },
                "authors": [
                    {
                        "name": "Puyu Yang"
                    },
                    {
                        "name": "Laifa Tao"
                    },
                    {
                        "name": "Zijian Huang"
                    },
                    {
                        "name": "Haifei Liu"
                    },
                    {
                        "name": "Wenyan Cao"
                    },
                    {
                        "name": "Hao Ji"
                    },
                    {
                        "name": "Jianan Qiu"
                    },
                    {
                        "name": "Qixuan Huang"
                    },
                    {
                        "name": "Xuanyuan Su"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shangyu Li"
                    },
                    {
                        "name": "Chen Lu"
                    },
                    {
                        "name": "Zhixuan Lian"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Lian"
                },
                "author": "Zhixuan Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12049v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12049v2",
                "updated": "2025-08-04T14:59:28Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    59,
                    28,
                    0,
                    216,
                    0
                ],
                "published": "2025-03-15T08:47:45Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    8,
                    47,
                    45,
                    5,
                    74,
                    0
                ],
                "title": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TACO: Taming Diffusion for in-the-wild Video Amodal Completion"
                },
                "summary": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Humans can infer complete shapes and appearances of objects from limited\nvisual cues, relying on extensive prior knowledge of the physical world.\nHowever, completing partially observable objects while ensuring consistency\nacross video frames remains challenging for existing models, especially for\nunstructured, in-the-wild videos. This paper tackles the task of Video Amodal\nCompletion (VAC), which aims to generate the complete object consistently\nthroughout the video given a visual prompt specifying the object of interest.\nLeveraging the rich, consistent manifolds learned by pre-trained video\ndiffusion models, we propose a conditional diffusion model, TACO, that\nrepurposes these manifolds for VAC. To enable its effective and robust\ngeneralization to challenging in-the-wild scenarios, we curate a large-scale\nsynthetic dataset with multiple difficulty levels by systematically imposing\nocclusions onto un-occluded videos. Building on this, we devise a progressive\nfine-tuning paradigm that starts with simpler recovery tasks and gradually\nadvances to more complex ones. We demonstrate TACO's versatility on a wide\nrange of in-the-wild videos from Internet, as well as on diverse, unseen\ndatasets commonly used in autonomous driving, robotic manipulation, and scene\nunderstanding. Moreover, we show that TACO can be effectively applied to\nvarious downstream tasks like object reconstruction and pose estimation,\nhighlighting its potential to facilitate physical world understanding and\nreasoning. Our project page is available at https://jason-aplp.github.io/TACO."
                },
                "authors": [
                    {
                        "name": "Ruijie Lu"
                    },
                    {
                        "name": "Yixin Chen"
                    },
                    {
                        "name": "Yu Liu"
                    },
                    {
                        "name": "Jiaxiang Tang"
                    },
                    {
                        "name": "Junfeng Ni"
                    },
                    {
                        "name": "Diwen Wan"
                    },
                    {
                        "name": "Gang Zeng"
                    },
                    {
                        "name": "Siyuan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Siyuan Huang"
                },
                "author": "Siyuan Huang",
                "arxiv_comment": "Accepted by ICCV 2025.Project page: https://jason-aplp.github.io/TACO",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12049v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12049v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20243v3",
                "updated": "2025-08-04T14:52:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    52,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-05-26T17:21:26Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    17,
                    21,
                    26,
                    0,
                    146,
                    0
                ],
                "title": "It's High Time: A Survey of Temporal Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's High Time: A Survey of Temporal Question Answering"
                },
                "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nQuestion Answering (TQA), a research area that focuses on answering questions\ninvolving temporal constraints or context. As the amount of time-stamped\ncontent from sources like news articles, web archives, and knowledge bases\nincreases, systems must address challenges such as detecting temporal intent,\nnormalizing time expressions, ordering events, and reasoning over evolving or\nambiguous facts. We focus on recent advances in TQA enabled by neural\narchitectures, especially transformer-based models and Large Language Models\n(LLMs), highlighting progress in temporal language modeling,\nretrieval-augmented generation (RAG), and temporal reasoning. We also discuss\nbenchmark datasets and evaluation strategies designed to test temporal\nrobustness, recency awareness, and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nQuestion Answering (TQA), a research area that focuses on answering questions\ninvolving temporal constraints or context. As the amount of time-stamped\ncontent from sources like news articles, web archives, and knowledge bases\nincreases, systems must address challenges such as detecting temporal intent,\nnormalizing time expressions, ordering events, and reasoning over evolving or\nambiguous facts. We focus on recent advances in TQA enabled by neural\narchitectures, especially transformer-based models and Large Language Models\n(LLMs), highlighting progress in temporal language modeling,\nretrieval-augmented generation (RAG), and temporal reasoning. We also discuss\nbenchmark datasets and evaluation strategies designed to test temporal\nrobustness, recency awareness, and generalization."
                },
                "authors": [
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Avishek Anand"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02480v1",
                "updated": "2025-08-04T14:47:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    47,
                    17,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:47:17Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    47,
                    17,
                    0,
                    216,
                    0
                ],
                "title": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding"
                },
                "summary": "Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting."
                },
                "authors": [
                    {
                        "name": "Wenwen Zeng"
                    },
                    {
                        "name": "Yonghuang Wu"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Xuan Xie"
                    },
                    {
                        "name": "Chengqian Zhao"
                    },
                    {
                        "name": "Feiyu Yin"
                    },
                    {
                        "name": "Guoqing Wu"
                    },
                    {
                        "name": "Jinhua Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Yu"
                },
                "author": "Jinhua Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11336v2",
                "updated": "2025-08-04T14:42:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    42,
                    2,
                    0,
                    216,
                    0
                ],
                "published": "2025-05-16T15:02:19Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    15,
                    2,
                    19,
                    4,
                    136,
                    0
                ],
                "title": "XtraGPT: Context-Aware and Controllable Academic Paper Revision via\n  Human-AI Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XtraGPT: Context-Aware and Controllable Academic Paper Revision via\n  Human-AI Collaboration"
                },
                "summary": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Andre Lin HuiKai"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "Preprint. The model report is available at\n  https://arxiv.org/abs/2505.11336v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02473v1",
                "updated": "2025-08-04T14:37:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    37,
                    32,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:37:32Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    37,
                    32,
                    0,
                    216,
                    0
                ],
                "title": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human\n  Instructions in IDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human\n  Instructions in IDEs"
                },
                "summary": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY."
                },
                "authors": [
                    {
                        "name": "Xinfang Chen"
                    },
                    {
                        "name": "Siyang Xiao"
                    },
                    {
                        "name": "Xianying Zhu"
                    },
                    {
                        "name": "Junhong Xie"
                    },
                    {
                        "name": "Ming Liang"
                    },
                    {
                        "name": "Dajun Chen"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Peng Di"
                    }
                ],
                "author_detail": {
                    "name": "Peng Di"
                },
                "author": "Peng Di",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3; D.1.2; I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02471v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02471v1",
                "updated": "2025-08-04T14:36:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    36,
                    34,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:36:34Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    36,
                    34,
                    0,
                    216,
                    0
                ],
                "title": "Inverse harmonic clustering for multi-pitch estimation: an optimal\n  transport approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inverse harmonic clustering for multi-pitch estimation: an optimal\n  transport approach"
                },
                "summary": "In this work, we consider the problem of multi-pitch estimation, i.e.,\nidentifying super-imposed truncated harmonic series from noisy measurements. We\nphrase this as recovering a harmonically-structured measure on the unit circle,\nwhere the structure is enforced using regularizers based on optimal transport\ntheory. In the resulting framework, a signal's spectral content is\nsimultaneously inferred and assigned, or transported, to a small set of\nharmonic series defined by their corresponding fundamental frequencies. In\ncontrast to existing methods from the compressed sensing paradigm, the proposed\nframework decouples regularization and dictionary design and mitigates\ncoherency problems. As a direct consequence, this also introduces robustness to\nthe phenomenon of inharmonicity. From this framework, we derive two estimation\nmethods, one for stochastic and one for deterministic signals, and propose\nefficient numerical algorithms implementing them. In numerical studies on both\nsynthetic and real data, the proposed methods are shown to achieve better\nestimation performance as compared to other methods from statistical signal\nprocessing literature. Furthermore, they perform comparably or better than\nnetwork-based methods, except when the latter are specially trained on the\ndata-type considered and are given access to considerably more data during\ninference.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we consider the problem of multi-pitch estimation, i.e.,\nidentifying super-imposed truncated harmonic series from noisy measurements. We\nphrase this as recovering a harmonically-structured measure on the unit circle,\nwhere the structure is enforced using regularizers based on optimal transport\ntheory. In the resulting framework, a signal's spectral content is\nsimultaneously inferred and assigned, or transported, to a small set of\nharmonic series defined by their corresponding fundamental frequencies. In\ncontrast to existing methods from the compressed sensing paradigm, the proposed\nframework decouples regularization and dictionary design and mitigates\ncoherency problems. As a direct consequence, this also introduces robustness to\nthe phenomenon of inharmonicity. From this framework, we derive two estimation\nmethods, one for stochastic and one for deterministic signals, and propose\nefficient numerical algorithms implementing them. In numerical studies on both\nsynthetic and real data, the proposed methods are shown to achieve better\nestimation performance as compared to other methods from statistical signal\nprocessing literature. Furthermore, they perform comparably or better than\nnetwork-based methods, except when the latter are specially trained on the\ndata-type considered and are given access to considerably more data during\ninference."
                },
                "authors": [
                    {
                        "name": "Anton Bjrkman"
                    },
                    {
                        "name": "Filip Elvander"
                    }
                ],
                "author_detail": {
                    "name": "Filip Elvander"
                },
                "author": "Filip Elvander",
                "arxiv_comment": "13 pages, 8 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02471v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02471v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02464v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02464v1",
                "updated": "2025-08-04T14:31:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    31,
                    11,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:31:11Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    31,
                    11,
                    0,
                    216,
                    0
                ],
                "title": "SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with\n  Vision Foundation Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAMPO: Visual Preference Optimization for Intent-Aware Segmentation with\n  Vision Foundation Models"
                },
                "summary": "Foundation models like Segment Anything Model (SAM) excel in promptable\nsegmentation but suffer from an intent gap: they segment only explicitly\nprompted objects, failing to generalize to semantically related instances\nimplicitly desired by users. This limitation is critical in domains with dense\nhomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual\nprompts typically yield incomplete results, rendering dense annotations\nimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO\n(Segment Anything Model with Preference Optimization), a novel framework that\nteaches visual foundation models to infer high-level categorical intent from\nsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO\noptimizes models to implicitly capture target-class characteristics through\npreference optimization. This approach, which operates without dependency on\nlanguage models, enables robust multi-object segmentation even under sparse\nprompting and demonstrates superior data efficiency during fine-tuning.\nValidated on three medical segmentation tasks, SAMPO achieves state-of-the-art\nperformance: on challenging tasks like PanNuke-T2, our method, when fine-tuned\nwith only 10% of the training data, significantly outperforms all existing\nmethods trained on the full 100% dataset, achieving an improvement of over 9\npercentage points compared to the best baseline. Our work establishes a new\nparadigm for intent-aware alignment in visual foundation models, removing\ndependencies on auxiliary prompt generators or language-model-assisted\npreference learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models like Segment Anything Model (SAM) excel in promptable\nsegmentation but suffer from an intent gap: they segment only explicitly\nprompted objects, failing to generalize to semantically related instances\nimplicitly desired by users. This limitation is critical in domains with dense\nhomogeneous objects (e.g., biomedical nuclei segmentation), where sparse visual\nprompts typically yield incomplete results, rendering dense annotations\nimpractical due to prohibitive cost. To bridge this gap, we introduce SAMPO\n(Segment Anything Model with Preference Optimization), a novel framework that\nteaches visual foundation models to infer high-level categorical intent from\nsparse visual interactions. Unlike conventional pixel-level fine-tuning, SAMPO\noptimizes models to implicitly capture target-class characteristics through\npreference optimization. This approach, which operates without dependency on\nlanguage models, enables robust multi-object segmentation even under sparse\nprompting and demonstrates superior data efficiency during fine-tuning.\nValidated on three medical segmentation tasks, SAMPO achieves state-of-the-art\nperformance: on challenging tasks like PanNuke-T2, our method, when fine-tuned\nwith only 10% of the training data, significantly outperforms all existing\nmethods trained on the full 100% dataset, achieving an improvement of over 9\npercentage points compared to the best baseline. Our work establishes a new\nparadigm for intent-aware alignment in visual foundation models, removing\ndependencies on auxiliary prompt generators or language-model-assisted\npreference learning."
                },
                "authors": [
                    {
                        "name": "Yonghuang Wu"
                    },
                    {
                        "name": "Wenwen Zeng"
                    },
                    {
                        "name": "Xuan Xie"
                    },
                    {
                        "name": "Chengqian Zhao"
                    },
                    {
                        "name": "Guoqing Wu"
                    },
                    {
                        "name": "Jinhua Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Yu"
                },
                "author": "Jinhua Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02464v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02464v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02458v2",
                "updated": "2025-08-05T09:17:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    17,
                    57,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T14:24:30Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    24,
                    30,
                    0,
                    216,
                    0
                ],
                "title": "From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via\n  Bilateral Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via\n  Bilateral Reinforcement Learning"
                },
                "summary": "Large Language Models show promise in emotion understanding, social\nreasoning, and empathy, yet they struggle with psychologically grounded tasks\nthat require inferring implicit mental states in context-rich, ambiguous\nsettings. These limitations arise from the absence of theory-aligned\nsupervision and the difficulty of capturing nuanced mental processes in\nreal-world narratives. To address this gap, we leverage expert-labeled,\npsychologically rich scenarios and propose a trajectory-aware reinforcement\nlearning framework that explicitly imitates expert psychological thought\npatterns. By integrating real-world stimuli with structured reasoning guidance,\nour approach enables compact models to internalize social-cognitive principles,\nperform nuanced psychological inference, and support continual\nself-improvement. Comprehensive experiments across multiple benchmarks further\ndemonstrate that our models achieve expert-level interpretive capabilities,\nexhibiting strong out-of-distribution generalization and robust continual\nlearning across diverse, challenging, and psychologically grounded tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models show promise in emotion understanding, social\nreasoning, and empathy, yet they struggle with psychologically grounded tasks\nthat require inferring implicit mental states in context-rich, ambiguous\nsettings. These limitations arise from the absence of theory-aligned\nsupervision and the difficulty of capturing nuanced mental processes in\nreal-world narratives. To address this gap, we leverage expert-labeled,\npsychologically rich scenarios and propose a trajectory-aware reinforcement\nlearning framework that explicitly imitates expert psychological thought\npatterns. By integrating real-world stimuli with structured reasoning guidance,\nour approach enables compact models to internalize social-cognitive principles,\nperform nuanced psychological inference, and support continual\nself-improvement. Comprehensive experiments across multiple benchmarks further\ndemonstrate that our models achieve expert-level interpretive capabilities,\nexhibiting strong out-of-distribution generalization and robust continual\nlearning across diverse, challenging, and psychologically grounded tasks."
                },
                "authors": [
                    {
                        "name": "Feng Yichao"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02452v1",
                "updated": "2025-08-04T14:17:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    17,
                    29,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:17:29Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    17,
                    29,
                    0,
                    216,
                    0
                ],
                "title": "LatentPrompt: Optimizing Promts in Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatentPrompt: Optimizing Promts in Latent Space"
                },
                "summary": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks."
                },
                "authors": [
                    {
                        "name": "Mateusz Bystroski"
                    },
                    {
                        "name": "Grzegorz Piotrowski"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Tomasz Kajdanowicz"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kajdanowicz"
                },
                "author": "Tomasz Kajdanowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12601v3",
                "updated": "2025-08-04T14:11:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    11,
                    45,
                    0,
                    216,
                    0
                ],
                "published": "2024-10-16T14:21:52Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    21,
                    52,
                    2,
                    290,
                    0
                ],
                "title": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization"
                },
                "summary": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning."
                },
                "authors": [
                    {
                        "name": "Yixi Ding"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Yanxia Qin"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for\n  Scientific and Societal Advances",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02442v1",
                "updated": "2025-08-04T14:02:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    2,
                    12,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:02:12Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    2,
                    12,
                    0,
                    216,
                    0
                ],
                "title": "Assessing the Reliability and Validity of Large Language Models for\n  Automated Assessment of Student Essays in Higher Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Reliability and Validity of Large Language Models for\n  Automated Assessment of Student Essays in Higher Education"
                },
                "summary": "This study investigates the reliability and validity of five advanced Large\nLanguage Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral\n24B, for automated essay scoring in a real world higher education context. A\ntotal of 67 Italian-language student essays, written as part of a university\npsychology course, were evaluated using a four-criterion rubric (Pertinence,\nCoherence, Originality, Feasibility). Each model scored all essays across three\nprompt replications to assess intra-model stability. Human-LLM agreement was\nconsistently low and non-significant (Quadratic Weighted Kappa), and\nwithin-model reliability across replications was similarly weak (median\nKendall's W < 0.30). Systematic scoring divergences emerged, including a\ntendency to inflate Coherence and inconsistent handling of context-dependent\ndimensions. Inter-model agreement analysis revealed moderate convergence for\nCoherence and Originality, but negligible concordance for Pertinence and\nFeasibility. Although limited in scope, these findings suggest that current\nLLMs may struggle to replicate human judgment in tasks requiring disciplinary\ninsight and contextual sensitivity. Human oversight remains critical when\nevaluating open-ended academic work, particularly in interpretive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the reliability and validity of five advanced Large\nLanguage Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral\n24B, for automated essay scoring in a real world higher education context. A\ntotal of 67 Italian-language student essays, written as part of a university\npsychology course, were evaluated using a four-criterion rubric (Pertinence,\nCoherence, Originality, Feasibility). Each model scored all essays across three\nprompt replications to assess intra-model stability. Human-LLM agreement was\nconsistently low and non-significant (Quadratic Weighted Kappa), and\nwithin-model reliability across replications was similarly weak (median\nKendall's W < 0.30). Systematic scoring divergences emerged, including a\ntendency to inflate Coherence and inconsistent handling of context-dependent\ndimensions. Inter-model agreement analysis revealed moderate convergence for\nCoherence and Originality, but negligible concordance for Pertinence and\nFeasibility. Although limited in scope, these findings suggest that current\nLLMs may struggle to replicate human judgment in tasks requiring disciplinary\ninsight and contextual sensitivity. Human oversight remains critical when\nevaluating open-ended academic work, particularly in interpretive domains."
                },
                "authors": [
                    {
                        "name": "Andrea Gaggioli"
                    },
                    {
                        "name": "Giuseppe Casaburi"
                    },
                    {
                        "name": "Leonardo Ercolani"
                    },
                    {
                        "name": "Francesco Collova'"
                    },
                    {
                        "name": "Pietro Torre"
                    },
                    {
                        "name": "Fabrizio Davide"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Davide"
                },
                "author": "Fabrizio Davide",
                "arxiv_comment": "24 pages (including appendix), 12 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15822v2",
                "updated": "2025-08-04T13:56:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    56,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-21T17:30:16Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    30,
                    16,
                    0,
                    202,
                    0
                ],
                "title": "Do AI models help produce verified bug fixes?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do AI models help produce verified bug fixes?"
                },
                "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair."
                },
                "authors": [
                    {
                        "name": "Li Huang"
                    },
                    {
                        "name": "Ilgiz Mustafin"
                    },
                    {
                        "name": "Marco Piccioni"
                    },
                    {
                        "name": "Alessandro Schena"
                    },
                    {
                        "name": "Reto Weber"
                    },
                    {
                        "name": "Bertrand Meyer"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Meyer"
                },
                "author": "Bertrand Meyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18937v2",
                "updated": "2025-08-04T13:54:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    54,
                    40,
                    0,
                    216,
                    0
                ],
                "published": "2024-05-29T09:43:48Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    43,
                    48,
                    2,
                    150,
                    0
                ],
                "title": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description"
                },
                "summary": "In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a\nchallenging task aimed at advancing 3D multimodal learning for fine-grained,\npart-aware segmentation grounding and detailed explanation of 3D objects.\nExisting 3D datasets largely focus on either vision-only part segmentation or\nvision-language scene segmentation, lacking the fine-grained multimodal\nsegmentation needed for robotic navigation and interaction in real-world\nenvironments. To address this gap, we present the 3DCoMPaT Grounded\nInstructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich\npoint cloud descriptions with corresponding part-level segmentation masks. This\ndataset encompasses extensive samples designed for both PaPGD and fine-grained\nsingle-part grounding tasks. To tackle the inherent challenges of grounding\nobjects and generating grounded descriptions at the part level, we propose\nKestrel, a part-aware 3D multimodal large language model that integrates an\nadvanced language model for nuanced language comprehension with multi-level\npoint feature propagation and query refinement mechanism to enhance spatial\nreasoning at the part level. The extensive experiments demonstrate that Kestrel\neffectively bridges the gap between part-aware language understanding and 3D\nsegmentation grounding, paving the way for more robust and interpretable 3D\nobject comprehension that meets the demands of real-world robotic applications.\nProject page at https://feielysia.github.io/Kestrel.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a\nchallenging task aimed at advancing 3D multimodal learning for fine-grained,\npart-aware segmentation grounding and detailed explanation of 3D objects.\nExisting 3D datasets largely focus on either vision-only part segmentation or\nvision-language scene segmentation, lacking the fine-grained multimodal\nsegmentation needed for robotic navigation and interaction in real-world\nenvironments. To address this gap, we present the 3DCoMPaT Grounded\nInstructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich\npoint cloud descriptions with corresponding part-level segmentation masks. This\ndataset encompasses extensive samples designed for both PaPGD and fine-grained\nsingle-part grounding tasks. To tackle the inherent challenges of grounding\nobjects and generating grounded descriptions at the part level, we propose\nKestrel, a part-aware 3D multimodal large language model that integrates an\nadvanced language model for nuanced language comprehension with multi-level\npoint feature propagation and query refinement mechanism to enhance spatial\nreasoning at the part level. The extensive experiments demonstrate that Kestrel\neffectively bridges the gap between part-aware language understanding and 3D\nsegmentation grounding, paving the way for more robust and interpretable 3D\nobject comprehension that meets the demands of real-world robotic applications.\nProject page at https://feielysia.github.io/Kestrel.github.io/"
                },
                "authors": [
                    {
                        "name": "Mahmoud Ahmed"
                    },
                    {
                        "name": "Junjie Fei"
                    },
                    {
                        "name": "Jian Ding"
                    },
                    {
                        "name": "Eslam Mohamed Bakr"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Elhoseiny"
                },
                "author": "Mohamed Elhoseiny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02435v1",
                "updated": "2025-08-04T13:50:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    50,
                    44,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:50:44Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    50,
                    44,
                    0,
                    216,
                    0
                ],
                "title": "Beyond Chunks and Graphs: Retrieval-Augmented Generation through\n  Triplet-Driven Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Chunks and Graphs: Retrieval-Augmented Generation through\n  Triplet-Driven Thinking"
                },
                "summary": "Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG"
                },
                "authors": [
                    {
                        "name": "Shengbo Gong"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Wei jin"
                    }
                ],
                "author_detail": {
                    "name": "Wei jin"
                },
                "author": "Wei jin",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02430v1",
                "updated": "2025-08-04T13:49:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    49,
                    30,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:49:30Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    49,
                    30,
                    0,
                    216,
                    0
                ],
                "title": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large\n  Language Model Applications"
                },
                "summary": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation."
                },
                "authors": [
                    {
                        "name": "Robin Nowak"
                    },
                    {
                        "name": "Patrick Figge"
                    },
                    {
                        "name": "Carolin Haeussler"
                    }
                ],
                "author_detail": {
                    "name": "Carolin Haeussler"
                },
                "author": "Carolin Haeussler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02427v1",
                "updated": "2025-08-04T13:48:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    48,
                    32,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:48:32Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    48,
                    32,
                    0,
                    216,
                    0
                ],
                "title": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through\n  Composing Ready-to-Use Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through\n  Composing Ready-to-Use Models"
                },
                "summary": "Composable AI offers a scalable and effective paradigm for tackling complex\nAI tasks by decomposing them into sub-tasks and solving each sub-task using\nready-to-use well-trained models. However, systematically evaluating methods\nunder this setting remains largely unexplored. In this paper, we introduce\nCABENCH, the first public benchmark comprising 70 realistic composable AI\ntasks, along with a curated pool of 700 models across multiple modalities and\ndomains. We also propose an evaluation framework to enable end-to-end\nassessment of composable AI solutions. To establish initial baselines, we\nprovide human-designed reference solutions and compare their performance with\ntwo LLM-based approaches. Our results illustrate the promise of composable AI\nin addressing complex real-world problems while highlighting the need for\nmethods that can fully unlock its potential by automatically generating\neffective execution pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composable AI offers a scalable and effective paradigm for tackling complex\nAI tasks by decomposing them into sub-tasks and solving each sub-task using\nready-to-use well-trained models. However, systematically evaluating methods\nunder this setting remains largely unexplored. In this paper, we introduce\nCABENCH, the first public benchmark comprising 70 realistic composable AI\ntasks, along with a curated pool of 700 models across multiple modalities and\ndomains. We also propose an evaluation framework to enable end-to-end\nassessment of composable AI solutions. To establish initial baselines, we\nprovide human-designed reference solutions and compare their performance with\ntwo LLM-based approaches. Our results illustrate the promise of composable AI\nin addressing complex real-world problems while highlighting the need for\nmethods that can fully unlock its potential by automatically generating\neffective execution pipelines."
                },
                "authors": [
                    {
                        "name": "Tung-Thuy Pham"
                    },
                    {
                        "name": "Duy-Quan Luong"
                    },
                    {
                        "name": "Minh-Quan Duong"
                    },
                    {
                        "name": "Trung-Hieu Nguyen"
                    },
                    {
                        "name": "Thu-Trang Nguyen"
                    },
                    {
                        "name": "Son Nguyen"
                    },
                    {
                        "name": "Hieu Dinh Vo"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Dinh Vo"
                },
                "author": "Hieu Dinh Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.13123v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.13123v2",
                "updated": "2025-08-04T13:46:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    46,
                    44,
                    0,
                    216,
                    0
                ],
                "published": "2025-03-17T12:48:29Z",
                "published_parsed": [
                    2025,
                    3,
                    17,
                    12,
                    48,
                    29,
                    0,
                    76,
                    0
                ],
                "title": "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIXPINN: Mixed-Material Simulations by Physics-Informed Neural Network"
                },
                "summary": "Simulating the complex interactions between soft tissues and rigid anatomy is\ncritical for applications in surgical training, planning, and robotic-assisted\ninterventions. Traditional Finite Element Method (FEM)-based simulations, while\naccurate, are computationally expensive and impractical for real-time\nscenarios. Learning-based approaches have shown promise in accelerating\npredictions but have fallen short in modeling soft-rigid interactions\neffectively. We introduce MIXPINN, a physics-informed Graph Neural Network\n(GNN) framework for mixed-material simulations, explicitly capturing soft-rigid\ninteractions using graph-based augmentations. Our approach integrates Virtual\nNodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint\nsatisfaction while preserving computational efficiency. By leveraging a\ngraph-based representation of biomechanical structures, MIXPINN learns\nhigh-fidelity deformations from FEM-generated data and achieves real-time\ninference with sub-millimeter accuracy. We validate our method in a realistic\nclinical scenario, demonstrating superior performance compared to baseline GNN\nmodels and traditional FEM methods. Our results show that MIXPINN reduces\ncomputational cost by an order of magnitude while maintaining high physical\naccuracy, making it a viable solution for real-time surgical simulation and\nrobotic-assisted procedures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulating the complex interactions between soft tissues and rigid anatomy is\ncritical for applications in surgical training, planning, and robotic-assisted\ninterventions. Traditional Finite Element Method (FEM)-based simulations, while\naccurate, are computationally expensive and impractical for real-time\nscenarios. Learning-based approaches have shown promise in accelerating\npredictions but have fallen short in modeling soft-rigid interactions\neffectively. We introduce MIXPINN, a physics-informed Graph Neural Network\n(GNN) framework for mixed-material simulations, explicitly capturing soft-rigid\ninteractions using graph-based augmentations. Our approach integrates Virtual\nNodes (VNs) and Virtual Edges (VEs) to enhance rigid body constraint\nsatisfaction while preserving computational efficiency. By leveraging a\ngraph-based representation of biomechanical structures, MIXPINN learns\nhigh-fidelity deformations from FEM-generated data and achieves real-time\ninference with sub-millimeter accuracy. We validate our method in a realistic\nclinical scenario, demonstrating superior performance compared to baseline GNN\nmodels and traditional FEM methods. Our results show that MIXPINN reduces\ncomputational cost by an order of magnitude while maintaining high physical\naccuracy, making it a viable solution for real-time surgical simulation and\nrobotic-assisted procedures."
                },
                "authors": [
                    {
                        "name": "Xintian Yuan"
                    },
                    {
                        "name": "Yunke Ao"
                    },
                    {
                        "name": "Boqi Chen"
                    },
                    {
                        "name": "Philipp Fuernstahl"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Fuernstahl"
                },
                "author": "Philipp Fuernstahl",
                "arxiv_comment": "Accepted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.13123v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.13123v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01281v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01281v4",
                "updated": "2025-08-04T13:46:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    46,
                    22,
                    0,
                    216,
                    0
                ],
                "published": "2024-11-02T15:23:28Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    23,
                    28,
                    5,
                    307,
                    0
                ],
                "title": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons"
                },
                "summary": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}"
                },
                "authors": [
                    {
                        "name": "Seonil Son"
                    },
                    {
                        "name": "Ju-Min Oh"
                    },
                    {
                        "name": "Heegon Jin"
                    },
                    {
                        "name": "Cheolhun Jang"
                    },
                    {
                        "name": "Jeongbeom Jeong"
                    },
                    {
                        "name": "Kuntae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kuntae Kim"
                },
                "author": "Kuntae Kim",
                "arxiv_comment": "8 pages for main body, 19 pages in total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01281v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01281v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16792v2",
                "updated": "2025-08-04T13:42:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    42,
                    52,
                    0,
                    216,
                    0
                ],
                "published": "2025-06-20T07:16:47Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    16,
                    47,
                    4,
                    171,
                    0
                ],
                "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning"
                },
                "summary": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks -- methods\ndesigned to elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version -- order-determining optimization. We\nconduct extensive experiments on two datasets using two open-source and four\nclosed-source models. Results show that MIST achieves competitive attack\nsuccess rate, relatively low query count, and fair transferability,\noutperforming or matching state-of-the-art jailbreak methods. Additionally, we\nconduct analysis on computational efficiency to validate the practical\nviability of MIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks -- methods\ndesigned to elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version -- order-determining optimization. We\nconduct extensive experiments on two datasets using two open-source and four\nclosed-source models. Results show that MIST achieves competitive attack\nsuccess rate, relatively low query count, and fair transferability,\noutperforming or matching state-of-the-art jailbreak methods. Additionally, we\nconduct analysis on computational efficiency to validate the practical\nviability of MIST."
                },
                "authors": [
                    {
                        "name": "Muyang Zheng"
                    },
                    {
                        "name": "Yuanzhi Yao"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Caihong Kai"
                    }
                ],
                "author_detail": {
                    "name": "Caihong Kai"
                },
                "author": "Caihong Kai",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05108v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05108v4",
                "updated": "2025-08-04T13:41:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    41,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2025-04-07T14:14:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    14,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning"
                },
                "summary": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non combinatorial optimization tasks demonstrate that integrating RL with\nevolutionary search accelerates the discovery of superior algorithms,\nshowcasing the potential of RL-enhanced evolutionary strategies for algorithm\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non combinatorial optimization tasks demonstrate that integrating RL with\nevolutionary search accelerates the discovery of superior algorithms,\nshowcasing the potential of RL-enhanced evolutionary strategies for algorithm\ndesign."
                },
                "authors": [
                    {
                        "name": "Anja Surina"
                    },
                    {
                        "name": "Amin Mansouri"
                    },
                    {
                        "name": "Lars Quaedvlieg"
                    },
                    {
                        "name": "Amal Seddas"
                    },
                    {
                        "name": "Maryna Viazovska"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05108v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05108v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02419v1",
                "updated": "2025-08-04T13:40:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    40,
                    59,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:40:59Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    40,
                    59,
                    0,
                    216,
                    0
                ],
                "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination\n  via Attention Lens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination\n  via Attention Lens"
                },
                "summary": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy."
                },
                "authors": [
                    {
                        "name": "Haohan Zheng"
                    },
                    {
                        "name": "Zhenguo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguo Zhang"
                },
                "author": "Zhenguo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02404v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02404v1",
                "updated": "2025-08-04T13:28:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    28,
                    12,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:28:12Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    28,
                    12,
                    0,
                    216,
                    0
                ],
                "title": "Robust Simulation Based Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust Simulation Based Inference"
                },
                "summary": "Simulation-Based Inference (SBI) is an approach to statistical inference\nwhere simulations from an assumed model are used to construct estimators and\nconfidence sets. SBI is often used when the likelihood is intractable and to\nconstruct confidence sets that do not rely on asymptotic methods or regularity\nconditions. Traditional SBI methods assume that the model is correct, but, as\nalways, this can lead to invalid inference when the model is misspecified. This\npaper introduces robust methods that allow for valid frequentist inference in\nthe presence of model misspecification. We propose a framework where the target\nof inference is a projection parameter that minimizes a discrepancy between the\ntrue distribution and the assumed model. The method guarantees valid inference,\neven when the model is incorrectly specified and even if the standard\nregularity conditions fail. Alternatively, we introduce model expansion through\nexponential tilting as another way to account for model misspecification. We\nalso develop an SBI based goodness-of-fit test to detect model\nmisspecification. Finally, we propose two ideas that are useful in the SBI\nframework beyond robust inference: an SBI based method to obtain closed form\napproximations of intractable models and an active learning approach to more\nefficiently sample the parameter space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simulation-Based Inference (SBI) is an approach to statistical inference\nwhere simulations from an assumed model are used to construct estimators and\nconfidence sets. SBI is often used when the likelihood is intractable and to\nconstruct confidence sets that do not rely on asymptotic methods or regularity\nconditions. Traditional SBI methods assume that the model is correct, but, as\nalways, this can lead to invalid inference when the model is misspecified. This\npaper introduces robust methods that allow for valid frequentist inference in\nthe presence of model misspecification. We propose a framework where the target\nof inference is a projection parameter that minimizes a discrepancy between the\ntrue distribution and the assumed model. The method guarantees valid inference,\neven when the model is incorrectly specified and even if the standard\nregularity conditions fail. Alternatively, we introduce model expansion through\nexponential tilting as another way to account for model misspecification. We\nalso develop an SBI based goodness-of-fit test to detect model\nmisspecification. Finally, we propose two ideas that are useful in the SBI\nframework beyond robust inference: an SBI based method to obtain closed form\napproximations of intractable models and an active learning approach to more\nefficiently sample the parameter space."
                },
                "authors": [
                    {
                        "name": "Lorenzo Tomaselli"
                    },
                    {
                        "name": "Valrie Ventura"
                    },
                    {
                        "name": "Larry Wasserman"
                    }
                ],
                "author_detail": {
                    "name": "Larry Wasserman"
                },
                "author": "Larry Wasserman",
                "arxiv_comment": "44 pages (29 main text + references, 15 appendix); 12 figures (8 main\n  text, 4 appendix); 5 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02404v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02404v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02391v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02391v1",
                "updated": "2025-08-04T13:17:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    17,
                    49,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:17:49Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    17,
                    49,
                    0,
                    216,
                    0
                ],
                "title": "Inference-time Scaling for Diffusion-based Audio Super-resolution",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inference-time Scaling for Diffusion-based Audio Super-resolution"
                },
                "summary": "Diffusion models have demonstrated remarkable success in generative tasks,\nincluding audio super-resolution (SR). In many applications like movie\npost-production and album mastering, substantial computational budgets are\navailable for achieving superior audio quality. However, while existing\ndiffusion approaches typically increase sampling steps to improve quality, the\nperformance remains fundamentally limited by the stochastic nature of the\nsampling process, leading to high-variance and quality-limited outputs. Here,\nrather than simply increasing the number of sampling steps, we propose a\ndifferent paradigm through inference-time scaling for SR, which explores\nmultiple solution trajectories during the sampling process. Different\ntask-specific verifiers are developed, and two search algorithms, including the\nrandom search and zero-order search for SR, are introduced. By actively guiding\nthe exploration of the high-dimensional solution space through\nverifier-algorithm combinations, we enable more robust and higher-quality\noutputs. Through extensive validation across diverse audio domains (speech,\nmusic, sound effects) and frequency ranges, we demonstrate consistent\nperformance gains, achieving improvements of up to 9.70% in aesthetics, 5.88%\nin speaker similarity, 15.20% in word error rate, and 46.98% in spectral\ndistance for speech SR from 4kHz to 24kHz, showcasing the effectiveness of our\napproach. Audio samples are available at:\nhttps://racerk.github.io/tt-scale-audiosr/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion models have demonstrated remarkable success in generative tasks,\nincluding audio super-resolution (SR). In many applications like movie\npost-production and album mastering, substantial computational budgets are\navailable for achieving superior audio quality. However, while existing\ndiffusion approaches typically increase sampling steps to improve quality, the\nperformance remains fundamentally limited by the stochastic nature of the\nsampling process, leading to high-variance and quality-limited outputs. Here,\nrather than simply increasing the number of sampling steps, we propose a\ndifferent paradigm through inference-time scaling for SR, which explores\nmultiple solution trajectories during the sampling process. Different\ntask-specific verifiers are developed, and two search algorithms, including the\nrandom search and zero-order search for SR, are introduced. By actively guiding\nthe exploration of the high-dimensional solution space through\nverifier-algorithm combinations, we enable more robust and higher-quality\noutputs. Through extensive validation across diverse audio domains (speech,\nmusic, sound effects) and frequency ranges, we demonstrate consistent\nperformance gains, achieving improvements of up to 9.70% in aesthetics, 5.88%\nin speaker similarity, 15.20% in word error rate, and 46.98% in spectral\ndistance for speech SR from 4kHz to 24kHz, showcasing the effectiveness of our\napproach. Audio samples are available at:\nhttps://racerk.github.io/tt-scale-audiosr/."
                },
                "authors": [
                    {
                        "name": "Yizhu Jin"
                    },
                    {
                        "name": "Zhen Ye"
                    },
                    {
                        "name": "Zeyue Tian"
                    },
                    {
                        "name": "Haohe Liu"
                    },
                    {
                        "name": "Qiuqiang Kong"
                    },
                    {
                        "name": "Yike Guo"
                    },
                    {
                        "name": "Wei Xue"
                    }
                ],
                "author_detail": {
                    "name": "Wei Xue"
                },
                "author": "Wei Xue",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02391v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02391v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SD",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SD",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.11130v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.11130v2",
                "updated": "2025-08-04T13:14:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    14,
                    13,
                    0,
                    216,
                    0
                ],
                "published": "2024-06-17T01:21:28Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    1,
                    21,
                    28,
                    0,
                    169,
                    0
                ],
                "title": "Dynamic Order Template Prediction for Generative Aspect-Based Sentiment\n  Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dynamic Order Template Prediction for Generative Aspect-Based Sentiment\n  Analysis"
                },
                "summary": "Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific\naspects within texts, resulting in detailed sentiment tuples. Previous ABSA\nmodels often use static templates to predict all of the elements in the tuples,\nand these models often fail to accurately capture dependencies between\nelements. Multi-view prompting method improves the performance of ABSA by\npredicting tuples with various templates and then ensembling the results.\nHowever, this method suffers from inefficiencies and out-of-distribution\nerrors. In this paper, we propose a Dynamic Order Template (DOT) method for\nABSA, which dynamically generates necessary views for each instance based on\ninstance-level entropy. Ensuring the diverse and relevant view generation, our\nproposed method improves F1-scores on ASQP and ACOS datasets while\nsignificantly reducing inference time.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aspect-based sentiment analysis (ABSA) assesses sentiments towards specific\naspects within texts, resulting in detailed sentiment tuples. Previous ABSA\nmodels often use static templates to predict all of the elements in the tuples,\nand these models often fail to accurately capture dependencies between\nelements. Multi-view prompting method improves the performance of ABSA by\npredicting tuples with various templates and then ensembling the results.\nHowever, this method suffers from inefficiencies and out-of-distribution\nerrors. In this paper, we propose a Dynamic Order Template (DOT) method for\nABSA, which dynamically generates necessary views for each instance based on\ninstance-level entropy. Ensuring the diverse and relevant view generation, our\nproposed method improves F1-scores on ASQP and ACOS datasets while\nsignificantly reducing inference time."
                },
                "authors": [
                    {
                        "name": "Yonghyun Jun"
                    },
                    {
                        "name": "Hwanhee Lee"
                    }
                ],
                "author_detail": {
                    "name": "Hwanhee Lee"
                },
                "author": "Hwanhee Lee",
                "arxiv_comment": "ACL 2025 Main",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.11130v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.11130v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.09607v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.09607v3",
                "updated": "2025-08-04T13:08:58Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    8,
                    58,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-13T12:24:02Z",
                "published_parsed": [
                    2025,
                    7,
                    13,
                    12,
                    24,
                    2,
                    6,
                    194,
                    0
                ],
                "title": "Efficient Private Inference Based on Helper-Assisted Malicious Security\n  Dishonest Majority MPC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Private Inference Based on Helper-Assisted Malicious Security\n  Dishonest Majority MPC"
                },
                "summary": "The existing MPC-based private inference frameworks either rely on\nimpractical real-world assumptions, or adopt the strongest security model\n(Malicious Security Dishonest Majority, MSDM) and then suffer from severe\nefficiency limitations. To balance security and efficiency, we propose a novel,\nthree-layer private inference framework based on the Helper-Assisted MSDM\n(HA-MSDM) model. The first is the primitive layer, where we extend computations\nfrom prime fields to rings for efficient fixed-point arithmetic and then better\nsupport inference operations. The second is the MPC layer, where we design six\nfixed-round MPC protocols to reduce latency for core operations like\nmultiplication, polynomial evaluation, and batch check. The third is the\ninference layer, which can achieve efficient and high-accuracy CNN inference.\nThe efficiency is achieved by applying our designed MPC protocols. The\nhigh-accuracy private inference in deep CNNs is achieved by designing a\nco-optimized strategy, which employs high-precision polynomial approximation\nfor activation functions and uses parameter-adjusted Batch Normalization layers\nto constrain inputs. Benchmarks on LeNet and AlexNet show our framework\nachieves up to a 2.4-25.7x speedup in LAN and a 1.3-9.5x acceleration in WAN\nover the state-of-the-art MSDM frameworks with only 0.04-1.08% relative error.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The existing MPC-based private inference frameworks either rely on\nimpractical real-world assumptions, or adopt the strongest security model\n(Malicious Security Dishonest Majority, MSDM) and then suffer from severe\nefficiency limitations. To balance security and efficiency, we propose a novel,\nthree-layer private inference framework based on the Helper-Assisted MSDM\n(HA-MSDM) model. The first is the primitive layer, where we extend computations\nfrom prime fields to rings for efficient fixed-point arithmetic and then better\nsupport inference operations. The second is the MPC layer, where we design six\nfixed-round MPC protocols to reduce latency for core operations like\nmultiplication, polynomial evaluation, and batch check. The third is the\ninference layer, which can achieve efficient and high-accuracy CNN inference.\nThe efficiency is achieved by applying our designed MPC protocols. The\nhigh-accuracy private inference in deep CNNs is achieved by designing a\nco-optimized strategy, which employs high-precision polynomial approximation\nfor activation functions and uses parameter-adjusted Batch Normalization layers\nto constrain inputs. Benchmarks on LeNet and AlexNet show our framework\nachieves up to a 2.4-25.7x speedup in LAN and a 1.3-9.5x acceleration in WAN\nover the state-of-the-art MSDM frameworks with only 0.04-1.08% relative error."
                },
                "authors": [
                    {
                        "name": "Kaiwen Wang"
                    },
                    {
                        "name": "Xiaolin Chang"
                    },
                    {
                        "name": "Junchao Fan"
                    },
                    {
                        "name": "Yuehan Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuehan Dong"
                },
                "author": "Yuehan Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.09607v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.09607v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02381v1",
                "updated": "2025-08-04T13:08:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    8,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:08:35Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    8,
                    35,
                    0,
                    216,
                    0
                ],
                "title": "Beyond Manually Designed Pruning Policies with Second-Level Performance\n  Prediction: A Pruning Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Manually Designed Pruning Policies with Second-Level Performance\n  Prediction: A Pruning Framework for LLMs"
                },
                "summary": "Non-uniform structured network pruning methods can effectively reduce Large\nLanguage Model (LLM) size by eliminating redundant channels or layers, offering\nlower performance degradation than uniform strategies. However, existing\nnon-uniform methods rely heavily on manually designed pruning policies (e.g.,\nlayer importance and scaling factors), and therefore cannot efficiently adapt\nto scenarios with dynamic pruning ratio requirements. Additionly, a critical\nbottleneck -- the time-consuming evaluation of pruning policies -- further\nlimits the feasibility of iteratively and dynamically finding optimal pruning\npolicies. To address these limitations, we propose PPF (Predictive Pruning\nFramework), a novel pruning framework for LLMs that eliminates manual design\ndependencies via second-level performance prediction. PPF not only supports\nreal-time pruning decisions under dynamic pruning ratios but is also applicable\nto static pruning scenarios. It employs an agent for producing adaptive and\nreal-time pruning actions, while a lightweight performance predictor that can\nevaluate a pruning policy in seconds, significantly speeding up the iterative\noptimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can\ngenerate dynamic/static pruning policies and it reduces perplexity by up to\n33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,\noutperforming manually designed pruning policies. The performance predictor\nachieves second-level performance prediction with high accuracy (prediction\nerror < 0.0011). It reduces the mean evaluation latency from minute-level (1\nminute and 38.02 seconds of test-set evaluation methods) to second-level (1.52\nsecond), achieving over 64 times speedup. Our code will be available at\nhttps://github.com/Ma-zx/PPF .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-uniform structured network pruning methods can effectively reduce Large\nLanguage Model (LLM) size by eliminating redundant channels or layers, offering\nlower performance degradation than uniform strategies. However, existing\nnon-uniform methods rely heavily on manually designed pruning policies (e.g.,\nlayer importance and scaling factors), and therefore cannot efficiently adapt\nto scenarios with dynamic pruning ratio requirements. Additionly, a critical\nbottleneck -- the time-consuming evaluation of pruning policies -- further\nlimits the feasibility of iteratively and dynamically finding optimal pruning\npolicies. To address these limitations, we propose PPF (Predictive Pruning\nFramework), a novel pruning framework for LLMs that eliminates manual design\ndependencies via second-level performance prediction. PPF not only supports\nreal-time pruning decisions under dynamic pruning ratios but is also applicable\nto static pruning scenarios. It employs an agent for producing adaptive and\nreal-time pruning actions, while a lightweight performance predictor that can\nevaluate a pruning policy in seconds, significantly speeding up the iterative\noptimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can\ngenerate dynamic/static pruning policies and it reduces perplexity by up to\n33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,\noutperforming manually designed pruning policies. The performance predictor\nachieves second-level performance prediction with high accuracy (prediction\nerror < 0.0011). It reduces the mean evaluation latency from minute-level (1\nminute and 38.02 seconds of test-set evaluation methods) to second-level (1.52\nsecond), achieving over 64 times speedup. Our code will be available at\nhttps://github.com/Ma-zx/PPF ."
                },
                "authors": [
                    {
                        "name": "Zuxin Ma"
                    },
                    {
                        "name": "Yunhe Cui"
                    },
                    {
                        "name": "Yongbin Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Qin"
                },
                "author": "Yongbin Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20957v2",
                "updated": "2025-08-04T13:06:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    6,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-28T16:09:38Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    9,
                    38,
                    0,
                    209,
                    0
                ],
                "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis"
                },
                "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract the latent preferences of models and\nmeasure their persistence. Focusing on sector, size, and momentum, our analysis\nreveals distinct, model-specific tendencies. In particular, we observe a\nconsistent preference for large-cap stocks and contrarian strategies across\nmost models. These preferences often harden into confirmation bias, with models\nclinging to initial judgments despite counter-evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract the latent preferences of models and\nmeasure their persistence. Focusing on sector, size, and momentum, our analysis\nreveals distinct, model-specific tendencies. In particular, we observe a\nconsistent preference for large-cap stocks and contrarian strategies across\nmost models. These preferences often harden into confirmation bias, with models\nclinging to initial judgments despite counter-evidence."
                },
                "authors": [
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Junhyuk Seo"
                    },
                    {
                        "name": "Suhwan Park"
                    },
                    {
                        "name": "Junhyeong Lee"
                    },
                    {
                        "name": "Wonbin Ahn"
                    },
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05386v2",
                "updated": "2025-08-04T12:55:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    55,
                    0,
                    0,
                    216,
                    0
                ],
                "published": "2025-06-03T12:59:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    59,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for\n  Clinical Notes"
                },
                "summary": "Clinical note generation aims to produce free-text summaries of a patient's\ncondition and diagnostic process, with discharge instructions being a\nrepresentative long-form example. While recent LLM-based methods pre-trained on\ngeneral clinical corpora show promise in clinical text generation, they fall\nshort in producing long-form notes from limited patient information. In this\npaper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG)\nfor long-form discharge instructions based on pre-admission information.\nReinRAG retrieves reasoning paths from a medical knowledge graph to provide\nexplicit semantic guidance to the LLM. To bridge the information gap, we\npropose group-based retriever optimization (GRO) which improves retrieval\nquality with group-normalized rewards, encouraging reasoning leaps for deeper\ninference by the LLM. Comprehensive experiments on the real-world dataset show\nthat ReinRAG outperforms baselines in both clinical efficacy and natural\nlanguage generation metrics. Further analysis reveals that ReinRAG fills\nsemantic gaps in sparse input scenarios, and retrieved reasoning paths help\nLLMs avoid clinical misinterpretation by focusing on key evidence and following\ncoherent reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical note generation aims to produce free-text summaries of a patient's\ncondition and diagnostic process, with discharge instructions being a\nrepresentative long-form example. While recent LLM-based methods pre-trained on\ngeneral clinical corpora show promise in clinical text generation, they fall\nshort in producing long-form notes from limited patient information. In this\npaper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG)\nfor long-form discharge instructions based on pre-admission information.\nReinRAG retrieves reasoning paths from a medical knowledge graph to provide\nexplicit semantic guidance to the LLM. To bridge the information gap, we\npropose group-based retriever optimization (GRO) which improves retrieval\nquality with group-normalized rewards, encouraging reasoning leaps for deeper\ninference by the LLM. Comprehensive experiments on the real-world dataset show\nthat ReinRAG outperforms baselines in both clinical efficacy and natural\nlanguage generation metrics. Further analysis reveals that ReinRAG fills\nsemantic gaps in sparse input scenarios, and retrieved reasoning paths help\nLLMs avoid clinical misinterpretation by focusing on key evidence and following\ncoherent reasoning."
                },
                "authors": [
                    {
                        "name": "Lo Pang-Yun Ting"
                    },
                    {
                        "name": "Chengshuai Zhao"
                    },
                    {
                        "name": "Yu-Hua Zeng"
                    },
                    {
                        "name": "Yuan Jee Lim"
                    },
                    {
                        "name": "Kun-Ta Chuang"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02366v1",
                "updated": "2025-08-04T12:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    52,
                    11,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:52:11Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    52,
                    11,
                    0,
                    216,
                    0
                ],
                "title": "Language Model Guided Reinforcement Learning in Quantitative Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Guided Reinforcement Learning in Quantitative Trading"
                },
                "summary": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL."
                },
                "authors": [
                    {
                        "name": "Adam Darmanin"
                    },
                    {
                        "name": "Vince Vella"
                    }
                ],
                "author_detail": {
                    "name": "Vince Vella"
                },
                "author": "Vince Vella",
                "arxiv_comment": "12 pages (4 pages appendix and references), 6 figures, preprint under\n  review for FLLM 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15113v2",
                "updated": "2025-08-04T12:51:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    51,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-19T17:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    55,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture"
                },
                "summary": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million and 1\nbillion parameters, focusing on attention head values, with results also\nindicating improved performance at these larger and more naturalistic scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million and 1\nbillion parameters, focusing on attention head values, with results also\nindicating improved performance at these larger and more naturalistic scales."
                },
                "authors": [
                    {
                        "name": "Thomas F Burns"
                    },
                    {
                        "name": "Tomoki Fukai"
                    },
                    {
                        "name": "Christopher J Earls"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J Earls"
                },
                "author": "Christopher J Earls",
                "arxiv_comment": "35 pages, 14 figures, 6 tables; accepted and published in TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B20, 68T01, 68T37, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5; I.7; J.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13626v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13626v2",
                "updated": "2025-08-04T12:34:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    34,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-04-18T11:07:19Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    11,
                    7,
                    19,
                    4,
                    108,
                    0
                ],
                "title": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Thought Manipulation: External Thought Can Be Efficient for Large\n  Reasoning Models"
                },
                "summary": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities on various tasks. However, LRMs often suffer from an\n``overthinking'' problem, where the model generates excessively redundant\nreasoning steps with limited performance gains. In this work, we empirically\nreveal an important characteristic of LRM behaviors that placing external CoTs\ngenerated by smaller models between the thinking token (\\texttt{<think>} and\n\\texttt{</think>}) can effectively manipulate the model to generate fewer\nthoughts. Building on this finding, we propose a simple yet efficient pipeline,\n\\Method, to enable LRMs to bypass unnecessary intermediate steps, thereby\nsignificantly reducing computational costs. We conduct extensive experiments to\nevaluate the utility and efficiency of \\Method. For instance, when applied to\nQwQ-32B on the LiveBench/Code dataset, \\Method keeps the original performance\nwhile reducing output token counts by approximately 30\\%, with minimal overhead\nintroduced by the CoT generator. Furthermore, we identify two suboptimal modes,\nblindly following flawed external thoughts and unnecessary rethinking, and show\nthat simple mitigations, such as difficulty-aware fallbacks, can further\nimprove performance. Overall, \\Method offers a practical, general, and\nefficient way to optimize LRM inference, making powerful reasoning models more\naccessible and scalable for real-world applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in large reasoning models (LRMs) have demonstrated the\neffectiveness of scaling test-time computation to enhance reasoning\ncapabilities on various tasks. However, LRMs often suffer from an\n``overthinking'' problem, where the model generates excessively redundant\nreasoning steps with limited performance gains. In this work, we empirically\nreveal an important characteristic of LRM behaviors that placing external CoTs\ngenerated by smaller models between the thinking token (\\texttt{<think>} and\n\\texttt{</think>}) can effectively manipulate the model to generate fewer\nthoughts. Building on this finding, we propose a simple yet efficient pipeline,\n\\Method, to enable LRMs to bypass unnecessary intermediate steps, thereby\nsignificantly reducing computational costs. We conduct extensive experiments to\nevaluate the utility and efficiency of \\Method. For instance, when applied to\nQwQ-32B on the LiveBench/Code dataset, \\Method keeps the original performance\nwhile reducing output token counts by approximately 30\\%, with minimal overhead\nintroduced by the CoT generator. Furthermore, we identify two suboptimal modes,\nblindly following flawed external thoughts and unnecessary rethinking, and show\nthat simple mitigations, such as difficulty-aware fallbacks, can further\nimprove performance. Overall, \\Method offers a practical, general, and\nefficient way to optimize LRM inference, making powerful reasoning models more\naccessible and scalable for real-world applications."
                },
                "authors": [
                    {
                        "name": "Yule Liu"
                    },
                    {
                        "name": "Jingyi Zheng"
                    },
                    {
                        "name": "Zhen Sun"
                    },
                    {
                        "name": "Zifan Peng"
                    },
                    {
                        "name": "Wenhan Dong"
                    },
                    {
                        "name": "Zeyang Sha"
                    },
                    {
                        "name": "Shiwen Cui"
                    },
                    {
                        "name": "Weiqiang Wang"
                    },
                    {
                        "name": "Xinlei He"
                    }
                ],
                "author_detail": {
                    "name": "Xinlei He"
                },
                "author": "Xinlei He",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13626v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13626v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02348v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02348v1",
                "updated": "2025-08-04T12:31:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    31,
                    11,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:31:11Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    31,
                    11,
                    0,
                    216,
                    0
                ],
                "title": "mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at\n  T-Junctions Utilizing Road Layout Extraction via Camera",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "mmWave Radar-Based Non-Line-of-Sight Pedestrian Localization at\n  T-Junctions Utilizing Road Layout Extraction via Camera"
                },
                "summary": "Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban\nenvironments poses a significant challenge for autonomous driving systems.\nWhile mmWave radar has demonstrated potential for detecting objects in such\nscenarios, the 2D radar point cloud (PCD) data is susceptible to distortions\ncaused by multipath reflections, making accurate spatial inference difficult.\nAdditionally, although camera images provide high-resolution visual\ninformation, they lack depth perception and cannot directly observe objects in\nNLoS regions. In this paper, we propose a novel framework that interprets radar\nPCD through road layout inferred from camera for localization of NLoS\npedestrians. The proposed method leverages visual information from the camera\nto interpret 2D radar PCD, enabling spatial scene reconstruction. The\neffectiveness of the proposed approach is validated through experiments\nconducted using a radar-camera system mounted on a real vehicle. The\nlocalization performance is evaluated using a dataset collected in outdoor NLoS\ndriving environments, demonstrating the practical applicability of the method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pedestrians Localization in Non-Line-of-Sight (NLoS) regions within urban\nenvironments poses a significant challenge for autonomous driving systems.\nWhile mmWave radar has demonstrated potential for detecting objects in such\nscenarios, the 2D radar point cloud (PCD) data is susceptible to distortions\ncaused by multipath reflections, making accurate spatial inference difficult.\nAdditionally, although camera images provide high-resolution visual\ninformation, they lack depth perception and cannot directly observe objects in\nNLoS regions. In this paper, we propose a novel framework that interprets radar\nPCD through road layout inferred from camera for localization of NLoS\npedestrians. The proposed method leverages visual information from the camera\nto interpret 2D radar PCD, enabling spatial scene reconstruction. The\neffectiveness of the proposed approach is validated through experiments\nconducted using a radar-camera system mounted on a real vehicle. The\nlocalization performance is evaluated using a dataset collected in outdoor NLoS\ndriving environments, demonstrating the practical applicability of the method."
                },
                "authors": [
                    {
                        "name": "Byeonggyu Park"
                    },
                    {
                        "name": "Hee-Yeun Kim"
                    },
                    {
                        "name": "Byonghyok Choi"
                    },
                    {
                        "name": "Hansang Cho"
                    },
                    {
                        "name": "Byungkwan Kim"
                    },
                    {
                        "name": "Soomok Lee"
                    },
                    {
                        "name": "Mingu Jeon"
                    },
                    {
                        "name": "Seong-Woo Kim"
                    }
                ],
                "author_detail": {
                    "name": "Seong-Woo Kim"
                },
                "author": "Seong-Woo Kim",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02348v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02348v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02344v1",
                "updated": "2025-08-04T12:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    25,
                    19,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    25,
                    19,
                    0,
                    216,
                    0
                ],
                "title": "Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal\n  Control Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal\n  Control Systems"
                },
                "summary": "Traffic signal control (TSC) is vital for mitigating congestion and\nsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation\nmodel with human-like reasoning for TSC systems. Our model is developed through\nself-exploration and iteration of reinforced large language models (LLMs) with\nexpert guidance in a simulated traffic environment. Compared to traditional\nreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers\nthree significant advantages. First, Traffic-R1 delivers zero-shot\ngeneralisation, transferring unchanged to new road networks and\nout-of-distribution incidents by utilizing its internal traffic control\npolicies and human-like reasoning. Second, its 3B-parameter architecture is\nlightweight enough for real-time inference on mobile-class chips, enabling\nlarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSC\nprocess and facilitates multi-intersection communication through its\nself-iteration and a new synchronous communication network. Extensive\nbenchmarks demonstrate that Traffic-R1 sets a new state of the art,\noutperforming strong baselines and training-intensive RL controllers. In\npractice, the model now manages signals for more than 55,000 drivers daily,\nshortening average queues by over 5% and halving operator workload. Our\ncheckpoint is available at https://huggingface.co/Season998/Traffic-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic signal control (TSC) is vital for mitigating congestion and\nsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation\nmodel with human-like reasoning for TSC systems. Our model is developed through\nself-exploration and iteration of reinforced large language models (LLMs) with\nexpert guidance in a simulated traffic environment. Compared to traditional\nreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers\nthree significant advantages. First, Traffic-R1 delivers zero-shot\ngeneralisation, transferring unchanged to new road networks and\nout-of-distribution incidents by utilizing its internal traffic control\npolicies and human-like reasoning. Second, its 3B-parameter architecture is\nlightweight enough for real-time inference on mobile-class chips, enabling\nlarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSC\nprocess and facilitates multi-intersection communication through its\nself-iteration and a new synchronous communication network. Extensive\nbenchmarks demonstrate that Traffic-R1 sets a new state of the art,\noutperforming strong baselines and training-intensive RL controllers. In\npractice, the model now manages signals for more than 55,000 drivers daily,\nshortening average queues by over 5% and halving operator workload. Our\ncheckpoint is available at https://huggingface.co/Season998/Traffic-R1."
                },
                "authors": [
                    {
                        "name": "Xingchen Zou"
                    },
                    {
                        "name": "Yuhao Yang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xixuan Hao"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02343v1",
                "updated": "2025-08-04T12:22:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    22,
                    39,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:22:39Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    22,
                    39,
                    0,
                    216,
                    0
                ],
                "title": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling\n  Formats for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling\n  Formats for Large Language Models"
                },
                "summary": "Quantization significantly accelerates inference in large language models\n(LLMs) by replacing original high-precision matrices with low-precision\ncounterparts. Recent advances in weight-activation quantization have primarily\nfocused on mapping both weights and activations to the INT4 format. Although\nthe new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x\nspeedup over FP16, existing INT4-based kernels fail to fully exploit this\ncapability due to mismatched data formats. To bridge this gap, we propose\nMicroMix, a co-designed mixed-precision quantization algorithm and matrix\nmultiplication kernel based on Microscaling (MX) data formats. Tailored for the\nBlackwell architecture, the MicroMix kernel supports arbitrary combinations of\nMXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a\nfavorable trade-off between accuracy and efficiency for each linear layer, we\nintroduce quantization thresholds that identify activation elements where\nlower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.\nOur algorithm selectively allocates higher-precision channels to preserve\naccuracy while maintaining compute efficiency. MicroMix achieves competitive or\nsuperior performance across diverse downstream tasks, including zero-shot and\nfew-shot learning, language modeling, code generation, and mathematical\nreasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX\n5090) GPUs, our kernel delivers at least 20% faster execution than\nTensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,\nMicroMix consistently improves prefill latency and memory efficiency across a\nrange of batch sizes compared to TensorRT baselines. Our code is available at\nhttps://github.com/lwy2020/MicroMix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization significantly accelerates inference in large language models\n(LLMs) by replacing original high-precision matrices with low-precision\ncounterparts. Recent advances in weight-activation quantization have primarily\nfocused on mapping both weights and activations to the INT4 format. Although\nthe new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x\nspeedup over FP16, existing INT4-based kernels fail to fully exploit this\ncapability due to mismatched data formats. To bridge this gap, we propose\nMicroMix, a co-designed mixed-precision quantization algorithm and matrix\nmultiplication kernel based on Microscaling (MX) data formats. Tailored for the\nBlackwell architecture, the MicroMix kernel supports arbitrary combinations of\nMXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a\nfavorable trade-off between accuracy and efficiency for each linear layer, we\nintroduce quantization thresholds that identify activation elements where\nlower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.\nOur algorithm selectively allocates higher-precision channels to preserve\naccuracy while maintaining compute efficiency. MicroMix achieves competitive or\nsuperior performance across diverse downstream tasks, including zero-shot and\nfew-shot learning, language modeling, code generation, and mathematical\nreasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX\n5090) GPUs, our kernel delivers at least 20% faster execution than\nTensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,\nMicroMix consistently improves prefill latency and memory efficiency across a\nrange of batch sizes compared to TensorRT baselines. Our code is available at\nhttps://github.com/lwy2020/MicroMix."
                },
                "authors": [
                    {
                        "name": "Wenyuan Liu"
                    },
                    {
                        "name": "Haoqian Meng"
                    },
                    {
                        "name": "Yilun Luo"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Xindian Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xindian Ma"
                },
                "author": "Xindian Ma",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02342v1",
                "updated": "2025-08-04T12:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    22,
                    25,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    22,
                    25,
                    0,
                    216,
                    0
                ],
                "title": "Agentic Personalized Fashion Recommendation in the Age of Generative AI:\n  Challenges, Opportunities, and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Personalized Fashion Recommendation in the Age of Generative AI:\n  Challenges, Opportunities, and Evaluation"
                },
                "summary": "Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands."
                },
                "authors": [
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Nima Rafiee"
                    },
                    {
                        "name": "Mahdyar Ravanbakhsh"
                    }
                ],
                "author_detail": {
                    "name": "Mahdyar Ravanbakhsh"
                },
                "author": "Mahdyar Ravanbakhsh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02337v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02337v1",
                "updated": "2025-08-04T12:20:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    20,
                    17,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:20:17Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    20,
                    17,
                    0,
                    216,
                    0
                ],
                "title": "Posterior Sampling of Probabilistic Word Embeddings",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Posterior Sampling of Probabilistic Word Embeddings"
                },
                "summary": "Quantifying uncertainty in word embeddings is crucial for reliable inference\nfrom textual data. However, existing Bayesian methods such as Hamiltonian Monte\nCarlo (HMC) and mean-field variational inference (MFVI) are either\ncomputationally infeasible for large data or rely on restrictive assumptions.\n  We propose a scalable Gibbs sampler using Polya-Gamma augmentation as well as\nLaplace approximation and compare them with MFVI and HMC for word embeddings.\nIn addition, we address non-identifiability in word embeddings. Our Gibbs\nsampler and HMC correctly estimate uncertainties, while MFVI does not, and\nLaplace approximation only does so on large sample sizes, as expected. Applying\nthe Gibbs sampler to the US Congress and the Movielens datasets, we demonstrate\nthe feasibility on larger real data. Finally, as a result of having draws from\nthe full posterior, we show that the posterior mean of word embeddings improves\nover maximum a posteriori (MAP) estimates in terms of hold-out likelihood,\nespecially for smaller sampling sizes, further strengthening the need for\nposterior sampling of word embeddings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying uncertainty in word embeddings is crucial for reliable inference\nfrom textual data. However, existing Bayesian methods such as Hamiltonian Monte\nCarlo (HMC) and mean-field variational inference (MFVI) are either\ncomputationally infeasible for large data or rely on restrictive assumptions.\n  We propose a scalable Gibbs sampler using Polya-Gamma augmentation as well as\nLaplace approximation and compare them with MFVI and HMC for word embeddings.\nIn addition, we address non-identifiability in word embeddings. Our Gibbs\nsampler and HMC correctly estimate uncertainties, while MFVI does not, and\nLaplace approximation only does so on large sample sizes, as expected. Applying\nthe Gibbs sampler to the US Congress and the Movielens datasets, we demonstrate\nthe feasibility on larger real data. Finally, as a result of having draws from\nthe full posterior, we show that the posterior mean of word embeddings improves\nover maximum a posteriori (MAP) estimates in terms of hold-out likelihood,\nespecially for smaller sampling sizes, further strengthening the need for\nposterior sampling of word embeddings."
                },
                "authors": [
                    {
                        "name": "Vin Yrjninen"
                    },
                    {
                        "name": "Isac Bostrm"
                    },
                    {
                        "name": "Mns Magnusson"
                    },
                    {
                        "name": "Johan Jonasson"
                    }
                ],
                "author_detail": {
                    "name": "Johan Jonasson"
                },
                "author": "Johan Jonasson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02337v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02337v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19214v2",
                "updated": "2025-08-04T12:11:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    11,
                    37,
                    0,
                    216,
                    0
                ],
                "published": "2025-02-26T15:15:01Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    15,
                    1,
                    2,
                    57,
                    0
                ],
                "title": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation"
                },
                "summary": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP)."
                },
                "authors": [
                    {
                        "name": "Anthony M. Smaldone"
                    },
                    {
                        "name": "Yu Shee"
                    },
                    {
                        "name": "Gregory W. Kyro"
                    },
                    {
                        "name": "Marwa H. Farag"
                    },
                    {
                        "name": "Zohim Chandani"
                    },
                    {
                        "name": "Elica Kyoseva"
                    },
                    {
                        "name": "Victor S. Batista"
                    }
                ],
                "author_detail": {
                    "name": "Victor S. Batista"
                },
                "author": "Victor S. Batista",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02322v1",
                "updated": "2025-08-04T11:42:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    42,
                    48,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:42:48Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    42,
                    48,
                    0,
                    216,
                    0
                ],
                "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis"
                },
                "summary": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuanchi Zhang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02317v2",
                "updated": "2025-08-05T03:34:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    3,
                    34,
                    20,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T11:33:04Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    33,
                    4,
                    0,
                    216,
                    0
                ],
                "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo"
                },
                "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs."
                },
                "authors": [
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Yaowei Zheng"
                    },
                    {
                        "name": "Zhelun Shi"
                    },
                    {
                        "name": "Zhongkai Zhao"
                    },
                    {
                        "name": "Bin Jia"
                    },
                    {
                        "name": "Ziyue Huang"
                    },
                    {
                        "name": "Zhiqi Lin"
                    },
                    {
                        "name": "Youjie Li"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.02724v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.02724v3",
                "updated": "2025-08-04T11:32:04Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    32,
                    4,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-03T15:41:04Z",
                "published_parsed": [
                    2025,
                    7,
                    3,
                    15,
                    41,
                    4,
                    3,
                    184,
                    0
                ],
                "title": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Multi-Label Contrastive Learning for Protein-Protein\n  Interaction Prediction Across Organisms"
                },
                "summary": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in AI for science have highlighted the power of contrastive\nlearning in bridging heterogeneous biological data modalities. Building on this\nparadigm, we propose HIPPO (HIerarchical Protein-Protein interaction prediction\nacross Organisms), a hierarchical contrastive framework for protein-protein\ninteraction(PPI) prediction, where protein sequences and their hierarchical\nattributes are aligned through multi-tiered biological representation matching.\nThe proposed approach incorporates hierarchical contrastive loss functions that\nemulate the structured relationship among functional classes of proteins. The\nframework adaptively incorporates domain and family knowledge through a\ndata-driven penalty mechanism, enforcing consistency between the learned\nembedding space and the intrinsic hierarchy of protein functions. Experiments\non benchmark datasets demonstrate that HIPPO achieves state-of-the-art\nperformance, outperforming existing methods and showing robustness in low-data\nregimes. Notably, the model demonstrates strong zero-shot transferability to\nother species without retraining, enabling reliable PPI prediction and\nfunctional inference even in less characterized or rare organisms where\nexperimental data are limited. Further analysis reveals that hierarchical\nfeature fusion is critical for capturing conserved interaction determinants,\nsuch as binding motifs and functional annotations. This work advances\ncross-species PPI prediction and provides a unified framework for interaction\nprediction in scenarios with sparse or imbalanced multi-species data."
                },
                "authors": [
                    {
                        "name": "Shiyi Liu"
                    },
                    {
                        "name": "Buwen Liang"
                    },
                    {
                        "name": "Yuetong Fang"
                    },
                    {
                        "name": "Zixuan Jiang"
                    },
                    {
                        "name": "Renjing Xu"
                    }
                ],
                "author_detail": {
                    "name": "Renjing Xu"
                },
                "author": "Renjing Xu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.02724v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.02724v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.BM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02312v1",
                "updated": "2025-08-04T11:28:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    28,
                    34,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:28:34Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    28,
                    34,
                    0,
                    216,
                    0
                ],
                "title": "A Survey on Data Security in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Data Security in Large Language Models"
                },
                "summary": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs."
                },
                "authors": [
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Yuanguo Lin"
                    },
                    {
                        "name": "Jinhe Su"
                    },
                    {
                        "name": "Yuanhui Yu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Fan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Fan Lin"
                },
                "author": "Fan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13422v2",
                "updated": "2025-08-04T11:27:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    27,
                    55,
                    0,
                    216,
                    0
                ],
                "published": "2025-02-19T04:45:05Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    4,
                    45,
                    5,
                    2,
                    50,
                    0
                ],
                "title": "Towards Question Answering over Large Semi-structured Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Question Answering over Large Semi-structured Tables"
                },
                "summary": "Table Question Answering (TableQA) attracts strong interests due to the\nprevalence of web information presented in the form of semi-structured tables.\nDespite many efforts, TableQA over large tables remains an open challenge. This\nis because large tables may overwhelm models that try to comprehend them in\nfull to locate question answers. Recent studies reduce input table size by\ndecomposing tables into smaller, question-relevant sub-tables via generating\nprograms to parse the tables. However, such solutions are subject to program\ngeneration and execution errors and are difficult to ensure decomposition\nquality. To address this issue, we propose TaDRe, a TableQA model that\nincorporates both pre- and post-table decomposition refinements to ensure table\ndecomposition quality, hence achieving highly accurate TableQA results. To\nevaluate TaDRe, we construct two new large-table TableQA benchmarks via\nLLM-driven table expansion and QA pair generation. Extensive experiments on\nboth the new and public benchmarks show that TaDRe achieves state-of-the-art\nperformance on large-table TableQA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table Question Answering (TableQA) attracts strong interests due to the\nprevalence of web information presented in the form of semi-structured tables.\nDespite many efforts, TableQA over large tables remains an open challenge. This\nis because large tables may overwhelm models that try to comprehend them in\nfull to locate question answers. Recent studies reduce input table size by\ndecomposing tables into smaller, question-relevant sub-tables via generating\nprograms to parse the tables. However, such solutions are subject to program\ngeneration and execution errors and are difficult to ensure decomposition\nquality. To address this issue, we propose TaDRe, a TableQA model that\nincorporates both pre- and post-table decomposition refinements to ensure table\ndecomposition quality, hence achieving highly accurate TableQA results. To\nevaluate TaDRe, we construct two new large-table TableQA benchmarks via\nLLM-driven table expansion and QA pair generation. Extensive experiments on\nboth the new and public benchmarks show that TaDRe achieves state-of-the-art\nperformance on large-table TableQA tasks."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wang"
                    },
                    {
                        "name": "Junhao Gan"
                    },
                    {
                        "name": "Jianzhong Qi"
                    }
                ],
                "author_detail": {
                    "name": "Jianzhong Qi"
                },
                "author": "Jianzhong Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02308v2",
                "updated": "2025-08-05T02:16:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    16,
                    8,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T11:22:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    22,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive\n  Long-context Scaling Without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive\n  Long-context Scaling Without Training"
                },
                "summary": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE."
                },
                "authors": [
                    {
                        "name": "Sikui Zhang"
                    },
                    {
                        "name": "Guangze Gao"
                    },
                    {
                        "name": "Ziyun Gan"
                    },
                    {
                        "name": "Chunfeng Yuan"
                    },
                    {
                        "name": "Zefeng Lin"
                    },
                    {
                        "name": "Houwen Peng"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Weiming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Hu"
                },
                "author": "Weiming Hu",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08113v2",
                "updated": "2025-08-04T11:18:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    18,
                    21,
                    0,
                    216,
                    0
                ],
                "published": "2025-04-10T20:19:50Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    19,
                    50,
                    3,
                    100,
                    0
                ],
                "title": "Test Amplification for REST APIs via Single and Multi-Agent LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Amplification for REST APIs via Single and Multi-Agent LLM Systems"
                },
                "summary": "REST APIs (Representational State Transfer Application Programming\nInterfaces) play a vital role in modern cloud-native applications. As these\nAPIs grow in complexity and scale, ensuring their correctness and robustness\nbecomes increasingly important. Automated testing is essential for identifying\nhidden bugs, particularly those that appear in edge cases or under unexpected\ninputs. However, creating comprehensive and effective test suites for REST APIs\nis challenging and often demands significant effort. In this paper, we\ninvestigate the use of large language model (LLM) systems, both single-agent\nand multi-agent setups, for amplifying existing REST API test suites. These\nsystems generate additional test cases that aim to push the boundaries of the\nAPI, uncovering behaviors that might otherwise go untested. We present a\ncomparative evaluation of the two approaches across several dimensions,\nincluding test coverage, bug detection effectiveness, and practical\nconsiderations such as computational cost and energy usage. Our evaluation\ndemonstrates increased API coverage, identification of numerous bugs in the API\nunder test, and insights into the computational cost and energy consumption of\nboth approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REST APIs (Representational State Transfer Application Programming\nInterfaces) play a vital role in modern cloud-native applications. As these\nAPIs grow in complexity and scale, ensuring their correctness and robustness\nbecomes increasingly important. Automated testing is essential for identifying\nhidden bugs, particularly those that appear in edge cases or under unexpected\ninputs. However, creating comprehensive and effective test suites for REST APIs\nis challenging and often demands significant effort. In this paper, we\ninvestigate the use of large language model (LLM) systems, both single-agent\nand multi-agent setups, for amplifying existing REST API test suites. These\nsystems generate additional test cases that aim to push the boundaries of the\nAPI, uncovering behaviors that might otherwise go untested. We present a\ncomparative evaluation of the two approaches across several dimensions,\nincluding test coverage, bug detection effectiveness, and practical\nconsiderations such as computational cost and energy usage. Our evaluation\ndemonstrates increased API coverage, identification of numerous bugs in the API\nunder test, and insights into the computational cost and energy consumption of\nboth approaches."
                },
                "authors": [
                    {
                        "name": "Robbe Nooyens"
                    },
                    {
                        "name": "Tolgahan Bardakci"
                    },
                    {
                        "name": "Mutlu Beyazit"
                    },
                    {
                        "name": "Serge Demeyer"
                    }
                ],
                "author_detail": {
                    "name": "Serge Demeyer"
                },
                "author": "Serge Demeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02304v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02304v1",
                "updated": "2025-08-04T11:17:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    17,
                    21,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:17:21Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    17,
                    21,
                    0,
                    216,
                    0
                ],
                "title": "ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant\n  Neural Rendering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASDR: Exploiting Adaptive Sampling and Data Reuse for CIM-based Instant\n  Neural Rendering"
                },
                "summary": "Neural Radiance Fields (NeRF) offer significant promise for generating\nphotorealistic images and videos. However, existing mainstream neural rendering\nmodels often fall short in meeting the demands for immediacy and power\nefficiency in practical applications. Specifically, these models frequently\nexhibit irregular access patterns and substantial computational overhead,\nleading to undesirable inference latency and high power consumption.\nComputing-in-memory (CIM), an emerging computational paradigm, has the\npotential to address these access bottlenecks and reduce the power consumption\nassociated with model execution.\n  To bridge the gap between model performance and real-world scene\nrequirements, we propose an algorithm-architecture co-design approach,\nabbreviated as ASDR, a CIM-based accelerator supporting efficient neural\nrendering. At the algorithmic level, we propose two rendering optimization\nschemes: (1) Dynamic sampling by online sensing of the rendering difficulty of\ndifferent pixels, thus reducing access memory and computational overhead. (2)\nReducing MLP overhead by decoupling and approximating the volume rendering of\ncolor and density. At the architecture level, we design an efficient\nReRAM-based CIM architecture with efficient data mapping and reuse\nmicroarchitecture. Experiments demonstrate that our design can achieve up to\n$9.55\\times$ and $69.75\\times$ speedup over state-of-the-art NeRF accelerators\nand Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Radiance Fields (NeRF) offer significant promise for generating\nphotorealistic images and videos. However, existing mainstream neural rendering\nmodels often fall short in meeting the demands for immediacy and power\nefficiency in practical applications. Specifically, these models frequently\nexhibit irregular access patterns and substantial computational overhead,\nleading to undesirable inference latency and high power consumption.\nComputing-in-memory (CIM), an emerging computational paradigm, has the\npotential to address these access bottlenecks and reduce the power consumption\nassociated with model execution.\n  To bridge the gap between model performance and real-world scene\nrequirements, we propose an algorithm-architecture co-design approach,\nabbreviated as ASDR, a CIM-based accelerator supporting efficient neural\nrendering. At the algorithmic level, we propose two rendering optimization\nschemes: (1) Dynamic sampling by online sensing of the rendering difficulty of\ndifferent pixels, thus reducing access memory and computational overhead. (2)\nReducing MLP overhead by decoupling and approximating the volume rendering of\ncolor and density. At the architecture level, we design an efficient\nReRAM-based CIM architecture with efficient data mapping and reuse\nmicroarchitecture. Experiments demonstrate that our design can achieve up to\n$9.55\\times$ and $69.75\\times$ speedup over state-of-the-art NeRF accelerators\nand Xavier NX GPU in graphics rendering tasks with only $0.1$ PSNR loss."
                },
                "authors": [
                    {
                        "name": "Fangxin Liu"
                    },
                    {
                        "name": "Haomin Li"
                    },
                    {
                        "name": "Bowen Zhu"
                    },
                    {
                        "name": "Zongwu Wang"
                    },
                    {
                        "name": "Zhuoran Song"
                    },
                    {
                        "name": "Habing Guan"
                    },
                    {
                        "name": "Li Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Li Jiang"
                },
                "author": "Li Jiang",
                "arxiv_comment": "Accepted by the 2025 International Conference on Architectural\n  Support for Programming Languages and Operating Systems (ASPLOS 2025). The\n  paper will be presented at ASPLOS 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02304v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02304v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02298v1",
                "updated": "2025-08-04T11:06:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    6,
                    8,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:06:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    6,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative\n  Credit Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative\n  Credit Assignment"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks."
                },
                "authors": [
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Yunsheng Shi"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02296v1",
                "updated": "2025-08-04T11:04:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    4,
                    54,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:04:54Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    4,
                    54,
                    0,
                    216,
                    0
                ],
                "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Methods Defend RAG Systems Well Against Real-World Attacks"
                },
                "summary": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance."
                },
                "authors": [
                    {
                        "name": "Ilias Triantafyllopoulos"
                    },
                    {
                        "name": "Renyi Qu"
                    },
                    {
                        "name": "Salvatore Giorgi"
                    },
                    {
                        "name": "Brenda Curtis"
                    },
                    {
                        "name": "Lyle H. Ungar"
                    },
                    {
                        "name": "Joo Sedoc"
                    }
                ],
                "author_detail": {
                    "name": "Joo Sedoc"
                },
                "author": "Joo Sedoc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02292v1",
                "updated": "2025-08-04T11:02:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    2,
                    34,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:02:34Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    2,
                    34,
                    0,
                    216,
                    0
                ],
                "title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI\n  Research and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI\n  Research and Deployment"
                },
                "summary": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}."
                },
                "authors": [
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Yilei Zhao"
                    },
                    {
                        "name": "Chuqiao Zong"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18397v2",
                "updated": "2025-08-04T11:01:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    1,
                    59,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-24T13:28:44Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    13,
                    28,
                    44,
                    3,
                    205,
                    0
                ],
                "title": "Correlation and Redundancy of Time-Delay Interferometry Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Correlation and Redundancy of Time-Delay Interferometry Configurations"
                },
                "summary": "Time-Delay Interferometry (TDI) is essential for space-based gravitational\nwave (GW) missions, as it suppresses laser frequency noise and achieve the\nrequired sensitivity. Beyond the standard Michelson configuration, a variety of\nsecond-generation TDI schemes have been proposed, each utilizing different\ncombinations of inter-spacecraft laser links. In this work, we conduct a\ncomparative study of several representative TDI configurations with varying\ntime spans and demonstrate that their (quasi-)orthogonal channels are highly\ncorrelated, indicating substantial redundancy among these schemes. In the\nlow-frequency regime, the performance of different TDI configurations are\nnearly identical. Their distinctions emerge primarily at high frequencies,\nwhere the GW wavelength becomes comparable to the arm length. In this regime,\nshorter TDI time spans with minimal null frequencies facilitate more accurate\nwaveform modeling and parameter recovery in frequency domain. In contrast,\nconfigurations with longer time spans and more null frequencies, such as the\nMichelson, are more susceptible to frequency aliasing and waveform modulation\neffects, which degrade inference accuracy. However, if signal modeling and\nanalysis are performed in the time domain, all TDI configurations become\neffectively equivalent. Considering the usability in both frequency and time\ndomain, the short-span PD4L scheme, which exhibits minimal nulls and superior\nperformance in high frequencies, emerges as a promising candidate for future\nspace-based GW mission designs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time-Delay Interferometry (TDI) is essential for space-based gravitational\nwave (GW) missions, as it suppresses laser frequency noise and achieve the\nrequired sensitivity. Beyond the standard Michelson configuration, a variety of\nsecond-generation TDI schemes have been proposed, each utilizing different\ncombinations of inter-spacecraft laser links. In this work, we conduct a\ncomparative study of several representative TDI configurations with varying\ntime spans and demonstrate that their (quasi-)orthogonal channels are highly\ncorrelated, indicating substantial redundancy among these schemes. In the\nlow-frequency regime, the performance of different TDI configurations are\nnearly identical. Their distinctions emerge primarily at high frequencies,\nwhere the GW wavelength becomes comparable to the arm length. In this regime,\nshorter TDI time spans with minimal null frequencies facilitate more accurate\nwaveform modeling and parameter recovery in frequency domain. In contrast,\nconfigurations with longer time spans and more null frequencies, such as the\nMichelson, are more susceptible to frequency aliasing and waveform modulation\neffects, which degrade inference accuracy. However, if signal modeling and\nanalysis are performed in the time domain, all TDI configurations become\neffectively equivalent. Considering the usability in both frequency and time\ndomain, the short-span PD4L scheme, which exhibits minimal nulls and superior\nperformance in high frequencies, emerges as a promising candidate for future\nspace-based GW mission designs."
                },
                "authors": [
                    {
                        "name": "Gang Wang"
                    }
                ],
                "author_detail": {
                    "name": "Gang Wang"
                },
                "author": "Gang Wang",
                "arxiv_comment": "18 pages, 11 figures, Part of a series of related work:\n  arXiv:2406.14173, arXiv:2403.01726, arXiv:2403.01490, arXiv:2406.11305,\n  arXiv:2502.03983",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "gr-qc",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02282v1",
                "updated": "2025-08-04T10:53:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    53,
                    10,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:53:10Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    53,
                    10,
                    0,
                    216,
                    0
                ],
                "title": "Distillation-Enhanced Clustering Acceleration for Encrypted Traffic\n  Classification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Distillation-Enhanced Clustering Acceleration for Encrypted Traffic\n  Classification"
                },
                "summary": "Traffic classification plays a significant role in network service\nmanagement. The advancement of deep learning has established pretrained models\nas a robust approach for this task. However, contemporary encrypted traffic\nclassification systems face dual limitations. Firstly, pretrained models\ntypically exhibit large-scale architectures, where their extensive\nparameterization results in slow inference speeds and high computational\nlatency. Secondly, reliance on labeled data for fine-tuning restricts these\nmodels to predefined supervised classes, creating a bottleneck when novel\ntraffic types emerge in the evolving Internet landscape. To address these\nchallenges, we propose NetClus, a novel framework integrating pretrained models\nwith distillation-enhanced clustering acceleration. During fine-tuning, NetClus\nfirst introduces a cluster-friendly loss to jointly reshape the latent space\nfor both classification and clustering. With the fine-tuned model, it distills\nthe model into a lightweight Feed-Forward Neural Network model to retain\nsemantics. During inference, NetClus performs heuristic merge with near-linear\nruntime, and valid the cluster purity with newly proposed metrics ASI to\nidentify emergent traffic types while expediting classification. Benchmarked\nagainst existing pretrained methods, NetClus achieves up to 6.2x acceleration\nwhile maintaining classification degradation below 1%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic classification plays a significant role in network service\nmanagement. The advancement of deep learning has established pretrained models\nas a robust approach for this task. However, contemporary encrypted traffic\nclassification systems face dual limitations. Firstly, pretrained models\ntypically exhibit large-scale architectures, where their extensive\nparameterization results in slow inference speeds and high computational\nlatency. Secondly, reliance on labeled data for fine-tuning restricts these\nmodels to predefined supervised classes, creating a bottleneck when novel\ntraffic types emerge in the evolving Internet landscape. To address these\nchallenges, we propose NetClus, a novel framework integrating pretrained models\nwith distillation-enhanced clustering acceleration. During fine-tuning, NetClus\nfirst introduces a cluster-friendly loss to jointly reshape the latent space\nfor both classification and clustering. With the fine-tuned model, it distills\nthe model into a lightweight Feed-Forward Neural Network model to retain\nsemantics. During inference, NetClus performs heuristic merge with near-linear\nruntime, and valid the cluster purity with newly proposed metrics ASI to\nidentify emergent traffic types while expediting classification. Benchmarked\nagainst existing pretrained methods, NetClus achieves up to 6.2x acceleration\nwhile maintaining classification degradation below 1%."
                },
                "authors": [
                    {
                        "name": "Ziyue Huang"
                    },
                    {
                        "name": "Chungang Lin"
                    },
                    {
                        "name": "Weiyao Zhang"
                    },
                    {
                        "name": "Xuying Meng"
                    },
                    {
                        "name": "Yujun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yujun Zhang"
                },
                "author": "Yujun Zhang",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13795v2",
                "updated": "2025-08-04T10:49:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    49,
                    54,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-18T12:39:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    39,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN."
                },
                "authors": [
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2508.02668v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02668v1",
                "updated": "2025-08-04T17:58:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    58,
                    22,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:58:22Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    58,
                    22,
                    0,
                    216,
                    0
                ],
                "title": "LOST: Low-rank and Sparse Pre-training for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LOST: Low-rank and Sparse Pre-training for Large Language Models"
                },
                "summary": "While large language models (LLMs) have achieved remarkable performance\nacross a wide range of tasks, their massive scale incurs prohibitive\ncomputational and memory costs for pre-training from scratch. Recent studies\nhave investigated the use of low-rank parameterization as a means of reducing\nmodel size and training cost. In this context, sparsity is often employed as a\ncomplementary technique to recover important information lost in low-rank\ncompression by capturing salient features in the residual space. However,\nexisting approaches typically combine low-rank and sparse components in a\nsimplistic or ad hoc manner, often resulting in undesirable performance\ndegradation compared to full-rank training. In this paper, we propose\n\\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for\nLLMs, a novel method that ingeniously integrates low-rank and sparse structures\nto enable effective training of LLMs from scratch under strict efficiency\nconstraints. LOST applies singular value decomposition to weight matrices,\npreserving the dominant low-rank components, while allocating the remaining\nsingular values to construct channel-wise sparse components to complement the\nexpressiveness of low-rank training. We evaluate LOST on LLM pretraining\nranging from 60M to 7B parameters. Our experiments show that LOST achieves\ncompetitive or superior performance compared to full-rank models, while\nsignificantly reducing both memory and compute overhead. Moreover, Code is\navailable at\n\\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST\nRepo}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While large language models (LLMs) have achieved remarkable performance\nacross a wide range of tasks, their massive scale incurs prohibitive\ncomputational and memory costs for pre-training from scratch. Recent studies\nhave investigated the use of low-rank parameterization as a means of reducing\nmodel size and training cost. In this context, sparsity is often employed as a\ncomplementary technique to recover important information lost in low-rank\ncompression by capturing salient features in the residual space. However,\nexisting approaches typically combine low-rank and sparse components in a\nsimplistic or ad hoc manner, often resulting in undesirable performance\ndegradation compared to full-rank training. In this paper, we propose\n\\textbf{LO}w-rank and \\textbf{S}parse pre-\\textbf{T}raining (\\textbf{LOST}) for\nLLMs, a novel method that ingeniously integrates low-rank and sparse structures\nto enable effective training of LLMs from scratch under strict efficiency\nconstraints. LOST applies singular value decomposition to weight matrices,\npreserving the dominant low-rank components, while allocating the remaining\nsingular values to construct channel-wise sparse components to complement the\nexpressiveness of low-rank training. We evaluate LOST on LLM pretraining\nranging from 60M to 7B parameters. Our experiments show that LOST achieves\ncompetitive or superior performance compared to full-rank models, while\nsignificantly reducing both memory and compute overhead. Moreover, Code is\navailable at\n\\href{https://github.com/JiaxiLi1/LOST-Low-rank-and-Sparse-Training-for-Large-Language-Models}{LOST\nRepo}"
                },
                "authors": [
                    {
                        "name": "Jiaxi Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Jinjin Xu"
                    },
                    {
                        "name": "Liwu Xu"
                    },
                    {
                        "name": "Tianjin Huang"
                    },
                    {
                        "name": "Wenwu Wang"
                    },
                    {
                        "name": "Shiwei Liu"
                    },
                    {
                        "name": "Xilu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Xilu Wang"
                },
                "author": "Xilu Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02668v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02668v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02635v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02635v1",
                "updated": "2025-08-04T17:22:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    22,
                    8,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:22:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    22,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "Test Set Quality in Multilingual LLM Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Set Quality in Multilingual LLM Evaluation"
                },
                "summary": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Several multilingual benchmark datasets have been developed in a\nsemi-automatic manner in the recent past to measure progress and understand the\nstate-of-the-art in the multilingual capabilities of Large Language Models.\nHowever, there is not a lot of attention paid to the quality of the datasets\nthemselves, despite the existence of previous work in identifying errors in\neven fully human-annotated test sets. In this paper, we manually analyze recent\nmultilingual evaluation sets in two languages - French and Telugu, identifying\nseveral errors in the process. We compare the performance difference across\nseveral LLMs with the original and revised versions of the datasets and\nidentify large differences (almost 10% in some cases) in both languages). Based\non these results, we argue that test sets should not be considered immutable\nand should be revisited, checked for correctness, and potentially versioned. We\nend with some recommendations for both the dataset creators as well as\nconsumers on addressing the dataset quality issues."
                },
                "authors": [
                    {
                        "name": "Kranti Chalamalasetti"
                    },
                    {
                        "name": "Gabriel Bernier-Colborne"
                    },
                    {
                        "name": "Yvan Gauthier"
                    },
                    {
                        "name": "Sowmya Vajjala"
                    }
                ],
                "author_detail": {
                    "name": "Sowmya Vajjala"
                },
                "author": "Sowmya Vajjala",
                "arxiv_comment": "Accepted at the 1st Workshop on Multilingual Data Quality Signals,\n  COLM 2025, Short paper. 10 pages in total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02635v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02635v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02622v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02622v1",
                "updated": "2025-08-04T17:10:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    10,
                    8,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:10:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    10,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Noosemia: toward a Cognitive and Phenomenological Account of\n  Intentionality Attribution in Human-Generative AI Interaction"
                },
                "summary": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces and formalizes Noosemia, a novel\ncognitive-phenomenological phenomenon emerging from human interaction with\ngenerative AI systems, particularly those enabling dialogic or multimodal\nexchanges. We propose a multidisciplinary framework to explain how, under\ncertain conditions, users attribute intentionality, agency, and even\ninteriority to these systems - a process grounded not in physical resemblance,\nbut in linguistic performance, epistemic opacity, and emergent technological\ncomplexity. By linking an LLM declination of meaning holism to our technical\nnotion of the LLM Contextual Cognitive Field, we clarify how LLMs construct\nmeaning relationally and how coherence and a simulacrum of agency arise at the\nhuman-AI interface. The analysis situates noosemia alongside pareidolia,\nanimism, the intentional stance and the uncanny valley, distinguishing its\nunique characteristics. We also introduce a-noosemia to describe the\nphenomenological withdrawal of such projections. The paper concludes with\nreflections on the broader philosophical, epistemological, and social\nimplications of noosemic dynamics and directions for future research."
                },
                "authors": [
                    {
                        "name": "Enrico De Santis"
                    },
                    {
                        "name": "Antonello Rizzi"
                    }
                ],
                "author_detail": {
                    "name": "Antonello Rizzi"
                },
                "author": "Antonello Rizzi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02622v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02622v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02618v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02618v1",
                "updated": "2025-08-04T17:06:23Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    6,
                    23,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:06:23Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    6,
                    23,
                    0,
                    216,
                    0
                ],
                "title": "Mitigating Attention Hacking in Preference-Based Reward Modeling via\n  Interaction Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mitigating Attention Hacking in Preference-Based Reward Modeling via\n  Interaction Distillation"
                },
                "summary": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reward model (RM), as the core component of reinforcement learning from\nhuman feedback (RLHF) for large language models (LLMs), responsible for\nproviding reward signals to generated responses. However, mainstream preference\nmodeling in RM is inadequate in terms of token-level interaction, making its\njudgment signals vulnerable to being hacked by misallocated attention to\ncontext. This stems from two fundamental limitations: (1) Current preference\nmodeling employs decoder-only architectures, where the unidirectional causal\nattention mechanism leads to forward-decaying intra-sequence attention within\nthe prompt-response sequence. (2) The independent Siamese-encoding paradigm\ninduces the absence of token-level inter-sequence attention between chosen and\nrejected sequences. To address this \"attention hacking\", we propose\n\"Interaction Distillation\", a novel training framework for more adequate\npreference modeling through attention-level optimization. The method introduces\nan interaction-based natural language understanding model as the teacher to\nprovide sophisticated token interaction patterns via comprehensive attention,\nand guides the preference modeling to simulate teacher model's interaction\npattern through an attentional alignment objective. Through extensive\nexperiments, interaction distillation has demonstrated its ability to provide\nmore stable and generalizable reward signals compared to state-of-the-art RM\noptimization methods that target data noise, highlighting the attention hacking\nconstitute a more fundamental limitation in RM."
                },
                "authors": [
                    {
                        "name": "Jianxiang Zang"
                    },
                    {
                        "name": "Meiling Ning"
                    },
                    {
                        "name": "Shihan Dou"
                    },
                    {
                        "name": "Jiazheng Zhang"
                    },
                    {
                        "name": "Tao Gui"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Xuanjing Huang"
                    }
                ],
                "author_detail": {
                    "name": "Xuanjing Huang"
                },
                "author": "Xuanjing Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02618v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02618v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02611v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02611v1",
                "updated": "2025-08-04T17:01:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    1,
                    10,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T17:01:10Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    17,
                    1,
                    10,
                    0,
                    216,
                    0
                ],
                "title": "Meta-RAG on Large Codebases Using Code Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Meta-RAG on Large Codebases Using Code Summarization"
                },
                "summary": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) systems have been at the forefront of applied\nArtificial Intelligence (AI) research in a multitude of domains. One such\ndomain is software development, where researchers have pushed the automation of\na number of code tasks through LLM agents. Software development is a complex\necosystem, that stretches far beyond code implementation and well into the\nrealm of code maintenance. In this paper, we propose a multi-agent system to\nlocalize bugs in large pre-existing codebases using information retrieval and\nLLMs. Our system introduces a novel Retrieval Augmented Generation (RAG)\napproach, Meta-RAG, where we utilize summaries to condense codebases by an\naverage of 79.8\\%, into a compact, structured, natural language representation.\nWe then use an LLM agent to determine which parts of the codebase are critical\nfor bug resolution, i.e. bug localization. We demonstrate the usefulness of\nMeta-RAG through evaluation with the SWE-bench Lite dataset. Meta-RAG scores\n84.67 % and 53.0 % for file-level and function-level correct localization\nrates, respectively, achieving state-of-the-art performance."
                },
                "authors": [
                    {
                        "name": "Vali Tawosia"
                    },
                    {
                        "name": "Salwa Alamir"
                    },
                    {
                        "name": "Xiaomo Liu"
                    },
                    {
                        "name": "Manuela Veloso"
                    }
                ],
                "author_detail": {
                    "name": "Manuela Veloso"
                },
                "author": "Manuela Veloso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02611v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02611v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02601v1",
                "updated": "2025-08-04T16:55:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    55,
                    2,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:55:02Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    55,
                    2,
                    0,
                    216,
                    0
                ],
                "title": "StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis\n  in Low-Data Regimes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StructSynth: Leveraging LLMs for Structure-Aware Tabular Data Synthesis\n  in Low-Data Regimes"
                },
                "summary": "The application of machine learning on tabular data in specialized domains is\nseverely limited by data scarcity. While generative models offer a solution,\ntraditional methods falter in low-data regimes, and recent Large Language\nModels (LLMs) often ignore the explicit dependency structure of tabular data,\nleading to low-fidelity synthetics. To address these limitations, we introduce\nStructSynth, a novel framework that integrates the generative power of LLMs\nwith robust structural control. StructSynth employs a two-stage architecture.\nFirst, it performs explicit structure discovery to learn a Directed Acyclic\nGraph (DAG) from the available data. Second, this learned structure serves as a\nhigh-fidelity blueprint to steer the LLM's generation process, forcing it to\nadhere to the learned feature dependencies and thereby ensuring the generated\ndata respects the underlying structure by design. Our extensive experiments\ndemonstrate that StructSynth produces synthetic data with significantly higher\nstructural integrity and downstream utility than state-of-the-art methods. It\nproves especially effective in challenging low-data scenarios, successfully\nnavigating the trade-off between privacy preservation and statistical fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of machine learning on tabular data in specialized domains is\nseverely limited by data scarcity. While generative models offer a solution,\ntraditional methods falter in low-data regimes, and recent Large Language\nModels (LLMs) often ignore the explicit dependency structure of tabular data,\nleading to low-fidelity synthetics. To address these limitations, we introduce\nStructSynth, a novel framework that integrates the generative power of LLMs\nwith robust structural control. StructSynth employs a two-stage architecture.\nFirst, it performs explicit structure discovery to learn a Directed Acyclic\nGraph (DAG) from the available data. Second, this learned structure serves as a\nhigh-fidelity blueprint to steer the LLM's generation process, forcing it to\nadhere to the learned feature dependencies and thereby ensuring the generated\ndata respects the underlying structure by design. Our extensive experiments\ndemonstrate that StructSynth produces synthetic data with significantly higher\nstructural integrity and downstream utility than state-of-the-art methods. It\nproves especially effective in challenging low-data scenarios, successfully\nnavigating the trade-off between privacy preservation and statistical fidelity."
                },
                "authors": [
                    {
                        "name": "Siyi Liu"
                    },
                    {
                        "name": "Yujia Zheng"
                    },
                    {
                        "name": "Yongqi Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongqi Zhang"
                },
                "author": "Yongqi Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.22967v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.22967v2",
                "updated": "2025-08-04T16:54:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    54,
                    44,
                    0,
                    216,
                    0
                ],
                "published": "2025-06-28T17:57:58Z",
                "published_parsed": [
                    2025,
                    6,
                    28,
                    17,
                    57,
                    58,
                    5,
                    179,
                    0
                ],
                "title": "ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ActAlign: Zero-Shot Fine-Grained Video Classification via\n  Language-Guided Sequence Alignment"
                },
                "summary": "We address the task of zero-shot video classification for extremely\nfine-grained actions (e.g., Windmill Dunk in basketball), where no video\nexamples or temporal annotations are available for unseen classes. While\nimage-language models (e.g., CLIP, SigLIP) show strong open-set recognition,\nthey lack temporal modeling needed for video understanding. We propose\nActAlign, a truly zero-shot, training-free method that formulates video\nclassification as a sequence alignment problem, preserving the generalization\nstrength of pretrained image-language models. For each class, a large language\nmodel (LLM) generates an ordered sequence of sub-actions, which we align with\nvideo frames using Dynamic Time Warping (DTW) in a shared embedding space.\nWithout any video-text supervision or fine-tuning, ActAlign achieves 30.5%\naccuracy on ActionAtlas--the most diverse benchmark of fine-grained actions\nacross multiple sports--where human performance is only 61.6%. ActAlign\noutperforms billion-parameter video-language models while using 8x fewer\nparameters. Our approach is model-agnostic and domain-general, demonstrating\nthat structured language priors combined with classical alignment methods can\nunlock the open-set recognition potential of image-language models for\nfine-grained video understanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We address the task of zero-shot video classification for extremely\nfine-grained actions (e.g., Windmill Dunk in basketball), where no video\nexamples or temporal annotations are available for unseen classes. While\nimage-language models (e.g., CLIP, SigLIP) show strong open-set recognition,\nthey lack temporal modeling needed for video understanding. We propose\nActAlign, a truly zero-shot, training-free method that formulates video\nclassification as a sequence alignment problem, preserving the generalization\nstrength of pretrained image-language models. For each class, a large language\nmodel (LLM) generates an ordered sequence of sub-actions, which we align with\nvideo frames using Dynamic Time Warping (DTW) in a shared embedding space.\nWithout any video-text supervision or fine-tuning, ActAlign achieves 30.5%\naccuracy on ActionAtlas--the most diverse benchmark of fine-grained actions\nacross multiple sports--where human performance is only 61.6%. ActAlign\noutperforms billion-parameter video-language models while using 8x fewer\nparameters. Our approach is model-agnostic and domain-general, demonstrating\nthat structured language priors combined with classical alignment methods can\nunlock the open-set recognition potential of image-language models for\nfine-grained video understanding."
                },
                "authors": [
                    {
                        "name": "Amir Aghdam"
                    },
                    {
                        "name": "Vincent Tao Hu"
                    },
                    {
                        "name": "Bjrn Ommer"
                    }
                ],
                "author_detail": {
                    "name": "Bjrn Ommer"
                },
                "author": "Bjrn Ommer",
                "arxiv_comment": "Preprint manuscript - Project page:\n  https://amir-aghdam.github.io/act-align/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.22967v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.22967v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02591v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02591v1",
                "updated": "2025-08-04T16:46:15Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    46,
                    15,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:46:15Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    46,
                    15,
                    0,
                    216,
                    0
                ],
                "title": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CharBench: Evaluating the Role of Tokenization in Character-Level Tasks"
                },
                "summary": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tasks that require character-level reasoning, such as counting or locating\ncharacters within words, remain challenging for contemporary language models. A\ncommon conjecture is that language models' reliance on subword units, rather\nthan characters, contributes to their struggles with character-level tasks, yet\nrecent studies offer conflicting conclusions about the role of tokenization,\nleaving its impact unclear. To address this gap, we introduce CharBench, a\ncomprehensive benchmark of character-level tasks that is two orders of\nmagnitude larger than existing alternatives. We evaluate a diverse range of\nleading open-weight and proprietary models on CharBench and find that it\npresents a significant challenge to modern LLMs, with an average accuracy of\n43.6% and 32.3% on some tasks. We present an in-depth analysis of how intrinsic\nproperties of words and their segmentations into tokens correspond to model\nperformance. For counting tasks, we find that tokenization properties are\nweakly correlated with correctness, while the length of the queried word and\nthe actual character count play a more significant part. In contrast, for tasks\nrequiring intra-word positional understanding, performance is negatively\ncorrelated with the length of the token containing the queried character,\nsuggesting that longer tokens obscure character position information for LLMs.\nWe encourage future work to build on the benchmark and evaluation methodology\nintroduced here as tools for improving model performance on such tasks."
                },
                "authors": [
                    {
                        "name": "Omri Uzan"
                    },
                    {
                        "name": "Yuval Pinter"
                    }
                ],
                "author_detail": {
                    "name": "Yuval Pinter"
                },
                "author": "Yuval Pinter",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02591v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02591v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02584v1",
                "updated": "2025-08-04T16:40:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    40,
                    2,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:40:02Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    40,
                    2,
                    0,
                    216,
                    0
                ],
                "title": "MArgE: Meshing Argumentative Evidence from Multiple Large Language\n  Models for Justifiable Claim Verification",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MArgE: Meshing Argumentative Evidence from Multiple Large Language\n  Models for Justifiable Claim Verification"
                },
                "summary": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging outputs from multiple large language models (LLMs) is emerging as\na method for harnessing their power across a wide range of tasks while\nmitigating their capacity for making errors, e.g., hallucinations. However,\ncurrent approaches to combining insights from multiple LLMs often involve\nunstructured interactions (e.g., free debate), resulting in model generations\nthat are not faithfully justifiable. In this work, we introduce MArgE, a novel\nframework to provide formal structure to the evidence from each LLM, in the\nform of a tree of extracted arguments, for the task of claim verification. We\nuse a variant of Argumentative LLMs (ArgLLMs), i.e. LLMs driven by frameworks\nand semantics from the field of computational argumentation, to construct\nstructured argument trees for given claims. This process creates an inspectable\npathway from the initial arguments to the final claim verification decisions,\nproviding a faithful justification thereof. We show experimentally that MArgE\ncan significantly outperform single LLMs, including three open-source models\n(4B to 8B parameters), GPT-4o-mini and existing ArgLLMs, as well as prior\nmethods for unstructured multi-LLM debates. We thus demonstrate the advantages\nof incorporating formal, argumentative reasoning mechanisms when combining\nmultiple LLM outputs."
                },
                "authors": [
                    {
                        "name": "Ming Pok Ng"
                    },
                    {
                        "name": "Junqi Jiang"
                    },
                    {
                        "name": "Gabriel Freedman"
                    },
                    {
                        "name": "Antonio Rago"
                    },
                    {
                        "name": "Francesca Toni"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Toni"
                },
                "author": "Francesca Toni",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02583v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02583v1",
                "updated": "2025-08-04T16:39:24Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    39,
                    24,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:39:24Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    39,
                    24,
                    0,
                    216,
                    0
                ],
                "title": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMA: Enhancing Mathematical Reasoning in Large Language Models with\n  Causal Knowledge"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance across a\nwide range of tasks, yet they still struggle with complex mathematical\nreasoning, a challenge fundamentally rooted in deep structural dependencies. To\naddress this challenge, we propose \\textbf{CA}usal \\textbf{MA}thematician\n(\\textbf{CAMA}), a two-stage causal framework that equips LLMs with explicit,\nreusable mathematical structure. In the learning stage, CAMA first constructs\nthe \\textbf{M}athematical \\textbf{C}ausal \\textbf{G}raph (\\textbf{MCG}), a\nhigh-level representation of solution strategies, by combining LLM priors with\ncausal discovery algorithms applied to a corpus of question-solution pairs. The\nresulting MCG encodes essential knowledge points and their causal dependencies.\nTo better align the graph with downstream reasoning tasks, CAMA further refines\nthe MCG through iterative feedback derived from a selected subset of the\nquestion-solution pairs. In the reasoning stage, given a new question, CAMA\ndynamically extracts a task-relevant subgraph from the MCG, conditioned on both\nthe question content and the LLM's intermediate reasoning trace. This subgraph,\nwhich encodes the most pertinent knowledge points and their causal\ndependencies, is then injected back into the LLM to guide its reasoning\nprocess. Empirical results on real-world datasets show that CAMA significantly\nimproves LLM performance on challenging mathematical problems. Furthermore, our\nexperiments demonstrate that structured guidance consistently outperforms\nunstructured alternatives, and that incorporating asymmetric causal\nrelationships yields greater improvements than using symmetric associations\nalone."
                },
                "authors": [
                    {
                        "name": "Lei Zan"
                    },
                    {
                        "name": "Keli Zhang"
                    },
                    {
                        "name": "Ruichu Cai"
                    },
                    {
                        "name": "Lujia Pan"
                    }
                ],
                "author_detail": {
                    "name": "Lujia Pan"
                },
                "author": "Lujia Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02583v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02583v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.07927v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.07927v3",
                "updated": "2025-08-04T16:37:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    37,
                    0,
                    0,
                    216,
                    0
                ],
                "published": "2025-01-14T08:30:49Z",
                "published_parsed": [
                    2025,
                    1,
                    14,
                    8,
                    30,
                    49,
                    1,
                    14,
                    0
                ],
                "title": "Gandalf the Red: Adaptive Security for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gandalf the Red: Adaptive Security for LLMs"
                },
                "summary": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Current evaluations of defenses against prompt attacks in large language\nmodel (LLM) applications often overlook two critical factors: the dynamic\nnature of adversarial behavior and the usability penalties imposed on\nlegitimate users by restrictive defenses. We propose D-SEC (Dynamic Security\nUtility Threat Model), which explicitly separates attackers from legitimate\nusers, models multi-step interactions, and expresses the security-utility in an\noptimizable form. We further address the shortcomings in existing evaluations\nby introducing Gandalf, a crowd-sourced, gamified red-teaming platform designed\nto generate realistic, adaptive attack. Using Gandalf, we collect and release a\ndataset of 279k prompt attacks. Complemented by benign user data, our analysis\nreveals the interplay between security and utility, showing that defenses\nintegrated in the LLM (e.g., system prompts) can degrade usability even without\nblocking requests. We demonstrate that restricted application domains,\ndefense-in-depth, and adaptive defenses are effective strategies for building\nsecure and useful LLM applications."
                },
                "authors": [
                    {
                        "name": "Niklas Pfister"
                    },
                    {
                        "name": "Vclav Volhejn"
                    },
                    {
                        "name": "Manuel Knott"
                    },
                    {
                        "name": "Santiago Arias"
                    },
                    {
                        "name": "Julia Baziska"
                    },
                    {
                        "name": "Mykhailo Bichurin"
                    },
                    {
                        "name": "Alan Commike"
                    },
                    {
                        "name": "Janet Darling"
                    },
                    {
                        "name": "Peter Dienes"
                    },
                    {
                        "name": "Matthew Fiedler"
                    },
                    {
                        "name": "David Haber"
                    },
                    {
                        "name": "Matthias Kraft"
                    },
                    {
                        "name": "Marco Lancini"
                    },
                    {
                        "name": "Max Mathys"
                    },
                    {
                        "name": "Damin Pascual-Ortiz"
                    },
                    {
                        "name": "Jakub Podolak"
                    },
                    {
                        "name": "Adri Romero-Lpez"
                    },
                    {
                        "name": "Kyriacos Shiarlis"
                    },
                    {
                        "name": "Andreas Signer"
                    },
                    {
                        "name": "Zsolt Terek"
                    },
                    {
                        "name": "Athanasios Theocharis"
                    },
                    {
                        "name": "Daniel Timbrell"
                    },
                    {
                        "name": "Samuel Trautwein"
                    },
                    {
                        "name": "Samuel Watts"
                    },
                    {
                        "name": "Yun-Han Wu"
                    },
                    {
                        "name": "Mateo Rojas-Carulla"
                    }
                ],
                "author_detail": {
                    "name": "Mateo Rojas-Carulla"
                },
                "author": "Mateo Rojas-Carulla",
                "arxiv_comment": "Niklas Pfister, V\\'aclav Volhejn and Manuel Knott contributed equally",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.07927v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.07927v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02577v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02577v1",
                "updated": "2025-08-04T16:33:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    33,
                    44,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:33:44Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    33,
                    44,
                    0,
                    216,
                    0
                ],
                "title": "EDGES-3: Instrument Design and Commissioning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDGES-3: Instrument Design and Commissioning"
                },
                "summary": "EDGES-3 is the third iteration of the EDGES experiment, designed to measure\nthe predicted global absorption feature in the radio spectrum produced by\nneutral hydrogen gas at cosmic dawn, a critical observation determining when\nand how the first stars populated the universe. The EDGES-3 instrument has been\nredesigned to include both the analog and digital electronics within the\nantenna, allowing for in-situ calibration and removal of the lossy balun found\nin EDGES-2. EDGES-3 has been on multiple deployments in the past 4 years; to\nOregon, Devon Island, Adak Island, and is currently installed and taking data\nin the outback of Western Australia. This paper provides an accounting of the\nchallenges inherent in the detection of the global, cosmological 21-cm signal,\nthe strategies EDGES employs to mitigate each of these challenges, a\ndescription of the instrument, and a report on the Western Australia deployment\nalong with observational data.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "EDGES-3 is the third iteration of the EDGES experiment, designed to measure\nthe predicted global absorption feature in the radio spectrum produced by\nneutral hydrogen gas at cosmic dawn, a critical observation determining when\nand how the first stars populated the universe. The EDGES-3 instrument has been\nredesigned to include both the analog and digital electronics within the\nantenna, allowing for in-situ calibration and removal of the lossy balun found\nin EDGES-2. EDGES-3 has been on multiple deployments in the past 4 years; to\nOregon, Devon Island, Adak Island, and is currently installed and taking data\nin the outback of Western Australia. This paper provides an accounting of the\nchallenges inherent in the detection of the global, cosmological 21-cm signal,\nthe strategies EDGES employs to mitigate each of these challenges, a\ndescription of the instrument, and a report on the Western Australia deployment\nalong with observational data."
                },
                "authors": [
                    {
                        "name": "Rigel C. Cappallo"
                    },
                    {
                        "name": "Alan E. E. Rogers"
                    },
                    {
                        "name": "Colin J. Lonsdale"
                    },
                    {
                        "name": "Judd D. Bowman"
                    },
                    {
                        "name": "John P. Barrett"
                    },
                    {
                        "name": "Steven G. Murray"
                    },
                    {
                        "name": "Nivedita Mahesh"
                    },
                    {
                        "name": "Peter Sims"
                    },
                    {
                        "name": "Akshatha K. Vydula"
                    },
                    {
                        "name": "Raul A. Monsalve"
                    },
                    {
                        "name": "Christopher J. Eckert"
                    },
                    {
                        "name": "Parker Steen"
                    },
                    {
                        "name": "Kenneth M. Wilson"
                    }
                ],
                "author_detail": {
                    "name": "Kenneth M. Wilson"
                },
                "author": "Kenneth M. Wilson",
                "arxiv_comment": "22 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02577v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02577v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02573v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02573v1",
                "updated": "2025-08-04T16:27:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    27,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:27:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    27,
                    56,
                    0,
                    216,
                    0
                ],
                "title": "Guess or Recall? Training CNNs to Classify and Localize Memorization in\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guess or Recall? Training CNNs to Classify and Localize Memorization in\n  LLMs"
                },
                "summary": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Verbatim memorization in Large Language Models (LLMs) is a multifaceted\nphenomenon involving distinct underlying mechanisms. We introduce a novel\nmethod to analyze the different forms of memorization described by the existing\ntaxonomy. Specifically, we train Convolutional Neural Networks (CNNs) on the\nattention weights of the LLM and evaluate the alignment between this taxonomy\nand the attention weights involved in decoding.\n  We find that the existing taxonomy performs poorly and fails to reflect\ndistinct mechanisms within the attention blocks. We propose a new taxonomy that\nmaximizes alignment with the attention weights, consisting of three categories:\nmemorized samples that are guessed using language modeling abilities, memorized\nsamples that are recalled due to high duplication in the training set, and\nnon-memorized samples. Our results reveal that few-shot verbatim memorization\ndoes not correspond to a distinct attention mechanism. We also show that a\nsignificant proportion of extractable samples are in fact guessed by the model\nand should therefore be studied separately. Finally, we develop a custom visual\ninterpretability technique to localize the regions of the attention weights\ninvolved in each form of memorization."
                },
                "authors": [
                    {
                        "name": "Jrmie Dentan"
                    },
                    {
                        "name": "Davide Buscaldi"
                    },
                    {
                        "name": "Sonia Vanier"
                    }
                ],
                "author_detail": {
                    "name": "Sonia Vanier"
                },
                "author": "Sonia Vanier",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02573v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02573v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02571v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02571v1",
                "updated": "2025-08-04T16:26:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    26,
                    17,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:26:17Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    26,
                    17,
                    0,
                    216,
                    0
                ],
                "title": "ASINT: Learning AS-to-Organization Mapping from Internet Metadata",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ASINT: Learning AS-to-Organization Mapping from Internet Metadata"
                },
                "summary": "Accurately mapping Autonomous Systems (ASNs) to their owning or operating\norganizations underpins Internet measurement research and security\napplications. Yet existing approaches commonly rely solely on WHOIS or\nPeeringDB, missing important relationships (e.g., cross-regional aliases,\nparent-child ownership) and failing to unify organizations scattered across\ndifferent RIR identifiers. We introduce ASINT, an end-to-end pipeline that\nfuses bulk registry data with unstructured Web sources, then employs\nretrieval-augmented generation (RAG) to guide large language model (LLM)\ninference. Through a multi-stage procedure, ASINT merges ASNs into\n\"organization families,\" capturing nuanced ties beyond the scope of simpler\nheuristics.\n  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both\nAS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g.,\noperator aliases, rebrands) that other datasets overlook. Moreover, our refined\nmappings enhance multiple security and measurement tasks: ASINT exposes 27.5%\nmore intra-organizational RPKI misconfigurations, cuts false-positive hijack\nalarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.\n  Finally, ASINT supports periodic updates and cost-sensitive LLM selection,\ndemonstrating that broader Web evidence can provide a more accurate, evolving\nview of the Internet's organizational structure.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately mapping Autonomous Systems (ASNs) to their owning or operating\norganizations underpins Internet measurement research and security\napplications. Yet existing approaches commonly rely solely on WHOIS or\nPeeringDB, missing important relationships (e.g., cross-regional aliases,\nparent-child ownership) and failing to unify organizations scattered across\ndifferent RIR identifiers. We introduce ASINT, an end-to-end pipeline that\nfuses bulk registry data with unstructured Web sources, then employs\nretrieval-augmented generation (RAG) to guide large language model (LLM)\ninference. Through a multi-stage procedure, ASINT merges ASNs into\n\"organization families,\" capturing nuanced ties beyond the scope of simpler\nheuristics.\n  ASINT maps 111,470 ASNs to 81,233 organization families; compared to both\nAS2ORG+ and AS-Sibling, ASINT identifies more cross-regional groupings (e.g.,\noperator aliases, rebrands) that other datasets overlook. Moreover, our refined\nmappings enhance multiple security and measurement tasks: ASINT exposes 27.5%\nmore intra-organizational RPKI misconfigurations, cuts false-positive hijack\nalarms by 9.4%, and lowers erroneous IP leasing inferences by 5.9%.\n  Finally, ASINT supports periodic updates and cost-sensitive LLM selection,\ndemonstrating that broader Web evidence can provide a more accurate, evolving\nview of the Internet's organizational structure."
                },
                "authors": [
                    {
                        "name": "Yongzhe Xu"
                    },
                    {
                        "name": "Weitong Li"
                    },
                    {
                        "name": "Eeshan Umrani"
                    },
                    {
                        "name": "Taejoong Chung"
                    }
                ],
                "author_detail": {
                    "name": "Taejoong Chung"
                },
                "author": "Taejoong Chung",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02571v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02571v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02558v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02558v1",
                "updated": "2025-08-04T16:14:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:14:03Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    14,
                    3,
                    0,
                    216,
                    0
                ],
                "title": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse-dLLM: Accelerating Diffusion LLMs with Dynamic Cache Eviction"
                },
                "summary": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Large Language Models (dLLMs) enable breakthroughs in reasoning and\nparallel decoding but suffer from prohibitive quadratic computational\ncomplexity and memory overhead during inference. Current caching techniques\naccelerate decoding by storing full-layer states, yet impose substantial memory\nusage that limit long-context applications. Our analysis of attention patterns\nin dLLMs reveals persistent cross-layer sparsity, with pivotal tokens remaining\nsalient across decoding steps and low-relevance tokens staying unimportant,\nmotivating selective cache eviction. We propose Sparse-dLLM, the first\ntraining-free framework integrating dynamic cache eviction with sparse\nattention via delayed bidirectional sparse caching. By leveraging the stability\nof token saliency over steps, it retains critical tokens and dynamically evicts\nunimportant prefix/suffix entries using an attention-guided strategy. Extensive\nexperiments on LLaDA and Dream series demonstrate Sparse-dLLM achieves up to\n10$\\times$ higher throughput than vanilla dLLMs, with comparable performance\nand similar peak memory costs, outperforming previous methods in efficiency and\neffectiveness."
                },
                "authors": [
                    {
                        "name": "Yuerong Song"
                    },
                    {
                        "name": "Xiaoran Liu"
                    },
                    {
                        "name": "Ruixiao Li"
                    },
                    {
                        "name": "Zhigeng Liu"
                    },
                    {
                        "name": "Zengfeng Huang"
                    },
                    {
                        "name": "Qipeng Guo"
                    },
                    {
                        "name": "Ziwei He"
                    },
                    {
                        "name": "Xipeng Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Xipeng Qiu"
                },
                "author": "Xipeng Qiu",
                "arxiv_comment": "11 pages, 6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02558v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02558v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02556v1",
                "updated": "2025-08-04T16:08:49Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    8,
                    49,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:08:49Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    8,
                    49,
                    0,
                    216,
                    0
                ],
                "title": "Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU\n  Neural Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated SNOMED CT Concept Annotation in Clinical Text Using Bi-GRU\n  Neural Networks"
                },
                "summary": "Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automated annotation of clinical text with standardized medical concepts is\ncritical for enabling structured data extraction and decision support. SNOMED\nCT provides a rich ontology for labeling clinical entities, but manual\nannotation is labor-intensive and impractical at scale. This study introduces a\nneural sequence labeling approach for SNOMED CT concept recognition using a\nBidirectional GRU model. Leveraging a subset of MIMIC-IV, we preprocess text\nwith domain-adapted SpaCy and SciBERT-based tokenization, segmenting sentences\ninto overlapping 19-token chunks enriched with contextual, syntactic, and\nmorphological features. The Bi-GRU model assigns IOB tags to identify concept\nspans and achieves strong performance with a 90 percent F1-score on the\nvalidation set. These results surpass traditional rule-based systems and match\nor exceed existing neural models. Qualitative analysis shows effective handling\nof ambiguous terms and misspellings. Our findings highlight that lightweight\nRNN-based architectures can deliver high-quality clinical concept annotation\nwith significantly lower computational cost than transformer-based models,\nmaking them well-suited for real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ali Noori"
                    },
                    {
                        "name": "Pratik Devkota"
                    },
                    {
                        "name": "Somya Mohanty"
                    },
                    {
                        "name": "Prashanti Manda"
                    }
                ],
                "author_detail": {
                    "name": "Prashanti Manda"
                },
                "author": "Prashanti Manda",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02549v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02549v1",
                "updated": "2025-08-04T16:01:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    1,
                    30,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T16:01:30Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    16,
                    1,
                    30,
                    0,
                    216,
                    0
                ],
                "title": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MonoDream: Monocular Vision-Language Navigation with Panoramic Dreaming"
                },
                "summary": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth\ninputs to provide rich spatial cues for action planning, but these sensors can\nbe costly or less accessible in real-world deployments. Recent approaches based\non Vision-Language Action (VLA) models achieve strong results with monocular\ninput, yet they still lag behind methods using panoramic RGB-D information. We\npresent MonoDream, a lightweight VLA framework that enables monocular agents to\nlearn a Unified Navigation Representation (UNR). This shared feature\nrepresentation jointly aligns navigation-relevant visual semantics (e.g.,\nglobal layout, depth, and future cues) and language-grounded action intent,\nenabling more reliable action prediction. MonoDream further introduces Latent\nPanoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to\npredict latent features of panoramic RGB and depth observations at both current\nand future steps based on only monocular input. Experiments on multiple VLN\nbenchmarks show that MonoDream consistently improves monocular navigation\nperformance and significantly narrows the gap with panoramic-based agents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision-Language Navigation (VLN) tasks often leverage panoramic RGB and depth\ninputs to provide rich spatial cues for action planning, but these sensors can\nbe costly or less accessible in real-world deployments. Recent approaches based\non Vision-Language Action (VLA) models achieve strong results with monocular\ninput, yet they still lag behind methods using panoramic RGB-D information. We\npresent MonoDream, a lightweight VLA framework that enables monocular agents to\nlearn a Unified Navigation Representation (UNR). This shared feature\nrepresentation jointly aligns navigation-relevant visual semantics (e.g.,\nglobal layout, depth, and future cues) and language-grounded action intent,\nenabling more reliable action prediction. MonoDream further introduces Latent\nPanoramic Dreaming (LPD) tasks to supervise the UNR, which train the model to\npredict latent features of panoramic RGB and depth observations at both current\nand future steps based on only monocular input. Experiments on multiple VLN\nbenchmarks show that MonoDream consistently improves monocular navigation\nperformance and significantly narrows the gap with panoramic-based agents."
                },
                "authors": [
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Wanting Li"
                    },
                    {
                        "name": "Yucheng Wang"
                    },
                    {
                        "name": "Maiyue Chen"
                    },
                    {
                        "name": "Kaihui Wang"
                    },
                    {
                        "name": "Zhizhong Su"
                    },
                    {
                        "name": "Xudong Cai"
                    },
                    {
                        "name": "Yeying Jin"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Zhaoxin Fan"
                    }
                ],
                "author_detail": {
                    "name": "Zhaoxin Fan"
                },
                "author": "Zhaoxin Fan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02549v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02549v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18182v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18182v2",
                "updated": "2025-08-04T15:53:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    53,
                    52,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-24T08:28:17Z",
                "published_parsed": [
                    2025,
                    7,
                    24,
                    8,
                    28,
                    17,
                    3,
                    205,
                    0
                ],
                "title": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SCOPE: Stochastic and Counterbiased Option Placement for Evaluating\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) can achieve inflated scores on multiple-choice\ntasks by exploiting inherent biases in option positions or labels, rather than\ndemonstrating genuine understanding. This study introduces SCOPE, an evaluation\nframework designed to measure and mitigate such selection bias in a\ndataset-independent manner. By repeatedly invoking a null prompt that lacks\nsemantic content, SCOPE estimates each model's unique position-bias\ndistribution. It then redistributes the answer slot according to the\ninverse-bias distribution, thereby equalizing the lucky-rate, the probability\nof selecting the correct answer by chance. Furthermore, it prevents\nsemantically similar distractors from being placed adjacent to the answer,\nthereby blocking near-miss guesses based on superficial proximity cues. Across\nmultiple benchmark experiments, SCOPE consistently outperformed existing\ndebiasing methods in terms of stable performance improvements and showed\nclearer confidence distributions over correct options. This framework thus\noffers a new standard for enhancing the fairness and reliability of LLM\nevaluations."
                },
                "authors": [
                    {
                        "name": "Wonjun Jeong"
                    },
                    {
                        "name": "Dongseok Kim"
                    },
                    {
                        "name": "Taegkeun Whangbo"
                    }
                ],
                "author_detail": {
                    "name": "Taegkeun Whangbo"
                },
                "author": "Taegkeun Whangbo",
                "arxiv_comment": "Comments: 34 pages, 1 figure. v2: All \"Consequence.\" statements in\n  the Theoretical Analysis section relabeled as \"Corollary.\"; duplicated values\n  in Table 20 (previously identical to Table 15) corrected",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18182v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18182v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.03336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.03336v2",
                "updated": "2025-08-04T15:48:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    48,
                    55,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-04T06:49:02Z",
                "published_parsed": [
                    2025,
                    7,
                    4,
                    6,
                    49,
                    2,
                    4,
                    185,
                    0
                ],
                "title": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs\n  More Realistic and Less Risky"
                },
                "summary": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are increasingly tasked with invoking enterprise\nAPIs, yet they routinely falter when near-duplicate tools vie for the same user\nintent or when required arguments are left underspecified. We introduce\nDiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a\ndisambiguation-centric, three-stage pipeline that (i) synthesizes\npersona-driven, multi-turn dialogues in which the assistant must distinguish\namong highly similar tools, (ii) performs supervised fine-tuning of open-source\nmodels with reasoning traces across 3B - 70B parameters, and (iii) evaluates\nreal-world readiness via a dynamic suite that redeploys each model in a live\nagentic loop and reports end-to-end goal completion alongside conventional\nstatic metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE\nraise tool-invocation success by 27 pp over GPT-4o and by 49 pp over\nClaude-3.5-Sonnet, both under optimized prompting. To spur further research, we\nrelease an open corpus of 5000 production-grade enterprise API specifications\npaired with rigorously validated, disambiguation-focused dialogues, offering a\npractical blueprint for building reliable, enterprise-ready tool-calling\nagents."
                },
                "authors": [
                    {
                        "name": "Ashutosh Hathidara"
                    },
                    {
                        "name": "Julien Yu"
                    },
                    {
                        "name": "Sebastian Schreiber"
                    }
                ],
                "author_detail": {
                    "name": "Sebastian Schreiber"
                },
                "author": "Sebastian Schreiber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.03336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.03336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02534v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02534v1",
                "updated": "2025-08-04T15:42:53Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    42,
                    53,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:42:53Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    42,
                    53,
                    0,
                    216,
                    0
                ],
                "title": "Communication and Computation Efficient Split Federated Learning in\n  O-RAN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Communication and Computation Efficient Split Federated Learning in\n  O-RAN"
                },
                "summary": "The hierarchical architecture of Open Radio Access Network (O-RAN) has\nenabled a new Federated Learning (FL) paradigm that trains models using data\nfrom non- and near-real-time (near-RT) Radio Intelligent Controllers (RICs).\nHowever, the ever-increasing model size leads to longer training time,\njeopardizing the deadline requirements for both non-RT and near-RT RICs. To\naddress this issue, split federated learning (SFL) offers an approach by\noffloading partial model layers from near-RT-RIC to high-performance\nnon-RT-RIC. Nonetheless, its deployment presents two challenges: (i) Frequent\ndata/gradient transfers between near-RT-RIC and non-RT-RIC in SFL incur\nsignificant communication cost in O-RAN. (ii) Proper allocation of\ncomputational and communication resources in O-RAN is vital to satisfying the\ndeadline and affects SFL convergence. Therefore, we propose SplitMe, an SFL\nframework that exploits mutual learning to alternately and independently train\nthe near-RT-RIC's model and the non-RT-RIC's inverse model, eliminating\nfrequent transfers. The ''inverse'' of the inverse model is derived via a\nzeroth-order technique to integrate the final model. Then, we solve a joint\noptimization problem for SplitMe to minimize overall resource costs with\ndeadline-aware selection of near-RT-RICs and adaptive local updates. Our\nnumerical results demonstrate that SplitMe remarkably outperforms FL frameworks\nlike SFL, FedAvg and O-RANFed regarding costs and convergence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The hierarchical architecture of Open Radio Access Network (O-RAN) has\nenabled a new Federated Learning (FL) paradigm that trains models using data\nfrom non- and near-real-time (near-RT) Radio Intelligent Controllers (RICs).\nHowever, the ever-increasing model size leads to longer training time,\njeopardizing the deadline requirements for both non-RT and near-RT RICs. To\naddress this issue, split federated learning (SFL) offers an approach by\noffloading partial model layers from near-RT-RIC to high-performance\nnon-RT-RIC. Nonetheless, its deployment presents two challenges: (i) Frequent\ndata/gradient transfers between near-RT-RIC and non-RT-RIC in SFL incur\nsignificant communication cost in O-RAN. (ii) Proper allocation of\ncomputational and communication resources in O-RAN is vital to satisfying the\ndeadline and affects SFL convergence. Therefore, we propose SplitMe, an SFL\nframework that exploits mutual learning to alternately and independently train\nthe near-RT-RIC's model and the non-RT-RIC's inverse model, eliminating\nfrequent transfers. The ''inverse'' of the inverse model is derived via a\nzeroth-order technique to integrate the final model. Then, we solve a joint\noptimization problem for SplitMe to minimize overall resource costs with\ndeadline-aware selection of near-RT-RICs and adaptive local updates. Our\nnumerical results demonstrate that SplitMe remarkably outperforms FL frameworks\nlike SFL, FedAvg and O-RANFed regarding costs and convergence."
                },
                "authors": [
                    {
                        "name": "Shunxian Gu"
                    },
                    {
                        "name": "Chaoqun You"
                    },
                    {
                        "name": "Bangbang Ren"
                    },
                    {
                        "name": "Deke Guo"
                    }
                ],
                "author_detail": {
                    "name": "Deke Guo"
                },
                "author": "Deke Guo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02534v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02534v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.18661v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.18661v3",
                "updated": "2025-08-04T15:39:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    39,
                    40,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-23T16:58:44Z",
                "published_parsed": [
                    2025,
                    7,
                    23,
                    16,
                    58,
                    44,
                    2,
                    204,
                    0
                ],
                "title": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by\n  Reinforcement Learning from Visual Map Feed Back",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eyes Will Shut: A Vision-Based Next GPS Location Prediction Model by\n  Reinforcement Learning from Visual Map Feed Back"
                },
                "summary": "Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Next Location Prediction is a fundamental task in the study of human\nmobility, with wide-ranging applications in transportation planning, urban\ngovernance, and epidemic forecasting. In practice, when humans attempt to\npredict the next location in a trajectory, they often visualize the trajectory\non a map and reason based on road connectivity and movement trends. However,\nthe vast majority of existing next-location prediction models do not reason\nover maps \\textbf{in the way that humans do}. Fortunately, the recent\ndevelopment of Vision-Language Models (VLMs) has demonstrated strong\ncapabilities in visual perception and even visual reasoning. This opens up a\nnew possibility: by rendering both the road network and trajectory onto an\nimage and leveraging the reasoning abilities of VLMs, we can enable models to\nperform trajectory inference in a human-like manner. To explore this idea, we\nfirst propose a method called Vision-Guided Location Search (VGLS), which\nevaluates whether a general-purpose VLM is capable of trajectory-based\nreasoning without modifying any of its internal parameters. Based on insights\nfrom the VGLS results, we further propose our main approach: VLMLocPredictor,\nwhich is composed of two stages: In the first stage, we design two Supervised\nFine-Tuning (SFT) tasks that help the VLM understand road network and\ntrajectory structures and acquire basic reasoning ability on such visual\ninputs. In the second stage, we introduce Reinforcement Learning from Visual\nMap Feedback, enabling the model to self-improve its next-location prediction\nability through interaction with the environment. Experiments conducted on\ndatasets from four different cities show that our method achieves\nstate-of-the-art (SOTA) performance and exhibits superior cross-city\ngeneralization compared to other LLM-based approaches."
                },
                "authors": [
                    {
                        "name": "Ruixing Zhang"
                    },
                    {
                        "name": "Yang Zhang"
                    },
                    {
                        "name": "Tongyu Zhu"
                    },
                    {
                        "name": "Leilei Sun"
                    },
                    {
                        "name": "Weifeng Lv"
                    }
                ],
                "author_detail": {
                    "name": "Weifeng Lv"
                },
                "author": "Weifeng Lv",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.18661v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.18661v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02529v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02529v1",
                "updated": "2025-08-04T15:38:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    38,
                    13,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:38:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    38,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Failure-Aware Multi-Robot Coordination for Resilient and Adaptive Target\n  Tracking"
                },
                "summary": "Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-robot coordination is crucial for autonomous systems, yet real-world\ndeployments often encounter various failures. These include both temporary and\npermanent disruptions in sensing and communication, which can significantly\ndegrade system robustness and performance if not explicitly modeled. Despite\nits practical importance, failure-aware coordination remains underexplored in\nthe literature. To bridge the gap between idealized conditions and the\ncomplexities of real-world environments, we propose a unified failure-aware\ncoordination framework designed to enable resilient and adaptive multi-robot\ntarget tracking under both temporary and permanent failure conditions. Our\napproach systematically distinguishes between two classes of failures: (1)\nprobabilistic and temporary disruptions, where robots recover from intermittent\nsensing or communication losses by dynamically adapting paths and avoiding\ninferred danger zones, and (2) permanent failures, where robots lose sensing or\ncommunication capabilities irreversibly, requiring sustained, decentralized\nbehavioral adaptation. To handle these scenarios, the robot team is partitioned\ninto subgroups. Robots that remain connected form a communication group and\ncollaboratively plan using partially centralized nonlinear optimization. Robots\nexperiencing permanent disconnection or failure continue to operate\nindependently through decentralized or individual optimization, allowing them\nto contribute to the task within their local context. We extensively evaluate\nour method across a range of benchmark variations and conduct a comprehensive\nassessment under diverse real-world failure scenarios. Results show that our\nframework consistently achieves robust performance in realistic environments\nwith unknown danger zones, offering a practical and generalizable solution for\nthe multi-robot systems community."
                },
                "authors": [
                    {
                        "name": "Peihan Li"
                    },
                    {
                        "name": "Jiazhen Liu"
                    },
                    {
                        "name": "Yuwei Wu"
                    },
                    {
                        "name": "Gaurav S. Sukhatme"
                    },
                    {
                        "name": "Vijay Kumar"
                    },
                    {
                        "name": "Lifeng Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Lifeng Zhou"
                },
                "author": "Lifeng Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02529v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02529v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02523v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02523v1",
                "updated": "2025-08-04T15:34:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    34,
                    25,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:34:25Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    34,
                    25,
                    0,
                    216,
                    0
                ],
                "title": "Transportation Cyber Incident Awareness through Generative AI-Based\n  Incident Analysis and Retrieval-Augmented Question-Answering Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transportation Cyber Incident Awareness through Generative AI-Based\n  Incident Analysis and Retrieval-Augmented Question-Answering Systems"
                },
                "summary": "Technological advancements have revolutionized numerous industries, including\ntransportation. While digitalization, automation, and connectivity have\nenhanced safety and efficiency, they have also introduced new vulnerabilities.\nWith 95% of data breaches attributed to human error, promoting cybersecurity\nawareness in transportation is increasingly critical. Despite numerous\ncyberattacks on transportation systems worldwide, comprehensive and centralized\nrecords of these incidents remain scarce. To address this gap and enhance cyber\nawareness, this paper presents a large language model (LLM) based approach to\nextract and organize transportation related cyber incidents from publicly\navailable datasets. A key contribution of this work is the use of generative AI\nto transform unstructured, heterogeneous cyber incident data into structured\nformats. Incidents were sourced from the Center for Strategic & International\nStudies (CSIS) List of Significant Cyber Incidents, the University of Maryland\nCyber Events Database (UMCED), the European Repository of Cyber Incidents\n(EuRepoC), the Maritime Cyber Attack Database (MCAD), and the U.S. DOT\nTransportation Cybersecurity and Resiliency (TraCR) Examples of Cyber Attacks\nin Transportation (2018 to 2022). These were classified by a fine tuned LLM\ninto five transportation modes: aviation, maritime, rail, road, and multimodal,\nforming a transportation specific cyber incident database. Another key\ncontribution of this work is the development of a Retrieval Augmented\nGeneration question answering system, designed to enhance accessibility and\npractical use by enabling users to query the curated database for specific\ndetails on transportation related cyber incidents. By leveraging LLMs for both\ndata extraction and user interaction, this study contributes a novel,\naccessible tool for improving cybersecurity awareness in the transportation\nsector.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Technological advancements have revolutionized numerous industries, including\ntransportation. While digitalization, automation, and connectivity have\nenhanced safety and efficiency, they have also introduced new vulnerabilities.\nWith 95% of data breaches attributed to human error, promoting cybersecurity\nawareness in transportation is increasingly critical. Despite numerous\ncyberattacks on transportation systems worldwide, comprehensive and centralized\nrecords of these incidents remain scarce. To address this gap and enhance cyber\nawareness, this paper presents a large language model (LLM) based approach to\nextract and organize transportation related cyber incidents from publicly\navailable datasets. A key contribution of this work is the use of generative AI\nto transform unstructured, heterogeneous cyber incident data into structured\nformats. Incidents were sourced from the Center for Strategic & International\nStudies (CSIS) List of Significant Cyber Incidents, the University of Maryland\nCyber Events Database (UMCED), the European Repository of Cyber Incidents\n(EuRepoC), the Maritime Cyber Attack Database (MCAD), and the U.S. DOT\nTransportation Cybersecurity and Resiliency (TraCR) Examples of Cyber Attacks\nin Transportation (2018 to 2022). These were classified by a fine tuned LLM\ninto five transportation modes: aviation, maritime, rail, road, and multimodal,\nforming a transportation specific cyber incident database. Another key\ncontribution of this work is the development of a Retrieval Augmented\nGeneration question answering system, designed to enhance accessibility and\npractical use by enabling users to query the curated database for specific\ndetails on transportation related cyber incidents. By leveraging LLMs for both\ndata extraction and user interaction, this study contributes a novel,\naccessible tool for improving cybersecurity awareness in the transportation\nsector."
                },
                "authors": [
                    {
                        "name": "Ostonya Thomas"
                    },
                    {
                        "name": "Muhaimin Bin Munir"
                    },
                    {
                        "name": "Jean-Michel Tine"
                    },
                    {
                        "name": "Mizanur Rahman"
                    },
                    {
                        "name": "Yuchen Cai"
                    },
                    {
                        "name": "Khandakar Ashrafi Akbar"
                    },
                    {
                        "name": "Md Nahiyan Uddin"
                    },
                    {
                        "name": "Latifur Khan"
                    },
                    {
                        "name": "Trayce Hockstad"
                    },
                    {
                        "name": "Mashrur Chowdhury"
                    }
                ],
                "author_detail": {
                    "name": "Mashrur Chowdhury"
                },
                "author": "Mashrur Chowdhury",
                "arxiv_comment": "This paper has been submitted to the Transportation Research Board\n  (TRB) for consideration for presentation at the 2026 Annual Meeting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02523v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02523v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02520v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02520v2",
                "updated": "2025-08-05T13:45:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    13,
                    45,
                    27,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T15:30:57Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    30,
                    57,
                    0,
                    216,
                    0
                ],
                "title": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384"
                },
                "summary": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in\nlarge-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in\nrecent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is\nscaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s\nhigh-speed interconnects. Running large MoE models on SuperPod-scale hardware\nbrings new challenges. It requires new execution models, scalable scheduling,\nefficient expert load balancing, and elimination of single points of failure.\nThis paper presents xDeepServe, Huawei Cloud's LLM serving system designed for\nSuperPod-scale infrastructure. At its core is Transformerless, a disaggregated\narchitecture that decomposes transformer models into modular units--attention,\nfeedforward, and MoE--executed independently on NPUs connected via high-speed\nfabric. We implement this design in two forms: disaggregated prefill-decode and\ndisaggregated MoE-attention. This fully disaggregated setup enables independent\nscaling of compute and memory without sacrificing performance. To support this\narchitecture, we propose XCCL, a communication library that leverages\nCloudMatrix384's global shared memory to implement efficient point-to-point and\nall-to-all primitives. We also extend our serving engine FlowServe with\nsystem-level techniques, enabling scalable inference across hundreds of NPUs."
                },
                "authors": [
                    {
                        "name": "Ao Xiao"
                    },
                    {
                        "name": "Bangzheng He"
                    },
                    {
                        "name": "Baoquan Zhang"
                    },
                    {
                        "name": "Baoxing Huai"
                    },
                    {
                        "name": "Bingji Wang"
                    },
                    {
                        "name": "Bo Wang"
                    },
                    {
                        "name": "Bo Xu"
                    },
                    {
                        "name": "Boyi Hou"
                    },
                    {
                        "name": "Chan Yang"
                    },
                    {
                        "name": "Changhong Liu"
                    },
                    {
                        "name": "Cheng Cui"
                    },
                    {
                        "name": "Chenyu Zhu"
                    },
                    {
                        "name": "Cong Feng"
                    },
                    {
                        "name": "Daohui Wang"
                    },
                    {
                        "name": "Dayun Lin"
                    },
                    {
                        "name": "Duo Zhao"
                    },
                    {
                        "name": "Fengshao Zou"
                    },
                    {
                        "name": "Fu Wang"
                    },
                    {
                        "name": "Gangqiang Zhang"
                    },
                    {
                        "name": "Gengyuan Dan"
                    },
                    {
                        "name": "Guanjie Chen"
                    },
                    {
                        "name": "Guodong Guan"
                    },
                    {
                        "name": "Guodong Yang"
                    },
                    {
                        "name": "Haifeng Li"
                    },
                    {
                        "name": "Haipei Zhu"
                    },
                    {
                        "name": "Hao Feng"
                    },
                    {
                        "name": "Hao Huang"
                    },
                    {
                        "name": "Hao Xu"
                    },
                    {
                        "name": "Hengrui Ma"
                    },
                    {
                        "name": "Hengtao Fan"
                    },
                    {
                        "name": "Hui Liu"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jiang Liu"
                    },
                    {
                        "name": "Jiang Xu"
                    },
                    {
                        "name": "Jie Meng"
                    },
                    {
                        "name": "Jinhan Xin"
                    },
                    {
                        "name": "Junhao Hu"
                    },
                    {
                        "name": "Juwei Chen"
                    },
                    {
                        "name": "Lan Yu"
                    },
                    {
                        "name": "Lanxin Miao"
                    },
                    {
                        "name": "Liang Liu"
                    },
                    {
                        "name": "Linan Jing"
                    },
                    {
                        "name": "Lu Zhou"
                    },
                    {
                        "name": "Meina Han"
                    },
                    {
                        "name": "Mingkun Deng"
                    },
                    {
                        "name": "Mingyu Deng"
                    },
                    {
                        "name": "Naitian Deng"
                    },
                    {
                        "name": "Nizhong Lin"
                    },
                    {
                        "name": "Peihan Zhao"
                    },
                    {
                        "name": "Peng Pan"
                    },
                    {
                        "name": "Pengfei Shen"
                    },
                    {
                        "name": "Ping Li"
                    },
                    {
                        "name": "Qi Zhang"
                    },
                    {
                        "name": "Qin Zhang"
                    },
                    {
                        "name": "Qingrong Xia"
                    },
                    {
                        "name": "Qingyi Zhang"
                    },
                    {
                        "name": "Qunchao Fu"
                    },
                    {
                        "name": "Ren Guo"
                    },
                    {
                        "name": "Ruimin Gao"
                    },
                    {
                        "name": "Shaochun Li"
                    },
                    {
                        "name": "Sheng Long"
                    },
                    {
                        "name": "Shentian Li"
                    },
                    {
                        "name": "Shining Wan"
                    },
                    {
                        "name": "Shuai Shen"
                    },
                    {
                        "name": "Shuangfu Zeng"
                    },
                    {
                        "name": "Shuming Jing"
                    },
                    {
                        "name": "Siqi Yang"
                    },
                    {
                        "name": "Song Zhang"
                    },
                    {
                        "name": "Tao Xu"
                    },
                    {
                        "name": "Tianlin Du"
                    },
                    {
                        "name": "Ting Chen"
                    },
                    {
                        "name": "Wanxu Wu"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Weinan Tong"
                    },
                    {
                        "name": "Weiwei Chen"
                    },
                    {
                        "name": "Wen Peng"
                    },
                    {
                        "name": "Wenli Zhou"
                    },
                    {
                        "name": "Wenquan Yang"
                    },
                    {
                        "name": "Wenxin Liang"
                    },
                    {
                        "name": "Xiang Liu"
                    },
                    {
                        "name": "Xiaoli Zhou"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Xinyu Duan"
                    },
                    {
                        "name": "Xu Li"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Xusheng Chen"
                    },
                    {
                        "name": "Yalong Shan"
                    },
                    {
                        "name": "Yang Gan"
                    },
                    {
                        "name": "Yao Lu"
                    },
                    {
                        "name": "Yi Deng"
                    },
                    {
                        "name": "Yi Zheng"
                    },
                    {
                        "name": "Yingfei Zheng"
                    },
                    {
                        "name": "Yiyun Zheng"
                    },
                    {
                        "name": "Yizhou Shan"
                    },
                    {
                        "name": "Yong Gao"
                    },
                    {
                        "name": "Yongqiang Yang"
                    },
                    {
                        "name": "Yuanjin Gong"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Yuetao Chen"
                    },
                    {
                        "name": "Yukun Zhu"
                    },
                    {
                        "name": "Yulong He"
                    },
                    {
                        "name": "Yusu Zhao"
                    },
                    {
                        "name": "Yuyan Wu"
                    },
                    {
                        "name": "Zenan Zhang"
                    },
                    {
                        "name": "Zhaojin Zhuo"
                    },
                    {
                        "name": "Zhaoyang Ji"
                    },
                    {
                        "name": "Zhefeng Wang"
                    },
                    {
                        "name": "Zheng Wang"
                    },
                    {
                        "name": "Zhenhua Yang"
                    },
                    {
                        "name": "Zhenli Sheng"
                    },
                    {
                        "name": "Zhibin Yu"
                    },
                    {
                        "name": "Zhigang Ji"
                    },
                    {
                        "name": "Zhihao Ren"
                    },
                    {
                        "name": "Zhipeng Bian"
                    },
                    {
                        "name": "Zhixia Liu"
                    },
                    {
                        "name": "Zhiyu Dong"
                    },
                    {
                        "name": "Zhonghua Li"
                    },
                    {
                        "name": "Zhou Yu"
                    },
                    {
                        "name": "Zhuoming Shen"
                    },
                    {
                        "name": "Zhuwei Peng"
                    },
                    {
                        "name": "Zi Ye"
                    },
                    {
                        "name": "Zihao Xiang"
                    },
                    {
                        "name": "Zimin Fu"
                    },
                    {
                        "name": "Zixuan Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zixuan Zhang"
                },
                "author": "Zixuan Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02520v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02520v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02518v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02518v1",
                "updated": "2025-08-04T15:25:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    25,
                    48,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:25:48Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    25,
                    48,
                    0,
                    216,
                    0
                ],
                "title": "AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via\n  Multi-modal LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AnalogCoder-Pro: Unifying Analog Circuit Generation and Optimization via\n  Multi-modal LLMs"
                },
                "summary": "Despite advances in analog design automation, analog front-end design still\nheavily depends on expert intuition and iterative simulations, underscoring\ncritical gaps in fully automated optimization for performance-critical\napplications. Recently, the rapid development of Large Language Models (LLMs)\nhas brought new promise to analog design automation. However, existing work\nremains in its early stages, and holistic joint optimization for practical\nend-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a\nunified multimodal LLM-based framework that integrates generative capabilities\nand optimization techniques to jointly explore circuit topologies and optimize\ndevice sizing, automatically generating performance-specific, fully sized\nschematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning\nLLMs on high-quality synthesized circuit data and introduces a multimodal\ndiagnosis and repair workflow based on functional specifications and waveform\nimages. By leveraging LLMs to interpret generated circuit netlists,\nAnalogCoder-Pro automates the extraction of critical design parameters and the\nformulation of parameter spaces, establishing an end-to-end workflow for\nsimultaneous topology generation and device sizing optimization. Extensive\nexperiments demonstrate that these orthogonal approaches significantly improve\nthe success rate of analog circuit design and enhance circuit performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite advances in analog design automation, analog front-end design still\nheavily depends on expert intuition and iterative simulations, underscoring\ncritical gaps in fully automated optimization for performance-critical\napplications. Recently, the rapid development of Large Language Models (LLMs)\nhas brought new promise to analog design automation. However, existing work\nremains in its early stages, and holistic joint optimization for practical\nend-to-end solutions remains largely unexplored. We propose AnalogCoder-Pro, a\nunified multimodal LLM-based framework that integrates generative capabilities\nand optimization techniques to jointly explore circuit topologies and optimize\ndevice sizing, automatically generating performance-specific, fully sized\nschematic netlists. AnalogCoder-Pro employs rejection sampling for fine-tuning\nLLMs on high-quality synthesized circuit data and introduces a multimodal\ndiagnosis and repair workflow based on functional specifications and waveform\nimages. By leveraging LLMs to interpret generated circuit netlists,\nAnalogCoder-Pro automates the extraction of critical design parameters and the\nformulation of parameter spaces, establishing an end-to-end workflow for\nsimultaneous topology generation and device sizing optimization. Extensive\nexperiments demonstrate that these orthogonal approaches significantly improve\nthe success rate of analog circuit design and enhance circuit performance."
                },
                "authors": [
                    {
                        "name": "Yao Lai"
                    },
                    {
                        "name": "Souradip Poddar"
                    },
                    {
                        "name": "Sungyoung Lee"
                    },
                    {
                        "name": "Guojin Chen"
                    },
                    {
                        "name": "Mengkang Hu"
                    },
                    {
                        "name": "Bei Yu"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "David Z. Pan"
                    }
                ],
                "author_detail": {
                    "name": "David Z. Pan"
                },
                "author": "David Z. Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02518v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02518v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02515v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02515v1",
                "updated": "2025-08-04T15:19:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    19,
                    22,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:19:22Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    19,
                    22,
                    0,
                    216,
                    0
                ],
                "title": "PoeTone: A Framework for Constrained Generation of Structured Chinese\n  Songci with LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PoeTone: A Framework for Constrained Generation of Structured Chinese\n  Songci with LLMs"
                },
                "summary": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a systematic investigation into the constrained\ngeneration capabilities of large language models (LLMs) in producing Songci, a\nclassical Chinese poetry form characterized by strict structural, tonal, and\nrhyme constraints defined by Cipai templates. We first develop a comprehensive,\nmulti-faceted evaluation framework that includes: (i) a formal conformity\nscore, (ii) automated quality assessment using LLMs, (iii) human evaluation,\nand (iv) classification-based probing tasks. Using this framework, we evaluate\nthe generative performance of 18 LLMs, including 3 proprietary models and 15\nopen-source models across four families, under five prompting strategies:\nzero-shot, one-shot, completion-based, instruction-tuned, and chain-of-thought.\nFinally, we propose a Generate-Critic architecture in which the evaluation\nframework functions as an automated critic. Leveraging the critic's feedback as\na reward signal, we fine-tune three lightweight open-source LLMs via supervised\nfine-tuning (SFT), resulting in improvements of up to 5.88% in formal\nconformity. Our findings offer new insights into the generative strengths and\nlimitations of LLMs in producing culturally significant and formally\nconstrained literary texts."
                },
                "authors": [
                    {
                        "name": "Zhan Qu"
                    },
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Michael Frber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Frber"
                },
                "author": "Michael Frber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02515v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02515v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02513v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02513v1",
                "updated": "2025-08-04T15:18:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    18,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:18:41Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    18,
                    41,
                    0,
                    216,
                    0
                ],
                "title": "Modular Arithmetic: Language Models Solve Math Digit by Digit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modular Arithmetic: Language Models Solve Math Digit by Digit"
                },
                "summary": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While recent work has begun to uncover the internal strategies that Large\nLanguage Models (LLMs) employ for simple arithmetic tasks, a unified\nunderstanding of their underlying mechanisms is still lacking. We extend recent\nfindings showing that LLMs represent numbers in a digit-wise manner and present\nevidence for the existence of digit-position-specific circuits that LLMs use to\nperform simple arithmetic tasks, i.e. modular subgroups of MLP neurons that\noperate independently on different digit positions (units, tens, hundreds).\nNotably, such circuits exist independently of model size and of tokenization\nstrategy, i.e. both for models that encode longer numbers digit-by-digit and as\none token. Using Feature Importance and Causal Interventions, we identify and\nvalidate the digit-position-specific circuits, revealing a compositional and\ninterpretable structure underlying the solving of arithmetic problems in LLMs.\nOur interventions selectively alter the model's prediction at targeted digit\npositions, demonstrating the causal role of digit-position circuits in solving\narithmetic tasks."
                },
                "authors": [
                    {
                        "name": "Tanja Baeumel"
                    },
                    {
                        "name": "Daniil Gurgurov"
                    },
                    {
                        "name": "Yusser al Ghussin"
                    },
                    {
                        "name": "Josef van Genabith"
                    },
                    {
                        "name": "Simon Ostermann"
                    }
                ],
                "author_detail": {
                    "name": "Simon Ostermann"
                },
                "author": "Simon Ostermann",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02513v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02513v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02511v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02511v1",
                "updated": "2025-08-04T15:17:13Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    17,
                    13,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:17:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    17,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "Test-time Prompt Intervention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time Prompt Intervention"
                },
                "summary": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test-time compute has led to remarkable success in the large language model\n(LLM) community, particularly for complex tasks, where longer chains of thought\n(CoTs) are generated to enhance reasoning capabilities. However, growing\nevidence reveals that such reasoning models often produce CoTs plagued by\nexcessive redundancy, including unnecessary verification steps and repetitive\nreasoning shifts. The root cause lies in post-training of them that overly rely\non outcome reward paradigms, as the data of process reward paradigms, which\nregulate intermediate reasoning steps, is difficult to construct at scale. To\naddress this, we propose PI, a novel framework for Test-time Prompt\nIntervention. PI provides an interface to dynamically guide and regulate\nreasoning paths during inference through timely (When module) and proper (How\nmodule) interventions and post-intervention sampling (Which module). This\nallows human problem-solving expertise and cognitive science principles to be\nseamlessly integrated into LLMs' reasoning processes, enhancing controllability\nand interpretability. Extensive experiments across multiple models and datasets\ndemonstrate that PI significantly shortens CoTs while reducing hallucination,\nyielding more concise and reliable reasoning."
                },
                "authors": [
                    {
                        "name": "Chenxu Yang"
                    },
                    {
                        "name": "Qingyi Si"
                    },
                    {
                        "name": "Mz Dai"
                    },
                    {
                        "name": "Dingyu Yao"
                    },
                    {
                        "name": "Mingyu Zheng"
                    },
                    {
                        "name": "Minghui Chen"
                    },
                    {
                        "name": "Zheng Lin"
                    },
                    {
                        "name": "Weiping Wang"
                    }
                ],
                "author_detail": {
                    "name": "Weiping Wang"
                },
                "author": "Weiping Wang",
                "arxiv_comment": "23 pages, 16 figures, under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02511v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02511v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02506v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02506v1",
                "updated": "2025-08-04T15:14:09Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    14,
                    9,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:14:09Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    14,
                    9,
                    0,
                    216,
                    0
                ],
                "title": "Decomposed Reasoning with Reinforcement Learning for Relevance\n  Assessment in UGC Platforms",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposed Reasoning with Reinforcement Learning for Relevance\n  Assessment in UGC Platforms"
                },
                "summary": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) plays a critical role in user-generated\ncontent (UGC) platforms, but its effectiveness depends heavily on accurate\nrelevance assessment of query-document pairs. Despite recent advances in\napplying large language models (LLMs) to relevance modeling, UGC platforms\npresent unique challenges: 1) ambiguous user intent due to sparse user feedback\nin RAG scenarios, and 2) substantial noise introduced by informal and\nunstructured language. To address these issues, we propose the Reinforced\nReasoning Model for Relevance Assessment (R3A), which introduces a decomposed\nreasoning framework over queries and candidate documents before scoring. R3A\nfirst leverages auxiliary high-ranked documents within the platform to infer\nlatent query intent. It then performs verbatim fragment extraction to justify\nrelevance decisions, thereby reducing errors caused by noisy UGC. Based on a\nreinforcement learning framework, R3A is optimized to mitigate distortions\narising from ambiguous queries and unstructured content. Experimental results\nshow that R3A significantly outperforms existing baseline methods in terms of\nrelevance accuracy, across both offline benchmarks and online experiments."
                },
                "authors": [
                    {
                        "name": "Xiaowei Yuan"
                    },
                    {
                        "name": "Lei Jin"
                    },
                    {
                        "name": "Haoxin Zhang"
                    },
                    {
                        "name": "Yan Gao"
                    },
                    {
                        "name": "Yi Wu"
                    },
                    {
                        "name": "Yao Hu"
                    },
                    {
                        "name": "Ziyang Huang"
                    },
                    {
                        "name": "Jun Zhao"
                    },
                    {
                        "name": "Kang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Kang Liu"
                },
                "author": "Kang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02506v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02506v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02505v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02505v1",
                "updated": "2025-08-04T15:13:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    13,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:13:56Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    13,
                    56,
                    0,
                    216,
                    0
                ],
                "title": "Would you let a humanoid play storytelling with your child? A usability\n  study on LLM-powered narrative Humanoid-Robot Interaction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Would you let a humanoid play storytelling with your child? A usability\n  study on LLM-powered narrative Humanoid-Robot Interaction"
                },
                "summary": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A key challenge in human-robot interaction research lies in developing\nrobotic systems that can effectively perceive and interpret social cues,\nfacilitating natural and adaptive interactions. In this work, we present a\nnovel framework for enhancing the attention of the iCub humanoid robot by\nintegrating advanced perceptual abilities to recognise social cues, understand\nsurroundings through generative models, such as ChatGPT, and respond with\ncontextually appropriate social behaviour. Specifically, we propose an\ninteraction task implementing a narrative protocol (storytelling task) in which\nthe human and the robot create a short imaginary story together, exchanging in\nturn cubes with creative images placed on them. To validate the protocol and\nthe framework, experiments were performed to quantify the degree of usability\nand the quality of experience perceived by participants interacting with the\nsystem. Such a system can be beneficial in promoting effective human robot\ncollaborations, especially in assistance, education and rehabilitation\nscenarios where the social awareness and the robot responsiveness play a\npivotal role."
                },
                "authors": [
                    {
                        "name": "Maria Lombardi"
                    },
                    {
                        "name": "Carmela Calabrese"
                    },
                    {
                        "name": "Davide Ghiglino"
                    },
                    {
                        "name": "Caterina Foglino"
                    },
                    {
                        "name": "Davide De Tommaso"
                    },
                    {
                        "name": "Giulia Da Lisca"
                    },
                    {
                        "name": "Lorenzo Natale"
                    },
                    {
                        "name": "Agnieszka Wykowska"
                    }
                ],
                "author_detail": {
                    "name": "Agnieszka Wykowska"
                },
                "author": "Agnieszka Wykowska",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02505v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02505v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02503v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02503v1",
                "updated": "2025-08-04T15:11:51Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    11,
                    51,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:11:51Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    11,
                    51,
                    0,
                    216,
                    0
                ],
                "title": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical\n  Modeling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OptiHive: Ensemble Selection for LLM-Based Optimization via Statistical\n  Modeling"
                },
                "summary": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based solvers have emerged as a promising means of automating problem\nmodeling and solving. However, they remain unreliable and often depend on\niterative repair loops that result in significant latency. We introduce\nOptiHive, an LLM-based framework that produces high-quality solvers for\noptimization problems from natural-language descriptions without iterative\nself-correction. OptiHive uses a single batched LLM query to generate diverse\ncomponents (solvers, problem instances, and validation tests) and filters out\nerroneous components to ensure fully interpretable outputs. Taking into account\nthe imperfection of the generated components, we employ a statistical model to\ninfer their true performance, enabling principled uncertainty quantification\nand solver selection. On tasks ranging from traditional optimization problems\nto challenging variants of the Multi-Depot Vehicle Routing Problem, OptiHive\nsignificantly outperforms baselines, increasing the optimality rate from 5\\% to\n92\\% on the most complex problems."
                },
                "authors": [
                    {
                        "name": "Maxime Bouscary"
                    },
                    {
                        "name": "Saurabh Amin"
                    }
                ],
                "author_detail": {
                    "name": "Saurabh Amin"
                },
                "author": "Saurabh Amin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02503v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02503v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.03165v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.03165v2",
                "updated": "2025-08-04T15:10:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    10,
                    59,
                    0,
                    216,
                    0
                ],
                "published": "2025-04-04T04:43:13Z",
                "published_parsed": [
                    2025,
                    4,
                    4,
                    4,
                    43,
                    13,
                    4,
                    94,
                    0
                ],
                "title": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Dynamic Clustering-Based Document Compression for\n  Retrieval-Augmented-Generation"
                },
                "summary": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) has emerged as a widely adopted approach\nfor knowledge injection during large language model (LLM) inference in recent\nyears. However, due to their limited ability to exploit fine-grained\ninter-document relationships, current RAG implementations face challenges in\neffectively addressing the retrieved noise and redundancy content, which may\ncause error in the generation results. To address these limitations, we propose\nan Efficient Dynamic Clustering-based document Compression framework (EDC2-RAG)\nthat utilizes latent inter-document relationships while simultaneously removing\nirrelevant information and redundant content. We validate our approach, built\nupon GPT-3.5-Turbo and GPT-4o-mini, on widely used knowledge-QA and\nHallucination-Detection datasets. Experimental results show that our method\nachieves consistent performance improvements across various scenarios and\nexperimental settings, demonstrating strong robustness and applicability. Our\ncode and datasets are available at https://github.com/Tsinghua-dhy/EDC-2-RAG."
                },
                "authors": [
                    {
                        "name": "Weitao Li"
                    },
                    {
                        "name": "Kaiming Liu"
                    },
                    {
                        "name": "Xiangyu Zhang"
                    },
                    {
                        "name": "Xuanyu Lei"
                    },
                    {
                        "name": "Weizhi Ma"
                    },
                    {
                        "name": "Yang Liu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Liu"
                },
                "author": "Yang Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.03165v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.03165v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02502v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02502v1",
                "updated": "2025-08-04T15:10:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    10,
                    44,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:10:44Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    10,
                    44,
                    0,
                    216,
                    0
                ],
                "title": "From Monolingual to Bilingual: Investigating Language Conditioning in\n  Large Language Models for Psycholinguistic Tasks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Monolingual to Bilingual: Investigating Language Conditioning in\n  Large Language Models for Psycholinguistic Tasks"
                },
                "summary": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) exhibit strong linguistic capabilities, but\nlittle is known about how they encode psycholinguistic knowledge across\nlanguages. We investigate whether and how LLMs exhibit human-like\npsycholinguistic responses under different linguistic identities using two\ntasks: sound symbolism and word valence. We evaluate two models,\nLlama-3.3-70B-Instruct and Qwen2.5-72B-Instruct, under monolingual and\nbilingual prompting in English, Dutch, and Chinese. Behaviorally, both models\nadjust their outputs based on prompted language identity, with Qwen showing\ngreater sensitivity and sharper distinctions between Dutch and Chinese. Probing\nanalysis reveals that psycholinguistic signals become more decodable in deeper\nlayers, with Chinese prompts yielding stronger and more stable valence\nrepresentations than Dutch. Our results demonstrate that language identity\nconditions both output behavior and internal representations in LLMs, providing\nnew insights into their application as models of cross-linguistic cognition."
                },
                "authors": [
                    {
                        "name": "Shuzhou Yuan"
                    },
                    {
                        "name": "Zhan Qu"
                    },
                    {
                        "name": "Mario Tawfelis"
                    },
                    {
                        "name": "Michael Frber"
                    }
                ],
                "author_detail": {
                    "name": "Michael Frber"
                },
                "author": "Michael Frber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02502v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02502v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02497v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02497v1",
                "updated": "2025-08-04T15:07:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    7,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:07:35Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    7,
                    35,
                    0,
                    216,
                    0
                ],
                "title": "Bridging Language Gaps in Open-Source Documentation with\n  Large-Language-Model Translation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bridging Language Gaps in Open-Source Documentation with\n  Large-Language-Model Translation"
                },
                "summary": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "While open source communities attract diverse contributors globally, few\nrepositories provide essential documentation in languages other than English.\nLarge language models (LLMs) have demonstrated remarkable capabilities in\nsoftware engineering tasks and translations across domains. However, little is\nknown about LLM capabilities in translating open-source technical\ndocumentation, which mixes natural language, code, URLs, and markdown\nformatting. To understand the need and potential for LLMs in technical\ndocumentation translation, we evaluated community translation activity and\nEnglish-to-German translations of 50 README files using OpenAI's ChatGPT 4 and\nAnthropic's Claude. We found scarce translation activity, mostly in larger\nrepositories and community-driven in nature. LLM performance comparison\nsuggests they can provide accurate translations. However, analysis revealed\nfidelity challenges: both models struggled to preserve structural components\n(e.g., hyperlinks) and exhibited formatting inconsistencies. These findings\nhighlight both promise and challenges of LLM-assisted documentation\ninternationalization. As a first step toward translation-aware continuous\nintegration pipelines, we introduce TRIFID, an early-stage translation fidelity\nscoring framework that automatically checks how well translations preserve\ncode, links, and formatting. Our efforts provide a foundation for automated\nLLM-driven support for creating and maintaining open source documentation."
                },
                "authors": [
                    {
                        "name": "Elijah Kayode Adejumo"
                    },
                    {
                        "name": "Brittany Johnson"
                    },
                    {
                        "name": "Mariam Guizani"
                    }
                ],
                "author_detail": {
                    "name": "Mariam Guizani"
                },
                "author": "Mariam Guizani",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02497v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02497v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02490v1",
                "updated": "2025-08-04T15:01:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    1,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T15:01:41Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    15,
                    1,
                    41,
                    0,
                    216,
                    0
                ],
                "title": "PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic\n  Evaluation of Large Models in Prognostics and Health Management",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PHM-Bench: A Domain-Specific Benchmarking Framework for Systematic\n  Evaluation of Large Models in Prognostics and Health Management"
                },
                "summary": "With the rapid advancement of generative artificial intelligence, large\nlanguage models (LLMs) are increasingly adopted in industrial domains, offering\nnew opportunities for Prognostics and Health Management (PHM). These models\nhelp address challenges such as high development costs, long deployment cycles,\nand limited generalizability. However, despite the growing synergy between PHM\nand LLMs, existing evaluation methodologies often fall short in structural\ncompleteness, dimensional comprehensiveness, and evaluation granularity. This\nhampers the in-depth integration of LLMs into the PHM domain. To address these\nlimitations, this study proposes PHM-Bench, a novel three-dimensional\nevaluation framework for PHM-oriented large models. Grounded in the triadic\nstructure of fundamental capability, core task, and entire lifecycle, PHM-Bench\nis tailored to the unique demands of PHM system engineering. It defines\nmulti-level evaluation metrics spanning knowledge comprehension, algorithmic\ngeneration, and task optimization. These metrics align with typical PHM tasks,\nincluding condition monitoring, fault diagnosis, RUL prediction, and\nmaintenance decision-making. Utilizing both curated case sets and publicly\navailable industrial datasets, our study enables multi-dimensional evaluation\nof general-purpose and domain-specific models across diverse PHM tasks.\nPHM-Bench establishes a methodological foundation for large-scale assessment of\nLLMs in PHM and offers a critical benchmark to guide the transition from\ngeneral-purpose to PHM-specialized models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid advancement of generative artificial intelligence, large\nlanguage models (LLMs) are increasingly adopted in industrial domains, offering\nnew opportunities for Prognostics and Health Management (PHM). These models\nhelp address challenges such as high development costs, long deployment cycles,\nand limited generalizability. However, despite the growing synergy between PHM\nand LLMs, existing evaluation methodologies often fall short in structural\ncompleteness, dimensional comprehensiveness, and evaluation granularity. This\nhampers the in-depth integration of LLMs into the PHM domain. To address these\nlimitations, this study proposes PHM-Bench, a novel three-dimensional\nevaluation framework for PHM-oriented large models. Grounded in the triadic\nstructure of fundamental capability, core task, and entire lifecycle, PHM-Bench\nis tailored to the unique demands of PHM system engineering. It defines\nmulti-level evaluation metrics spanning knowledge comprehension, algorithmic\ngeneration, and task optimization. These metrics align with typical PHM tasks,\nincluding condition monitoring, fault diagnosis, RUL prediction, and\nmaintenance decision-making. Utilizing both curated case sets and publicly\navailable industrial datasets, our study enables multi-dimensional evaluation\nof general-purpose and domain-specific models across diverse PHM tasks.\nPHM-Bench establishes a methodological foundation for large-scale assessment of\nLLMs in PHM and offers a critical benchmark to guide the transition from\ngeneral-purpose to PHM-specialized models."
                },
                "authors": [
                    {
                        "name": "Puyu Yang"
                    },
                    {
                        "name": "Laifa Tao"
                    },
                    {
                        "name": "Zijian Huang"
                    },
                    {
                        "name": "Haifei Liu"
                    },
                    {
                        "name": "Wenyan Cao"
                    },
                    {
                        "name": "Hao Ji"
                    },
                    {
                        "name": "Jianan Qiu"
                    },
                    {
                        "name": "Qixuan Huang"
                    },
                    {
                        "name": "Xuanyuan Su"
                    },
                    {
                        "name": "Yuhang Xie"
                    },
                    {
                        "name": "Jun Zhang"
                    },
                    {
                        "name": "Shangyu Li"
                    },
                    {
                        "name": "Chen Lu"
                    },
                    {
                        "name": "Zhixuan Lian"
                    }
                ],
                "author_detail": {
                    "name": "Zhixuan Lian"
                },
                "author": "Zhixuan Lian",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.20243v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.20243v3",
                "updated": "2025-08-04T14:52:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    52,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-05-26T17:21:26Z",
                "published_parsed": [
                    2025,
                    5,
                    26,
                    17,
                    21,
                    26,
                    0,
                    146,
                    0
                ],
                "title": "It's High Time: A Survey of Temporal Question Answering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It's High Time: A Survey of Temporal Question Answering"
                },
                "summary": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nQuestion Answering (TQA), a research area that focuses on answering questions\ninvolving temporal constraints or context. As the amount of time-stamped\ncontent from sources like news articles, web archives, and knowledge bases\nincreases, systems must address challenges such as detecting temporal intent,\nnormalizing time expressions, ordering events, and reasoning over evolving or\nambiguous facts. We focus on recent advances in TQA enabled by neural\narchitectures, especially transformer-based models and Large Language Models\n(LLMs), highlighting progress in temporal language modeling,\nretrieval-augmented generation (RAG), and temporal reasoning. We also discuss\nbenchmark datasets and evaluation strategies designed to test temporal\nrobustness, recency awareness, and generalization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time plays a critical role in how information is generated, retrieved, and\ninterpreted. In this survey, we provide a comprehensive overview of Temporal\nQuestion Answering (TQA), a research area that focuses on answering questions\ninvolving temporal constraints or context. As the amount of time-stamped\ncontent from sources like news articles, web archives, and knowledge bases\nincreases, systems must address challenges such as detecting temporal intent,\nnormalizing time expressions, ordering events, and reasoning over evolving or\nambiguous facts. We focus on recent advances in TQA enabled by neural\narchitectures, especially transformer-based models and Large Language Models\n(LLMs), highlighting progress in temporal language modeling,\nretrieval-augmented generation (RAG), and temporal reasoning. We also discuss\nbenchmark datasets and evaluation strategies designed to test temporal\nrobustness, recency awareness, and generalization."
                },
                "authors": [
                    {
                        "name": "Bhawna Piryani"
                    },
                    {
                        "name": "Abdelrahman Abdallah"
                    },
                    {
                        "name": "Jamshid Mozafari"
                    },
                    {
                        "name": "Avishek Anand"
                    },
                    {
                        "name": "Adam Jatowt"
                    }
                ],
                "author_detail": {
                    "name": "Adam Jatowt"
                },
                "author": "Adam Jatowt",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.20243v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.20243v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02480v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02480v1",
                "updated": "2025-08-04T14:47:17Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    47,
                    17,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:47:17Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    47,
                    17,
                    0,
                    216,
                    0
                ],
                "title": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MindShot: Multi-Shot Video Reconstruction from fMRI with LLM Decoding"
                },
                "summary": "Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reconstructing dynamic videos from fMRI is important for understanding visual\ncognition and enabling vivid brain-computer interfaces. However, current\nmethods are critically limited to single-shot clips, failing to address the\nmulti-shot nature of real-world experiences. Multi-shot reconstruction faces\nfundamental challenges: fMRI signal mixing across shots, the temporal\nresolution mismatch between fMRI and video obscuring rapid scene changes, and\nthe lack of dedicated multi-shot fMRI-video datasets. To overcome these\nlimitations, we propose a novel divide-and-decode framework for multi-shot fMRI\nvideo reconstruction. Our core innovations are: (1) A shot boundary predictor\nmodule explicitly decomposing mixed fMRI signals into shot-specific segments.\n(2) Generative keyframe captioning using LLMs, which decodes robust textual\ndescriptions from each segment, overcoming temporal blur by leveraging\nhigh-level semantics. (3) Novel large-scale data synthesis (20k samples) from\nexisting datasets. Experimental results demonstrate our framework outperforms\nstate-of-the-art methods in multi-shot reconstruction fidelity. Ablation\nstudies confirm the critical role of fMRI decomposition and semantic\ncaptioning, with decomposition significantly improving decoded caption CLIP\nsimilarity by 71.8%. This work establishes a new paradigm for multi-shot fMRI\nreconstruction, enabling accurate recovery of complex visual narratives through\nexplicit decomposition and semantic prompting."
                },
                "authors": [
                    {
                        "name": "Wenwen Zeng"
                    },
                    {
                        "name": "Yonghuang Wu"
                    },
                    {
                        "name": "Yifan Chen"
                    },
                    {
                        "name": "Xuan Xie"
                    },
                    {
                        "name": "Chengqian Zhao"
                    },
                    {
                        "name": "Feiyu Yin"
                    },
                    {
                        "name": "Guoqing Wu"
                    },
                    {
                        "name": "Jinhua Yu"
                    }
                ],
                "author_detail": {
                    "name": "Jinhua Yu"
                },
                "author": "Jinhua Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02480v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02480v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.11336v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.11336v2",
                "updated": "2025-08-04T14:42:02Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    42,
                    2,
                    0,
                    216,
                    0
                ],
                "published": "2025-05-16T15:02:19Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    15,
                    2,
                    19,
                    4,
                    136,
                    0
                ],
                "title": "XtraGPT: Context-Aware and Controllable Academic Paper Revision via\n  Human-AI Collaboration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XtraGPT: Context-Aware and Controllable Academic Paper Revision via\n  Human-AI Collaboration"
                },
                "summary": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite the growing adoption of large language models (LLMs) in academic\nworkflows, their capabilities remain limited when it comes to supporting\nhigh-quality scientific writing. Most existing systems are designed for\ngeneral-purpose scientific text generation and fail to meet the sophisticated\ndemands of research communication beyond surface-level polishing, such as\nconceptual coherence across sections. Furthermore, academic writing is\ninherently iterative and revision-driven, a process not well supported by\ndirect prompting-based paradigms. To address these scenarios, we propose a\nhuman-AI collaboration framework for academic paper revision. We first\nintroduce a comprehensive dataset of 7,040 research papers from top-tier venues\nannotated with over 140,000 instruction-response pairs that reflect realistic,\nsection-level scientific revisions. Building on the dataset, we develop\nXtraGPT, the first suite of open-source LLMs, designed to provide\ncontext-aware, instruction-guided writing assistance, ranging from 1.5B to 14B\nparameters. Extensive experiments validate that XtraGPT significantly\noutperforms same-scale baselines and approaches the quality of proprietary\nsystems. Both automated preference assessments and human evaluations confirm\nthe effectiveness of our models in improving scientific drafts."
                },
                "authors": [
                    {
                        "name": "Nuo Chen"
                    },
                    {
                        "name": "Andre Lin HuiKai"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Junyi Hou"
                    },
                    {
                        "name": "Zining Zhang"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Xidong Wang"
                    },
                    {
                        "name": "Bingsheng He"
                    }
                ],
                "author_detail": {
                    "name": "Bingsheng He"
                },
                "author": "Bingsheng He",
                "arxiv_comment": "Preprint. The model report is available at\n  https://arxiv.org/abs/2505.11336v1",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.11336v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.11336v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02473v1",
                "updated": "2025-08-04T14:37:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    37,
                    32,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:37:32Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    37,
                    32,
                    0,
                    216,
                    0
                ],
                "title": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human\n  Instructions in IDEs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Efficient and Adaptive Next Edit Suggestion Framework with Zero Human\n  Instructions in IDEs"
                },
                "summary": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code editing, including modifying, refactoring, and maintaining existing\ncode, is the most frequent task in software development and has garnered\nsignificant attention from AI-powered tools. However, existing solutions that\ntranslate explicit natural language instructions into code edits face critical\nlimitations, such as heavy reliance on human instruction input and high\nlatency, which hinder their effective integration into a developer's workflow.\nWe observe that developers' habitual behaviors and coding objectives are often\nreflected in their historical editing patterns, making this data key to\naddressing existing limitations. To leverage these insights, we propose NES\n(Next Edit Suggestion), an LLM-driven code editing framework that delivers an\ninstruction-free and low-latency experience. Built on a dual-model architecture\nand trained with our high-quality SFT and DAPO datasets, NES enhances\nproductivity by understanding developer intent while optimizing inference to\nminimize latency. NES is a scalable, industry-ready solution with a continuous\nTab key interaction workflow, seamlessly adopted by a FinTech company with over\n20,000 developers. Evaluations on real-world datasets show NES achieves 75.6%\nand 81.6% accuracy in two tasks of predicting next edit locations, alongside\n91.36% ES and 27.7% EMR for intent-aligned edits, outperforming SOTA models.\nOur open-sourced SFT and DAPO datasets have been demonstrated to enhance the\nperformance of open-source CodeLLMs. The demonstration of NES is available at\nhttps://youtu.be/yGoyYOe6fbY."
                },
                "authors": [
                    {
                        "name": "Xinfang Chen"
                    },
                    {
                        "name": "Siyang Xiao"
                    },
                    {
                        "name": "Xianying Zhu"
                    },
                    {
                        "name": "Junhong Xie"
                    },
                    {
                        "name": "Ming Liang"
                    },
                    {
                        "name": "Dajun Chen"
                    },
                    {
                        "name": "Wei Jiang"
                    },
                    {
                        "name": "Yong Li"
                    },
                    {
                        "name": "Peng Di"
                    }
                ],
                "author_detail": {
                    "name": "Peng Di"
                },
                "author": "Peng Di",
                "arxiv_comment": "13 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68N30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.3; D.1.2; I.2.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.02662v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.02662v3",
                "updated": "2025-08-04T14:30:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    30,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-03-04T14:25:51Z",
                "published_parsed": [
                    2025,
                    3,
                    4,
                    14,
                    25,
                    51,
                    1,
                    63,
                    0
                ],
                "title": "10K is Enough: An Ultra-Lightweight Binarized Network for Infrared\n  Small-Target Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "10K is Enough: An Ultra-Lightweight Binarized Network for Infrared\n  Small-Target Detection"
                },
                "summary": "The widespread deployment of Infrared Small-Target Detection (IRSTD)\nalgorithms on edge devices necessitates the exploration of model compression\ntechniques. Binarized neural networks (BNNs) are distinguished by their\nexceptional efficiency in model compression. However, the small size of\ninfrared targets introduces stringent precision requirements for the IRSTD\ntask, while the inherent precision loss during binarization presents a\nsignificant challenge. To address this, we propose the Binarized Infrared\nSmall-Target Detection Network (BiisNet), which preserves the core operations\nof binarized convolutions while integrating full-precision features into the\nnetwork's information flow. Specifically, we propose the Dot Binary\nConvolution, which retains fine-grained semantic information in feature maps\nwhile still leveraging the binarized convolution operations. In addition, we\nintroduce a smooth and adaptive Dynamic Softsign function, which provides more\ncomprehensive and progressively finer gradient during backpropagation,\nenhancing model stability and promoting an optimal weight distribution.\nExperimental results demonstrate that BiisNet not only significantly\noutperforms other binary architectures but also has strong competitiveness\namong state-of-the-art full-precision models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The widespread deployment of Infrared Small-Target Detection (IRSTD)\nalgorithms on edge devices necessitates the exploration of model compression\ntechniques. Binarized neural networks (BNNs) are distinguished by their\nexceptional efficiency in model compression. However, the small size of\ninfrared targets introduces stringent precision requirements for the IRSTD\ntask, while the inherent precision loss during binarization presents a\nsignificant challenge. To address this, we propose the Binarized Infrared\nSmall-Target Detection Network (BiisNet), which preserves the core operations\nof binarized convolutions while integrating full-precision features into the\nnetwork's information flow. Specifically, we propose the Dot Binary\nConvolution, which retains fine-grained semantic information in feature maps\nwhile still leveraging the binarized convolution operations. In addition, we\nintroduce a smooth and adaptive Dynamic Softsign function, which provides more\ncomprehensive and progressively finer gradient during backpropagation,\nenhancing model stability and promoting an optimal weight distribution.\nExperimental results demonstrate that BiisNet not only significantly\noutperforms other binary architectures but also has strong competitiveness\namong state-of-the-art full-precision models."
                },
                "authors": [
                    {
                        "name": "Biqiao Xin"
                    },
                    {
                        "name": "Qianchen Mao"
                    },
                    {
                        "name": "Bingshu Wang"
                    },
                    {
                        "name": "Jiangbin Zheng"
                    },
                    {
                        "name": "Yong Zhao"
                    },
                    {
                        "name": "C. L. Philip Chen"
                    }
                ],
                "author_detail": {
                    "name": "C. L. Philip Chen"
                },
                "author": "C. L. Philip Chen",
                "arxiv_comment": "We found the paper has insufficient workload after review. No\n  substitute manuscript can be ready soon. To ensure academic quality, we\n  withdraw it and plan to resubmit when improved",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.02662v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.02662v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.16525v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.16525v4",
                "updated": "2025-08-04T14:28:05Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    28,
                    5,
                    0,
                    216,
                    0
                ],
                "published": "2024-06-24T11:01:43Z",
                "published_parsed": [
                    2024,
                    6,
                    24,
                    11,
                    1,
                    43,
                    0,
                    176,
                    0
                ],
                "title": "Enhancing OOD Detection Using Latent Diffusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing OOD Detection Using Latent Diffusion"
                },
                "summary": "Out-of-distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in real-world scenarios, enabling the identification of\nunknown samples or objects. A prominent approach to enhance OOD detection\nperformance involves leveraging auxiliary datasets for training. Recent efforts\nhave explored using generative models, such as Stable Diffusion (SD), to\nsynthesize outlier data in the pixel space. However, synthesizing OOD data in\nthe pixel space can lead to reduced robustness due to over-generation. To\naddress this challenge, we propose Outlier-Aware Learning (OAL), a novel\nframework that generates synthetic OOD training data within the latent space,\ntaking a further step to study how to utilize Stable Diffusion for developing a\nlatent-based outlier synthesis approach. This improvement facilitates network\ntraining with fewer outliers and less computational cost. Besides, to\nregularize the model's decision boundary, we develop a mutual information-based\ncontrastive learning module (MICL) that amplifies the distinction between\nIn-Distribution (ID) and collected OOD data. Moreover, we develop a knowledge\ndistillation module to prevent the degradation of ID classification accuracy\nwhen training with OOD data. The superior performance of our method on several\nbenchmark datasets demonstrates its efficiency and effectiveness. Source code\nis available in https://github.com/HengGao12/OAL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Out-of-distribution (OOD) detection is crucial for the reliable deployment of\nmachine learning models in real-world scenarios, enabling the identification of\nunknown samples or objects. A prominent approach to enhance OOD detection\nperformance involves leveraging auxiliary datasets for training. Recent efforts\nhave explored using generative models, such as Stable Diffusion (SD), to\nsynthesize outlier data in the pixel space. However, synthesizing OOD data in\nthe pixel space can lead to reduced robustness due to over-generation. To\naddress this challenge, we propose Outlier-Aware Learning (OAL), a novel\nframework that generates synthetic OOD training data within the latent space,\ntaking a further step to study how to utilize Stable Diffusion for developing a\nlatent-based outlier synthesis approach. This improvement facilitates network\ntraining with fewer outliers and less computational cost. Besides, to\nregularize the model's decision boundary, we develop a mutual information-based\ncontrastive learning module (MICL) that amplifies the distinction between\nIn-Distribution (ID) and collected OOD data. Moreover, we develop a knowledge\ndistillation module to prevent the degradation of ID classification accuracy\nwhen training with OOD data. The superior performance of our method on several\nbenchmark datasets demonstrates its efficiency and effectiveness. Source code\nis available in https://github.com/HengGao12/OAL."
                },
                "authors": [
                    {
                        "name": "Heng Gao"
                    },
                    {
                        "name": "Jun Li"
                    }
                ],
                "author_detail": {
                    "name": "Jun Li"
                },
                "author": "Jun Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.16525v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.16525v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02459v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02459v1",
                "updated": "2025-08-04T14:26:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    26,
                    48,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:26:48Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    26,
                    48,
                    0,
                    216,
                    0
                ],
                "title": "Quantum Convolutional Neural Network with Nonlinear Effects and Barren\n  Plateau Mitigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum Convolutional Neural Network with Nonlinear Effects and Barren\n  Plateau Mitigation"
                },
                "summary": "Quantum neural networks (QNNs) leverage quantum entanglement and\nsuperposition to enable large-scale parallel linear computation, offering a\npotential solution to the scalability limits of classical deep learning.\nHowever, their practical deployment is hampered by two key challenges: the lack\nof intrinsic nonlinear operations and the barren plateau phenomenon. We propose\na quantum convolutional neural network (QCNN) architecture that simultaneously\naddresses both issues. Nonlinear effects are introduced via orthonormal basis\nexpansions of power series, while barren plateaus are mitigated by directly\nparameterizing unitary matrices rather than stacking multiple parameterized\ngates. Our design further incorporates quantum analogs of convolutional kernels\nand strides for scalable circuit construction. Experiments on MNIST and\nFashion-MNIST datasets achieve 99.0% and 88.0% test accuracy, respectively.\nConsistency between PyTorch-based matrix simulation and Qiskit-based quantum\ncircuit simulation validates the physical fidelity of the model. These results\ndemonstrate a flexible and effective quantum architecture that faithfully\nintegrates classical convolutional mechanisms into a quantum framework, paving\nthe way for practical and expressive QNNs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantum neural networks (QNNs) leverage quantum entanglement and\nsuperposition to enable large-scale parallel linear computation, offering a\npotential solution to the scalability limits of classical deep learning.\nHowever, their practical deployment is hampered by two key challenges: the lack\nof intrinsic nonlinear operations and the barren plateau phenomenon. We propose\na quantum convolutional neural network (QCNN) architecture that simultaneously\naddresses both issues. Nonlinear effects are introduced via orthonormal basis\nexpansions of power series, while barren plateaus are mitigated by directly\nparameterizing unitary matrices rather than stacking multiple parameterized\ngates. Our design further incorporates quantum analogs of convolutional kernels\nand strides for scalable circuit construction. Experiments on MNIST and\nFashion-MNIST datasets achieve 99.0% and 88.0% test accuracy, respectively.\nConsistency between PyTorch-based matrix simulation and Qiskit-based quantum\ncircuit simulation validates the physical fidelity of the model. These results\ndemonstrate a flexible and effective quantum architecture that faithfully\nintegrates classical convolutional mechanisms into a quantum framework, paving\nthe way for practical and expressive QNNs."
                },
                "authors": [
                    {
                        "name": "Pei-Kun Yang"
                    }
                ],
                "author_detail": {
                    "name": "Pei-Kun Yang"
                },
                "author": "Pei-Kun Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02459v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02459v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02458v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02458v2",
                "updated": "2025-08-05T09:17:57Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    9,
                    17,
                    57,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T14:24:30Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    24,
                    30,
                    0,
                    216,
                    0
                ],
                "title": "From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via\n  Bilateral Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Stimuli to Minds: Enhancing Psychological Reasoning in LLMs via\n  Bilateral Reinforcement Learning"
                },
                "summary": "Large Language Models show promise in emotion understanding, social\nreasoning, and empathy, yet they struggle with psychologically grounded tasks\nthat require inferring implicit mental states in context-rich, ambiguous\nsettings. These limitations arise from the absence of theory-aligned\nsupervision and the difficulty of capturing nuanced mental processes in\nreal-world narratives. To address this gap, we leverage expert-labeled,\npsychologically rich scenarios and propose a trajectory-aware reinforcement\nlearning framework that explicitly imitates expert psychological thought\npatterns. By integrating real-world stimuli with structured reasoning guidance,\nour approach enables compact models to internalize social-cognitive principles,\nperform nuanced psychological inference, and support continual\nself-improvement. Comprehensive experiments across multiple benchmarks further\ndemonstrate that our models achieve expert-level interpretive capabilities,\nexhibiting strong out-of-distribution generalization and robust continual\nlearning across diverse, challenging, and psychologically grounded tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models show promise in emotion understanding, social\nreasoning, and empathy, yet they struggle with psychologically grounded tasks\nthat require inferring implicit mental states in context-rich, ambiguous\nsettings. These limitations arise from the absence of theory-aligned\nsupervision and the difficulty of capturing nuanced mental processes in\nreal-world narratives. To address this gap, we leverage expert-labeled,\npsychologically rich scenarios and propose a trajectory-aware reinforcement\nlearning framework that explicitly imitates expert psychological thought\npatterns. By integrating real-world stimuli with structured reasoning guidance,\nour approach enables compact models to internalize social-cognitive principles,\nperform nuanced psychological inference, and support continual\nself-improvement. Comprehensive experiments across multiple benchmarks further\ndemonstrate that our models achieve expert-level interpretive capabilities,\nexhibiting strong out-of-distribution generalization and robust continual\nlearning across diverse, challenging, and psychologically grounded tasks."
                },
                "authors": [
                    {
                        "name": "Feng Yichao"
                    },
                    {
                        "name": "Haoran Luo"
                    },
                    {
                        "name": "Lang Feng"
                    },
                    {
                        "name": "Shuai Zhao"
                    },
                    {
                        "name": "Anh Tuan Luu"
                    }
                ],
                "author_detail": {
                    "name": "Anh Tuan Luu"
                },
                "author": "Anh Tuan Luu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02458v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02458v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02452v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02452v1",
                "updated": "2025-08-04T14:17:29Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    17,
                    29,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:17:29Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    17,
                    29,
                    0,
                    216,
                    0
                ],
                "title": "LatentPrompt: Optimizing Promts in Latent Space",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LatentPrompt: Optimizing Promts in Latent Space"
                },
                "summary": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances have shown that optimizing prompts for Large Language Models\n(LLMs) can significantly improve task performance, yet many optimization\ntechniques rely on heuristics or manual exploration. We present LatentPrompt, a\nmodel-agnostic framework for prompt optimization that leverages latent semantic\nspace to automatically generate, evaluate, and refine candidate prompts without\nrequiring hand-crafted rules. Beginning with a set of seed prompts, our method\nembeds them in a continuous latent space and systematically explores this space\nto identify prompts that maximize task-specific performance. In a\nproof-of-concept study on the Financial PhraseBank sentiment classification\nbenchmark, LatentPrompt increased classification accuracy by approximately 3\npercent after a single optimization cycle. The framework is broadly applicable,\nrequiring only black-box access to an LLM and an automatic evaluation metric,\nmaking it suitable for diverse domains and tasks."
                },
                "authors": [
                    {
                        "name": "Mateusz Bystroski"
                    },
                    {
                        "name": "Grzegorz Piotrowski"
                    },
                    {
                        "name": "Nitesh V. Chawla"
                    },
                    {
                        "name": "Tomasz Kajdanowicz"
                    }
                ],
                "author_detail": {
                    "name": "Tomasz Kajdanowicz"
                },
                "author": "Tomasz Kajdanowicz",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02452v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02452v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.12601v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.12601v3",
                "updated": "2025-08-04T14:11:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    11,
                    45,
                    0,
                    216,
                    0
                ],
                "published": "2024-10-16T14:21:52Z",
                "published_parsed": [
                    2024,
                    10,
                    16,
                    14,
                    21,
                    52,
                    2,
                    290,
                    0
                ],
                "title": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CCSBench: Evaluating Compositional Controllability in LLMs for\n  Scientific Document Summarization"
                },
                "summary": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To broaden the dissemination of scientific knowledge to diverse audiences, it\nis desirable for scientific document summarization systems to simultaneously\ncontrol multiple attributes such as length and empirical focus. However,\nexisting research typically focuses on controlling single attributes, leaving\nthe compositional control of multiple attributes underexplored. To address this\ngap, we introduce CCSBench, the first evaluation benchmark for compositional\ncontrollable summarization in the scientific domain. Our benchmark enables\nfine-grained control over both explicit attributes (e.g., length), which are\nobjective and straightforward, and implicit attributes (e.g., conceptual or\nempirical focus), which are more subjective and abstract. We conduct extensive\nexperiments using various large language models (LLMs) under various settings,\nincluding in-context learning, parameter-efficient fine-tuning, and two-stage\nmodular methods for balancing control over different attributes. Our findings\nreveal significant limitations in LLMs capabilities in balancing trade-offs\nbetween control attributes, especially implicit ones that require deeper\nunderstanding and abstract reasoning."
                },
                "authors": [
                    {
                        "name": "Yixi Ding"
                    },
                    {
                        "name": "Jiaying Wu"
                    },
                    {
                        "name": "Tongyao Zhu"
                    },
                    {
                        "name": "Yanxia Qin"
                    },
                    {
                        "name": "Qian Liu"
                    },
                    {
                        "name": "Min-Yen Kan"
                    }
                ],
                "author_detail": {
                    "name": "Min-Yen Kan"
                },
                "author": "Min-Yen Kan",
                "arxiv_comment": "Accepted to KDD 2025 SciSoc LLM Workshop: Large Language Models for\n  Scientific and Societal Advances",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.12601v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.12601v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02442v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02442v1",
                "updated": "2025-08-04T14:02:12Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    2,
                    12,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T14:02:12Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    14,
                    2,
                    12,
                    0,
                    216,
                    0
                ],
                "title": "Assessing the Reliability and Validity of Large Language Models for\n  Automated Assessment of Student Essays in Higher Education",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing the Reliability and Validity of Large Language Models for\n  Automated Assessment of Student Essays in Higher Education"
                },
                "summary": "This study investigates the reliability and validity of five advanced Large\nLanguage Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral\n24B, for automated essay scoring in a real world higher education context. A\ntotal of 67 Italian-language student essays, written as part of a university\npsychology course, were evaluated using a four-criterion rubric (Pertinence,\nCoherence, Originality, Feasibility). Each model scored all essays across three\nprompt replications to assess intra-model stability. Human-LLM agreement was\nconsistently low and non-significant (Quadratic Weighted Kappa), and\nwithin-model reliability across replications was similarly weak (median\nKendall's W < 0.30). Systematic scoring divergences emerged, including a\ntendency to inflate Coherence and inconsistent handling of context-dependent\ndimensions. Inter-model agreement analysis revealed moderate convergence for\nCoherence and Originality, but negligible concordance for Pertinence and\nFeasibility. Although limited in scope, these findings suggest that current\nLLMs may struggle to replicate human judgment in tasks requiring disciplinary\ninsight and contextual sensitivity. Human oversight remains critical when\nevaluating open-ended academic work, particularly in interpretive domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study investigates the reliability and validity of five advanced Large\nLanguage Models (LLMs), Claude 3.5, DeepSeek v2, Gemini 2.5, GPT-4, and Mistral\n24B, for automated essay scoring in a real world higher education context. A\ntotal of 67 Italian-language student essays, written as part of a university\npsychology course, were evaluated using a four-criterion rubric (Pertinence,\nCoherence, Originality, Feasibility). Each model scored all essays across three\nprompt replications to assess intra-model stability. Human-LLM agreement was\nconsistently low and non-significant (Quadratic Weighted Kappa), and\nwithin-model reliability across replications was similarly weak (median\nKendall's W < 0.30). Systematic scoring divergences emerged, including a\ntendency to inflate Coherence and inconsistent handling of context-dependent\ndimensions. Inter-model agreement analysis revealed moderate convergence for\nCoherence and Originality, but negligible concordance for Pertinence and\nFeasibility. Although limited in scope, these findings suggest that current\nLLMs may struggle to replicate human judgment in tasks requiring disciplinary\ninsight and contextual sensitivity. Human oversight remains critical when\nevaluating open-ended academic work, particularly in interpretive domains."
                },
                "authors": [
                    {
                        "name": "Andrea Gaggioli"
                    },
                    {
                        "name": "Giuseppe Casaburi"
                    },
                    {
                        "name": "Leonardo Ercolani"
                    },
                    {
                        "name": "Francesco Collova'"
                    },
                    {
                        "name": "Pietro Torre"
                    },
                    {
                        "name": "Fabrizio Davide"
                    }
                ],
                "author_detail": {
                    "name": "Fabrizio Davide"
                },
                "author": "Fabrizio Davide",
                "arxiv_comment": "24 pages (including appendix), 12 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02442v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02442v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15822v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15822v2",
                "updated": "2025-08-04T13:56:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    56,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-21T17:30:16Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    17,
                    30,
                    16,
                    0,
                    202,
                    0
                ],
                "title": "Do AI models help produce verified bug fixes?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do AI models help produce verified bug fixes?"
                },
                "summary": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Among areas of software engineering where AI techniques -- particularly,\nLarge Language Models -- seem poised to yield dramatic improvements, an\nattractive candidate is Automatic Program Repair (APR), the production of\nsatisfactory corrections to software bugs. Does this expectation materialize in\npractice? How do we find out, making sure that proposed corrections actually\nwork? If programmers have access to LLMs, how do they actually use them to\ncomplement their own skills?\n  To answer these questions, we took advantage of the availability of a\nprogram-proving environment, which formally determines the correctness of\nproposed fixes, to conduct a study of program debugging with two randomly\nassigned groups of programmers, one with access to LLMs and the other without,\nboth validating their answers through the proof tools. The methodology relied\non a division into general research questions (Goals in the Goal-Query-Metric\napproach), specific elements admitting specific answers (Queries), and\nmeasurements supporting these answers (Metrics). While applied so far to a\nlimited sample size, the results are a first step towards delineating a proper\nrole for AI and LLMs in providing guaranteed-correct fixes to program bugs.\n  These results caused surprise as compared to what one might expect from the\nuse of AI for debugging and APR. The contributions also include: a detailed\nmethodology for experiments in the use of LLMs for debugging, which other\nprojects can reuse; a fine-grain analysis of programmer behavior, made possible\nby the use of full-session recording; a definition of patterns of use of LLMs,\nwith 7 distinct categories; and validated advice for getting the best of LLMs\nfor debugging and Automatic Program Repair."
                },
                "authors": [
                    {
                        "name": "Li Huang"
                    },
                    {
                        "name": "Ilgiz Mustafin"
                    },
                    {
                        "name": "Marco Piccioni"
                    },
                    {
                        "name": "Alessandro Schena"
                    },
                    {
                        "name": "Reto Weber"
                    },
                    {
                        "name": "Bertrand Meyer"
                    }
                ],
                "author_detail": {
                    "name": "Bertrand Meyer"
                },
                "author": "Bertrand Meyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15822v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15822v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.18937v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.18937v2",
                "updated": "2025-08-04T13:54:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    54,
                    40,
                    0,
                    216,
                    0
                ],
                "published": "2024-05-29T09:43:48Z",
                "published_parsed": [
                    2024,
                    5,
                    29,
                    9,
                    43,
                    48,
                    2,
                    150,
                    0
                ],
                "title": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kestrel: 3D Multimodal LLM for Part-Aware Grounded Description"
                },
                "summary": "In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a\nchallenging task aimed at advancing 3D multimodal learning for fine-grained,\npart-aware segmentation grounding and detailed explanation of 3D objects.\nExisting 3D datasets largely focus on either vision-only part segmentation or\nvision-language scene segmentation, lacking the fine-grained multimodal\nsegmentation needed for robotic navigation and interaction in real-world\nenvironments. To address this gap, we present the 3DCoMPaT Grounded\nInstructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich\npoint cloud descriptions with corresponding part-level segmentation masks. This\ndataset encompasses extensive samples designed for both PaPGD and fine-grained\nsingle-part grounding tasks. To tackle the inherent challenges of grounding\nobjects and generating grounded descriptions at the part level, we propose\nKestrel, a part-aware 3D multimodal large language model that integrates an\nadvanced language model for nuanced language comprehension with multi-level\npoint feature propagation and query refinement mechanism to enhance spatial\nreasoning at the part level. The extensive experiments demonstrate that Kestrel\neffectively bridges the gap between part-aware language understanding and 3D\nsegmentation grounding, paving the way for more robust and interpretable 3D\nobject comprehension that meets the demands of real-world robotic applications.\nProject page at https://feielysia.github.io/Kestrel.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce Part-Aware Point Grounded Description (PaPGD), a\nchallenging task aimed at advancing 3D multimodal learning for fine-grained,\npart-aware segmentation grounding and detailed explanation of 3D objects.\nExisting 3D datasets largely focus on either vision-only part segmentation or\nvision-language scene segmentation, lacking the fine-grained multimodal\nsegmentation needed for robotic navigation and interaction in real-world\nenvironments. To address this gap, we present the 3DCoMPaT Grounded\nInstructions (3DCoMPaT-GrIn) Dataset, a comprehensive resource that pairs rich\npoint cloud descriptions with corresponding part-level segmentation masks. This\ndataset encompasses extensive samples designed for both PaPGD and fine-grained\nsingle-part grounding tasks. To tackle the inherent challenges of grounding\nobjects and generating grounded descriptions at the part level, we propose\nKestrel, a part-aware 3D multimodal large language model that integrates an\nadvanced language model for nuanced language comprehension with multi-level\npoint feature propagation and query refinement mechanism to enhance spatial\nreasoning at the part level. The extensive experiments demonstrate that Kestrel\neffectively bridges the gap between part-aware language understanding and 3D\nsegmentation grounding, paving the way for more robust and interpretable 3D\nobject comprehension that meets the demands of real-world robotic applications.\nProject page at https://feielysia.github.io/Kestrel.github.io/"
                },
                "authors": [
                    {
                        "name": "Mahmoud Ahmed"
                    },
                    {
                        "name": "Junjie Fei"
                    },
                    {
                        "name": "Jian Ding"
                    },
                    {
                        "name": "Eslam Mohamed Bakr"
                    },
                    {
                        "name": "Mohamed Elhoseiny"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed Elhoseiny"
                },
                "author": "Mohamed Elhoseiny",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.18937v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.18937v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02435v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02435v1",
                "updated": "2025-08-04T13:50:44Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    50,
                    44,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:50:44Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    50,
                    44,
                    0,
                    216,
                    0
                ],
                "title": "Beyond Chunks and Graphs: Retrieval-Augmented Generation through\n  Triplet-Driven Thinking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Chunks and Graphs: Retrieval-Augmented Generation through\n  Triplet-Driven Thinking"
                },
                "summary": "Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-augmented generation (RAG) is critical for reducing hallucinations\nand incorporating external knowledge into Large Language Models (LLMs).\nHowever, advanced RAG systems face a trade-off between performance and\nefficiency. Multi-round RAG approaches achieve strong reasoning but incur\nexcessive LLM calls and token costs, while Graph RAG methods suffer from\ncomputationally expensive, error-prone graph construction and retrieval\nredundancy. To address these challenges, we propose T$^2$RAG, a novel framework\nthat operates on a simple, graph-free knowledge base of atomic triplets.\nT$^2$RAG leverages an LLM to decompose questions into searchable triplets with\nplaceholders, which it then iteratively resolves by retrieving evidence from\nthe triplet database. Empirical results show that T$^2$RAG significantly\noutperforms state-of-the-art multi-round and Graph RAG methods, achieving an\naverage performance gain of up to 11\\% across six datasets while reducing\nretrieval costs by up to 45\\%. Our code is available at\nhttps://github.com/rockcor/T2RAG"
                },
                "authors": [
                    {
                        "name": "Shengbo Gong"
                    },
                    {
                        "name": "Xianfeng Tang"
                    },
                    {
                        "name": "Carl Yang"
                    },
                    {
                        "name": "Wei jin"
                    }
                ],
                "author_detail": {
                    "name": "Wei jin"
                },
                "author": "Wei jin",
                "arxiv_comment": "19 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02435v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02435v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02430v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02430v1",
                "updated": "2025-08-04T13:49:30Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    49,
                    30,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:49:30Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    49,
                    30,
                    0,
                    216,
                    0
                ],
                "title": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI-Based Measurement of Innovation: Mapping Expert Insight into Large\n  Language Model Applications"
                },
                "summary": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring innovation often relies on context-specific proxies and on expert\nevaluation. Hence, empirical innovation research is often limited to settings\nwhere such data is available. We investigate how large language models (LLMs)\ncan be leveraged to overcome the constraints of manual expert evaluations and\nassist researchers in measuring innovation. We design an LLM framework that\nreliably approximates domain experts' assessment of innovation from\nunstructured text data. We demonstrate the performance and broad applicability\nof this framework through two studies in different contexts: (1) the\ninnovativeness of software application updates and (2) the originality of\nuser-generated feedback and improvement ideas in product reviews. We compared\nthe performance (F1-score) and reliability (consistency rate) of our LLM\nframework against alternative measures used in prior innovation studies, and to\nstate-of-the-art machine learning- and deep learning-based models. The LLM\nframework achieved higher F1-scores than the other approaches, and its results\nare highly consistent (i.e., results do not change across runs). This article\nequips R&D personnel in firms, as well as researchers, reviewers, and editors,\nwith the knowledge and tools to effectively use LLMs for measuring innovation\nand evaluating the performance of LLM-based innovation measures. In doing so,\nwe discuss, the impact of important design decisions-including model selection,\nprompt engineering, training data size, training data distribution, and\nparameter settings-on performance and reliability. Given the challenges\ninherent in using human expert evaluation and existing text-based measures, our\nframework has important implications for harnessing LLMs as reliable,\nincreasingly accessible, and broadly applicable research tools for measuring\ninnovation."
                },
                "authors": [
                    {
                        "name": "Robin Nowak"
                    },
                    {
                        "name": "Patrick Figge"
                    },
                    {
                        "name": "Carolin Haeussler"
                    }
                ],
                "author_detail": {
                    "name": "Carolin Haeussler"
                },
                "author": "Carolin Haeussler",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02430v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02430v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02427v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02427v1",
                "updated": "2025-08-04T13:48:32Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    48,
                    32,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:48:32Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    48,
                    32,
                    0,
                    216,
                    0
                ],
                "title": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through\n  Composing Ready-to-Use Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CABENCH: Benchmarking Composable AI for Solving Complex Tasks through\n  Composing Ready-to-Use Models"
                },
                "summary": "Composable AI offers a scalable and effective paradigm for tackling complex\nAI tasks by decomposing them into sub-tasks and solving each sub-task using\nready-to-use well-trained models. However, systematically evaluating methods\nunder this setting remains largely unexplored. In this paper, we introduce\nCABENCH, the first public benchmark comprising 70 realistic composable AI\ntasks, along with a curated pool of 700 models across multiple modalities and\ndomains. We also propose an evaluation framework to enable end-to-end\nassessment of composable AI solutions. To establish initial baselines, we\nprovide human-designed reference solutions and compare their performance with\ntwo LLM-based approaches. Our results illustrate the promise of composable AI\nin addressing complex real-world problems while highlighting the need for\nmethods that can fully unlock its potential by automatically generating\neffective execution pipelines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Composable AI offers a scalable and effective paradigm for tackling complex\nAI tasks by decomposing them into sub-tasks and solving each sub-task using\nready-to-use well-trained models. However, systematically evaluating methods\nunder this setting remains largely unexplored. In this paper, we introduce\nCABENCH, the first public benchmark comprising 70 realistic composable AI\ntasks, along with a curated pool of 700 models across multiple modalities and\ndomains. We also propose an evaluation framework to enable end-to-end\nassessment of composable AI solutions. To establish initial baselines, we\nprovide human-designed reference solutions and compare their performance with\ntwo LLM-based approaches. Our results illustrate the promise of composable AI\nin addressing complex real-world problems while highlighting the need for\nmethods that can fully unlock its potential by automatically generating\neffective execution pipelines."
                },
                "authors": [
                    {
                        "name": "Tung-Thuy Pham"
                    },
                    {
                        "name": "Duy-Quan Luong"
                    },
                    {
                        "name": "Minh-Quan Duong"
                    },
                    {
                        "name": "Trung-Hieu Nguyen"
                    },
                    {
                        "name": "Thu-Trang Nguyen"
                    },
                    {
                        "name": "Son Nguyen"
                    },
                    {
                        "name": "Hieu Dinh Vo"
                    }
                ],
                "author_detail": {
                    "name": "Hieu Dinh Vo"
                },
                "author": "Hieu Dinh Vo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02427v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02427v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.01281v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.01281v4",
                "updated": "2025-08-04T13:46:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    46,
                    22,
                    0,
                    216,
                    0
                ],
                "published": "2024-11-02T15:23:28Z",
                "published_parsed": [
                    2024,
                    11,
                    2,
                    15,
                    23,
                    28,
                    5,
                    307,
                    0
                ],
                "title": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Arena-Lite: Efficient and Reliable Large Language Model Evaluation via\n  Tournament-Based Direct Comparisons"
                },
                "summary": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) expand across domains, LLM judges have become\nessential for systems evaluation. Current benchmarks typically compare system\noutputs against baselines. This baseline-mediated approach, though convenient,\nyields lower reliability than direct comparison between systems. We propose\nArena-Lite which integrates tournament structure on top of head-to-head\ncomparison. The application of a tournament structure and direct comparison\neliminates the need for baseline outputs, reduces the number of required\ncomparisons, and allows higher reliability in system rankings. We conducted two\nexperiments: (1) controlled stochastic modeling and (2) empirical validation\nwith a real LLM judge. Those experiments collectively demonstrate that\nArena-Lite consistently achieves higher reliability with fewer comparisons,\neven with smaller datasets or weaker judges. We release an easy-to-use web\ndemonstration and code to foster adoption of Arena-Lite, streamlining model\nselection across research and industry communities. Arena-Lite demo and code\nare available on\n\\href{https://huggingface.co/spaces/NCSOFT/ArenaLite}{https://huggingface.co/spaces/NCSOFT/ArenaLite}"
                },
                "authors": [
                    {
                        "name": "Seonil Son"
                    },
                    {
                        "name": "Ju-Min Oh"
                    },
                    {
                        "name": "Heegon Jin"
                    },
                    {
                        "name": "Cheolhun Jang"
                    },
                    {
                        "name": "Jeongbeom Jeong"
                    },
                    {
                        "name": "Kuntae Kim"
                    }
                ],
                "author_detail": {
                    "name": "Kuntae Kim"
                },
                "author": "Kuntae Kim",
                "arxiv_comment": "8 pages for main body, 19 pages in total",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.01281v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.01281v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.16792v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.16792v2",
                "updated": "2025-08-04T13:42:52Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    42,
                    52,
                    0,
                    216,
                    0
                ],
                "published": "2025-06-20T07:16:47Z",
                "published_parsed": [
                    2025,
                    6,
                    20,
                    7,
                    16,
                    47,
                    4,
                    171,
                    0
                ],
                "title": "MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MIST: Jailbreaking Black-box Large Language Models via Iterative\n  Semantic Tuning"
                },
                "summary": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks -- methods\ndesigned to elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version -- order-determining optimization. We\nconduct extensive experiments on two datasets using two open-source and four\nclosed-source models. Results show that MIST achieves competitive attack\nsuccess rate, relatively low query count, and fair transferability,\noutperforming or matching state-of-the-art jailbreak methods. Additionally, we\nconduct analysis on computational efficiency to validate the practical\nviability of MIST.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite efforts to align large language models (LLMs) with societal and moral\nvalues, these models remain susceptible to jailbreak attacks -- methods\ndesigned to elicit harmful responses. Jailbreaking black-box LLMs is considered\nchallenging due to the discrete nature of token inputs, restricted access to\nthe target LLM, and limited query budget. To address the issues above, we\npropose an effective method for jailbreaking black-box large language Models\nvia Iterative Semantic Tuning, named MIST. MIST enables attackers to\niteratively refine prompts that preserve the original semantic intent while\ninducing harmful content. Specifically, to balance semantic similarity with\ncomputational efficiency, MIST incorporates two key strategies: sequential\nsynonym search, and its advanced version -- order-determining optimization. We\nconduct extensive experiments on two datasets using two open-source and four\nclosed-source models. Results show that MIST achieves competitive attack\nsuccess rate, relatively low query count, and fair transferability,\noutperforming or matching state-of-the-art jailbreak methods. Additionally, we\nconduct analysis on computational efficiency to validate the practical\nviability of MIST."
                },
                "authors": [
                    {
                        "name": "Muyang Zheng"
                    },
                    {
                        "name": "Yuanzhi Yao"
                    },
                    {
                        "name": "Changting Lin"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Caihong Kai"
                    }
                ],
                "author_detail": {
                    "name": "Caihong Kai"
                },
                "author": "Caihong Kai",
                "arxiv_comment": "14 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.16792v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.16792v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.05108v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.05108v4",
                "updated": "2025-08-04T13:41:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    41,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2025-04-07T14:14:15Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    14,
                    14,
                    15,
                    0,
                    97,
                    0
                ],
                "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement\n  Learning"
                },
                "summary": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non combinatorial optimization tasks demonstrate that integrating RL with\nevolutionary search accelerates the discovery of superior algorithms,\nshowcasing the potential of RL-enhanced evolutionary strategies for algorithm\ndesign.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Discovering efficient algorithms for solving complex problems has been an\noutstanding challenge in mathematics and computer science, requiring\nsubstantial human expertise over the years. Recent advancements in evolutionary\nsearch with large language models (LLMs) have shown promise in accelerating the\ndiscovery of algorithms across various domains, particularly in mathematics and\noptimization. However, existing approaches treat the LLM as a static generator,\nmissing the opportunity to update the model with the signal obtained from\nevolutionary exploration. In this work, we propose to augment LLM-based\nevolutionary search by continuously refining the search operator - the LLM -\nthrough reinforcement learning (RL) fine-tuning. Our method leverages\nevolutionary search as an exploration strategy to discover improved algorithms,\nwhile RL optimizes the LLM policy based on these discoveries. Our experiments\non combinatorial optimization tasks demonstrate that integrating RL with\nevolutionary search accelerates the discovery of superior algorithms,\nshowcasing the potential of RL-enhanced evolutionary strategies for algorithm\ndesign."
                },
                "authors": [
                    {
                        "name": "Anja Surina"
                    },
                    {
                        "name": "Amin Mansouri"
                    },
                    {
                        "name": "Lars Quaedvlieg"
                    },
                    {
                        "name": "Amal Seddas"
                    },
                    {
                        "name": "Maryna Viazovska"
                    },
                    {
                        "name": "Emmanuel Abbe"
                    },
                    {
                        "name": "Caglar Gulcehre"
                    }
                ],
                "author_detail": {
                    "name": "Caglar Gulcehre"
                },
                "author": "Caglar Gulcehre",
                "arxiv_comment": "34 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.05108v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.05108v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02419v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02419v1",
                "updated": "2025-08-04T13:40:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    40,
                    59,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:40:59Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    40,
                    59,
                    0,
                    216,
                    0
                ],
                "title": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination\n  via Attention Lens",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modality Bias in LVLMs: Analyzing and Mitigating Object Hallucination\n  via Attention Lens"
                },
                "summary": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large vision-language models (LVLMs) have demonstrated remarkable multimodal\ncomprehension and reasoning capabilities, but they still suffer from severe\nobject hallucination. Previous studies primarily attribute the flaw to\nlinguistic prior caused by the scale mismatch between visual encoders and large\nlanguage models (LLMs) in LVLMs. Specifically, as current LVLMs are built upon\nLLMs, they tend to over-rely on textual prompts and internal knowledge of LLMs,\ngenerating descriptions inconsistent with visual cues. However, through an\nin-depth investigation of the hallucinated mechanisms, we empirically reveal a\npreviously overlooked phenomenon: LVLMs may ignore not only visual information\nbut also textual modality during hallucination, a behavior termed as modality\nbias, which indicates that LVLMs struggle to simultaneously attend to both\nvisual and textual modalities, leading to fragmented understanding of\nuser-provided instructions. Based on this observation, we propose a simple yet\neffective training-free method to mitigate object hallucination. Concretely, we\nintervene and adjust the attention weights of textual and visual tokens,\nbalancing cross-modal compatibility for better alignment with user intentions.\nFurthermore, we adopt a contrastive decoding strategy to reduce the LVLM's\noverreliance on its parametric knowledge, synergistically enhancing our\nattention manipulation. Extensive experiments confirm the widespread presence\nof modality bias in LVLMs. Notably, our method effectively mitigates\nhallucination across multiple open-source LVLMs and benchmarks, highlighting\nits generalizability and efficacy."
                },
                "authors": [
                    {
                        "name": "Haohan Zheng"
                    },
                    {
                        "name": "Zhenguo Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Zhenguo Zhang"
                },
                "author": "Zhenguo Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02419v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02419v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02401v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02401v1",
                "updated": "2025-08-04T13:26:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:26:16Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    26,
                    16,
                    0,
                    216,
                    0
                ],
                "title": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CompressKV: Semantic Retrieval Heads Know What Tokens are Not Important\n  Before Generation"
                },
                "summary": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have significantly boosted\nlong-context processing. However, the increasing key-value (KV) cache size\nposes critical challenges to memory and execution efficiency. Most KV cache\ncompression methods rely on heuristic token eviction using all attention heads\nin Grouped Query Attention (GQA)-based LLMs. This method ignores the different\nfunctionalities of attention heads, leading to the eviction of critical tokens\nand thus degrades the performance of LLMs.\n  To address the issue above, instead of using all the attention heads in\nGQA-based LLMs to determine important tokens as in the previous work, we first\nidentify the attention heads in each layer that are not only capable of\nretrieving the initial and final tokens of a prompt, but also capable of\nretrieving important tokens within the text and attending to their surrounding\nsemantic context. Afterwards, we exploit such heads to determine the important\ntokens and retain their corresponding KV cache pairs. Furthermore, we analyze\nthe cache eviction error of each layer individually and introduce a\nlayer-adaptive KV cache allocation strategy. Experimental results demonstrate\nthe proposed CompressKV consistently outperforms state-of-the-art approaches\nunder various memory budgets on LongBench and Needle-in-a-Haystack benchmarks.\nOur code is publicly available at: https://github.com/TUDa-HWAI/CompressKV.git."
                },
                "authors": [
                    {
                        "name": "Xiaolin Lin"
                    },
                    {
                        "name": "Jingcun Wang"
                    },
                    {
                        "name": "Olga Kondrateva"
                    },
                    {
                        "name": "Yiyu Shi"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Grace Li Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Grace Li Zhang"
                },
                "author": "Grace Li Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02401v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02401v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02381v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02381v1",
                "updated": "2025-08-04T13:08:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    8,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T13:08:35Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    8,
                    35,
                    0,
                    216,
                    0
                ],
                "title": "Beyond Manually Designed Pruning Policies with Second-Level Performance\n  Prediction: A Pruning Framework for LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Manually Designed Pruning Policies with Second-Level Performance\n  Prediction: A Pruning Framework for LLMs"
                },
                "summary": "Non-uniform structured network pruning methods can effectively reduce Large\nLanguage Model (LLM) size by eliminating redundant channels or layers, offering\nlower performance degradation than uniform strategies. However, existing\nnon-uniform methods rely heavily on manually designed pruning policies (e.g.,\nlayer importance and scaling factors), and therefore cannot efficiently adapt\nto scenarios with dynamic pruning ratio requirements. Additionly, a critical\nbottleneck -- the time-consuming evaluation of pruning policies -- further\nlimits the feasibility of iteratively and dynamically finding optimal pruning\npolicies. To address these limitations, we propose PPF (Predictive Pruning\nFramework), a novel pruning framework for LLMs that eliminates manual design\ndependencies via second-level performance prediction. PPF not only supports\nreal-time pruning decisions under dynamic pruning ratios but is also applicable\nto static pruning scenarios. It employs an agent for producing adaptive and\nreal-time pruning actions, while a lightweight performance predictor that can\nevaluate a pruning policy in seconds, significantly speeding up the iterative\noptimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can\ngenerate dynamic/static pruning policies and it reduces perplexity by up to\n33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,\noutperforming manually designed pruning policies. The performance predictor\nachieves second-level performance prediction with high accuracy (prediction\nerror < 0.0011). It reduces the mean evaluation latency from minute-level (1\nminute and 38.02 seconds of test-set evaluation methods) to second-level (1.52\nsecond), achieving over 64 times speedup. Our code will be available at\nhttps://github.com/Ma-zx/PPF .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Non-uniform structured network pruning methods can effectively reduce Large\nLanguage Model (LLM) size by eliminating redundant channels or layers, offering\nlower performance degradation than uniform strategies. However, existing\nnon-uniform methods rely heavily on manually designed pruning policies (e.g.,\nlayer importance and scaling factors), and therefore cannot efficiently adapt\nto scenarios with dynamic pruning ratio requirements. Additionly, a critical\nbottleneck -- the time-consuming evaluation of pruning policies -- further\nlimits the feasibility of iteratively and dynamically finding optimal pruning\npolicies. To address these limitations, we propose PPF (Predictive Pruning\nFramework), a novel pruning framework for LLMs that eliminates manual design\ndependencies via second-level performance prediction. PPF not only supports\nreal-time pruning decisions under dynamic pruning ratios but is also applicable\nto static pruning scenarios. It employs an agent for producing adaptive and\nreal-time pruning actions, while a lightweight performance predictor that can\nevaluate a pruning policy in seconds, significantly speeding up the iterative\noptimization process. Experiments on Llama2-7B and Llama3-8B show that PPF can\ngenerate dynamic/static pruning policies and it reduces perplexity by up to\n33.4% (dynamic pruning) and 84.78% (static pruning) over existing methods,\noutperforming manually designed pruning policies. The performance predictor\nachieves second-level performance prediction with high accuracy (prediction\nerror < 0.0011). It reduces the mean evaluation latency from minute-level (1\nminute and 38.02 seconds of test-set evaluation methods) to second-level (1.52\nsecond), achieving over 64 times speedup. Our code will be available at\nhttps://github.com/Ma-zx/PPF ."
                },
                "authors": [
                    {
                        "name": "Zuxin Ma"
                    },
                    {
                        "name": "Yunhe Cui"
                    },
                    {
                        "name": "Yongbin Qin"
                    }
                ],
                "author_detail": {
                    "name": "Yongbin Qin"
                },
                "author": "Yongbin Qin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02381v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02381v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.20957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.20957v2",
                "updated": "2025-08-04T13:06:03Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    13,
                    6,
                    3,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-28T16:09:38Z",
                "published_parsed": [
                    2025,
                    7,
                    28,
                    16,
                    9,
                    38,
                    0,
                    209,
                    0
                ],
                "title": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Your AI, Not Your View: The Bias of LLMs in Investment Analysis"
                },
                "summary": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract the latent preferences of models and\nmeasure their persistence. Focusing on sector, size, and momentum, our analysis\nreveals distinct, model-specific tendencies. In particular, we observe a\nconsistent preference for large-cap stocks and contrarian strategies across\nmost models. These preferences often harden into confirmation bias, with models\nclinging to initial judgments despite counter-evidence.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In finance, Large Language Models (LLMs) face frequent knowledge conflicts\ndue to discrepancies between pre-trained parametric knowledge and real-time\nmarket data. These conflicts become particularly problematic when LLMs are\ndeployed in real-world investment services, where misalignment between a\nmodel's embedded preferences and those of the financial institution can lead to\nunreliable recommendations. Yet little research has examined what investment\nviews LLMs actually hold. We propose an experimental framework to investigate\nsuch conflicts, offering the first quantitative analysis of confirmation bias\nin LLM-based investment analysis. Using hypothetical scenarios with balanced\nand imbalanced arguments, we extract the latent preferences of models and\nmeasure their persistence. Focusing on sector, size, and momentum, our analysis\nreveals distinct, model-specific tendencies. In particular, we observe a\nconsistent preference for large-cap stocks and contrarian strategies across\nmost models. These preferences often harden into confirmation bias, with models\nclinging to initial judgments despite counter-evidence."
                },
                "authors": [
                    {
                        "name": "Hoyoung Lee"
                    },
                    {
                        "name": "Junhyuk Seo"
                    },
                    {
                        "name": "Suhwan Park"
                    },
                    {
                        "name": "Junhyeong Lee"
                    },
                    {
                        "name": "Wonbin Ahn"
                    },
                    {
                        "name": "Chanyeol Choi"
                    },
                    {
                        "name": "Alejandro Lopez-Lira"
                    },
                    {
                        "name": "Yongjae Lee"
                    }
                ],
                "author_detail": {
                    "name": "Yongjae Lee"
                },
                "author": "Yongjae Lee",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.20957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.20957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-fin.PM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-fin.PM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.05386v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.05386v2",
                "updated": "2025-08-04T12:55:00Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    55,
                    0,
                    0,
                    216,
                    0
                ],
                "published": "2025-06-03T12:59:52Z",
                "published_parsed": [
                    2025,
                    6,
                    3,
                    12,
                    59,
                    52,
                    1,
                    154,
                    0
                ],
                "title": "Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for\n  Clinical Notes",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leaps Beyond the Seen: Reinforced Reasoning Augmented Generation for\n  Clinical Notes"
                },
                "summary": "Clinical note generation aims to produce free-text summaries of a patient's\ncondition and diagnostic process, with discharge instructions being a\nrepresentative long-form example. While recent LLM-based methods pre-trained on\ngeneral clinical corpora show promise in clinical text generation, they fall\nshort in producing long-form notes from limited patient information. In this\npaper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG)\nfor long-form discharge instructions based on pre-admission information.\nReinRAG retrieves reasoning paths from a medical knowledge graph to provide\nexplicit semantic guidance to the LLM. To bridge the information gap, we\npropose group-based retriever optimization (GRO) which improves retrieval\nquality with group-normalized rewards, encouraging reasoning leaps for deeper\ninference by the LLM. Comprehensive experiments on the real-world dataset show\nthat ReinRAG outperforms baselines in both clinical efficacy and natural\nlanguage generation metrics. Further analysis reveals that ReinRAG fills\nsemantic gaps in sparse input scenarios, and retrieved reasoning paths help\nLLMs avoid clinical misinterpretation by focusing on key evidence and following\ncoherent reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical note generation aims to produce free-text summaries of a patient's\ncondition and diagnostic process, with discharge instructions being a\nrepresentative long-form example. While recent LLM-based methods pre-trained on\ngeneral clinical corpora show promise in clinical text generation, they fall\nshort in producing long-form notes from limited patient information. In this\npaper, we propose ReinRAG, a reinforced reasoning augmented generation (RAG)\nfor long-form discharge instructions based on pre-admission information.\nReinRAG retrieves reasoning paths from a medical knowledge graph to provide\nexplicit semantic guidance to the LLM. To bridge the information gap, we\npropose group-based retriever optimization (GRO) which improves retrieval\nquality with group-normalized rewards, encouraging reasoning leaps for deeper\ninference by the LLM. Comprehensive experiments on the real-world dataset show\nthat ReinRAG outperforms baselines in both clinical efficacy and natural\nlanguage generation metrics. Further analysis reveals that ReinRAG fills\nsemantic gaps in sparse input scenarios, and retrieved reasoning paths help\nLLMs avoid clinical misinterpretation by focusing on key evidence and following\ncoherent reasoning."
                },
                "authors": [
                    {
                        "name": "Lo Pang-Yun Ting"
                    },
                    {
                        "name": "Chengshuai Zhao"
                    },
                    {
                        "name": "Yu-Hua Zeng"
                    },
                    {
                        "name": "Yuan Jee Lim"
                    },
                    {
                        "name": "Kun-Ta Chuang"
                    },
                    {
                        "name": "Huan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Huan Liu"
                },
                "author": "Huan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.05386v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.05386v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02366v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02366v1",
                "updated": "2025-08-04T12:52:11Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    52,
                    11,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:52:11Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    52,
                    11,
                    0,
                    216,
                    0
                ],
                "title": "Language Model Guided Reinforcement Learning in Quantitative Trading",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language Model Guided Reinforcement Learning in Quantitative Trading"
                },
                "summary": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Algorithmic trading requires short-term decisions aligned with long-term\nfinancial goals. While reinforcement learning (RL) has been explored for such\ntactical decisions, its adoption remains limited by myopic behavior and opaque\npolicy rationale. In contrast, large language models (LLMs) have recently\ndemonstrated strategic reasoning and multi-modal financial signal\ninterpretation when guided by well-designed prompts.\n  We propose a hybrid system where LLMs generate high-level trading strategies\nto guide RL agents in their actions. We evaluate (i) the rationale of\nLLM-generated strategies via expert review, and (ii) the Sharpe Ratio (SR) and\nMaximum Drawdown (MDD) of LLM-guided agents versus unguided baselines. Results\nshow improved return and risk metrics over standard RL."
                },
                "authors": [
                    {
                        "name": "Adam Darmanin"
                    },
                    {
                        "name": "Vince Vella"
                    }
                ],
                "author_detail": {
                    "name": "Vince Vella"
                },
                "author": "Vince Vella",
                "arxiv_comment": "12 pages (4 pages appendix and references), 6 figures, preprint under\n  review for FLLM 2025 conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02366v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02366v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-fin.TR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.6; J.4",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.15113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.15113v2",
                "updated": "2025-08-04T12:51:56Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    51,
                    56,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-19T17:55:42Z",
                "published_parsed": [
                    2024,
                    12,
                    19,
                    17,
                    55,
                    42,
                    3,
                    354,
                    0
                ],
                "title": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Associative memory inspires improvements for in-context learning using a\n  novel attention residual stream architecture"
                },
                "summary": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million and 1\nbillion parameters, focusing on attention head values, with results also\nindicating improved performance at these larger and more naturalistic scales.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) demonstrate an impressive ability to utilise\ninformation within the context of their input sequences to appropriately\nrespond to data unseen by the LLM during its training procedure. This ability\nis known as in-context learning (ICL). Humans and non-human animals demonstrate\nsimilar abilities, however their neural architectures differ substantially from\nLLMs. Despite this, a critical component within LLMs, the attention mechanism,\nresembles modern associative memory models, widely used in and influenced by\nthe computational neuroscience community to model biological memory systems.\nUsing this connection, we introduce an associative memory model capable of\nperforming ICL. We use this as inspiration for a novel residual stream\narchitecture which allows information to directly flow between attention heads.\nWe test this architecture during training within a two-layer Transformer and\nshow its ICL abilities manifest more quickly than without this modification. We\nthen apply our architecture in small language models with 8 million and 1\nbillion parameters, focusing on attention head values, with results also\nindicating improved performance at these larger and more naturalistic scales."
                },
                "authors": [
                    {
                        "name": "Thomas F Burns"
                    },
                    {
                        "name": "Tomoki Fukai"
                    },
                    {
                        "name": "Christopher J Earls"
                    }
                ],
                "author_detail": {
                    "name": "Christopher J Earls"
                },
                "author": "Christopher J Earls",
                "arxiv_comment": "35 pages, 14 figures, 6 tables; accepted and published in TMLR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.15113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.15113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "92B20, 68T01, 68T37, 68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5; I.7; J.2; J.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02359v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02359v1",
                "updated": "2025-08-04T12:48:59Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    48,
                    59,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:48:59Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    48,
                    59,
                    0,
                    216,
                    0
                ],
                "title": "Toward a reliable PWM-based light-emitting diode visual stimulus for\n  improved SSVEP response with minimal visual fatigue",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Toward a reliable PWM-based light-emitting diode visual stimulus for\n  improved SSVEP response with minimal visual fatigue"
                },
                "summary": "Steady state visual evoked response (SSVEP) is widely used in visual-based\ndiagnosis and applications such as brain computer interfacing due to its high\ninformation transfer rate and the capability to activate commands through\nsimple gaze control. However, one major impediment in using flashing visual\nstimulus to obtain SSVEP is eye fatigue that prevents continued long term use\npreventing practical deployment. This combined with the difficulty in\nestablishing precise pulse-width modulation (PWM) that results in poorer\naccuracy warrants the development of appropriate approach to solve these\nissues. Various studies have suggested the usage of high frequencies of visual\nstimulus to reduce the visual fatigue for the user but this results in poor\nresponse performance. Here, the authors study the use of extremely high\nduty-cycles in the stimulus in the hope of solving these constraints.\nElectroencephalogram data was recorded with PWM duty-cycles of 50 to 95%\ngenerated by a precise custom-made light-emitting diode hardware and tested ten\nsubjects responded that increasing duty-cycles had less visual strain for all\nthe frequency values and the SSVEP exhibited a subject-independent peak\nresponse for duty-cycle of 85%. This could pave the way for increased usage of\nSSVEP for practical applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Steady state visual evoked response (SSVEP) is widely used in visual-based\ndiagnosis and applications such as brain computer interfacing due to its high\ninformation transfer rate and the capability to activate commands through\nsimple gaze control. However, one major impediment in using flashing visual\nstimulus to obtain SSVEP is eye fatigue that prevents continued long term use\npreventing practical deployment. This combined with the difficulty in\nestablishing precise pulse-width modulation (PWM) that results in poorer\naccuracy warrants the development of appropriate approach to solve these\nissues. Various studies have suggested the usage of high frequencies of visual\nstimulus to reduce the visual fatigue for the user but this results in poor\nresponse performance. Here, the authors study the use of extremely high\nduty-cycles in the stimulus in the hope of solving these constraints.\nElectroencephalogram data was recorded with PWM duty-cycles of 50 to 95%\ngenerated by a precise custom-made light-emitting diode hardware and tested ten\nsubjects responded that increasing duty-cycles had less visual strain for all\nthe frequency values and the SSVEP exhibited a subject-independent peak\nresponse for duty-cycle of 85%. This could pave the way for increased usage of\nSSVEP for practical applications."
                },
                "authors": [
                    {
                        "name": "Surej Mouli"
                    },
                    {
                        "name": "Ramaswamy Palaniappan"
                    }
                ],
                "author_detail": {
                    "name": "Ramaswamy Palaniappan"
                },
                "author": "Ramaswamy Palaniappan",
                "arxiv_doi": "10.1049/joe.2016.0314",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1049/joe.2016.0314",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.02359v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02359v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "The Journal of Engineering (JoE) 2017",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02344v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02344v1",
                "updated": "2025-08-04T12:25:19Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    25,
                    19,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:25:19Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    25,
                    19,
                    0,
                    216,
                    0
                ],
                "title": "Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal\n  Control Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic-R1: Reinforced LLMs Bring Human-Like Reasoning to Traffic Signal\n  Control Systems"
                },
                "summary": "Traffic signal control (TSC) is vital for mitigating congestion and\nsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation\nmodel with human-like reasoning for TSC systems. Our model is developed through\nself-exploration and iteration of reinforced large language models (LLMs) with\nexpert guidance in a simulated traffic environment. Compared to traditional\nreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers\nthree significant advantages. First, Traffic-R1 delivers zero-shot\ngeneralisation, transferring unchanged to new road networks and\nout-of-distribution incidents by utilizing its internal traffic control\npolicies and human-like reasoning. Second, its 3B-parameter architecture is\nlightweight enough for real-time inference on mobile-class chips, enabling\nlarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSC\nprocess and facilitates multi-intersection communication through its\nself-iteration and a new synchronous communication network. Extensive\nbenchmarks demonstrate that Traffic-R1 sets a new state of the art,\noutperforming strong baselines and training-intensive RL controllers. In\npractice, the model now manages signals for more than 55,000 drivers daily,\nshortening average queues by over 5% and halving operator workload. Our\ncheckpoint is available at https://huggingface.co/Season998/Traffic-R1.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traffic signal control (TSC) is vital for mitigating congestion and\nsustaining urban mobility. In this paper, we introduce Traffic-R1, a foundation\nmodel with human-like reasoning for TSC systems. Our model is developed through\nself-exploration and iteration of reinforced large language models (LLMs) with\nexpert guidance in a simulated traffic environment. Compared to traditional\nreinforcement learning (RL) and recent LLM-based methods, Traffic-R1 offers\nthree significant advantages. First, Traffic-R1 delivers zero-shot\ngeneralisation, transferring unchanged to new road networks and\nout-of-distribution incidents by utilizing its internal traffic control\npolicies and human-like reasoning. Second, its 3B-parameter architecture is\nlightweight enough for real-time inference on mobile-class chips, enabling\nlarge-scale edge deployment. Third, Traffic-R1 provides an explainable TSC\nprocess and facilitates multi-intersection communication through its\nself-iteration and a new synchronous communication network. Extensive\nbenchmarks demonstrate that Traffic-R1 sets a new state of the art,\noutperforming strong baselines and training-intensive RL controllers. In\npractice, the model now manages signals for more than 55,000 drivers daily,\nshortening average queues by over 5% and halving operator workload. Our\ncheckpoint is available at https://huggingface.co/Season998/Traffic-R1."
                },
                "authors": [
                    {
                        "name": "Xingchen Zou"
                    },
                    {
                        "name": "Yuhao Yang"
                    },
                    {
                        "name": "Zheng Chen"
                    },
                    {
                        "name": "Xixuan Hao"
                    },
                    {
                        "name": "Yiqi Chen"
                    },
                    {
                        "name": "Chao Huang"
                    },
                    {
                        "name": "Yuxuan Liang"
                    }
                ],
                "author_detail": {
                    "name": "Yuxuan Liang"
                },
                "author": "Yuxuan Liang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02344v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02344v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02343v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02343v1",
                "updated": "2025-08-04T12:22:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    22,
                    39,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:22:39Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    22,
                    39,
                    0,
                    216,
                    0
                ],
                "title": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling\n  Formats for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MicroMix: Efficient Mixed-Precision Quantization with Microscaling\n  Formats for Large Language Models"
                },
                "summary": "Quantization significantly accelerates inference in large language models\n(LLMs) by replacing original high-precision matrices with low-precision\ncounterparts. Recent advances in weight-activation quantization have primarily\nfocused on mapping both weights and activations to the INT4 format. Although\nthe new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x\nspeedup over FP16, existing INT4-based kernels fail to fully exploit this\ncapability due to mismatched data formats. To bridge this gap, we propose\nMicroMix, a co-designed mixed-precision quantization algorithm and matrix\nmultiplication kernel based on Microscaling (MX) data formats. Tailored for the\nBlackwell architecture, the MicroMix kernel supports arbitrary combinations of\nMXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a\nfavorable trade-off between accuracy and efficiency for each linear layer, we\nintroduce quantization thresholds that identify activation elements where\nlower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.\nOur algorithm selectively allocates higher-precision channels to preserve\naccuracy while maintaining compute efficiency. MicroMix achieves competitive or\nsuperior performance across diverse downstream tasks, including zero-shot and\nfew-shot learning, language modeling, code generation, and mathematical\nreasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX\n5090) GPUs, our kernel delivers at least 20% faster execution than\nTensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,\nMicroMix consistently improves prefill latency and memory efficiency across a\nrange of batch sizes compared to TensorRT baselines. Our code is available at\nhttps://github.com/lwy2020/MicroMix.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization significantly accelerates inference in large language models\n(LLMs) by replacing original high-precision matrices with low-precision\ncounterparts. Recent advances in weight-activation quantization have primarily\nfocused on mapping both weights and activations to the INT4 format. Although\nthe new FP4 Tensor Cores in NVIDIA's Blackwell architecture offer up to 4x\nspeedup over FP16, existing INT4-based kernels fail to fully exploit this\ncapability due to mismatched data formats. To bridge this gap, we propose\nMicroMix, a co-designed mixed-precision quantization algorithm and matrix\nmultiplication kernel based on Microscaling (MX) data formats. Tailored for the\nBlackwell architecture, the MicroMix kernel supports arbitrary combinations of\nMXFP4, MXFP6, and MXFP8 channels, and produces BFloat16 outputs. To achieve a\nfavorable trade-off between accuracy and efficiency for each linear layer, we\nintroduce quantization thresholds that identify activation elements where\nlower-precision formats (MXFP4 or MXFP6) incur excessive quantization error.\nOur algorithm selectively allocates higher-precision channels to preserve\naccuracy while maintaining compute efficiency. MicroMix achieves competitive or\nsuperior performance across diverse downstream tasks, including zero-shot and\nfew-shot learning, language modeling, code generation, and mathematical\nreasoning. On both consumer-grade (RTX 5070Ti laptop) and server-grade (RTX\n5090) GPUs, our kernel delivers at least 20% faster execution than\nTensorRT-FP8. Furthermore, when applied to various Llama and Qwen models,\nMicroMix consistently improves prefill latency and memory efficiency across a\nrange of batch sizes compared to TensorRT baselines. Our code is available at\nhttps://github.com/lwy2020/MicroMix."
                },
                "authors": [
                    {
                        "name": "Wenyuan Liu"
                    },
                    {
                        "name": "Haoqian Meng"
                    },
                    {
                        "name": "Yilun Luo"
                    },
                    {
                        "name": "Peng Zhang"
                    },
                    {
                        "name": "Xindian Ma"
                    }
                ],
                "author_detail": {
                    "name": "Xindian Ma"
                },
                "author": "Xindian Ma",
                "arxiv_comment": "12 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02343v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02343v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02342v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02342v1",
                "updated": "2025-08-04T12:22:25Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    22,
                    25,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T12:22:25Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    22,
                    25,
                    0,
                    216,
                    0
                ],
                "title": "Agentic Personalized Fashion Recommendation in the Age of Generative AI:\n  Challenges, Opportunities, and Evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Agentic Personalized Fashion Recommendation in the Age of Generative AI:\n  Challenges, Opportunities, and Evaluation"
                },
                "summary": "Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fashion recommender systems (FaRS) face distinct challenges due to rapid\ntrend shifts, nuanced user preferences, intricate item-item compatibility, and\nthe complex interplay among consumers, brands, and influencers. Traditional\nrecommendation approaches, largely static and retrieval-focused, struggle to\neffectively capture these dynamic elements, leading to decreased user\nsatisfaction and elevated return rates. This paper synthesizes both academic\nand industrial viewpoints to map the distinctive output space and stakeholder\necosystem of modern FaRS, identifying the complex interplay among users,\nbrands, platforms, and influencers, and highlighting the unique data and\nmodeling challenges that arise.\n  We outline a research agenda for industrial FaRS, centered on five\nrepresentative scenarios spanning static queries, outfit composition, and\nmulti-turn dialogue, and argue that mixed-modality refinement-the ability to\ncombine image-based references (anchors) with nuanced textual constraints-is a\nparticularly critical task for real-world deployment. To this end, we propose\nan Agentic Mixed-Modality Refinement (AMMR) pipeline, which fuses multimodal\nencoders with agentic LLM planners and dynamic retrieval, bridging the gap\nbetween expressive user intent and fast-changing fashion inventories. Our work\nshows that moving beyond static retrieval toward adaptive, generative, and\nstakeholder-aware systems is essential to satisfy the evolving expectations of\nfashion consumers and brands."
                },
                "authors": [
                    {
                        "name": "Yashar Deldjoo"
                    },
                    {
                        "name": "Nima Rafiee"
                    },
                    {
                        "name": "Mahdyar Ravanbakhsh"
                    }
                ],
                "author_detail": {
                    "name": "Mahdyar Ravanbakhsh"
                },
                "author": "Mahdyar Ravanbakhsh",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02342v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02342v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19214v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19214v2",
                "updated": "2025-08-04T12:11:37Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    12,
                    11,
                    37,
                    0,
                    216,
                    0
                ],
                "published": "2025-02-26T15:15:01Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    15,
                    15,
                    1,
                    2,
                    57,
                    0
                ],
                "title": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Hybrid Transformer Architecture with a Quantized Self-Attention\n  Mechanism Applied to Molecular Generation"
                },
                "summary": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of the self-attention mechanism in classical machine learning\nmodels has inspired the development of quantum analogs aimed at reducing\ncomputational overhead. Self-attention integrates learnable query and key\nmatrices to calculate attention scores between all pairs of tokens in a\nsequence. These scores are then multiplied by a learnable value matrix to\nobtain the output self-attention matrix, enabling the model to effectively\ncapture long-range dependencies within the input sequence. Here, we propose a\nhybrid quantum-classical self-attention mechanism as part of a transformer\ndecoder, the architecture underlying large language models (LLMs). To\ndemonstrate its utility in chemistry, we train this model on the QM9 dataset\nfor conditional generation, using SMILES strings as input, each labeled with a\nset of physicochemical properties that serve as conditions during inference.\nOur theoretical analysis shows that the time complexity of the query-key dot\nproduct is reduced from $\\mathcal{O}(n^2 d)$ in a classical model to\n$\\mathcal{O}(n^2\\log d)$ in our quantum model, where $n$ and $d$ represent the\nsequence length and embedding dimension, respectively. We perform simulations\nusing NVIDIA's CUDA-Q platform, which is designed for efficient GPU\nscalability. This work provides a promising avenue for quantum-enhanced natural\nlanguage processing (NLP)."
                },
                "authors": [
                    {
                        "name": "Anthony M. Smaldone"
                    },
                    {
                        "name": "Yu Shee"
                    },
                    {
                        "name": "Gregory W. Kyro"
                    },
                    {
                        "name": "Marwa H. Farag"
                    },
                    {
                        "name": "Zohim Chandani"
                    },
                    {
                        "name": "Elica Kyoseva"
                    },
                    {
                        "name": "Victor S. Batista"
                    }
                ],
                "author_detail": {
                    "name": "Victor S. Batista"
                },
                "author": "Victor S. Batista",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19214v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19214v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00103v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00103v2",
                "updated": "2025-08-04T11:52:16Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    52,
                    16,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-31T18:56:01Z",
                "published_parsed": [
                    2025,
                    7,
                    31,
                    18,
                    56,
                    1,
                    3,
                    212,
                    0
                ],
                "title": "A Mixed User-Centered Approach to Enable Augmented Intelligence in\n  Intelligent Tutoring Systems: The Case of MathAIde app",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Mixed User-Centered Approach to Enable Augmented Intelligence in\n  Intelligent Tutoring Systems: The Case of MathAIde app"
                },
                "summary": "Integrating Artificial Intelligence in Education (AIED) aims to enhance\nlearning experiences through technologies like Intelligent Tutoring Systems\n(ITS), offering personalized learning, increased engagement, and improved\nretention rates. However, AIED faces three main challenges: the critical role\nof teachers in the design process, the limitations and reliability of AI tools,\nand the accessibility of technological resources. Augmented Intelligence (AuI)\naddresses these challenges by enhancing human capabilities rather than\nreplacing them, allowing systems to suggest solutions. In contrast, humans\nprovide final assessments, thus improving AI over time. In this sense, this\nstudy focuses on designing, developing, and evaluating MathAIde, an ITS that\ncorrects mathematics exercises using computer vision and AI and provides\nfeedback based on photos of student work. The methodology included\nbrainstorming sessions with potential users, high-fidelity prototyping, A/B\ntesting, and a case study involving real-world classroom environments for\nteachers and students. Our research identified several design possibilities for\nimplementing AuI in ITSs, emphasizing a balance between user needs and\ntechnological feasibility. Prioritization and validation through prototyping\nand testing highlighted the importance of efficiency metrics, ultimately\nleading to a solution that offers pre-defined remediation alternatives for\nteachers. Real-world deployment demonstrated the usefulness of the proposed\nsolution. Our research contributes to the literature by providing a usable,\nteacher-centered design approach that involves teachers in all design phases.\nAs a practical implication, we highlight that the user-centered design approach\nincreases the usefulness and adoption potential of AIED systems, especially in\nresource-limited environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating Artificial Intelligence in Education (AIED) aims to enhance\nlearning experiences through technologies like Intelligent Tutoring Systems\n(ITS), offering personalized learning, increased engagement, and improved\nretention rates. However, AIED faces three main challenges: the critical role\nof teachers in the design process, the limitations and reliability of AI tools,\nand the accessibility of technological resources. Augmented Intelligence (AuI)\naddresses these challenges by enhancing human capabilities rather than\nreplacing them, allowing systems to suggest solutions. In contrast, humans\nprovide final assessments, thus improving AI over time. In this sense, this\nstudy focuses on designing, developing, and evaluating MathAIde, an ITS that\ncorrects mathematics exercises using computer vision and AI and provides\nfeedback based on photos of student work. The methodology included\nbrainstorming sessions with potential users, high-fidelity prototyping, A/B\ntesting, and a case study involving real-world classroom environments for\nteachers and students. Our research identified several design possibilities for\nimplementing AuI in ITSs, emphasizing a balance between user needs and\ntechnological feasibility. Prioritization and validation through prototyping\nand testing highlighted the importance of efficiency metrics, ultimately\nleading to a solution that offers pre-defined remediation alternatives for\nteachers. Real-world deployment demonstrated the usefulness of the proposed\nsolution. Our research contributes to the literature by providing a usable,\nteacher-centered design approach that involves teachers in all design phases.\nAs a practical implication, we highlight that the user-centered design approach\nincreases the usefulness and adoption potential of AIED systems, especially in\nresource-limited environments."
                },
                "authors": [
                    {
                        "name": "Guilherme Guerino"
                    },
                    {
                        "name": "Luiz Rodrigues"
                    },
                    {
                        "name": "Luana Bianchini"
                    },
                    {
                        "name": "Mariana Alves"
                    },
                    {
                        "name": "Marcelo Marinho"
                    },
                    {
                        "name": "Thomaz Veloso"
                    },
                    {
                        "name": "Valmir Macario"
                    },
                    {
                        "name": "Diego Dermeval"
                    },
                    {
                        "name": "Thales Vieira"
                    },
                    {
                        "name": "Ig Bittencourt"
                    },
                    {
                        "name": "Seiji Isotani"
                    }
                ],
                "author_detail": {
                    "name": "Seiji Isotani"
                },
                "author": "Seiji Isotani",
                "arxiv_comment": "Article accepted in the International Journal of Human-Computer\n  Interaction",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00103v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00103v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T01",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.0; I.2.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.15322v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.15322v3",
                "updated": "2025-08-04T11:48:41Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    48,
                    41,
                    0,
                    216,
                    0
                ],
                "published": "2024-10-20T07:32:16Z",
                "published_parsed": [
                    2024,
                    10,
                    20,
                    7,
                    32,
                    16,
                    6,
                    294,
                    0
                ],
                "title": "UoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion\n  Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UoMo: A Foundation Model for Mobile Traffic Forecasting with Diffusion\n  Model"
                },
                "summary": "Mobile traffic forecasting allows operators to anticipate network dynamics\nand performance in advance, offering substantial potential for enhancing\nservice quality and improving user experience. However, existing models are\noften task-oriented and are trained with tailored data, which limits their\neffectiveness in diverse mobile network tasks of Base Station (BS) deployment,\nresource allocation, energy optimization, etc. and hinders generalization\nacross different urban environments. Foundation models have made remarkable\nstrides across various domains of NLP and CV due to their multi-tasking\nadaption and zero/few-shot learning capabilities. In this paper, we propose an\ninnovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to\nhandle diverse forecasting tasks of short/long-term predictions and\ndistribution generation across multiple cities to support network planning and\noptimization. FoMo combines diffusion models and transformers, where various\nspatio-temporal masks are proposed to enable FoMo to learn intrinsic features\nof different tasks, and a contrastive learning strategy is developed to capture\nthe correlations between mobile traffic and urban contexts, thereby improving\nits transfer learning capability. Extensive experiments on 9 real-world\ndatasets demonstrate that FoMo outperforms current models concerning diverse\nforecasting tasks and zero/few-shot learning, showcasing a strong universality.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile traffic forecasting allows operators to anticipate network dynamics\nand performance in advance, offering substantial potential for enhancing\nservice quality and improving user experience. However, existing models are\noften task-oriented and are trained with tailored data, which limits their\neffectiveness in diverse mobile network tasks of Base Station (BS) deployment,\nresource allocation, energy optimization, etc. and hinders generalization\nacross different urban environments. Foundation models have made remarkable\nstrides across various domains of NLP and CV due to their multi-tasking\nadaption and zero/few-shot learning capabilities. In this paper, we propose an\ninnovative Foundation model for Mo}bile traffic forecasting (FoMo), aiming to\nhandle diverse forecasting tasks of short/long-term predictions and\ndistribution generation across multiple cities to support network planning and\noptimization. FoMo combines diffusion models and transformers, where various\nspatio-temporal masks are proposed to enable FoMo to learn intrinsic features\nof different tasks, and a contrastive learning strategy is developed to capture\nthe correlations between mobile traffic and urban contexts, thereby improving\nits transfer learning capability. Extensive experiments on 9 real-world\ndatasets demonstrate that FoMo outperforms current models concerning diverse\nforecasting tasks and zero/few-shot learning, showcasing a strong universality."
                },
                "authors": [
                    {
                        "name": "Haoye Chai"
                    },
                    {
                        "name": "Shiyuan Zhang"
                    },
                    {
                        "name": "Xiaoqian Qi"
                    },
                    {
                        "name": "Baohua Qiu"
                    },
                    {
                        "name": "Yong Li"
                    }
                ],
                "author_detail": {
                    "name": "Yong Li"
                },
                "author": "Yong Li",
                "arxiv_doi": "10.1145/3711896.3737272",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3711896.3737272",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2410.15322v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.15322v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "2025 ACM SIGKDD International Conference on Knowledge Discovery and\n  Data Mining, KDD 2025",
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02322v1",
                "updated": "2025-08-04T11:42:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    42,
                    48,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:42:48Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    42,
                    48,
                    0,
                    216,
                    0
                ],
                "title": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAMERA: Multi-Matrix Joint Compression for MoE Models via Micro-Expert\n  Redundancy Analysis"
                },
                "summary": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) with Mixture-of-Experts (MoE) architectures are\ndistinguished by their strong performance scaling with increasing parameters\nacross a wide range of tasks, yet they also suffer from substantial\ncomputational and storage overheads. Notably, the performance gains of MoE\nmodels do not scale proportionally with the growth in expert parameters. While\nprior works attempt to reduce parameters via expert-level pruning, merging, or\ndecomposition, they still suffer from challenges in both performance and\ncomputational efficiency. In this paper, we address these challenges by\nintroducing micro-expert as a finer-grained compression unit that spans across\nmatrices. We first establish a more fundamental perspective, viewing MoE layers\nas mixtures of micro-experts, and present CAMERA, a lightweight and\ntraining-free framework for identifying micro-expert redundancy. Our analysis\nuncovers significant variance in micro-expert contributions during decoding.\nBased on this insight, we further propose CAMERA-P, a structured micro-expert\npruning framework, and CAMERA-Q, a mixed-precision quantization idea designed\nfor micro-experts. Extensive experiments on nine downstream tasks show that\nCAMERA-P consistently outperforms strong baselines under pruning ratios ranging\nfrom 20% to 60%. Furthermore, CAMERA-Q achieves superior results under\naggressive 2-bit quantization, surpassing existing matrix- and channel-level\nideas. Notably, our method enables complete micro-expert analysis of\nQwen2-57B-A14B in less than 5 minutes on a single NVIDIA A100-40GB GPU."
                },
                "authors": [
                    {
                        "name": "Yuzhuang Xu"
                    },
                    {
                        "name": "Xu Han"
                    },
                    {
                        "name": "Yuanchi Zhang"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Yijun Liu"
                    },
                    {
                        "name": "Shiyu Ji"
                    },
                    {
                        "name": "Qingfu Zhu"
                    },
                    {
                        "name": "Wanxiang Che"
                    }
                ],
                "author_detail": {
                    "name": "Wanxiang Che"
                },
                "author": "Wanxiang Che",
                "arxiv_comment": "16 pages, 9 figures, 7 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02317v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02317v2",
                "updated": "2025-08-05T03:34:20Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    3,
                    34,
                    20,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T11:33:04Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    33,
                    4,
                    0,
                    216,
                    0
                ],
                "title": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VeOmni: Scaling Any Modality Model Training with Model-Centric\n  Distributed Recipe Zoo"
                },
                "summary": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models (LLMs) have driven impressive\nprogress in omni-modal understanding and generation. However, training\nomni-modal LLMs remains a significant challenge due to the heterogeneous model\narchitectures required to process diverse modalities, necessitating\nsophisticated system design for efficient large-scale training. Existing\nframeworks typically entangle model definition with parallel logic, incurring\nlimited scalability and substantial engineering overhead for end-to-end\nomni-modal training. We present VeOmni, a modular and efficient training\nframework to accelerate the development of omni-modal LLMs. VeOmni introduces\nmodel-centric distributed recipes that decouples communication from\ncomputation, enabling efficient 3D parallelism on omni-modal LLMs. VeOmni also\nfeatures a flexible configuration interface supporting seamless integration of\nnew modalities with minimal code change. Using VeOmni, a omni-modal\nmixture-of-experts (MoE) model with 30B parameters can be trained with over\n2,800 tokens/sec/GPU throughput and scale to 160K context lengths via 3D\nparallelism on 128 GPUs, showcasing its superior efficiency and scalability for\ntraining large omni-modal LLMs."
                },
                "authors": [
                    {
                        "name": "Qianli Ma"
                    },
                    {
                        "name": "Yaowei Zheng"
                    },
                    {
                        "name": "Zhelun Shi"
                    },
                    {
                        "name": "Zhongkai Zhao"
                    },
                    {
                        "name": "Bin Jia"
                    },
                    {
                        "name": "Ziyue Huang"
                    },
                    {
                        "name": "Zhiqi Lin"
                    },
                    {
                        "name": "Youjie Li"
                    },
                    {
                        "name": "Jiacheng Yang"
                    },
                    {
                        "name": "Yanghua Peng"
                    },
                    {
                        "name": "Zhi Zhang"
                    },
                    {
                        "name": "Xin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Xin Liu"
                },
                "author": "Xin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02317v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02317v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02312v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02312v1",
                "updated": "2025-08-04T11:28:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    28,
                    34,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:28:34Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    28,
                    34,
                    0,
                    216,
                    0
                ],
                "title": "A Survey on Data Security in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Data Security in Large Language Models"
                },
                "summary": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs), now a foundation in advancing natural language\nprocessing, power applications such as text generation, machine translation,\nand conversational systems. Despite their transformative potential, these\nmodels inherently rely on massive amounts of training data, often collected\nfrom diverse and uncurated sources, which exposes them to serious data security\nrisks. Harmful or malicious data can compromise model behavior, leading to\nissues such as toxic output, hallucinations, and vulnerabilities to threats\nsuch as prompt injection or data poisoning. As LLMs continue to be integrated\ninto critical real-world systems, understanding and addressing these\ndata-centric security risks is imperative to safeguard user trust and system\nreliability. This survey offers a comprehensive overview of the main data\nsecurity risks facing LLMs and reviews current defense strategies, including\nadversarial training, RLHF, and data augmentation. Additionally, we categorize\nand analyze relevant datasets used for assessing robustness and security across\ndifferent domains, providing guidance for future research. Finally, we\nhighlight key research directions that focus on secure model updates,\nexplainability-driven defenses, and effective governance frameworks, aiming to\npromote the safe and responsible development of LLM technology. This work aims\nto inform researchers, practitioners, and policymakers, driving progress toward\ndata security in LLMs."
                },
                "authors": [
                    {
                        "name": "Kang Chen"
                    },
                    {
                        "name": "Xiuze Zhou"
                    },
                    {
                        "name": "Yuanguo Lin"
                    },
                    {
                        "name": "Jinhe Su"
                    },
                    {
                        "name": "Yuanhui Yu"
                    },
                    {
                        "name": "Li Shen"
                    },
                    {
                        "name": "Fan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Fan Lin"
                },
                "author": "Fan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02312v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02312v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13422v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13422v2",
                "updated": "2025-08-04T11:27:55Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    27,
                    55,
                    0,
                    216,
                    0
                ],
                "published": "2025-02-19T04:45:05Z",
                "published_parsed": [
                    2025,
                    2,
                    19,
                    4,
                    45,
                    5,
                    2,
                    50,
                    0
                ],
                "title": "Towards Question Answering over Large Semi-structured Tables",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Question Answering over Large Semi-structured Tables"
                },
                "summary": "Table Question Answering (TableQA) attracts strong interests due to the\nprevalence of web information presented in the form of semi-structured tables.\nDespite many efforts, TableQA over large tables remains an open challenge. This\nis because large tables may overwhelm models that try to comprehend them in\nfull to locate question answers. Recent studies reduce input table size by\ndecomposing tables into smaller, question-relevant sub-tables via generating\nprograms to parse the tables. However, such solutions are subject to program\ngeneration and execution errors and are difficult to ensure decomposition\nquality. To address this issue, we propose TaDRe, a TableQA model that\nincorporates both pre- and post-table decomposition refinements to ensure table\ndecomposition quality, hence achieving highly accurate TableQA results. To\nevaluate TaDRe, we construct two new large-table TableQA benchmarks via\nLLM-driven table expansion and QA pair generation. Extensive experiments on\nboth the new and public benchmarks show that TaDRe achieves state-of-the-art\nperformance on large-table TableQA tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Table Question Answering (TableQA) attracts strong interests due to the\nprevalence of web information presented in the form of semi-structured tables.\nDespite many efforts, TableQA over large tables remains an open challenge. This\nis because large tables may overwhelm models that try to comprehend them in\nfull to locate question answers. Recent studies reduce input table size by\ndecomposing tables into smaller, question-relevant sub-tables via generating\nprograms to parse the tables. However, such solutions are subject to program\ngeneration and execution errors and are difficult to ensure decomposition\nquality. To address this issue, we propose TaDRe, a TableQA model that\nincorporates both pre- and post-table decomposition refinements to ensure table\ndecomposition quality, hence achieving highly accurate TableQA results. To\nevaluate TaDRe, we construct two new large-table TableQA benchmarks via\nLLM-driven table expansion and QA pair generation. Extensive experiments on\nboth the new and public benchmarks show that TaDRe achieves state-of-the-art\nperformance on large-table TableQA tasks."
                },
                "authors": [
                    {
                        "name": "Yuxiang Wang"
                    },
                    {
                        "name": "Junhao Gan"
                    },
                    {
                        "name": "Jianzhong Qi"
                    }
                ],
                "author_detail": {
                    "name": "Jianzhong Qi"
                },
                "author": "Jianzhong Qi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13422v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13422v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02308v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02308v2",
                "updated": "2025-08-05T02:16:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    16,
                    8,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T11:22:13Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    22,
                    13,
                    0,
                    216,
                    0
                ],
                "title": "LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive\n  Long-context Scaling Without Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LaMPE: Length-aware Multi-grained Positional Encoding for Adaptive\n  Long-context Scaling Without Training"
                },
                "summary": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) experience significant performance degradation\nwhen the input exceeds the pretraining context window, primarily due to the\nout-of-distribution (OOD) behavior of Rotary Position Embedding (RoPE). Recent\nstudies mitigate this problem by remapping OOD positions into the\nin-distribution range with fixed mapping strategies, ignoring the dynamic\nrelationship between input length and the model's effective context window. To\nthis end, we propose Length-aware Multi-grained Positional Encoding (LaMPE), a\ntraining-free method that fully utilizes the model's effective context window\nfor adaptive long-context scaling in LLMs. Motivated by the left-skewed\nfrequency distribution of relative positions, LaMPE establishes a dynamic\nrelationship between mapping length and input length through a parametric\nscaled sigmoid function to adaptively allocate positional capacity across\nvarying input lengths. Meanwhile, LaMPE devises a novel multi-grained attention\nmechanism that strategically allocates positional resolution across different\nsequence regions to capture both fine-grained locality and long-range\ndependencies. Our method can be seamlessly applied to a wide range of\nRoPE-based LLMs without training. Extensive experiments on three representative\nLLMs across five mainstream long-context benchmarks demonstrate that LaMPE\nachieves significant performance improvements compared to existing length\nextrapolation methods. The code will be released at\nhttps://github.com/scar-on/LaMPE."
                },
                "authors": [
                    {
                        "name": "Sikui Zhang"
                    },
                    {
                        "name": "Guangze Gao"
                    },
                    {
                        "name": "Ziyun Gan"
                    },
                    {
                        "name": "Chunfeng Yuan"
                    },
                    {
                        "name": "Zefeng Lin"
                    },
                    {
                        "name": "Houwen Peng"
                    },
                    {
                        "name": "Bing Li"
                    },
                    {
                        "name": "Weiming Hu"
                    }
                ],
                "author_detail": {
                    "name": "Weiming Hu"
                },
                "author": "Weiming Hu",
                "arxiv_comment": "13 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02308v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02308v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.08113v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.08113v2",
                "updated": "2025-08-04T11:18:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    18,
                    21,
                    0,
                    216,
                    0
                ],
                "published": "2025-04-10T20:19:50Z",
                "published_parsed": [
                    2025,
                    4,
                    10,
                    20,
                    19,
                    50,
                    3,
                    100,
                    0
                ],
                "title": "Test Amplification for REST APIs via Single and Multi-Agent LLM Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Test Amplification for REST APIs via Single and Multi-Agent LLM Systems"
                },
                "summary": "REST APIs (Representational State Transfer Application Programming\nInterfaces) play a vital role in modern cloud-native applications. As these\nAPIs grow in complexity and scale, ensuring their correctness and robustness\nbecomes increasingly important. Automated testing is essential for identifying\nhidden bugs, particularly those that appear in edge cases or under unexpected\ninputs. However, creating comprehensive and effective test suites for REST APIs\nis challenging and often demands significant effort. In this paper, we\ninvestigate the use of large language model (LLM) systems, both single-agent\nand multi-agent setups, for amplifying existing REST API test suites. These\nsystems generate additional test cases that aim to push the boundaries of the\nAPI, uncovering behaviors that might otherwise go untested. We present a\ncomparative evaluation of the two approaches across several dimensions,\nincluding test coverage, bug detection effectiveness, and practical\nconsiderations such as computational cost and energy usage. Our evaluation\ndemonstrates increased API coverage, identification of numerous bugs in the API\nunder test, and insights into the computational cost and energy consumption of\nboth approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "REST APIs (Representational State Transfer Application Programming\nInterfaces) play a vital role in modern cloud-native applications. As these\nAPIs grow in complexity and scale, ensuring their correctness and robustness\nbecomes increasingly important. Automated testing is essential for identifying\nhidden bugs, particularly those that appear in edge cases or under unexpected\ninputs. However, creating comprehensive and effective test suites for REST APIs\nis challenging and often demands significant effort. In this paper, we\ninvestigate the use of large language model (LLM) systems, both single-agent\nand multi-agent setups, for amplifying existing REST API test suites. These\nsystems generate additional test cases that aim to push the boundaries of the\nAPI, uncovering behaviors that might otherwise go untested. We present a\ncomparative evaluation of the two approaches across several dimensions,\nincluding test coverage, bug detection effectiveness, and practical\nconsiderations such as computational cost and energy usage. Our evaluation\ndemonstrates increased API coverage, identification of numerous bugs in the API\nunder test, and insights into the computational cost and energy consumption of\nboth approaches."
                },
                "authors": [
                    {
                        "name": "Robbe Nooyens"
                    },
                    {
                        "name": "Tolgahan Bardakci"
                    },
                    {
                        "name": "Mutlu Beyazit"
                    },
                    {
                        "name": "Serge Demeyer"
                    }
                ],
                "author_detail": {
                    "name": "Serge Demeyer"
                },
                "author": "Serge Demeyer",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.08113v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.08113v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02298v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02298v1",
                "updated": "2025-08-04T11:06:08Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    6,
                    8,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:06:08Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    6,
                    8,
                    0,
                    216,
                    0
                ],
                "title": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative\n  Credit Assignment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAPO: Towards Enhancing LLM Reasoning through Verifiable Generative\n  Credit Assignment"
                },
                "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reinforcement Learning with Verifiable Rewards (RLVR) has improved the\nreasoning abilities of Large Language Models (LLMs) by using rule-based binary\nfeedback, helping to mitigate reward hacking. However, current RLVR methods\ntypically treat whole responses as single actions, assigning the same reward to\nevery token. This coarse-grained feedback hampers precise credit assignment,\nmaking it hard for models to identify which reasoning steps lead to success or\nfailure, and often results in suboptimal policies and inefficient learning.\nMethods like PPO provide credit assignment through value estimation, but often\nyield inaccurate and unverifiable signals due to limited sampling. On the other\nhand, methods using Process Reward Models can provide step-by-step judgments\nfor each reasoning step, but they require high-quality process supervision\nlabels and are time-consuming when applied in online reinforcement learning\n(RL). To overcome these limitations, we introduce a simple but efficient method\nCredit Assignment Policy Optimization (CAPO). Given a reasoning response\nrollout from the policy model, CAPO directly leverages an off-the-shelf,\ngeneral-purpose LLM as a Generative Process Reward Model (LLM-as-GenPRM) to\ngenerate all step-wise critique by one pass, thereby providing verifiable\ntoken-level rewards to refine the tokens that were originally assigned\nidentical rule-based rewards. This enables more fine-grained credit assignment\nin an effective way. Furthermore, to enhance the accuracy and robustness of\nCAPO, we employ voting mechanisms that scale with the number of generated\ncritiques. Extensive experiments using different backbones like Llama and Qwen\nmodels and in different sizes show that CAPO consistently outperforms\nsupervised learning-based and RL-based fine-tuning methods across six\nchallenging mathematical benchmarks and three out-of-domain benchmarks."
                },
                "authors": [
                    {
                        "name": "Guofu Xie"
                    },
                    {
                        "name": "Yunsheng Shi"
                    },
                    {
                        "name": "Hongtao Tian"
                    },
                    {
                        "name": "Ting Yao"
                    },
                    {
                        "name": "Xiao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xiao Zhang"
                },
                "author": "Xiao Zhang",
                "arxiv_comment": "Work in progress",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02298v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02298v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02296v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02296v1",
                "updated": "2025-08-04T11:04:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    4,
                    54,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:04:54Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    4,
                    54,
                    0,
                    216,
                    0
                ],
                "title": "Simple Methods Defend RAG Systems Well Against Real-World Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple Methods Defend RAG Systems Well Against Real-World Attacks"
                },
                "summary": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ensuring safety and in-domain responses for Retrieval-Augmented Generation\n(RAG) systems is paramount in safety-critical applications, yet remains a\nsignificant challenge. To address this, we evaluate four methodologies for\nOut-Of-Domain (OOD) query detection: GPT-4o, regression-based, Principal\nComponent Analysis (PCA)-based, and Neural Collapse (NC), to ensure the RAG\nsystem only responds to queries confined to the system's knowledge base.\nSpecifically, our evaluation explores two novel dimensionality reduction and\nfeature separation strategies: \\textit{PCA}, where top components are selected\nusing explained variance or OOD separability, and an adaptation of\n\\textit{Neural Collapse Feature Separation}. We validate our approach on\nstandard datasets (StackExchange and MSMARCO) and real-world applications\n(Substance Use and COVID-19), including tests against LLM-simulated and actual\nattacks on a COVID-19 vaccine chatbot. Through human and LLM-based evaluations\nof response correctness and relevance, we confirm that an external OOD detector\nis crucial for maintaining response relevance."
                },
                "authors": [
                    {
                        "name": "Ilias Triantafyllopoulos"
                    },
                    {
                        "name": "Renyi Qu"
                    },
                    {
                        "name": "Salvatore Giorgi"
                    },
                    {
                        "name": "Brenda Curtis"
                    },
                    {
                        "name": "Lyle H. Ungar"
                    },
                    {
                        "name": "Joo Sedoc"
                    }
                ],
                "author_detail": {
                    "name": "Joo Sedoc"
                },
                "author": "Joo Sedoc",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02296v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02296v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02292v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02292v1",
                "updated": "2025-08-04T11:02:34Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    2,
                    34,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T11:02:34Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    11,
                    2,
                    34,
                    0,
                    216,
                    0
                ],
                "title": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI\n  Research and Deployment",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinWorld: An All-in-One Open-Source Platform for End-to-End Financial AI\n  Research and Deployment"
                },
                "summary": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Financial AI holds great promise for transforming modern finance, with the\npotential to support a wide range of tasks such as market forecasting,\nportfolio management, quantitative trading, and automated analysis. However,\nexisting platforms remain limited in task coverage, lack robust multimodal data\nintegration, and offer insufficient support for the training and deployment of\nlarge language models (LLMs). In response to these limitations, we present\nFinWorld, an all-in-one open-source platform that provides end-to-end support\nfor the entire financial AI workflow, from data acquisition to experimentation\nand deployment. FinWorld distinguishes itself through native integration of\nheterogeneous financial data, unified support for diverse AI paradigms, and\nadvanced agent automation, enabling seamless development and deployment.\nLeveraging data from 2 representative markets, 4 stock pools, and over 800\nmillion financial data points, we conduct comprehensive experiments on 4 key\nfinancial AI tasks. These experiments systematically evaluate deep learning and\nreinforcement learning algorithms, with particular emphasis on RL-based\nfinetuning for LLMs and LLM Agents. The empirical results demonstrate that\nFinWorld significantly enhances reproducibility, supports transparent\nbenchmarking, and streamlines deployment, thereby providing a strong foundation\nfor future research and real-world applications. Code is available at\nGithub~\\footnote{https://github.com/DVampire/FinWorld}."
                },
                "authors": [
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Yilei Zhao"
                    },
                    {
                        "name": "Chuqiao Zong"
                    },
                    {
                        "name": "Xinrun Wang"
                    },
                    {
                        "name": "Bo An"
                    }
                ],
                "author_detail": {
                    "name": "Bo An"
                },
                "author": "Bo An",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02292v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02292v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02291v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02291v1",
                "updated": "2025-08-04T10:59:07Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    59,
                    7,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:59:07Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    59,
                    7,
                    0,
                    216,
                    0
                ],
                "title": "Flexible Automatic Identification and Removal (FAIR)-Pruner: An\n  Efficient Neural Network Pruning Method",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Flexible Automatic Identification and Removal (FAIR)-Pruner: An\n  Efficient Neural Network Pruning Method"
                },
                "summary": "Neural network pruning is a critical compression technique that facilitates\nthe deployment of large-scale neural networks on resource-constrained edge\ndevices, typically by identifying and eliminating redundant or insignificant\nparameters to reduce computational and memory overhead. This paper proposes the\nFlexible Automatic Identification and Removal (FAIR)-Pruner, a novel method for\nneural network structured pruning. Specifically, FAIR-Pruner first evaluates\nthe importance of each unit (e.g., neuron or channel) through the Utilization\nScore quantified by the Wasserstein distance. To reflect the performance\ndegradation after unit removal, it then introduces the Reconstruction Error,\nwhich is computed via the Taylor expansion of the loss function. Finally,\nFAIR-Pruner identifies superfluous units with negligible impact on model\nperformance by controlling the proposed Tolerance of Difference, which measures\ndifferences between unimportant units and those that cause performance\ndegradation. A major advantage of FAIR-Pruner lies in its capacity to\nautomatically determine the layer-wise pruning rates, which yields a more\nefficient subnetwork structure compared to applying a uniform pruning rate.\nAnother advantage of the FAIR-Pruner is its great one-shot performance without\npost-pruning fine-tuning. Furthermore, with utilization scores and\nreconstruction errors, users can flexibly obtain pruned models under different\npruning ratios. Comprehensive experimental validation on diverse benchmark\ndatasets (e.g., ImageNet) and various neural network architectures (e.g., VGG)\ndemonstrates that FAIR-Pruner achieves significant model compression while\nmaintaining high accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural network pruning is a critical compression technique that facilitates\nthe deployment of large-scale neural networks on resource-constrained edge\ndevices, typically by identifying and eliminating redundant or insignificant\nparameters to reduce computational and memory overhead. This paper proposes the\nFlexible Automatic Identification and Removal (FAIR)-Pruner, a novel method for\nneural network structured pruning. Specifically, FAIR-Pruner first evaluates\nthe importance of each unit (e.g., neuron or channel) through the Utilization\nScore quantified by the Wasserstein distance. To reflect the performance\ndegradation after unit removal, it then introduces the Reconstruction Error,\nwhich is computed via the Taylor expansion of the loss function. Finally,\nFAIR-Pruner identifies superfluous units with negligible impact on model\nperformance by controlling the proposed Tolerance of Difference, which measures\ndifferences between unimportant units and those that cause performance\ndegradation. A major advantage of FAIR-Pruner lies in its capacity to\nautomatically determine the layer-wise pruning rates, which yields a more\nefficient subnetwork structure compared to applying a uniform pruning rate.\nAnother advantage of the FAIR-Pruner is its great one-shot performance without\npost-pruning fine-tuning. Furthermore, with utilization scores and\nreconstruction errors, users can flexibly obtain pruned models under different\npruning ratios. Comprehensive experimental validation on diverse benchmark\ndatasets (e.g., ImageNet) and various neural network architectures (e.g., VGG)\ndemonstrates that FAIR-Pruner achieves significant model compression while\nmaintaining high accuracy."
                },
                "authors": [
                    {
                        "name": "Chenqing Lin"
                    },
                    {
                        "name": "Mostafa Hussien"
                    },
                    {
                        "name": "Chengyao Yu"
                    },
                    {
                        "name": "Mohamed Cheriet"
                    },
                    {
                        "name": "Osama Abdelrahman"
                    },
                    {
                        "name": "Ruixing Ming"
                    }
                ],
                "author_detail": {
                    "name": "Ruixing Ming"
                },
                "author": "Ruixing Ming",
                "arxiv_comment": "Submitted to AAAI 2026",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02291v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02291v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13795v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13795v2",
                "updated": "2025-08-04T10:49:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    49,
                    54,
                    0,
                    216,
                    0
                ],
                "published": "2024-12-18T12:39:53Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    39,
                    53,
                    2,
                    353,
                    0
                ],
                "title": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mix-LN: Unleashing the Power of Deeper Layers by Combining Pre-LN and\n  Post-LN"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success, yet recent\nfindings reveal that their deeper layers often contribute minimally and can be\npruned without affecting overall performance. While some view this as an\nopportunity for model compression, we identify it as a training shortfall\nrooted in the widespread use of Pre-Layer Normalization (Pre-LN). We\ndemonstrate that Pre-LN, commonly employed in models like GPT and LLaMA, leads\nto diminished gradient norms in its deeper layers, reducing their\neffectiveness. In contrast, Post-Layer Normalization (Post-LN) preserves larger\ngradient norms in deeper layers but suffers from vanishing gradients in earlier\nlayers. To address this, we introduce Mix-LN, a novel normalization technique\nthat combines the strengths of Pre-LN and Post-LN within the same model. Mix-LN\napplies Post-LN to the earlier layers and Pre-LN to the deeper layers, ensuring\nmore uniform gradients across layers. This allows all parts of the\nnetwork--both shallow and deep layers--to contribute effectively to training.\nExtensive experiments with various model sizes from 70M to 7B demonstrate that\nMix-LN consistently outperforms both Pre-LN and Post-LN, promoting more\nbalanced, healthier gradient norms throughout the network, and enhancing the\noverall quality of LLM pre-training. Furthermore, we demonstrate that models\npre-trained with Mix-LN learn better compared to those using Pre-LN or Post-LN\nduring supervised fine-tuning (SFT) and reinforcement learning from human\nfeedback (RLHF), highlighting the critical importance of high-quality deep\nlayers. By effectively addressing the inefficiencies of deep layers in current\nLLMs, Mix-LN unlocks their potential, enhancing model capacity without\nincreasing model size. Our code is available at\nhttps://github.com/pixeli99/MixLN."
                },
                "authors": [
                    {
                        "name": "Pengxiang Li"
                    },
                    {
                        "name": "Lu Yin"
                    },
                    {
                        "name": "Shiwei Liu"
                    }
                ],
                "author_detail": {
                    "name": "Shiwei Liu"
                },
                "author": "Shiwei Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13795v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13795v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02276v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02276v1",
                "updated": "2025-08-04T10:43:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    43,
                    31,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:43:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    43,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "CellForge: Agentic Design of Virtual Cell Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CellForge: Agentic Design of Virtual Cell Models"
                },
                "summary": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Virtual cell modeling represents an emerging frontier at the intersection of\nartificial intelligence and biology, aiming to predict quantities such as\nresponses to diverse perturbations quantitatively. However, autonomously\nbuilding computational models for virtual cells is challenging due to the\ncomplexity of biological systems, the heterogeneity of data modalities, and the\nneed for domain-specific expertise across multiple disciplines. Here, we\nintroduce CellForge, an agentic system that leverages a multi-agent framework\nthat transforms presented biological datasets and research objectives directly\ninto optimized computational models for virtual cells. More specifically, given\nonly raw single-cell multi-omics data and task descriptions as input, CellForge\noutputs both an optimized model architecture and executable code for training\nvirtual cell models and inference. The framework integrates three core modules:\nTask Analysis for presented dataset characterization and relevant literature\nretrieval, Method Design, where specialized agents collaboratively develop\noptimized modeling strategies, and Experiment Execution for automated\ngeneration of code. The agents in the Design module are separated into experts\nwith differing perspectives and a central moderator, and have to\ncollaboratively exchange solutions until they achieve a reasonable consensus.\nWe demonstrate CellForge's capabilities in single-cell perturbation prediction,\nusing six diverse datasets that encompass gene knockouts, drug treatments, and\ncytokine stimulations across multiple modalities. CellForge consistently\noutperforms task-specific state-of-the-art methods. Overall, CellForge\ndemonstrates how iterative interaction between LLM agents with differing\nperspectives provides better solutions than directly addressing a modeling\nchallenge. Our code is publicly available at\nhttps://github.com/gersteinlab/CellForge."
                },
                "authors": [
                    {
                        "name": "Xiangru Tang"
                    },
                    {
                        "name": "Zhuoyun Yu"
                    },
                    {
                        "name": "Jiapeng Chen"
                    },
                    {
                        "name": "Yan Cui"
                    },
                    {
                        "name": "Daniel Shao"
                    },
                    {
                        "name": "Weixu Wang"
                    },
                    {
                        "name": "Fang Wu"
                    },
                    {
                        "name": "Yuchen Zhuang"
                    },
                    {
                        "name": "Wenqi Shi"
                    },
                    {
                        "name": "Zhi Huang"
                    },
                    {
                        "name": "Arman Cohan"
                    },
                    {
                        "name": "Xihong Lin"
                    },
                    {
                        "name": "Fabian Theis"
                    },
                    {
                        "name": "Smita Krishnaswamy"
                    },
                    {
                        "name": "Mark Gerstein"
                    }
                ],
                "author_detail": {
                    "name": "Mark Gerstein"
                },
                "author": "Mark Gerstein",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02276v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02276v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.14999v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.14999v2",
                "updated": "2025-08-04T10:31:26Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    31,
                    26,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-20T15:10:43Z",
                "published_parsed": [
                    2025,
                    7,
                    20,
                    15,
                    10,
                    43,
                    6,
                    201,
                    0
                ],
                "title": "Clustered Federated Learning for Generalizable FDIA Detection in Smart\n  Grids with Heterogeneous Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clustered Federated Learning for Generalizable FDIA Detection in Smart\n  Grids with Heterogeneous Data"
                },
                "summary": "False Data Injection Attacks (FDIAs) pose severe security risks to smart\ngrids by manipulating measurement data collected from spatially distributed\ndevices such as SCADA systems and PMUs. These measurements typically exhibit\nNon-Independent and Identically Distributed (Non-IID) characteristics across\ndifferent regions, which significantly challenges the generalization ability of\ndetection models. Traditional centralized training approaches not only face\nprivacy risks and data sharing constraints but also incur high transmission\ncosts, limiting their scalability and deployment feasibility. To address these\nissues, this paper proposes a privacy-preserving federated learning framework,\ntermed Federated Cluster Average (FedClusAvg), designed to improve FDIA\ndetection in Non-IID and resource-constrained environments. FedClusAvg\nincorporates cluster-based stratified sampling and hierarchical communication\n(client-subserver-server) to enhance model generalization and reduce\ncommunication overhead. By enabling localized training and weighted parameter\naggregation, the algorithm achieves accurate model convergence without\ncentralizing sensitive data. Experimental results on benchmark smart grid\ndatasets demonstrate that FedClusAvg not only improves detection accuracy under\nheterogeneous data distributions but also significantly reduces communication\nrounds and bandwidth consumption. This work provides an effective solution for\nsecure and efficient FDIA detection in large-scale distributed power systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "False Data Injection Attacks (FDIAs) pose severe security risks to smart\ngrids by manipulating measurement data collected from spatially distributed\ndevices such as SCADA systems and PMUs. These measurements typically exhibit\nNon-Independent and Identically Distributed (Non-IID) characteristics across\ndifferent regions, which significantly challenges the generalization ability of\ndetection models. Traditional centralized training approaches not only face\nprivacy risks and data sharing constraints but also incur high transmission\ncosts, limiting their scalability and deployment feasibility. To address these\nissues, this paper proposes a privacy-preserving federated learning framework,\ntermed Federated Cluster Average (FedClusAvg), designed to improve FDIA\ndetection in Non-IID and resource-constrained environments. FedClusAvg\nincorporates cluster-based stratified sampling and hierarchical communication\n(client-subserver-server) to enhance model generalization and reduce\ncommunication overhead. By enabling localized training and weighted parameter\naggregation, the algorithm achieves accurate model convergence without\ncentralizing sensitive data. Experimental results on benchmark smart grid\ndatasets demonstrate that FedClusAvg not only improves detection accuracy under\nheterogeneous data distributions but also significantly reduces communication\nrounds and bandwidth consumption. This work provides an effective solution for\nsecure and efficient FDIA detection in large-scale distributed power systems."
                },
                "authors": [
                    {
                        "name": "Yunfeng Li"
                    },
                    {
                        "name": "Junhong Liu"
                    },
                    {
                        "name": "Zhaohui Yang"
                    },
                    {
                        "name": "Guofu Liao"
                    },
                    {
                        "name": "Chuyun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Chuyun Zhang"
                },
                "author": "Chuyun Zhang",
                "arxiv_comment": "10 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.14999v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.14999v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10939v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10939v2",
                "updated": "2025-08-04T10:29:40Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    29,
                    40,
                    0,
                    216,
                    0
                ],
                "published": "2025-05-16T07:23:59Z",
                "published_parsed": [
                    2025,
                    5,
                    16,
                    7,
                    23,
                    59,
                    4,
                    136,
                    0
                ],
                "title": "GenKnowSub: Improving Modularity and Reusability of LLMs through General\n  Knowledge Subtraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GenKnowSub: Improving Modularity and Reusability of LLMs through General\n  Knowledge Subtraction"
                },
                "summary": "Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models often struggle with zero-shot generalization, and\nseveral modular approaches have been proposed to address this challenge. Yet,\nwe hypothesize that a key limitation remains: the entanglement of general\nknowledge and task-specific adaptations. To overcome this, we propose a modular\nframework that disentangles these components by constructing a library of\ntask-specific LoRA modules alongside a general-domain LoRA. By subtracting this\ngeneral knowledge component from each task-specific module, we obtain residual\nmodules that focus more exclusively on task-relevant information, a method we\ncall general knowledge subtraction (GenKnowSub). Leveraging the refined\ntask-specific modules and the Arrow routing algorithm\n\\citep{ostapenko2024towards}, we dynamically select and combine modules for new\ninputs without additional training. Our studies on the Phi-3 model and standard\nArrow as baselines reveal that using general knowledge LoRAs derived from\ndiverse languages, including English, French, and German, yields consistent\nperformance gains in both monolingual and cross-lingual settings across a wide\nset of benchmarks. Further experiments on Phi-2 demonstrate how GenKnowSub\ngeneralizes to weaker LLMs. The complete code and data are available at\nhttps://github.com/saharsamr/Modular-LLM."
                },
                "authors": [
                    {
                        "name": "Mohammadtaha Bagherifard"
                    },
                    {
                        "name": "Sahar Rajabi"
                    },
                    {
                        "name": "Ali Edalat"
                    },
                    {
                        "name": "Yadollah Yaghoobzadeh"
                    }
                ],
                "author_detail": {
                    "name": "Yadollah Yaghoobzadeh"
                },
                "author": "Yadollah Yaghoobzadeh",
                "arxiv_comment": "Accepted to ACL 2025 (main conference, short paper), 10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10939v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10939v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02269v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02269v1",
                "updated": "2025-08-04T10:21:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    21,
                    47,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:21:47Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    21,
                    47,
                    0,
                    216,
                    0
                ],
                "title": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AirTrafficGen: Configurable Air Traffic Scenario Generation with Large\n  Language Models"
                },
                "summary": "The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, AirTrafficGen, that leverages large language models (LLMs)\nto automate and control the generation of complex ATC scenarios. Our method\nuses a purpose-built, graph-based representation to encode sector topology\n(including airspace geometry, routes, and fixes) into a format LLMs can\nprocess. Through rigorous benchmarking, we show that state-of-the-art models\nlike Gemini 2.5 Pro and OpenAI o3 can generate high-traffic scenarios whilst\nmaintaining operational realism. Our engineered prompting enables fine-grained\ncontrol over interaction presence, type, and location. Initial findings suggest\nthese models are also capable of iterative refinement, correcting flawed\nscenarios based on simple textual feedback. This approach provides a scalable\nalternative to manual scenario design, addressing the need for a greater volume\nand variety of ATC training and validation simulations. More broadly, this work\nshowcases the potential of LLMs for complex planning in safety-critical\ndomains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The manual design of scenarios for Air Traffic Control (ATC) training is a\ndemanding and time-consuming bottleneck that limits the diversity of\nsimulations available to controllers. To address this, we introduce a novel,\nend-to-end approach, AirTrafficGen, that leverages large language models (LLMs)\nto automate and control the generation of complex ATC scenarios. Our method\nuses a purpose-built, graph-based representation to encode sector topology\n(including airspace geometry, routes, and fixes) into a format LLMs can\nprocess. Through rigorous benchmarking, we show that state-of-the-art models\nlike Gemini 2.5 Pro and OpenAI o3 can generate high-traffic scenarios whilst\nmaintaining operational realism. Our engineered prompting enables fine-grained\ncontrol over interaction presence, type, and location. Initial findings suggest\nthese models are also capable of iterative refinement, correcting flawed\nscenarios based on simple textual feedback. This approach provides a scalable\nalternative to manual scenario design, addressing the need for a greater volume\nand variety of ATC training and validation simulations. More broadly, this work\nshowcases the potential of LLMs for complex planning in safety-critical\ndomains."
                },
                "authors": [
                    {
                        "name": "Dewi Sid William Gould"
                    },
                    {
                        "name": "George De Ath"
                    },
                    {
                        "name": "Ben Carvell"
                    },
                    {
                        "name": "Nick Pepper"
                    }
                ],
                "author_detail": {
                    "name": "Nick Pepper"
                },
                "author": "Nick Pepper",
                "arxiv_comment": "7 pages and appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02269v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02269v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.15343v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.15343v2",
                "updated": "2025-08-04T10:12:31Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    12,
                    31,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-21T07:58:03Z",
                "published_parsed": [
                    2025,
                    7,
                    21,
                    7,
                    58,
                    3,
                    0,
                    202,
                    0
                ],
                "title": "StackTrans: From Large Language Model to Large Pushdown Automata Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "StackTrans: From Large Language Model to Large Pushdown Automata Model"
                },
                "summary": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Transformer architecture has emerged as a landmark advancement within the\nbroad field of artificial intelligence, effectively catalyzing the advent of\nlarge language models (LLMs). However, despite its remarkable capabilities and\nthe substantial progress it has facilitated, the Transformer architecture still\nhas some limitations. One such intrinsic limitation is its inability to\neffectively capture the Chomsky hierarchy, such as regular expressions or\ndeterministic context-free grammars. Drawing inspiration from pushdown\nautomata, which efficiently resolve deterministic context-free grammars using\nstacks, we propose StackTrans to address the aforementioned issue within LLMs.\nUnlike previous approaches that modify the attention computation, StackTrans\nexplicitly incorporates hidden state stacks between Transformer layers. This\ndesign maintains compatibility with existing frameworks like flash-attention.\nSpecifically, our design features stack operations -- such as pushing and\npopping hidden states -- that are differentiable and can be learned in an\nend-to-end manner. Our comprehensive evaluation spans benchmarks for both\nChomsky hierarchies and large-scale natural languages. Across these diverse\ntasks, StackTrans consistently outperforms standard Transformer models and\nother baselines. We have successfully scaled StackTrans up from 360M to 7B\nparameters. In particular, our from-scratch pretrained model StackTrans-360M\noutperforms several larger open-source LLMs with 2-3x more parameters,\nshowcasing its superior efficiency and reasoning capability."
                },
                "authors": [
                    {
                        "name": "Kechi Zhang"
                    },
                    {
                        "name": "Ge Li"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Huangzhao Zhang"
                    },
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Jia Li"
                    },
                    {
                        "name": "Jingjing Xu"
                    },
                    {
                        "name": "Zhi Jin"
                    }
                ],
                "author_detail": {
                    "name": "Zhi Jin"
                },
                "author": "Zhi Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2507.15343v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.15343v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02260v1",
                "updated": "2025-08-04T10:08:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    8,
                    10,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T10:08:10Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    8,
                    10,
                    0,
                    216,
                    0
                ],
                "title": "Decomposing the Entropy-Performance Exchange: The Missing Keys to\n  Unlocking Effective Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decomposing the Entropy-Performance Exchange: The Missing Keys to\n  Unlocking Effective Reinforcement Learning"
                },
                "summary": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recently, reinforcement learning with verifiable rewards (RLVR) has been\nwidely used for enhancing the reasoning abilities of large language models\n(LLMs). A core challenge in RLVR involves managing the exchange between entropy\nand performance of policies. Despite the importance of this exchange, a\nfine-grained understanding of when and how this exchange operates most\neffectively remains limited. To bridge this gap, we conduct a systematic\nempirical analysis of the entropy-performance exchange mechanism of RLVR across\ndifferent levels of granularity. Specifically, we first divide the training\nprocess into two distinct stages based on entropy dynamics, i.e., rising stage\nand plateau stage, and then systematically investigate how this mechanism\nvaries across stage-level, instance-level, and token-level granularitiess. Our\nanalysis reveals that, in the rising stage, entropy reduction in negative\nsamples facilitates the learning of effective reasoning patterns, which in turn\ndrives rapid performance gains. Moreover, in the plateau stage, learning\nefficiency strongly correlates with high-entropy tokens present in\nlow-perplexity samples and those located at the end of sequences. Motivated by\nthese findings, we propose two methods that dynamically adjust the reward\nsignal using perplexity and positional information to focus RL updates on\ntokens that exhibit high learning potential, achieving improvements compared to\nthe baseline methods on various LLMs."
                },
                "authors": [
                    {
                        "name": "Jia Deng"
                    },
                    {
                        "name": "Jie Chen"
                    },
                    {
                        "name": "Zhipeng Chen"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Ji-Rong Wen"
                    }
                ],
                "author_detail": {
                    "name": "Ji-Rong Wen"
                },
                "author": "Ji-Rong Wen",
                "arxiv_comment": "7 pages, 20 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2507.10178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2507.10178v2",
                "updated": "2025-08-04T10:03:27Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    10,
                    3,
                    27,
                    0,
                    216,
                    0
                ],
                "published": "2025-07-14T11:40:17Z",
                "published_parsed": [
                    2025,
                    7,
                    14,
                    11,
                    40,
                    17,
                    0,
                    195,
                    0
                ],
                "title": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Pimba: A Processing-in-Memory Acceleration for Post-Transformer Large\n  Language Model Serving"
                },
                "summary": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 4.1x and 2.1x\nhigher token generation throughput, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformers are the driving force behind today's Large Language Models\n(LLMs), serving as the foundation for their performance and versatility. Yet,\ntheir compute and memory costs grow with sequence length, posing scalability\nchallenges for long-context inferencing. In response, the algorithm community\nis exploring alternative architectures, such as state space models (SSMs),\nlinear attention, and recurrent neural networks (RNNs), which we refer to as\npost-transformers. This shift presents a key challenge: building a serving\nsystem that efficiently supports both transformer and post-transformer LLMs\nwithin a unified framework. To address this challenge, we analyze the\nperformance characteristics of transformer and post-transformer LLMs. Despite\ntheir algorithmic differences, both are fundamentally limited by memory\nbandwidth under batched inference due to attention in transformers and state\nupdates in post-transformers. Further analyses suggest two additional insights:\n(1) state update operations, unlike attention, incur high hardware cost, making\nper-bank PIM acceleration inefficient, and (2) different low-precision\narithmetic methods offer varying accuracy-area tradeoffs, while we identify\nMicrosoft's MX as the Pareto-optimal choice. Building on these insights, we\ndesign Pimba as an array of State-update Processing Units (SPUs), each shared\nbetween two banks to enable interleaved access to PIM. Each SPU includes a\nState-update Processing Engine (SPE) that comprises element-wise multipliers\nand adders using MX-based quantized arithmetic, enabling efficient execution of\nstate update and attention operations. Our evaluation shows that, compared to\nLLM-optimized GPU and GPU+PIM systems, Pimba achieves up to 4.1x and 2.1x\nhigher token generation throughput, respectively."
                },
                "authors": [
                    {
                        "name": "Wonung Kim"
                    },
                    {
                        "name": "Yubin Lee"
                    },
                    {
                        "name": "Yoonsung Kim"
                    },
                    {
                        "name": "Jinwoo Hwang"
                    },
                    {
                        "name": "Seongryong Oh"
                    },
                    {
                        "name": "Jiyong Jung"
                    },
                    {
                        "name": "Aziz Huseynov"
                    },
                    {
                        "name": "Woong Gyu Park"
                    },
                    {
                        "name": "Chang Hyun Park"
                    },
                    {
                        "name": "Divya Mahajan"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3725843.3756121",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3725843.3756121",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2507.10178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2507.10178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "MICRO 2025",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02243v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02243v1",
                "updated": "2025-08-04T09:43:54Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    43,
                    54,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:43:54Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    43,
                    54,
                    0,
                    216,
                    0
                ],
                "title": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I2CR: Intra- and Inter-modal Collaborative Reflections for Multimodal\n  Entity Linking"
                },
                "summary": "Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multimodal entity linking plays a crucial role in a wide range of\napplications. Recent advances in large language model-based methods have become\nthe dominant paradigm for this task, effectively leveraging both textual and\nvisual modalities to enhance performance. Despite their success, these methods\nstill face two challenges, including unnecessary incorporation of image data in\ncertain scenarios and the reliance only on a one-time extraction of visual\nfeatures, which can undermine their effectiveness and accuracy. To address\nthese challenges, we propose a novel LLM-based framework for the multimodal\nentity linking task, called Intra- and Inter-modal Collaborative Reflections.\nThis framework prioritizes leveraging text information to address the task.\nWhen text alone is insufficient to link the correct entity through intra- and\ninter-modality evaluations, it employs a multi-round iterative strategy that\nintegrates key visual clues from various aspects of the image to support\nreasoning and enhance matching accuracy. Extensive experiments on three widely\nused public datasets demonstrate that our framework consistently outperforms\ncurrent state-of-the-art methods in the task, achieving improvements of 3.2%,\n5.1%, and 1.6%, respectively. Our code is available at\nhttps://github.com/ziyan-xiaoyu/I2CR/."
                },
                "authors": [
                    {
                        "name": "Ziyan Liu"
                    },
                    {
                        "name": "Junwen Li"
                    },
                    {
                        "name": "Kaiwen Li"
                    },
                    {
                        "name": "Tong Ruan"
                    },
                    {
                        "name": "Chao Wang"
                    },
                    {
                        "name": "Xinyan He"
                    },
                    {
                        "name": "Zongyu Wang"
                    },
                    {
                        "name": "Xuezhi Cao"
                    },
                    {
                        "name": "Jingping Liu"
                    }
                ],
                "author_detail": {
                    "name": "Jingping Liu"
                },
                "author": "Jingping Liu",
                "arxiv_doi": "10.1145/3746027.3755674",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3746027.3755674",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2508.02243v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02243v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "10 pages, 6 figures, accepted by ACMMM 2025",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02241v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02241v1",
                "updated": "2025-08-04T09:41:10Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    41,
                    10,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:41:10Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    41,
                    10,
                    0,
                    216,
                    0
                ],
                "title": "Isolating Culture Neurons in Multilingual Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Isolating Culture Neurons in Multilingual Large Language Models"
                },
                "summary": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Language and culture are deeply intertwined, yet it is so far unclear how and\nwhere multilingual large language models encode culture. Here, we extend upon\nan established methodology for identifying language-specific neurons and extend\nit to localize and isolate culture-specific neurons, carefully disentangling\ntheir overlap and interaction with language-specific neurons. To facilitate our\nexperiments, we introduce MUREL, a curated dataset of 85.2 million tokens\nspanning six different cultures. Our localization and intervention experiments\nshow that LLMs encode different cultures in distinct neuron populations,\npredominantly in upper layers, and that these culture neurons can be modulated\nindependently from language-specific neurons or those specific to other\ncultures. These findings suggest that cultural knowledge and propensities in\nmultilingual language models can be selectively isolated and edited - promoting\nfairness, inclusivity, and alignment. Code and data is available at\nhttps://github.com/namazifard/Culture_Neurons ."
                },
                "authors": [
                    {
                        "name": "Danial Namazifard"
                    },
                    {
                        "name": "Lukas Galke"
                    }
                ],
                "author_detail": {
                    "name": "Lukas Galke"
                },
                "author": "Lukas Galke",
                "arxiv_comment": "18 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02241v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02241v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02240v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02240v2",
                "updated": "2025-08-05T02:13:39Z",
                "updated_parsed": [
                    2025,
                    8,
                    5,
                    2,
                    13,
                    39,
                    1,
                    217,
                    0
                ],
                "published": "2025-08-04T09:39:31Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    31,
                    0,
                    216,
                    0
                ],
                "title": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Forecasting When to Forecast: Accelerating Diffusion Models with\n  Confidence-Gated Taylor"
                },
                "summary": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) have demonstrated remarkable performance in\nvisual generation tasks. However, their low inference speed limits their\ndeployment in low-resource applications. Recent training-free approaches\nexploit the redundancy of features across timesteps by caching and reusing past\nrepresentations to accelerate inference. Building on this idea, TaylorSeer\ninstead uses cached features to predict future ones via Taylor expansion.\nHowever, its module-level prediction across all transformer blocks (e.g.,\nattention or feedforward modules) requires storing fine-grained intermediate\nfeatures, leading to notable memory and computation overhead. Moreover, it\nadopts a fixed caching schedule without considering the varying accuracy of\npredictions across timesteps, which can lead to degraded outputs when\nprediction fails. To address these limitations, we propose a novel approach to\nbetter leverage Taylor-based acceleration. First, we shift the Taylor\nprediction target from the module level to the last block level, significantly\nreducing the number of cached features. Furthermore, observing strong\nsequential dependencies among Transformer blocks, we propose to use the error\nbetween the Taylor-estimated and actual outputs of the first block as an\nindicator of prediction reliability. If the error is small, we trust the Taylor\nprediction for the last block; otherwise, we fall back to full computation,\nthereby enabling a dynamic caching mechanism. Empirical results show that our\nmethod achieves a better balance between speed and quality, achieving a 3.17x\nacceleration on FLUX, 2.36x on DiT, and 4.14x on Wan Video with negligible\nquality drop. The Project Page is\n\\href{https://cg-taylor-acce.github.io/CG-Taylor/}{here.}"
                },
                "authors": [
                    {
                        "name": "Xiaoliu Guan"
                    },
                    {
                        "name": "Lielin Jiang"
                    },
                    {
                        "name": "Hanqi Chen"
                    },
                    {
                        "name": "Xu Zhang"
                    },
                    {
                        "name": "Jiaxing Yan"
                    },
                    {
                        "name": "Guanzhong Wang"
                    },
                    {
                        "name": "Yi Liu"
                    },
                    {
                        "name": "Zetao Zhang"
                    },
                    {
                        "name": "Yu Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wu"
                },
                "author": "Yu Wu",
                "arxiv_comment": "15 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02240v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02240v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14807v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14807v2",
                "updated": "2025-08-04T09:39:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    39,
                    21,
                    0,
                    216,
                    0
                ],
                "published": "2024-09-23T08:32:22Z",
                "published_parsed": [
                    2024,
                    9,
                    23,
                    8,
                    32,
                    22,
                    0,
                    267,
                    0
                ],
                "title": "Interpreting Multi-band Galaxy Observations with Large Language\n  Model-Based Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Interpreting Multi-band Galaxy Observations with Large Language\n  Model-Based Agents"
                },
                "summary": "Astronomical research traditionally relies on extensive domain knowledge to\ninterpret observations and narrow down hypotheses. We demonstrate that this\nprocess can be emulated using large language model-based agents to accelerate\nresearch workflows. We propose mephisto, a multi-agent collaboration framework\nthat mimics human reasoning to interpret multi-band galaxy observations.\nmephisto interacts with the CIGALE codebase, which includes spectral energy\ndistribution (SED) models to explain observations. In this open-world setting,\nmephisto learns from its self-play experience, performs tree search, and\naccumulates knowledge in a dynamically updated base. As a proof of concept, we\napply mephisto to the latest data from the James Webb Space Telescope. mephisto\nattains near-human proficiency in reasoning about galaxies' physical scenarios,\neven when dealing with a recently discovered population of \"Little Red Dot\"\ngalaxies. This represents the first demonstration of agentic research in\nastronomy, advancing towards end-to-end research via LLM agents and potentially\nexpediting astronomical discoveries.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Astronomical research traditionally relies on extensive domain knowledge to\ninterpret observations and narrow down hypotheses. We demonstrate that this\nprocess can be emulated using large language model-based agents to accelerate\nresearch workflows. We propose mephisto, a multi-agent collaboration framework\nthat mimics human reasoning to interpret multi-band galaxy observations.\nmephisto interacts with the CIGALE codebase, which includes spectral energy\ndistribution (SED) models to explain observations. In this open-world setting,\nmephisto learns from its self-play experience, performs tree search, and\naccumulates knowledge in a dynamically updated base. As a proof of concept, we\napply mephisto to the latest data from the James Webb Space Telescope. mephisto\nattains near-human proficiency in reasoning about galaxies' physical scenarios,\neven when dealing with a recently discovered population of \"Little Red Dot\"\ngalaxies. This represents the first demonstration of agentic research in\nastronomy, advancing towards end-to-end research via LLM agents and potentially\nexpediting astronomical discoveries."
                },
                "authors": [
                    {
                        "name": "Zechang Sun"
                    },
                    {
                        "name": "Yuan-Sen Ting"
                    },
                    {
                        "name": "Yaobo Liang"
                    },
                    {
                        "name": "Nan Duan"
                    },
                    {
                        "name": "Song Huang"
                    },
                    {
                        "name": "Zheng Cai"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Cai"
                },
                "author": "Zheng Cai",
                "arxiv_comment": "Accepted at the NIPS ML4PS Workshop 2024. The journal version is in\n  preparation. Code and data will be fully made public following the journal\n  publication. We welcome any comments and feedback",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14807v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14807v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.IM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02233v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02233v1",
                "updated": "2025-08-04T09:33:47Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    33,
                    47,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:33:47Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    33,
                    47,
                    0,
                    216,
                    0
                ],
                "title": "A Methodological Framework for LLM-Based Mining of Software Repositories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Methodological Framework for LLM-Based Mining of Software Repositories"
                },
                "summary": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are increasingly used in software engineering\nresearch, offering new opportunities for automating repository mining tasks.\nHowever, despite their growing popularity, the methodological integration of\nLLMs into Mining Software Repositories (MSR) remains poorly understood.\nExisting studies tend to focus on specific capabilities or performance\nbenchmarks, providing limited insight into how researchers utilize LLMs across\nthe full research pipeline. To address this gap, we conduct a mixed-method\nstudy that combines a rapid review and questionnaire survey in the field of\nLLM4MSR. We investigate (1) the approaches and (2) the threats that affect the\nempirical rigor of researchers involved in this field. Our findings reveal 15\nmethodological approaches, nine main threats, and 25 mitigation strategies.\nBuilding on these findings, we present PRIMES 2.0, a refined empirical\nframework organized into six stages, comprising 23 methodological substeps,\neach mapped to specific threats and corresponding mitigation strategies,\nproviding prescriptive and adaptive support throughout the lifecycle of\nLLM-based MSR studies. Our work contributes to establishing a more transparent\nand reproducible foundation for LLM-based MSR research."
                },
                "authors": [
                    {
                        "name": "Vincenzo De Martino"
                    },
                    {
                        "name": "Joel Castao"
                    },
                    {
                        "name": "Fabio Palomba"
                    },
                    {
                        "name": "Xavier Franch"
                    },
                    {
                        "name": "Silverio Martnez-Fernndez"
                    }
                ],
                "author_detail": {
                    "name": "Silverio Martnez-Fernndez"
                },
                "author": "Silverio Martnez-Fernndez",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02233v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02233v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02232v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02232v1",
                "updated": "2025-08-04T09:32:22Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    32,
                    22,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:32:22Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    32,
                    22,
                    0,
                    216,
                    0
                ],
                "title": "Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities\n  via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Eye2Recall: Exploring the Design of Enhancing Reminiscence Activities\n  via Eye Tracking-Based LLM-Powered Interaction Experience for Older Adults"
                },
                "summary": "Photo-based reminiscence has the potential to have a positive impact on older\nadults' reconnection with their personal history and improve their well-being.\nSupporting reminiscence in older adults through technological implementations\nis becoming an increasingly important area of research in the fields of HCI and\nCSCW. However, the impact of integrating gaze and speech as mixed-initiative\ninteractions in LLM-powered reminiscence conversations remains under-explored.\nTo address this, we conducted expert interviews to understand the challenges\nthat older adults face with LLM-powered, photo-based reminiscence experiences.\nBased on these design considerations, we developed Eye2Recall, a system that\nintegrates eye tracking for detecting visual interest with natural language\ninteraction to create a mixed-initiative reminiscence experience. We evaluated\nits effectiveness through a user study involving ten older adults. The results\nhave important implications for the future design of more accessible and\nempowering reminiscence technologies that better align with older adults'\nnatural interaction patterns and enhance their positive aging.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photo-based reminiscence has the potential to have a positive impact on older\nadults' reconnection with their personal history and improve their well-being.\nSupporting reminiscence in older adults through technological implementations\nis becoming an increasingly important area of research in the fields of HCI and\nCSCW. However, the impact of integrating gaze and speech as mixed-initiative\ninteractions in LLM-powered reminiscence conversations remains under-explored.\nTo address this, we conducted expert interviews to understand the challenges\nthat older adults face with LLM-powered, photo-based reminiscence experiences.\nBased on these design considerations, we developed Eye2Recall, a system that\nintegrates eye tracking for detecting visual interest with natural language\ninteraction to create a mixed-initiative reminiscence experience. We evaluated\nits effectiveness through a user study involving ten older adults. The results\nhave important implications for the future design of more accessible and\nempowering reminiscence technologies that better align with older adults'\nnatural interaction patterns and enhance their positive aging."
                },
                "authors": [
                    {
                        "name": "Lei Han"
                    },
                    {
                        "name": "Mingnan Wei"
                    },
                    {
                        "name": "Qiongyan Chen"
                    },
                    {
                        "name": "Anqi Wang"
                    },
                    {
                        "name": "Rong Pang"
                    },
                    {
                        "name": "Kefei Liu"
                    },
                    {
                        "name": "Rongrong Chen"
                    },
                    {
                        "name": "David Yip"
                    }
                ],
                "author_detail": {
                    "name": "David Yip"
                },
                "author": "David Yip",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02232v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02232v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.00737v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.00737v2",
                "updated": "2025-08-04T09:29:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    29,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-01T16:08:05Z",
                "published_parsed": [
                    2025,
                    8,
                    1,
                    16,
                    8,
                    5,
                    4,
                    213,
                    0
                ],
                "title": "How LLMs are Shaping the Future of Virtual Reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "How LLMs are Shaping the Future of Virtual Reality"
                },
                "summary": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The integration of Large Language Models (LLMs) into Virtual Reality (VR)\ngames marks a paradigm shift in the design of immersive, adaptive, and\nintelligent digital experiences. This paper presents a comprehensive review of\nrecent research at the intersection of LLMs and VR, examining how these models\nare transforming narrative generation, non-player character (NPC) interactions,\naccessibility, personalization, and game mastering. Drawing from an analysis of\n62 peer reviewed studies published between 2018 and 2025, we identify key\napplication domains ranging from emotionally intelligent NPCs and procedurally\ngenerated storytelling to AI-driven adaptive systems and inclusive gameplay\ninterfaces. We also address the major challenges facing this convergence,\nincluding real-time performance constraints, memory limitations, ethical risks,\nand scalability barriers. Our findings highlight that while LLMs significantly\nenhance realism, creativity, and user engagement in VR environments, their\neffective deployment requires robust design strategies that integrate\nmultimodal interaction, hybrid AI architectures, and ethical safeguards. The\npaper concludes by outlining future research directions in multimodal AI,\naffective computing, reinforcement learning, and open-source development,\naiming to guide the responsible advancement of intelligent and inclusive VR\nsystems."
                },
                "authors": [
                    {
                        "name": "Seda zkaya"
                    },
                    {
                        "name": "Santiago Berrezueta-Guzman"
                    },
                    {
                        "name": "Stefan Wagner"
                    }
                ],
                "author_detail": {
                    "name": "Stefan Wagner"
                },
                "author": "Stefan Wagner",
                "arxiv_comment": "Pre-print",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.00737v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.00737v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02230v1",
                "updated": "2025-08-04T09:27:35Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    27,
                    35,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:27:35Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    27,
                    35,
                    0,
                    216,
                    0
                ],
                "title": "FedAPTA: Federated Multi-task Learning in Computing Power Networks with\n  Adaptive Layer-wise Pruning and Task-aware Aggregation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FedAPTA: Federated Multi-task Learning in Computing Power Networks with\n  Adaptive Layer-wise Pruning and Task-aware Aggregation"
                },
                "summary": "Federated Learning (FL) has shown considerable promise in Computing Power\nNetworks (CPNs) for privacy protection, efficient data utilization, and dynamic\ncollaboration. Although it offers practical benefits, applying FL in CPNs\ncontinues to encounter a major obstacle, i.e., multi-task deployment. However,\nexisting work mainly focuses on mitigating FL's computation and communication\noverhead of a single task while overlooking the computing resource wastage\nissue of heterogeneous devices across multiple tasks in FL under CPNs. To\ntackle this, we design FedAPTA, a federated multi-task learning framework in\nCPNs. FedAPTA alleviates computing resource wastage through the developed\nlayer-wise model pruning technique, which reduces local model size while\nconsidering both data and device heterogeneity. To aggregate structurally\nheterogeneous local models of different tasks, we introduce a heterogeneous\nmodel recovery strategy and a task-aware model aggregation method that enables\nthe aggregation through infilling local model architecture with the shared\nglobal model and clustering local models according to their specific tasks. We\ndeploy FedAPTA on a realistic FL platform and benchmark it against nine SOTA FL\nmethods. The experimental outcomes demonstrate that the proposed FedAPTA\nconsiderably outperforms the state-of-the-art FL methods by up to 4.23%. Our\ncode is available at https://github.com/Zhenzovo/FedCPN.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) has shown considerable promise in Computing Power\nNetworks (CPNs) for privacy protection, efficient data utilization, and dynamic\ncollaboration. Although it offers practical benefits, applying FL in CPNs\ncontinues to encounter a major obstacle, i.e., multi-task deployment. However,\nexisting work mainly focuses on mitigating FL's computation and communication\noverhead of a single task while overlooking the computing resource wastage\nissue of heterogeneous devices across multiple tasks in FL under CPNs. To\ntackle this, we design FedAPTA, a federated multi-task learning framework in\nCPNs. FedAPTA alleviates computing resource wastage through the developed\nlayer-wise model pruning technique, which reduces local model size while\nconsidering both data and device heterogeneity. To aggregate structurally\nheterogeneous local models of different tasks, we introduce a heterogeneous\nmodel recovery strategy and a task-aware model aggregation method that enables\nthe aggregation through infilling local model architecture with the shared\nglobal model and clustering local models according to their specific tasks. We\ndeploy FedAPTA on a realistic FL platform and benchmark it against nine SOTA FL\nmethods. The experimental outcomes demonstrate that the proposed FedAPTA\nconsiderably outperforms the state-of-the-art FL methods by up to 4.23%. Our\ncode is available at https://github.com/Zhenzovo/FedCPN."
                },
                "authors": [
                    {
                        "name": "Yachao Yuan"
                    },
                    {
                        "name": "Zhen Yu"
                    },
                    {
                        "name": "Jin Wang"
                    },
                    {
                        "name": "Zhipeng Cheng"
                    },
                    {
                        "name": "Jianhua Hu"
                    }
                ],
                "author_detail": {
                    "name": "Jianhua Hu"
                },
                "author": "Jianhua Hu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.08724v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.08724v2",
                "updated": "2025-08-04T09:27:21Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    27,
                    21,
                    0,
                    216,
                    0
                ],
                "published": "2025-06-10T12:18:53Z",
                "published_parsed": [
                    2025,
                    6,
                    10,
                    12,
                    18,
                    53,
                    1,
                    161,
                    0
                ],
                "title": "Future Deployment and Flexibility of Distributed Energy Resources in the\n  Distribution Grids of Switzerland",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Future Deployment and Flexibility of Distributed Energy Resources in the\n  Distribution Grids of Switzerland"
                },
                "summary": "The decarbonization goals worldwide drive the energy transition of power\ndistribution grids, which operate under increasingly volatile conditions and\ncloser to their technical limits. In this context, localized operational data\nwith high temporal and spatial resolution is essential for their effective\nplanning and regulation. Nevertheless, information on grid-connected\ndistributed energy resources, such as electric vehicles, photovoltaic systems,\nand heat pumps, is often fragmented, inconsistent, and unavailable. This work\nintroduces a comprehensive database of distributed energy resources and\nnon-controllable loads allocated in Switzerland's medium- and low-voltage\ndistribution grid models, covering over 2 million points of connection.\nRemarkably, this data specifies the flexibility capabilities of the\ncontrollable devices, with a set of projections aligned with national forecasts\nfor 2030, 2040, and 2050. The database supports studies on flexibility\nprovision of distributed energy resources, distribution grid resilience, and\nnational energy policy, among other topics. Importantly, its modular structure\nallows users to extract national- and local-scale information across medium-\nand low-voltage systems, enabling broad applicability across locations.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The decarbonization goals worldwide drive the energy transition of power\ndistribution grids, which operate under increasingly volatile conditions and\ncloser to their technical limits. In this context, localized operational data\nwith high temporal and spatial resolution is essential for their effective\nplanning and regulation. Nevertheless, information on grid-connected\ndistributed energy resources, such as electric vehicles, photovoltaic systems,\nand heat pumps, is often fragmented, inconsistent, and unavailable. This work\nintroduces a comprehensive database of distributed energy resources and\nnon-controllable loads allocated in Switzerland's medium- and low-voltage\ndistribution grid models, covering over 2 million points of connection.\nRemarkably, this data specifies the flexibility capabilities of the\ncontrollable devices, with a set of projections aligned with national forecasts\nfor 2030, 2040, and 2050. The database supports studies on flexibility\nprovision of distributed energy resources, distribution grid resilience, and\nnational energy policy, among other topics. Importantly, its modular structure\nallows users to extract national- and local-scale information across medium-\nand low-voltage systems, enabling broad applicability across locations."
                },
                "authors": [
                    {
                        "name": "Lorenzo Zapparoli"
                    },
                    {
                        "name": "Alfredo Oneto"
                    },
                    {
                        "name": "Mara Parajeles Herrera"
                    },
                    {
                        "name": "Blazhe Gjorgiev"
                    },
                    {
                        "name": "Gabriela Hug"
                    },
                    {
                        "name": "Giovanni Sansavini"
                    }
                ],
                "author_detail": {
                    "name": "Giovanni Sansavini"
                },
                "author": "Giovanni Sansavini",
                "arxiv_comment": "The dataset can be accessed here:\n  https://doi.org/10.5281/zenodo.15056134",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.08724v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.08724v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02228v1",
                "updated": "2025-08-04T09:25:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    25,
                    48,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:25:48Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    25,
                    48,
                    0,
                    216,
                    0
                ],
                "title": "Guiding an Automatic Speech Recognition Decoder Using Large Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Guiding an Automatic Speech Recognition Decoder Using Large Language\n  Models"
                },
                "summary": "Automatic Speech Recognition (ASR) consists of an acoustic model (AM) and a\nlanguage model (LM). The AM estimates the probability of an acoustic signal\nbased on a sequence of linguistic units, typically phones, characters, or\ntokens, while the LM assesses the likelihood of a specific sequence of words or\ntokens. Although Large Language Models (LLMs) have demonstrated significant\npotential across various tasks, integrating them into ASR remains an open\nchallenge. By decomposing the maximum a posteriori (MAP) estimator of words (or\ntokens) given the acoustic signal, we derive an iterative procedure that\nfacilitates a novel integration of the AM and LLM, while maintaining their\nseparability. This approach enables each component to be independently trained\nand improved using its own data, thereby maximizing the system's performance by\nleveraging the strengths of both models without requiring joint optimization.\nWe illustrate the effectiveness of our method in comparison to three language\nmodels: N-gram, GCNN, and TransformerLM across multiple datasets spanning\nvarious speech styles, including ALLSSTAR, WSJ0, and TED-LIUM 3. Our\nexperiments involved two acoustic models (wav2vec 2.0 and HuBERT) and three\nLLMs (GPT-2, LLaMA 2, and Falcon). Notably, our method demonstrates particular\nefficacy in addressing complex speech sentences, acronyms, and domain-specific\nvocabulary.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Automatic Speech Recognition (ASR) consists of an acoustic model (AM) and a\nlanguage model (LM). The AM estimates the probability of an acoustic signal\nbased on a sequence of linguistic units, typically phones, characters, or\ntokens, while the LM assesses the likelihood of a specific sequence of words or\ntokens. Although Large Language Models (LLMs) have demonstrated significant\npotential across various tasks, integrating them into ASR remains an open\nchallenge. By decomposing the maximum a posteriori (MAP) estimator of words (or\ntokens) given the acoustic signal, we derive an iterative procedure that\nfacilitates a novel integration of the AM and LLM, while maintaining their\nseparability. This approach enables each component to be independently trained\nand improved using its own data, thereby maximizing the system's performance by\nleveraging the strengths of both models without requiring joint optimization.\nWe illustrate the effectiveness of our method in comparison to three language\nmodels: N-gram, GCNN, and TransformerLM across multiple datasets spanning\nvarious speech styles, including ALLSSTAR, WSJ0, and TED-LIUM 3. Our\nexperiments involved two acoustic models (wav2vec 2.0 and HuBERT) and three\nLLMs (GPT-2, LLaMA 2, and Falcon). Notably, our method demonstrates particular\nefficacy in addressing complex speech sentences, acronyms, and domain-specific\nvocabulary."
                },
                "authors": [
                    {
                        "name": "Eyal Cohen"
                    },
                    {
                        "name": "Bhiksha Raj"
                    },
                    {
                        "name": "Joseph Keshet"
                    }
                ],
                "author_detail": {
                    "name": "Joseph Keshet"
                },
                "arxiv_affiliation": "Technion - Israel Institute of Technology",
                "author": "Joseph Keshet",
                "arxiv_comment": "11 pages, 2 figures. This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.AS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.AS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02222v1",
                "updated": "2025-08-04T09:12:45Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    12,
                    45,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:12:45Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    12,
                    45,
                    0,
                    216,
                    0
                ],
                "title": "FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries\n  and Rich Relevance in Financial Chinese Passage Retrieval",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries\n  and Rich Relevance in Financial Chinese Passage Retrieval"
                },
                "summary": "In recent years, large language models (LLMs) have demonstrated significant\npotential in constructing passage retrieval datasets. However, existing methods\nstill face limitations in expressing cross-doc query needs and controlling\nannotation quality. To address these issues, this paper proposes a\nbidirectional generation pipeline, which aims to generate 3-level hierarchical\nqueries for both intra-doc and cross-doc scenarios and mine additional\nrelevance labels on top of direct mapping annotation. The pipeline introduces\ntwo query generation methods: bottom-up from single-doc text and top-down from\nmulti-doc titles. The bottom-up method uses LLMs to disassemble and generate\nstructured queries at both sentence-level and passage-level simultaneously from\nintra-doc passages. The top-down approach incorporates three key financial\nelements--industry, topic, and time--to divide report titles into clusters and\nprompts LLMs to generate topic-level queries from each cluster. For relevance\nannotation, our pipeline not only relies on direct mapping annotation from the\ngeneration relationship but also implements an indirect positives mining method\nto enrich the relevant query-passage pairs. Using this pipeline, we constructed\na Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k\nChinese financial research reports, which includes hierarchical queries and\nrich relevance labels. Through evaluations of mined relevance labels,\nbenchmarking and training experiments, we assessed the quality of FinCPRG and\nvalidated its effectiveness as a passage retrieval dataset for both training\nand benchmarking.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In recent years, large language models (LLMs) have demonstrated significant\npotential in constructing passage retrieval datasets. However, existing methods\nstill face limitations in expressing cross-doc query needs and controlling\nannotation quality. To address these issues, this paper proposes a\nbidirectional generation pipeline, which aims to generate 3-level hierarchical\nqueries for both intra-doc and cross-doc scenarios and mine additional\nrelevance labels on top of direct mapping annotation. The pipeline introduces\ntwo query generation methods: bottom-up from single-doc text and top-down from\nmulti-doc titles. The bottom-up method uses LLMs to disassemble and generate\nstructured queries at both sentence-level and passage-level simultaneously from\nintra-doc passages. The top-down approach incorporates three key financial\nelements--industry, topic, and time--to divide report titles into clusters and\nprompts LLMs to generate topic-level queries from each cluster. For relevance\nannotation, our pipeline not only relies on direct mapping annotation from the\ngeneration relationship but also implements an indirect positives mining method\nto enrich the relevant query-passage pairs. Using this pipeline, we constructed\na Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k\nChinese financial research reports, which includes hierarchical queries and\nrich relevance labels. Through evaluations of mined relevance labels,\nbenchmarking and training experiments, we assessed the quality of FinCPRG and\nvalidated its effectiveness as a passage retrieval dataset for both training\nand benchmarking."
                },
                "authors": [
                    {
                        "name": "Xuan Xu"
                    },
                    {
                        "name": "Beilin Chu"
                    },
                    {
                        "name": "Qinhong Lin"
                    },
                    {
                        "name": "Yixiao Zhong"
                    },
                    {
                        "name": "Fufang Wen"
                    },
                    {
                        "name": "Jiaqi Liu"
                    },
                    {
                        "name": "Binjie Fei"
                    },
                    {
                        "name": "Yu Li"
                    },
                    {
                        "name": "Zhongliang Yang"
                    },
                    {
                        "name": "Linna Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Linna Zhou"
                },
                "author": "Linna Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2506.10857v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2506.10857v2",
                "updated": "2025-08-04T09:11:48Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    11,
                    48,
                    0,
                    216,
                    0
                ],
                "published": "2025-06-12T16:17:17Z",
                "published_parsed": [
                    2025,
                    6,
                    12,
                    16,
                    17,
                    17,
                    3,
                    163,
                    0
                ],
                "title": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VRBench: A Benchmark for Multi-Step Reasoning in Long Narrative Videos"
                },
                "summary": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 960 long videos (with an average duration of\n1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs\nand 25,106 reasoning steps with timestamps. These videos are curated via a\nmulti-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present VRBench, the first long narrative video benchmark crafted for\nevaluating large models' multi-step reasoning capabilities, addressing\nlimitations in existing evaluations that overlook temporal reasoning and\nprocedural validity. It comprises 960 long videos (with an average duration of\n1.6 hours), along with 8,243 human-labeled multi-step question-answering pairs\nand 25,106 reasoning steps with timestamps. These videos are curated via a\nmulti-stage filtering process including expert inter-rater reviewing to\nprioritize plot coherence. We develop a human-AI collaborative framework that\ngenerates coherent reasoning chains, each requiring multiple temporally\ngrounded steps, spanning seven types (e.g., event attribution, implicit\ninference). VRBench designs a multi-phase evaluation pipeline that assesses\nmodels at both the outcome and process levels. Apart from the MCQs for the\nfinal results, we propose a progress-level LLM-guided scoring metric to\nevaluate the quality of the reasoning chain from multiple dimensions\ncomprehensively. Through extensive evaluations of 12 LLMs and 19 VLMs on\nVRBench, we undertake a thorough analysis and provide valuable insights that\nadvance the field of multi-step reasoning."
                },
                "authors": [
                    {
                        "name": "Jiashuo Yu"
                    },
                    {
                        "name": "Yue Wu"
                    },
                    {
                        "name": "Meng Chu"
                    },
                    {
                        "name": "Zhifei Ren"
                    },
                    {
                        "name": "Zizheng Huang"
                    },
                    {
                        "name": "Pei Chu"
                    },
                    {
                        "name": "Ruijie Zhang"
                    },
                    {
                        "name": "Yinan He"
                    },
                    {
                        "name": "Qirui Li"
                    },
                    {
                        "name": "Songze Li"
                    },
                    {
                        "name": "Zhenxiang Li"
                    },
                    {
                        "name": "Zhongying Tu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Yu Qiao"
                    },
                    {
                        "name": "Yali Wang"
                    },
                    {
                        "name": "Yi Wang"
                    },
                    {
                        "name": "Limin Wang"
                    }
                ],
                "author_detail": {
                    "name": "Limin Wang"
                },
                "author": "Limin Wang",
                "arxiv_comment": "ICCV2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2506.10857v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2506.10857v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02215v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02215v1",
                "updated": "2025-08-04T09:08:43Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:08:43Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    8,
                    43,
                    0,
                    216,
                    0
                ],
                "title": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LeanK: Learnable K Cache Channel Pruning for Efficient Decoding"
                },
                "summary": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) enable long-context tasks but face efficiency\nchallenges due to the growing key-value (KV) cache. We propose LeanK, a\nlearning-based method that prunes unimportant key (K) cache channels by\nleveraging static channel sparsity. With a novel two-stage training process,\nLeanK learns channel-wise static mask that could satisfy specific sparsity\nratio and hardware alignment requirement. LeanK reduces GPU memory and\naccelerates decoding without sacrificing accuracy. Experiments demonstrate up\nto 70% K cache and 16%-18% V cache memory reduction. Custom decoding kernel\nenables 1.3x speedup for attention computation. We also provide insights into\nmodel channels and attention heads during long-context inference by analyzing\nthe learned importance distribution. Our code is available at\nhttps://aka.ms/LeanK."
                },
                "authors": [
                    {
                        "name": "Yike Zhang"
                    },
                    {
                        "name": "Zhiyuan He"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Jianyong Wang"
                    },
                    {
                        "name": "Lili Qiu"
                    }
                ],
                "author_detail": {
                    "name": "Lili Qiu"
                },
                "author": "Lili Qiu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02215v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02215v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2508.02209v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2508.02209v1",
                "updated": "2025-08-04T09:00:01Z",
                "updated_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    0,
                    1,
                    0,
                    216,
                    0
                ],
                "published": "2025-08-04T09:00:01Z",
                "published_parsed": [
                    2025,
                    8,
                    4,
                    9,
                    0,
                    1,
                    0,
                    216,
                    0
                ],
                "title": "Balancing Information Accuracy and Response Timeliness in Networked LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Balancing Information Accuracy and Response Timeliness in Networked LLMs"
                },
                "summary": "Recent advancements in Large Language Models (LLMs) have transformed many\nfields including scientific discovery, content generation, biomedical text\nmining, and educational technology. However, the substantial requirements for\ntraining data, computational resources, and energy consumption pose significant\nchallenges for their practical deployment. A promising alternative is to\nleverage smaller, specialized language models and aggregate their outputs to\nimprove overall response quality. In this work, we investigate a networked LLM\nsystem composed of multiple users, a central task processor, and clusters of\ntopic-specialized LLMs. Each user submits categorical binary (true/false)\nqueries, which are routed by the task processor to a selected cluster of $m$\nLLMs. After gathering individual responses, the processor returns a final\naggregated answer to the user. We characterize both the information accuracy\nand response timeliness in this setting, and formulate a joint optimization\nproblem to balance these two competing objectives. Our extensive simulations\ndemonstrate that the aggregated responses consistently achieve higher accuracy\nthan those of individual LLMs. Notably, this improvement is more significant\nwhen the participating LLMs exhibit similar standalone performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advancements in Large Language Models (LLMs) have transformed many\nfields including scientific discovery, content generation, biomedical text\nmining, and educational technology. However, the substantial requirements for\ntraining data, computational resources, and energy consumption pose significant\nchallenges for their practical deployment. A promising alternative is to\nleverage smaller, specialized language models and aggregate their outputs to\nimprove overall response quality. In this work, we investigate a networked LLM\nsystem composed of multiple users, a central task processor, and clusters of\ntopic-specialized LLMs. Each user submits categorical binary (true/false)\nqueries, which are routed by the task processor to a selected cluster of $m$\nLLMs. After gathering individual responses, the processor returns a final\naggregated answer to the user. We characterize both the information accuracy\nand response timeliness in this setting, and formulate a joint optimization\nproblem to balance these two competing objectives. Our extensive simulations\ndemonstrate that the aggregated responses consistently achieve higher accuracy\nthan those of individual LLMs. Notably, this improvement is more significant\nwhen the participating LLMs exhibit similar standalone performance."
                },
                "authors": [
                    {
                        "name": "Yigit Turkmen"
                    },
                    {
                        "name": "Baturalp Buyukates"
                    },
                    {
                        "name": "Melih Bastopcu"
                    }
                ],
                "author_detail": {
                    "name": "Melih Bastopcu"
                },
                "author": "Melih Bastopcu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2508.02209v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2508.02209v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]