[
    {
        "keyword": "kv cache",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.10560v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10560v1",
                "updated": "2025-05-15T17:59:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:59:24Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    24,
                    3,
                    135,
                    0
                ],
                "title": "Approximation-First Timeseries Monitoring Query At Scale",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Approximation-First Timeseries Monitoring Query At Scale"
                },
                "summary": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Timeseries monitoring systems such as Prometheus play a crucial role in\ngaining observability of the underlying system components. These systems\ncollect timeseries metrics from various system components and perform\nmonitoring queries over periodic window-based aggregations (i.e., rule\nqueries). However, despite wide adoption, the operational costs and query\nlatency of rule queries remain high. In this paper, we identify major\nbottlenecks associated with repeated data scans and query computations\nconcerning window overlaps in rule queries, and present PromSketch, an\napproximation-first query framework as intermediate caches for monitoring\nsystems. It enables low operational costs and query latency, by combining\napproximate window-based query frameworks and sketch-based precomputation.\nPromSketch is implemented as a standalone module that can be integrated into\nPrometheus and VictoriaMetrics, covering 70% of Prometheus' aggregation over\ntime queries. Our evaluation shows that PromSketch achieves up to a two orders\nof magnitude reduction in query latency over Prometheus and VictoriaMetrics,\nwhile lowering operational dollar costs of query processing by two orders of\nmagnitude compared to Prometheus and by at least 4x compared to VictoriaMetrics\nwith at most 5% average errors across statistics. The source code has been made\navailable at https://github.com/Froot-NetSys/promsketch."
                },
                "authors": [
                    {
                        "name": "Zeying Zhu"
                    },
                    {
                        "name": "Jonathan Chamberlain"
                    },
                    {
                        "name": "Kenny Wu"
                    },
                    {
                        "name": "David Starobinski"
                    },
                    {
                        "name": "Zaoxing Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zaoxing Liu"
                },
                "author": "Zaoxing Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10560v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10560v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v4",
                "updated": "2025-05-15T17:18:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    18,
                    12,
                    3,
                    135,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.02380v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.02380v7",
                "updated": "2025-05-15T13:48:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    48,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-04T20:59:34Z",
                "published_parsed": [
                    2025,
                    1,
                    4,
                    20,
                    59,
                    34,
                    5,
                    4,
                    0
                ],
                "title": "Reciprocating Locks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reciprocating Locks"
                },
                "summary": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present \"Reciprocating Locks\", a novel mutual exclusion locking algorithm,\ntargeting cache-coherent shared memory (CC), that enjoys a number of desirable\nproperties. The doorway arrival phase and the release operation both run in\nconstant-time. Waiting threads use local spinning and only a single waiting\nelement is required per thread, regardless of the number of locks a thread\nmight hold at a given time. While our lock does not provide strict FIFO\nadmission, it bounds bypass and has strong anti-starvation properties. The lock\nis compact, space efficient, and has been intentionally designed to be readily\nusable in real-world general purpose computing environments such as the linux\nkernel, pthreads, or C++. We show the lock exhibits high throughput under\ncontention and low latency in the uncontended case. The performance of\nReciprocating Locks is competitive with and often better than the best\nstate-of-the-art scalable spin locks."
                },
                "authors": [
                    {
                        "name": "Dave Dice"
                    },
                    {
                        "name": "Alex Kogan"
                    }
                ],
                "author_detail": {
                    "name": "Alex Kogan"
                },
                "author": "Alex Kogan",
                "arxiv_comment": "Added additional variations in appendix, at the request of\n  collaborators who want to prove various properties",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.02380v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.02380v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.4.1",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.13779v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.13779v3",
                "updated": "2025-05-15T03:29:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    29,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2024-12-18T12:16:41Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    12,
                    16,
                    41,
                    2,
                    353,
                    0
                ],
                "title": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rehearsal-Free Continual Federated Learning with Synergistic Synaptic\n  Intelligence"
                },
                "summary": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Continual Federated Learning (CFL) allows distributed devices to\ncollaboratively learn novel concepts from continuously shifting training data\nwhile avoiding knowledge forgetting of previously seen tasks. To tackle this\nchallenge, most current CFL approaches rely on extensive rehearsal of previous\ndata. Despite effectiveness, rehearsal comes at a cost to memory, and it may\nalso violate data privacy. Considering these, we seek to apply regularization\ntechniques to CFL by considering their cost-efficient properties that do not\nrequire sample caching or rehearsal. Specifically, we first apply traditional\nregularization techniques to CFL and observe that existing regularization\ntechniques, especially synaptic intelligence, can achieve promising results\nunder homogeneous data distribution but fail when the data is heterogeneous.\nBased on this observation, we propose a simple yet effective regularization\nalgorithm for CFL named FedSSI, which tailors the synaptic intelligence for the\nCFL with heterogeneous data settings. FedSSI can not only reduce computational\noverhead without rehearsal but also address the data heterogeneity issue.\nExtensive experiments show that FedSSI achieves superior performance compared\nto state-of-the-art methods."
                },
                "authors": [
                    {
                        "name": "Yichen Li"
                    },
                    {
                        "name": "Yuying Wang"
                    },
                    {
                        "name": "Haozhao Wang"
                    },
                    {
                        "name": "Yining Qi"
                    },
                    {
                        "name": "Tianzhe Xiao"
                    },
                    {
                        "name": "Ruixuan Li"
                    }
                ],
                "author_detail": {
                    "name": "Ruixuan Li"
                },
                "author": "Ruixuan Li",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2403.05890",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.13779v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.13779v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.16112v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.16112v2",
                "updated": "2025-05-15T03:27:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    27,
                    28,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-20T13:00:36Z",
                "published_parsed": [
                    2025,
                    3,
                    20,
                    13,
                    0,
                    36,
                    3,
                    79,
                    0
                ],
                "title": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PromptMobile: Efficient Promptus for Low Bandwidth Mobile Video\n  Streaming"
                },
                "summary": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Traditional video compression algorithms exhibit significant quality\ndegradation at extremely low bitrates. Promptus emerges as a new paradigm for\nvideo streaming, substantially cutting down the bandwidth essential for video\nstreaming. However, Promptus is computationally intensive and can not run in\nreal-time on mobile devices. This paper presents PromptMobile, an efficient\nacceleration framework tailored for on-device Promptus. Specifically, we\npropose (1) a two-stage efficient generation framework to reduce computational\ncost by 8.1x, (2) a fine-grained inter-frame caching to reduce redundant\ncomputations by 16.6%, (3) system-level optimizations to further enhance\nefficiency. The evaluations demonstrate that compared with the original\nPromptus, PromptMobile achieves a 13.6x increase in image generation speed.\nCompared with other streaming methods, PromptMobile achives an average LPIPS\nimprovement of 0.016 (compared with H.265), reducing 60% of severely distorted\nframes (compared to VQGAN)."
                },
                "authors": [
                    {
                        "name": "Liming Liu"
                    },
                    {
                        "name": "Jiangkai Wu"
                    },
                    {
                        "name": "Haoyang Wang"
                    },
                    {
                        "name": "Peiheng Wang"
                    },
                    {
                        "name": "Zongming Guo"
                    },
                    {
                        "name": "Xinggong Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Xinggong Zhang"
                },
                "author": "Xinggong Zhang",
                "arxiv_doi": "10.1145/3735358.3735383",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3735358.3735383",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.16112v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.16112v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "6 pages (excluding references), 10 figures, to appear in APNET 2025",
                "arxiv_journal_ref": "Proc. 9th Asia-Pacific Workshop on Networking (APNET), Aug 2025,\n  Paper No. 24",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09928v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09928v1",
                "updated": "2025-05-15T03:25:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    25,
                    41,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T03:25:41Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    3,
                    25,
                    41,
                    3,
                    135,
                    0
                ],
                "title": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DeFeed: Secure Decentralized Cross-Contract Data Feed in Web 3.0 for\n  Connected Autonomous Vehicles"
                },
                "summary": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Smart contracts have been a topic of interest in blockchain research and are\na key enabling technology for Connected Autonomous Vehicles (CAVs) in the era\nof Web 3.0. These contracts enable trustless interactions without the need for\nintermediaries, as they operate based on predefined rules encoded on the\nblockchain. However, smart contacts face significant challenges in\ncross-contract communication and information sharing, making it difficult to\nestablish seamless connectivity and collaboration among CAVs with Web 3.0. In\nthis paper, we propose DeFeed, a novel secure protocol that incorporates\nvarious gas-saving functions for CAVs, originated from in-depth research into\nthe interaction among smart contracts for decentralized cross-contract data\nfeed in Web 3.0. DeFeed allows smart contracts to obtain information from other\ncontracts efficiently in a single click, without complicated operations. We\njudiciously design and complete various functions with DeFeed, including a pool\nfunction and a cache function for gas optimization, a subscribe function for\nfacilitating data access, and an update function for the future iteration of\nour protocol. Tailored for CAVs with Web 3.0 use cases, DeFeed enables\nefficient data feed between smart contracts underpinning decentralized\napplications and vehicle coordination. Implemented and tested on the Ethereum\nofficial test network, DeFeed demonstrates significant improvements in contract\ninteraction efficiency, reducing computational complexity and gas costs. Our\nsolution represents a critical step towards seamless, decentralized\ncommunication in Web 3.0 ecosystems."
                },
                "authors": [
                    {
                        "name": "Xingchen Sun"
                    },
                    {
                        "name": "Runhua Xu"
                    },
                    {
                        "name": "Wei Ni"
                    },
                    {
                        "name": "Li Duan"
                    },
                    {
                        "name": "Chao Li"
                    }
                ],
                "author_detail": {
                    "name": "Chao Li"
                },
                "author": "Chao Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09928v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09928v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09561v1",
                "updated": "2025-05-14T17:00:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T17:00:47Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    0,
                    47,
                    2,
                    134,
                    0
                ],
                "title": "Learning Long-Context Diffusion Policies via Past-Token Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Long-Context Diffusion Policies via Past-Token Prediction"
                },
                "summary": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Reasoning over long sequences of observations and actions is essential for\nmany robotic tasks. Yet, learning effective long-context policies from\ndemonstrations remains challenging. As context length increases, training\nbecomes increasingly expensive due to rising memory demands, and policy\nperformance often degrades as a result of spurious correlations. Recent methods\ntypically sidestep these issues by truncating context length, discarding\nhistorical information that may be critical for subsequent decisions. In this\npaper, we propose an alternative approach that explicitly regularizes the\nretention of past information. We first revisit the copycat problem in\nimitation learning and identify an opposite challenge in recent diffusion\npolicies: rather than over-relying on prior actions, they often fail to capture\nessential dependencies between past and future actions. To address this, we\nintroduce Past-Token Prediction (PTP), an auxiliary task in which the policy\nlearns to predict past action tokens alongside future ones. This regularization\nsignificantly improves temporal modeling in the policy head, with minimal\nreliance on visual representations. Building on this observation, we further\nintroduce a multistage training strategy: pre-train the visual encoder with\nshort contexts, and fine-tune the policy head using cached long-context\nembeddings. This strategy preserves the benefits of PTP while greatly reducing\nmemory and computational overhead. Finally, we extend PTP into a\nself-verification mechanism at test time, enabling the policy to score and\nselect candidates consistent with past actions during inference. Experiments\nacross four real-world and six simulated tasks demonstrate that our proposed\nmethod improves the performance of long-context diffusion policies by 3x and\naccelerates policy training by more than 10x."
                },
                "authors": [
                    {
                        "name": "Marcel Torne"
                    },
                    {
                        "name": "Andy Tang"
                    },
                    {
                        "name": "Yuejiang Liu"
                    },
                    {
                        "name": "Chelsea Finn"
                    }
                ],
                "author_detail": {
                    "name": "Chelsea Finn"
                },
                "author": "Chelsea Finn",
                "arxiv_comment": "Videos are available at https://long-context-dp.github.io",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06738v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06738v2",
                "updated": "2025-05-14T16:04:57Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    16,
                    4,
                    57,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-10T19:06:37Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    19,
                    6,
                    37,
                    5,
                    130,
                    0
                ],
                "title": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Said: Unveiling Hardware Cache Side-Channels in Local\n  Large Language Model Inference"
                },
                "summary": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) that can be deployed locally have recently\ngained popularity for privacy-sensitive tasks, with companies such as Meta,\nGoogle, and Intel playing significant roles in their development. However, the\nsecurity of local LLMs through the lens of hardware cache side-channels remains\nunexplored. In this paper, we unveil novel side-channel vulnerabilities in\nlocal LLM inference: token value and token position leakage, which can expose\nboth the victim's input and output text, thereby compromising user privacy.\nSpecifically, we found that adversaries can infer the token values from the\ncache access patterns of the token embedding operation, and deduce the token\npositions from the timing of autoregressive decoding phases. To demonstrate the\npotential of these leaks, we design a novel eavesdropping attack framework\ntargeting both open-source and proprietary LLM inference systems. The attack\nframework does not directly interact with the victim's LLM and can be executed\nwithout privilege.\n  We evaluate the attack on a range of practical local LLM deployments (e.g.,\nLlama, Falcon, and Gemma), and the results show that our attack achieves\npromising accuracy. The restored output and input text have an average edit\ndistance of 5.2% and 17.3% to the ground truth, respectively. Furthermore, the\nreconstructed texts achieve average cosine similarity scores of 98.7% (input)\nand 98.0% (output)."
                },
                "authors": [
                    {
                        "name": "Zibo Gao"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Feng Guo"
                    },
                    {
                        "name": "Yixin Zhang"
                    },
                    {
                        "name": "Yinglong Han"
                    },
                    {
                        "name": "Siyuan Liu"
                    },
                    {
                        "name": "Haiyang Li"
                    },
                    {
                        "name": "Zhiqiang Lv"
                    }
                ],
                "author_detail": {
                    "name": "Zhiqiang Lv"
                },
                "author": "Zhiqiang Lv",
                "arxiv_comment": "Submitted for review in January 22, 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06738v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06738v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10659v7",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10659v7",
                "updated": "2025-05-14T04:38:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    38,
                    42,
                    2,
                    134,
                    0
                ],
                "published": "2024-11-16T01:39:44Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    1,
                    39,
                    44,
                    5,
                    321,
                    0
                ],
                "title": "Spineless Traversal for Layout Invalidation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spineless Traversal for Layout Invalidation"
                },
                "summary": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Latency is a major concern for web rendering engines like those in Chrome,\nSafari, and Firefox. These engines reduce latency by using an incremental\nlayout algorithm to redraw the page when the user interacts with it. In such an\nalgorithm, elements that change frame-to-frame are marked dirty, and only those\nelements are processed to draw the next frame, dramatically reducing latency.\nHowever, the standard incremental layout algorithm must search the page for\ndirty elements, accessing auxiliary elements in the process. These auxiliary\nelements add cache misses and stalled cycles, and are responsible for a sizable\nfraction of all layout latency. We introduce a new, faster incremental layout\nalgorithm called Spineless Traversal. Spineless Traversal uses a\ncache-friendlier priority queue algorithm that avoids accessing auxiliary nodes\nand thus reduces cache traffic and stalls. This leads to dramatic speedups on\nthe most latency-critical interactions such as hovering, typing, and animation.\nMoreover, thanks to numerous low-level optimizations, Spineless Traversal is\ncompetitive across the whole spectrum of incremental layout workloads.\nSpineless Traversal is faster than the standard approach on 83.0% of 2216\nbenchmarks, with a mean speedup of 1.80x concentrated in the most\nlatency-critical interactions."
                },
                "authors": [
                    {
                        "name": "Marisa Kirisame"
                    },
                    {
                        "name": "Tiezhi Wang"
                    },
                    {
                        "name": "Pavel Panchekha"
                    }
                ],
                "author_detail": {
                    "name": "Pavel Panchekha"
                },
                "author": "Pavel Panchekha",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10659v7",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10659v7",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.18599v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.18599v2",
                "updated": "2025-05-14T04:22:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    4,
                    22,
                    24,
                    2,
                    134,
                    0
                ],
                "published": "2025-03-24T11:56:50Z",
                "published_parsed": [
                    2025,
                    3,
                    24,
                    11,
                    56,
                    50,
                    0,
                    83,
                    0
                ],
                "title": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Oaken: Fast and Efficient LLM Serving with Online-Offline Hybrid KV\n  Cache Quantization"
                },
                "summary": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern Large Language Model serving system batches multiple requests to\nachieve high throughput, while batching attention operations is challenging,\nrendering memory bandwidth a critical bottleneck. The community relies on\nhigh-end GPUs with multiple high-bandwidth memory channels. Unfortunately,\nHBM's high bandwidth often comes at the expense of limited memory capacity,\nwhich reduces core utilization and increases costs. Recent advancements\nenabling longer contexts for LLMs have substantially increased the key-value\ncache size, further intensifying the pressures on memory capacity. The\nliterature has explored KV cache quantization techniques, which commonly use\nlow bitwidth for most values, selectively using higher bitwidth for outlier\nvalues. While this approach helps achieve high accuracy and low bitwidth\nsimultaneously, it comes with the limitation that cost for online outlier\ndetection is excessively high, negating the advantages. We propose Oaken, an\nacceleration solution that achieves high accuracy and high performance\nsimultaneously through co-designing algorithm and hardware. To effectively find\na sweet spot in the accuracy-performance trade-off space of KV cache\nquantization, Oaken employs an online-offline hybrid approach, setting outlier\nthresholds offline, which are then used to determine the quantization scale\nonline. To translate the proposed algorithmic technique into tangible\nperformance gains, Oaken also comes with custom quantization engines and memory\nmanagement units that can be integrated with any LLM accelerators. We built an\nOaken accelerator on top of an LLM accelerator, LPU, and conducted a\ncomprehensive evaluation. Our experiments show that for a batch size of 256,\nOaken achieves up to 1.58x throughput improvement over NVIDIA A100 GPU,\nincurring a minimal accuracy loss of only 0.54\\% on average, compared to\nstate-of-the-art KV cache quantization techniques."
                },
                "authors": [
                    {
                        "name": "Minsu Kim"
                    },
                    {
                        "name": "Seongmin Hong"
                    },
                    {
                        "name": "RyeoWook Ko"
                    },
                    {
                        "name": "Soongyu Choi"
                    },
                    {
                        "name": "Hunjong Lee"
                    },
                    {
                        "name": "Junsoo Kim"
                    },
                    {
                        "name": "Joo-Young Kim"
                    },
                    {
                        "name": "Jongse Park"
                    }
                ],
                "author_detail": {
                    "name": "Jongse Park"
                },
                "author": "Jongse Park",
                "arxiv_doi": "10.1145/3695053.3731019",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3695053.3731019",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.18599v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.18599v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "16 pages, 14 figures, and 4 tables",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09081v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09081v1",
                "updated": "2025-05-14T02:29:46Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T02:29:46Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    2,
                    29,
                    46,
                    2,
                    134,
                    0
                ],
                "title": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SALM: A Multi-Agent Framework for Language Model-Driven Social Network\n  Simulation"
                },
                "summary": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Contemporary approaches to agent-based modeling (ABM) of social systems have\ntraditionally emphasized rule-based behaviors, limiting their ability to\ncapture nuanced dynamics by moving beyond predefined rules and leveraging\ncontextual understanding from LMs of human social interaction. This paper\npresents SALM (Social Agent LM Framework), a novel approach for integrating\nlanguage models (LMs) into social network simulation that achieves\nunprecedented temporal stability in multi-agent scenarios. Our primary\ncontributions include: (1) a hierarchical prompting architecture enabling\nstable simulation beyond 4,000 timesteps while reducing token usage by 73%, (2)\nan attention-based memory system achieving 80% cache hit rates (95% CI [78%,\n82%]) with sub-linear memory growth of 9.5%, and (3) formal bounds on\npersonality stability. Through extensive validation against SNAP ego networks,\nwe demonstrate the first LLM-based framework capable of modeling long-term\nsocial phenomena while maintaining empirically validated behavioral fidelity."
                },
                "authors": [
                    {
                        "name": "Gaurav Koley"
                    }
                ],
                "author_detail": {
                    "name": "Gaurav Koley"
                },
                "author": "Gaurav Koley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09081v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09081v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.MA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09040v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09040v1",
                "updated": "2025-05-14T00:41:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "published": "2025-05-14T00:41:44Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    0,
                    41,
                    44,
                    2,
                    134,
                    0
                ],
                "title": "RT-cache: Efficient Robot Trajectory Retrieval System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RT-cache: Efficient Robot Trajectory Retrieval System"
                },
                "summary": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper introduces RT-cache, a novel trajectorymemory pipeline that\naccelerates real-world robot inference by leveraging big-data retrieval and\nlearning from experience. While modern Vision-Language-Action (VLA) models can\nhandle diverse robotic tasks, they often incur high per-step inference costs,\nresulting in significant latency, sometimes minutes per task. In contrast,\nRT-cache stores a large-scale Memory of previously successful robot\ntrajectories and retrieves relevant multistep motion snippets, drastically\nreducing inference overhead. By integrating a Memory Builder with a Trajectory\nRetrieval, we develop an efficient retrieval process that remains tractable\neven for extremely large datasets. RT-cache flexibly accumulates real-world\nexperiences and replays them whenever the current scene matches past states,\nadapting quickly to new or unseen environments with only a few additional\nsamples. Experiments on the Open-X Embodiment Dataset and other real-world data\ndemonstrate that RT-cache completes tasks both faster and more successfully\nthan a baseline lacking retrieval, suggesting a practical, data-driven solution\nfor real-time manipulation."
                },
                "authors": [
                    {
                        "name": "Owen Kwon"
                    },
                    {
                        "name": "Abraham George"
                    },
                    {
                        "name": "Alison Bartsch"
                    },
                    {
                        "name": "Amir Barati Farimani"
                    }
                ],
                "author_detail": {
                    "name": "Amir Barati Farimani"
                },
                "author": "Amir Barati Farimani",
                "arxiv_comment": "9 pages, 5 figures. Submitted to an IEEE robotics conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09040v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09040v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08958v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08958v1",
                "updated": "2025-05-13T20:51:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T20:51:59Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    20,
                    51,
                    59,
                    1,
                    133,
                    0
                ],
                "title": "Adaptive Entanglement Generation for Quantum Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Adaptive Entanglement Generation for Quantum Routing"
                },
                "summary": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Entanglement generation in long-distance quantum networks is a difficult\nprocess due to resource limitations and the probabilistic nature of\nentanglement swapping. To maximize success probability, existing quantum\nrouting algorithms employ computationally expensive solutions (e.g., linear\nprogramming) to determine which links to entangle and use for end-to-end\nentanglement generation. Such optimization methods, however, cannot meet the\ndelay requirements of real-world quantum networks, necessitating swift yet\nefficient real-time optimization models. In this paper, we propose\nreinforcement learning (RL)-based models to determine which links to entangle\nand proactively swap to meet connection requests. We show that the proposed\nRL-based approach is 20x faster compared to linear programming. Moreover, we\nshow that one can take advantage of the longevity of entanglements to (i) cache\nentangled links for future use and (ii) proactively swap entanglement on\nhigh-demand path segments, thereby increasing the likelihood of request\nsuccess. Through comprehensive simulations, we demonstrate that caching unused\nentanglements leads to a 10-15% improvement in the performance of\nstate-of-the-art quantum routing algorithms. Complementing caching with\nproactive entanglement swapping further enhances the request success rate by up\nto 52.55%."
                },
                "authors": [
                    {
                        "name": "Tasdiqul Islam"
                    },
                    {
                        "name": "Md Arifuzzaman"
                    },
                    {
                        "name": "Engin Arslan"
                    }
                ],
                "author_detail": {
                    "name": "Engin Arslan"
                },
                "author": "Engin Arslan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08958v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08958v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.01723v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.01723v5",
                "updated": "2025-05-13T17:43:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    17,
                    43,
                    47,
                    1,
                    133,
                    0
                ],
                "published": "2024-10-02T16:34:29Z",
                "published_parsed": [
                    2024,
                    10,
                    2,
                    16,
                    34,
                    29,
                    2,
                    276,
                    0
                ],
                "title": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "HarmoniCa: Harmonizing Training and Inference for Better Feature Caching\n  in Diffusion Transformer Acceleration"
                },
                "summary": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformers (DiTs) excel in generative tasks but face practical\ndeployment challenges due to high inference costs. Feature caching, which\nstores and retrieves redundant computations, offers the potential for\nacceleration. Existing learning-based caching, though adaptive, overlooks the\nimpact of the prior timestep. It also suffers from misaligned\nobjectives--aligned predicted noise vs. high-quality images--between training\nand inference. These two discrepancies compromise both performance and\nefficiency. To this end, we harmonize training and inference with a novel\nlearning-based caching framework dubbed HarmoniCa. It first incorporates\nStep-Wise Denoising Training (SDT) to ensure the continuity of the denoising\nprocess, where prior steps can be leveraged. In addition, an Image Error\nProxy-Guided Objective (IEPO) is applied to balance image quality against cache\nutilization through an efficient proxy to approximate the image error.\nExtensive experiments across $8$ models, $4$ samplers, and resolutions from\n$256\\times256$ to $2K$ demonstrate superior performance and speedup of our\nframework. For instance, it achieves over $40\\%$ latency reduction (i.e.,\n$2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$.\nRemarkably, our image-free approach reduces training time by $25\\%$ compared\nwith the previous method. Our code is available at\nhttps://github.com/ModelTC/HarmoniCa."
                },
                "authors": [
                    {
                        "name": "Yushi Huang"
                    },
                    {
                        "name": "Zining Wang"
                    },
                    {
                        "name": "Ruihao Gong"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Xinjie Zhang"
                    },
                    {
                        "name": "Jinyang Guo"
                    },
                    {
                        "name": "Xianglong Liu"
                    },
                    {
                        "name": "Jun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhang"
                },
                "author": "Jun Zhang",
                "arxiv_comment": "Accepted by ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.01723v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.01723v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08587v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08587v1",
                "updated": "2025-05-13T13:58:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T13:58:22Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    13,
                    58,
                    22,
                    1,
                    133,
                    0
                ],
                "title": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Two-Level Sketching Alternating Anderson acceleration for Complex\n  Physics Applications"
                },
                "summary": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a novel two-level sketching extension of the Alternating\nAnderson-Picard (AAP) method for accelerating fixed-point iterations in\nchallenging single- and multi-physics simulations governed by discretized\npartial differential equations. Our approach combines a static, physics-based\nprojection that reduces the least-squares problem to the most informative field\n(e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage\ndriven by a backward stability analysis under Lipschitz continuity. We\nintroduce inexpensive estimators for stability thresholds and cache-aware\nrandomized selection strategies to balance computational cost against\nmemory-access overhead. The resulting algorithm solves reduced least-squares\nsystems in place, minimizes memory footprints, and seamlessly alternates\nbetween low-cost Picard updates and Anderson mixing. Implemented in Julia, our\ntwo-level sketching AAP achieves up to 50% time-to-solution reductions compared\nto standard Anderson acceleration-without degrading convergence rates-on\nbenchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes\nformulations at varying problem sizes. These results demonstrate the method's\nrobustness, scalability, and potential for integration into high-performance\nscientific computing frameworks. Our implementation is available open-source in\nthe AAP.jl library."
                },
                "authors": [
                    {
                        "name": "Nicols A. Barnafi"
                    },
                    {
                        "name": "Massimiliano Lupo Pasini"
                    }
                ],
                "author_detail": {
                    "name": "Massimiliano Lupo Pasini"
                },
                "author": "Massimiliano Lupo Pasini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08587v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08587v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "math.NA",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "math.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.NA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "65N12, 65N22, 65K10, 65F10, 65F99, 65B99",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13989v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13989v2",
                "updated": "2025-05-13T09:36:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    9,
                    36,
                    3,
                    1,
                    133,
                    0
                ],
                "published": "2025-04-18T13:46:58Z",
                "published_parsed": [
                    2025,
                    4,
                    18,
                    13,
                    46,
                    58,
                    4,
                    108,
                    0
                ],
                "title": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Gradual Binary Search and Dimension Expansion : A general method for\n  activation quantization in LLMs"
                },
                "summary": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have become pivotal in artificial intelligence,\ndemonstrating strong capabilities in reasoning, understanding, and generating\ndata. However, their deployment on edge devices is hindered by their\nsubstantial size, often reaching several billion parameters. Quantization is a\nwidely used method to reduce memory usage and inference time, however LLMs\npresent unique challenges due to the prevalence of outliers in their\nactivations. In this work, we leverage the theoretical advantages of Hadamard\nmatrices over random rotation matrices to push the boundaries of quantization\nin LLMs. We demonstrate that Hadamard matrices are more effective in reducing\noutliers, which are a significant obstacle in achieving low-bit quantization.\nOur method based on a gradual binary search enables 3-bit quantization for\nweights, activations, and key-value (KV) caches, resulting in a 40% increase in\naccuracy on common benchmarks compared to SoTA methods. We extend the use of\nrotation matrices to support non-power-of-2 embedding dimensions, similar to\nthe Qwen architecture, by employing the Paley algorithm. We theoretically\ndemonstrates the superiority of Hadamard matrices in reducing outliers.We\nachieved 3-bit quantization for weights, activations, and KV cache,\nsignificantly enhancing model performance. Our experimental results on multiple\nmodels family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of\nour approach, outperforming existing methods and enabling practical 3-bit\nquantization."
                },
                "authors": [
                    {
                        "name": "Lucas Maisonnave"
                    },
                    {
                        "name": "Cyril Moineau"
                    },
                    {
                        "name": "Olivier Bichler"
                    },
                    {
                        "name": "Fabrice Rastello"
                    }
                ],
                "author_detail": {
                    "name": "Fabrice Rastello"
                },
                "author": "Fabrice Rastello",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13989v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13989v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08261v1",
                "updated": "2025-05-13T06:24:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "published": "2025-05-13T06:24:48Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    6,
                    24,
                    48,
                    1,
                    133,
                    0
                ],
                "title": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual\n  Compression for Scalable Knowledge Integration"
                },
                "summary": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid progress in large language models (LLMs) has paved the way for\nnovel approaches in knowledge-intensive tasks. Among these, Cache-Augmented\nGeneration (CAG) has emerged as a promising alternative to Retrieval-Augmented\nGeneration (RAG). CAG minimizes retrieval latency and simplifies system design\nby preloading knowledge into the model's context. However, challenges persist\nin scaling CAG to accommodate large and dynamic knowledge bases effectively.\nThis paper introduces Adaptive Contextual Compression (ACC), an innovative\ntechnique designed to dynamically compress and manage context inputs, enabling\nefficient utilization of the extended memory capabilities of modern LLMs. To\nfurther address the limitations of standalone CAG, we propose a Hybrid CAG-RAG\nFramework, which integrates selective retrieval to augment preloaded contexts\nin scenarios requiring additional information. Comprehensive evaluations on\ndiverse datasets highlight the proposed methods' ability to enhance\nscalability, optimize efficiency, and improve multi-hop reasoning performance,\noffering practical solutions for real-world knowledge integration challenges."
                },
                "authors": [
                    {
                        "name": "Rishabh Agrawal"
                    },
                    {
                        "name": "Himanshu Kumar"
                    }
                ],
                "author_detail": {
                    "name": "Himanshu Kumar"
                },
                "author": "Himanshu Kumar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.08261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07692v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07692v1",
                "updated": "2025-05-12T15:58:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:58:39Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    58,
                    39,
                    0,
                    132,
                    0
                ],
                "title": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and\n  Dynamic Workloads in Large-scale Cloud Environments"
                },
                "summary": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-tenant architectures enhance the elasticity and resource utilization of\nNoSQL databases by allowing multiple tenants to co-locate and share resources.\nHowever, in large-scale cloud environments, the diverse and dynamic nature of\nworkloads poses significant challenges for multi-tenant NoSQL databases. Based\non our practical observations, we have identified three crucial challenges: (1)\nthe impact of caching on performance isolation, as cache hits alter request\nexecution and resource consumption, leading to inaccurate traffic control; (2)\nthe dynamic changes in traffic, with changes in tenant traffic trends causing\nthrottling or resource wastage, and changes in access distribution causing hot\nkey pressure or cache hit ratio drops; and (3) the imbalanced layout of data\nnodes due to tenants' diverse resource requirements, leading to low resource\nutilization. To address these challenges, we introduce ABase, a multi-tenant\nNoSQL serverless database developed at ByteDance. ABase introduces a two-layer\ncaching mechanism with a cache-aware isolation mechanism to ensure accurate\nresource consumption estimates. Furthermore, ABase employs a predictive\nautoscaling policy to dynamically adjust resources in response to tenant\ntraffic changes and a multi-resource rescheduling algorithm to balance resource\nutilization across data nodes. With these innovations, ABase has successfully\nserved ByteDance's large-scale cloud environment, supporting a total workload\nthat has achieved a peak QPS of over 13 billion and total storage exceeding 1\nEB."
                },
                "authors": [
                    {
                        "name": "Rong Kang"
                    },
                    {
                        "name": "Yanbin Chen"
                    },
                    {
                        "name": "Ye Liu"
                    },
                    {
                        "name": "Fuxin Jiang"
                    },
                    {
                        "name": "Qingshuo Li"
                    },
                    {
                        "name": "Miao Ma"
                    },
                    {
                        "name": "Jian Liu"
                    },
                    {
                        "name": "Guangliang Zhao"
                    },
                    {
                        "name": "Tieying Zhang"
                    },
                    {
                        "name": "Jianjun Chen"
                    },
                    {
                        "name": "Lei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Zhang"
                },
                "author": "Lei Zhang",
                "arxiv_comment": "SIGMOD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07692v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07692v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07680v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07680v1",
                "updated": "2025-05-12T15:46:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T15:46:28Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    15,
                    46,
                    28,
                    0,
                    132,
                    0
                ],
                "title": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in\n  Large Language Models"
                },
                "summary": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) present a critical trade-off between inference\nquality and computational cost: larger models offer superior capabilities but\nincur significant latency, while smaller models are faster but less powerful.\nExisting serving strategies often employ fixed model scales or static two-stage\nspeculative decoding, failing to dynamically adapt to the varying complexities\nof user requests or fluctuations in system performance. This paper introduces\n\\systemname{}, a novel framework that reimagines LLM inference as an adaptive\nrouting problem solved through multi-level speculative decoding. \\systemname{}\ndynamically constructs and optimizes inference \"paths\" (chains of models) based\non real-time feedback, addressing the limitations of static approaches. Our\ncontributions are threefold: (1) An \\textbf{adaptive model chain scheduling}\nmechanism that leverages performance profiling (execution times) and predictive\nsimilarity metrics (derived from token distribution divergence) to continuously\nselect the optimal sequence of draft and verifier models, minimizing predicted\nlatency per generated token. (2) A \\textbf{multi-level collaborative\nverification} framework where intermediate models within the selected chain can\nvalidate speculative tokens, reducing the verification burden on the final,\nmost powerful target model. (3) A \\textbf{synchronized state management} system\nproviding efficient, consistent KV cache handling across heterogeneous models\nin the chain, including precise, low-overhead rollbacks tailored for\nasynchronous batch processing inherent in multi-level speculation. Preliminary\nexperiments demonstrate the validity of our method."
                },
                "authors": [
                    {
                        "name": "Hang Wu"
                    },
                    {
                        "name": "Jianian Zhu"
                    },
                    {
                        "name": "Yinghui Li"
                    },
                    {
                        "name": "Haojie Wang"
                    },
                    {
                        "name": "Biao Hou"
                    },
                    {
                        "name": "Jidong Zhai"
                    }
                ],
                "author_detail": {
                    "name": "Jidong Zhai"
                },
                "author": "Jidong Zhai",
                "arxiv_comment": "10 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07680v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07680v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07350v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07350v1",
                "updated": "2025-05-12T08:44:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T08:44:10Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    44,
                    10,
                    0,
                    132,
                    0
                ],
                "title": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "All-optical electric field sensing with nanodiamond-doped polymer thin\n  films"
                },
                "summary": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The nitrogen-vacancy (NV) center is a photoluminescent defect in diamond that\nexists in different charge states, NV$^-$ and NV$^0$, that are sensitive to the\nNV's nanoscale environment. Here, we show that photoluminescence (PL) from NV\ncenters in fluorescent nanodiamonds (FNDs) can be employed for all-optical\nvoltage sensing based on electric field-induced NV charge state modulation.\nMore than 95% of FNDs integrated into a capacitor device show a transient\nincrease in NV$^-$ PL intensity of up to 31% within 0.1 ms after application of\nan external voltage, accompanied by a simultaneous decrease in NV$^0$ PL. The\nchange in NV$^-$ PL increases with increasing applied voltage from 0 to 100 V,\ncorresponding to an electric field of 0 to 625 kV cm$^ {-1}$ in our devices.\nThe electric field sensitivity of a single FND is 19 V cm$^{-1}$ Hz$^ {-1/2}$.\nWe investigate the NV charge state photodynamics on the millisecond timescale\nand find that the change in NV PL strongly depends on the rate of\nphotoexcitation. We propose a model that qualitatively explains the observed\nchanges in NV PL based on an electric field-induced redistribution of\nphotoexcited electrons from substitutional nitrogen defects to NV centers,\nleading to a transient conversion of NV$^0$ to NV$^-$ centers upon application\nof an external voltage. Our results contribute to the development of FNDs as\nreliable, all-optical, nanoscale electric field sensors in solid-state systems."
                },
                "authors": [
                    {
                        "name": "Roy Styles"
                    },
                    {
                        "name": "Mengke Han"
                    },
                    {
                        "name": "Toon Goris"
                    },
                    {
                        "name": "James Partridge"
                    },
                    {
                        "name": "Brett C. Johnson"
                    },
                    {
                        "name": "Blanca del Rosal"
                    },
                    {
                        "name": "Amanda N. Abraham"
                    },
                    {
                        "name": "Heike Ebendorff-Heidepriem"
                    },
                    {
                        "name": "Brant C. Gibson"
                    },
                    {
                        "name": "Nikolai Dontschuk"
                    },
                    {
                        "name": "Jean-Philippe Tetienne"
                    },
                    {
                        "name": "Philipp Reineck"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Reineck"
                },
                "author": "Philipp Reineck",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07350v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07350v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mes-hall",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mes-hall",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07274v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07274v1",
                "updated": "2025-05-12T06:53:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T06:53:24Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    6,
                    53,
                    24,
                    0,
                    132,
                    0
                ],
                "title": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cache-Efficient Posterior Sampling for Reinforcement Learning with\n  LLM-Derived Priors Across Discrete and Continuous Domains"
                },
                "summary": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Integrating large language models (LLMs) as priors in reinforcement learning\n(RL) offers significant advantages but comes with substantial computational\ncosts. We present a principled cache-efficient framework for posterior sampling\nwith LLM-derived priors that dramatically reduces these costs while maintaining\nhigh performance. At the core of our approach is an adaptive caching mechanism,\nwhere cache parameters are meta-optimized using surrogate gradients derived\nfrom policy performance. This design enables efficient inference across both\ndiscrete text environments (e.g., TextWorld, ALFWorld) and continuous control\ndomains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries\nand 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU)\nwhile retaining 96--98\\% of uncached performance. Our theoretical analysis\nprovides KL divergence bounds on approximation quality, validated empirically.\nThe framework extends to offline RL, where our CQL-Prior variant improves\nperformance by 14--29\\% and reduces training time by 38--40\\%. Extensive\nevaluations across a diverse suite of eight tasks demonstrate the\ngeneralizability and practical viability of LLM-guided RL in\nresource-constrained settings."
                },
                "authors": [
                    {
                        "name": "Ibne Farabi Shihab"
                    },
                    {
                        "name": "Sanjeda Akter"
                    },
                    {
                        "name": "Anuj Sharma"
                    }
                ],
                "author_detail": {
                    "name": "Anuj Sharma"
                },
                "author": "Anuj Sharma",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07274v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07274v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07239v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07239v1",
                "updated": "2025-05-12T05:29:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T05:29:30Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    29,
                    30,
                    0,
                    132,
                    0
                ],
                "title": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comet: Accelerating Private Inference for Large Language Model by\n  Predicting Activation Sparsity"
                },
                "summary": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the growing use of large language models (LLMs) hosted on cloud\nplatforms to offer inference services, privacy concerns about the potential\nleakage of sensitive information are escalating. Secure multi-party computation\n(MPC) is a promising solution to protect the privacy in LLM inference. However,\nMPC requires frequent inter-server communication, causing high performance\noverhead.\n  Inspired by the prevalent activation sparsity of LLMs, where most neuron are\nnot activated after non-linear activation functions, we propose an efficient\nprivate inference system, Comet. This system employs an accurate and fast\npredictor to predict the sparsity distribution of activation function output.\nAdditionally, we introduce a new private inference protocol. It efficiently and\nsecurely avoids computations involving zero values by exploiting the spatial\nlocality of the predicted sparse distribution. While this computation-avoidance\napproach impacts the spatiotemporal continuity of KV cache entries, we address\nthis challenge with a low-communication overhead cache refilling strategy that\nmerges miss requests and incorporates a prefetching mechanism. Finally, we\nevaluate Comet on four common LLMs and compare it with six state-of-the-art\nprivate inference systems. Comet achieves a 1.87x-2.63x speedup and a\n1.94x-2.64x communication reduction."
                },
                "authors": [
                    {
                        "name": "Guang Yan"
                    },
                    {
                        "name": "Yuhui Zhang"
                    },
                    {
                        "name": "Zimu Guo"
                    },
                    {
                        "name": "Lutan Zhao"
                    },
                    {
                        "name": "Xiaojun Chen"
                    },
                    {
                        "name": "Chen Wang"
                    },
                    {
                        "name": "Wenhao Wang"
                    },
                    {
                        "name": "Dan Meng"
                    },
                    {
                        "name": "Rui Hou"
                    }
                ],
                "author_detail": {
                    "name": "Rui Hou"
                },
                "author": "Rui Hou",
                "arxiv_doi": "10.1109/SP61157.2025.00182",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/SP61157.2025.00182",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.07239v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07239v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Accepted to SP 2025",
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07203v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07203v1",
                "updated": "2025-05-12T03:22:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "published": "2025-05-12T03:22:29Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    3,
                    22,
                    29,
                    0,
                    132,
                    0
                ],
                "title": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PrefillOnly: An Inference Engine for Prefill-only Workloads in Large\n  Language Model Applications"
                },
                "summary": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Besides typical generative applications, like ChatGPT, GitHub Copilot, and\nCursor, we observe an emerging trend that LLMs are increasingly used in\ntraditional discriminative tasks, such as recommendation, credit verification,\nand data labeling. The key characteristic of these emerging use cases is that\nthe LLM generates only a single output token, rather than an arbitrarily long\nsequence of tokens. We call this prefill-only workload. However, since existing\nLLM engines assume arbitrary output lengths, they fail to leverage the unique\nproperties of prefill-only workloads. In this paper, we present PrefillOnly,\nthe first LLM inference engine that improves the inference throughput and\nlatency by fully embracing the properties of prefill-only workloads. First,\nsince it generates only one token, PrefillOnly only needs to store the KV cache\nof only the last computed layer, rather than of all layers. This drastically\nreduces the GPU memory footprint of LLM inference and allows handling long\ninputs without using solutions that reduces throughput, such as cross-GPU KV\ncache parallelization. Second, because the output length is fixed, rather than\narbitrary, PrefillOnly can precisely determine the job completion time (JCT) of\neach prefill-only request before it starts. This enables efficient JCT-aware\nscheduling policies such as shortest remaining job first. PrefillOnly can\nprocess upto 4x larger queries per second without inflating average and P99\nlatency."
                },
                "authors": [
                    {
                        "name": "Kuntai Du"
                    },
                    {
                        "name": "Bowen Wang"
                    },
                    {
                        "name": "Chen Zhang"
                    },
                    {
                        "name": "Yiming Cheng"
                    },
                    {
                        "name": "Qing Lan"
                    },
                    {
                        "name": "Hejian Sang"
                    },
                    {
                        "name": "Yihua Cheng"
                    },
                    {
                        "name": "Jiayi Yao"
                    },
                    {
                        "name": "Xiaoxuan Liu"
                    },
                    {
                        "name": "Yifan Qiao"
                    },
                    {
                        "name": "Ion Stoica"
                    },
                    {
                        "name": "Junchen Jiang"
                    }
                ],
                "author_detail": {
                    "name": "Junchen Jiang"
                },
                "author": "Junchen Jiang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07203v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07203v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06901v1",
                "updated": "2025-05-11T08:44:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "published": "2025-05-11T08:44:31Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    8,
                    44,
                    31,
                    6,
                    131,
                    0
                ],
                "title": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware\n  Cache Compression"
                },
                "summary": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated transformative capabilities\nacross diverse artificial intelligence applications, yet their deployment is\nhindered by substantial memory and computational demands, especially in\nresource-constrained environments. Quantization techniques have emerged as a\ncritical solution, reducing data precision to enhance memory and computational\nefficiency. However, existing methods often suffer from high runtime overheads\nand potential accuracy degradation. To address these challenges, we propose\nEcco, an entropy-based cache compression technique tailored for LLMs. Ecco\ncombines group-wise and non-uniform quantization with pre-defined shared\nk-means patterns and Huffman coding to exploit the inherent entropy\ncharacteristics of LLM cache data. Recognizing the inefficiencies of\ntraditional Huffman coding in terms of parallelism and latency, we introduce a\nnovel parallel Huffman-based decoding process with a multi-stage pipeline\ndesign, reducing latency by two orders of magnitude and achieving throughput\ncomparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco\nachieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art\nAWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all\nwhile increasing memory capacity by nearly 4$\\times$ and maintaining\nstate-of-the-art LLM accuracy. These results underscore the effectiveness of\nour entropy-based cache compression in enhancing LLM performance and\nefficiency, paving the way for more deployable large-scale AI models."
                },
                "authors": [
                    {
                        "name": "Feng Cheng"
                    },
                    {
                        "name": "Cong Guo"
                    },
                    {
                        "name": "Chiyue Wei"
                    },
                    {
                        "name": "Junyao Zhang"
                    },
                    {
                        "name": "Changchun Zhou"
                    },
                    {
                        "name": "Edward Hanson"
                    },
                    {
                        "name": "Jiaqi Zhang"
                    },
                    {
                        "name": "Xiaoxiao Liu"
                    },
                    {
                        "name": "Hai \"Helen\" Li"
                    },
                    {
                        "name": "Yiran Chen"
                    }
                ],
                "author_detail": {
                    "name": "Yiran Chen"
                },
                "author": "Yiran Chen",
                "arxiv_comment": "ISCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06625v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06625v1",
                "updated": "2025-05-10T12:16:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T12:16:50Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    12,
                    16,
                    50,
                    5,
                    130,
                    0
                ],
                "title": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated\n  NPUs"
                },
                "summary": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the rapid development of DNN applications, multi-tenant execution, where\nmultiple DNNs are co-located on a single SoC, is becoming a prevailing trend.\nAlthough many methods are proposed in prior works to improve multi-tenant\nperformance, the impact of shared cache is not well studied. This paper\nproposes CaMDN, an architecture-scheduling co-design to enhance cache\nefficiency for multi-tenant DNNs on integrated NPUs. Specifically, a\nlightweight architecture is proposed to support model-exclusive, NPU-controlled\nregions inside shared cache to eliminate unexpected cache contention. Moreover,\na cache scheduling method is proposed to improve shared cache utilization. In\nparticular, it includes a cache-aware mapping method for adaptability to the\nvarying available cache capacity and a dynamic allocation algorithm to adjust\nthe usage among co-located DNNs at runtime. Compared to prior works, CaMDN\nreduces the memory access by 33.4% on average and achieves a model speedup of\nup to 2.56$\\times$ (1.88$\\times$ on average)."
                },
                "authors": [
                    {
                        "name": "Tianhao Cai"
                    },
                    {
                        "name": "Liang Wang"
                    },
                    {
                        "name": "Limin Xiao"
                    },
                    {
                        "name": "Meng Han"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Lin Sun"
                    },
                    {
                        "name": "Xiaojian Liao"
                    }
                ],
                "author_detail": {
                    "name": "Xiaojian Liao"
                },
                "author": "Xiaojian Liao",
                "arxiv_comment": "7 pages, 9 figures. This paper has been accepted to the 2025 Design\n  Automation Conference (DAC)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06625v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06625v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06556v1",
                "updated": "2025-05-10T07:57:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "published": "2025-05-10T07:57:02Z",
                "published_parsed": [
                    2025,
                    5,
                    10,
                    7,
                    57,
                    2,
                    5,
                    130,
                    0
                ],
                "title": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TierBase: A Workload-Driven Cost-Optimized Key-Value Store"
                },
                "summary": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the current era of data-intensive applications, the demand for\nhigh-performance, cost-effective storage solutions is paramount. This paper\nintroduces a Space-Performance Cost Model for key-value store, designed to\nguide cost-effective storage configuration decisions. The model quantifies the\ntrade-offs between performance and storage costs, providing a framework for\noptimizing resource allocation in large-scale data serving environments. Guided\nby this cost model, we present TierBase, a distributed key-value store\ndeveloped by Ant Group that optimizes total cost by strategically synchronizing\ndata between cache and storage tiers, maximizing resource utilization and\neffectively handling skewed workloads. To enhance cost-efficiency, TierBase\nincorporates several optimization techniques, including pre-trained data\ncompression, elastic threading mechanisms, and the utilization of persistent\nmemory. We detail TierBase's architecture, key components, and the\nimplementation of cost optimization strategies. Extensive evaluations using\nboth synthetic benchmarks and real-world workloads demonstrate TierBase's\nsuperior cost-effectiveness compared to existing solutions. Furthermore, case\nstudies from Ant Group's production environments showcase TierBase's ability to\nachieve up to 62% cost reduction in primary scenarios, highlighting its\npractical impact in large-scale online data serving."
                },
                "authors": [
                    {
                        "name": "Zhitao Shen"
                    },
                    {
                        "name": "Shiyu Yang"
                    },
                    {
                        "name": "Weibo Chen"
                    },
                    {
                        "name": "Kunming Wang"
                    },
                    {
                        "name": "Yue Li"
                    },
                    {
                        "name": "Jiabao Jin"
                    },
                    {
                        "name": "Wei Jia"
                    },
                    {
                        "name": "Junwei Chen"
                    },
                    {
                        "name": "Yuan Su"
                    },
                    {
                        "name": "Xiaoxia Duan"
                    },
                    {
                        "name": "Wei Chen"
                    },
                    {
                        "name": "Lei Wang"
                    },
                    {
                        "name": "Jie Song"
                    },
                    {
                        "name": "Ruoyi Ruan"
                    },
                    {
                        "name": "Xuemin Lin"
                    }
                ],
                "author_detail": {
                    "name": "Xuemin Lin"
                },
                "author": "Xuemin Lin",
                "arxiv_comment": "Accepted by ICDE 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DB",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07872v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07872v1",
                "updated": "2025-05-09T21:05:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T21:05:20Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    21,
                    5,
                    20,
                    4,
                    129,
                    0
                ],
                "title": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Revenue Optimization in Video Caching Networks with Privacy-Preserving\n  Demand Predictions"
                },
                "summary": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Performance of video streaming, which accounts for most of the traffic in\nwireless communication, can be significantly improved by caching popular videos\nat the wireless edge. Determining the cache content that optimizes performance\n(defined via a revenue function) is thus an important task, and prediction of\nthe future demands based on past history can make this process much more\nefficient. However, since practical video caching networks involve various\nparties (e.g., users, isp, and csp) that do not wish to reveal information such\nas past history to each other, privacy-preserving solutions are required.\nMotivated by this, we propose a proactive caching method based on users'\nprivacy-preserving multi-slot future demand predictions -- obtained from a\ntrained Transformer -- to optimize revenue. Specifically, we first use a\nprivacy-preserving fl algorithm to train a Transformer to predict multi-slot\nfuture demands of the users. However, prediction accuracy is not perfect and\ndecreases the farther into the future the prediction is done. We model the\nimpact of prediction errors invoking the file popularities, based on which we\nformulate a long-term system revenue optimization to make the cache placement\ndecisions. As the formulated problem is NP-hard, we use a greedy algorithm to\nefficiently obtain an approximate solution. Simulation results validate that\n(i) the fl solution achieves results close to the centralized\n(non-privacy-preserving) solution and (ii) optimization of revenue may provide\ndifferent solutions than the classical chr criterion."
                },
                "authors": [
                    {
                        "name": "Yijing Zhang"
                    },
                    {
                        "name": "Ferdous Pervej"
                    },
                    {
                        "name": "Andreas F. Molisch"
                    }
                ],
                "author_detail": {
                    "name": "Andreas F. Molisch"
                },
                "author": "Andreas F. Molisch",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07872v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07872v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.06095v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.06095v3",
                "updated": "2025-05-09T07:26:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    7,
                    26,
                    29,
                    4,
                    129,
                    0
                ],
                "published": "2024-06-10T08:26:27Z",
                "published_parsed": [
                    2024,
                    6,
                    10,
                    8,
                    26,
                    27,
                    0,
                    162,
                    0
                ],
                "title": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An extension of C++ with memory-centric specifications for HPC to reduce\n  memory footprints and streamline MPI development"
                },
                "summary": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The C++ programming language and its cousins lean towards a\nmemory-inefficient storage of structs: The compiler inserts helper bits such\nthat individual instance variables fit to byte or cache boundaries, while it is\nnot able to exploit knowledge about the range of integers, enums or bitsets.\nFurthermore, the language provides neither support for data exchange via MPI\nnor for arbitrary floating-point precisions. We propose C++ attributes through\nwhich developers can guide the compiler what memory arrangements would be\nbeneficial: Can multiple booleans or integers with limited range be squeezed\ninto one bit field, do floating point numbers hold fewer significant bits than\nin the IEEE standard, or does the code benefit from a MPI datatype for subsets\nof attributes? The extension offers the opportunity to fall back to normal\nalignment via plain C++ assignments, no dependencies upon external libraries\nare introduced, and the resulting code remains standard C++ subject to some\nweakened guarantees on addresses and pointer arithmetics. Our work implements\nthe language annotations within LLVM and demonstrates their potential impact,\nboth upon the runtime and the memory footprint, through smoothed particle\nhydrodynamics (SPH) benchmarks. They uncover the potential gains in terms of\nperformance and development productivity."
                },
                "authors": [
                    {
                        "name": "Pawel K. Radtke"
                    },
                    {
                        "name": "Cristian G. Barrera-Hinojosa"
                    },
                    {
                        "name": "Mladen Ivkovic"
                    },
                    {
                        "name": "Tobias Weinzierl"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Weinzierl"
                },
                "author": "Tobias Weinzierl",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.06095v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.06095v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MS",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05829v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05829v1",
                "updated": "2025-05-09T06:56:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T06:56:17Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    6,
                    56,
                    17,
                    4,
                    129,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Increment-Calibrated Caching with\n  Channel-Aware Singular Value Decomposition"
                },
                "summary": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion transformer (DiT) models have achieved remarkable success in image\ngeneration, thanks for their exceptional generative capabilities and\nscalability. Nonetheless, the iterative nature of diffusion models (DMs)\nresults in high computation complexity, posing challenges for deployment.\nAlthough existing cache-based acceleration methods try to utilize the inherent\ntemporal similarity to skip redundant computations of DiT, the lack of\ncorrection may induce potential quality degradation. In this paper, we propose\nincrement-calibrated caching, a training-free method for DiT acceleration,\nwhere the calibration parameters are generated from the pre-trained model\nitself with low-rank approximation. To deal with the possible correction\nfailure arising from outlier activations, we introduce channel-aware Singular\nValue Decomposition (SVD), which further strengthens the calibration effect.\nExperimental results show that our method always achieve better performance\nthan existing naive caching methods with a similar computation resource budget.\nWhen compared with 35-step DDIM, our method eliminates more than 45%\ncomputation and improves IS by 12 at the cost of less than 0.06 FID increase.\nCode is available at https://github.com/ccccczzy/icc."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Chen"
                    },
                    {
                        "name": "Keyi Li"
                    },
                    {
                        "name": "Yifan Jia"
                    },
                    {
                        "name": "Le Ye"
                    },
                    {
                        "name": "Yufei Ma"
                    }
                ],
                "author_detail": {
                    "name": "Yufei Ma"
                },
                "author": "Yufei Ma",
                "arxiv_comment": "accepted by CVPR2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05829v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05829v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05772v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05772v1",
                "updated": "2025-05-09T04:17:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "published": "2025-05-09T04:17:05Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    4,
                    17,
                    5,
                    4,
                    129,
                    0
                ],
                "title": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Sparse Attention Remapping with Clustering for Efficient LLM Decoding on\n  PIM"
                },
                "summary": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer-based models are the foundation of modern machine learning, but\ntheir execution, particularly during autoregressive decoding in large language\nmodels (LLMs), places significant pressure on memory systems due to frequent\nmemory accesses and growing key-value (KV) caches. This creates a bottleneck in\nmemory bandwidth, especially as context lengths increase. Processing-in-memory\n(PIM) architectures are a promising solution, offering high internal bandwidth\nand compute parallelism near memory. However, current PIM designs are primarily\noptimized for dense attention and struggle with the dynamic, irregular access\npatterns introduced by modern KV cache sparsity techniques. Consequently, they\nsuffer from workload imbalance, reducing throughput and resource utilization.\nIn this work, we propose STARC, a novel sparsity-optimized data mapping scheme\ntailored specifically for efficient LLM decoding on PIM architectures. STARC\nclusters KV pairs by semantic similarity and maps them to contiguous memory\nregions aligned with PIM bank structures. During decoding, queries retrieve\nrelevant tokens at cluster granularity by matching against precomputed\ncentroids, enabling selective attention and parallel processing without\nfrequent reclustering or data movement overhead. Experiments on the HBM-PIM\nsystem show that, compared to common token-wise sparsity methods, STARC reduces\nattention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a\nKV cache budget of 1024, it achieves up to 54%--74% latency reduction and\n45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC\nmaintains model accuracy comparable to state-of-the-art sparse attention\nmethods, demonstrating its effectiveness in enabling efficient and\nhardware-friendly long-context LLM inference on PIM architectures."
                },
                "authors": [
                    {
                        "name": "Zehao Fan"
                    },
                    {
                        "name": "Garrett Gagnon"
                    },
                    {
                        "name": "Zhenyu Liu"
                    },
                    {
                        "name": "Liu Liu"
                    }
                ],
                "author_detail": {
                    "name": "Liu Liu"
                },
                "author": "Liu Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05772v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05772v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.17264v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.17264v3",
                "updated": "2025-05-09T00:31:24Z",
                "updated_parsed": [
                    2025,
                    5,
                    9,
                    0,
                    31,
                    24,
                    4,
                    129,
                    0
                ],
                "published": "2024-09-25T18:21:05Z",
                "published_parsed": [
                    2024,
                    9,
                    25,
                    18,
                    21,
                    5,
                    2,
                    269,
                    0
                ],
                "title": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Medha: Efficiently Serving Multi-Million Context Length LLM Inference\n  Requests Without Approximations"
                },
                "summary": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As large language models (LLMs) handle increasingly longer contexts, serving\nlong inference requests of millions of tokens presents unique challenges. We\nshow that existing work for long context inference is largely based on\ntechniques from long context training, and does not handle the high variability\nin input lengths during inference. This leads to inefficient resource\nutilization, server fragmentation, and head-of-line (HOL) blocking.\n  We present Medha, an end-to-end system for efficient long-context LLM\ninference that addresses these challenges through fine-grained time sharing.\nMedha introduces three key innovations: (1) the mechanism of adaptive prefill\nchunking to help mitigate HOL blocking with preemption; (2) two new parallelism\nstrategies: Sequence Pipeline Parallelism (SPP) to reduce time-to-first-token\nby pipelining prefill chunks, and KV-Cache Parallelism (KVP) to lower\ntime-peroutput-token by distributing decoding across servers; and (3) a novel\ninput-length aware least remaining slack scheduling to meet Service Level\nObjectives (SLOs).\n  Medha enables exact inference scaling beyond 10 million tokens, maintaining\nhigh throughput and low latency across mixed-length workloads. Compared to\nstate-of-the-art systems, Medha reduces server fragmentation, cuts median\nlatency by up to 30x, and improves throughput by over 5x, delivering\nproduction-scale long-context inference without compromising performance on\nshorter requests."
                },
                "authors": [
                    {
                        "name": "Amey Agrawal"
                    },
                    {
                        "name": "Haoran Qiu"
                    },
                    {
                        "name": "Junda Chen"
                    },
                    {
                        "name": "igo Goiri"
                    },
                    {
                        "name": "Chaojie Zhang"
                    },
                    {
                        "name": "Rayyan Shahid"
                    },
                    {
                        "name": "Ramachandran Ramjee"
                    },
                    {
                        "name": "Alexey Tumanov"
                    },
                    {
                        "name": "Esha Choukse"
                    }
                ],
                "author_detail": {
                    "name": "Esha Choukse"
                },
                "author": "Esha Choukse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.17264v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.17264v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05251v1",
                "updated": "2025-05-08T13:56:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T13:56:20Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    13,
                    56,
                    20,
                    3,
                    128,
                    0
                ],
                "title": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Altitude Platform-Based Caching and Multicasting for Rural\n  Connectivity"
                },
                "summary": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Providing efficient and reliable content delivery in rural areas remains a\nsignificant challenge due to the lack of communication infrastructure. To\nbridge the digital divide, this paper investigates the potential of leveraging\nmultiple high-altitude platforms (HAPs) for energy-efficient content delivery\nin wide rural regions. Each caching-enabled HAP is equipped with both\nFree-Space Optical (FSO) transceivers for backhaul links and Radio Frequency\n(RF) antenna arrays for access links. To further enhance network efficiency, we\nconsider a network coding-based multicasting scheme, where different types of\ncontent are treated as distinct multicast sessions. With the objective of\nminimizing long-term power cost, we propose a hierarchical framework that\nintegrates deep reinforcement learn-ing (DRL) and convex optimization to\njointly optimize dynamic caching strategies and resource allocation across the\nnetwork. Simulation results demonstrate that our approach significantly reduces\npower cost compared to several baseline approaches, providing a practical\nsolution for improving rural connectivity."
                },
                "authors": [
                    {
                        "name": "Yongqiang Zhang"
                    },
                    {
                        "name": "Mustafa A. Kishk"
                    },
                    {
                        "name": "Mohamed-Slim Alouini"
                    }
                ],
                "author_detail": {
                    "name": "Mohamed-Slim Alouini"
                },
                "author": "Mohamed-Slim Alouini",
                "arxiv_comment": "13 pages, 8 figures, submitted to IEEE journals for possible\n  publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "49",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.4.0",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05130v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05130v1",
                "updated": "2025-05-08T11:07:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T11:07:35Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    11,
                    7,
                    35,
                    3,
                    128,
                    0
                ],
                "title": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheFL: Efficient Federated Cache Model Fine-Tuning for Vision-Language\n  Models"
                },
                "summary": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large pre-trained Vision-Language Models (VLMs), such as Contrastive\nLanguage-Image Pre-training (CLIP), have exhibited remarkable zero-shot\nperformance across various image classification tasks. Fine-tuning these models\non domain-specific datasets further enhances their effectiveness for downstream\napplications. However, fine-tuning in cloud environments raises significant\nconcerns regarding data security and privacy. Federated Learning (FL) offers a\ndecentralized solution by enabling model training across local clients without\ncentralizing sensitive data, but the high communication and computation costs\nof transmitting full pre-trained models during training limit its scalability.\nAdditionally, non-Independent and Identically Distributed (non-IID) data across\nlocal clients can negatively impact model convergence and performance. To\naddress these challenges, we propose CacheFL, a novel federated learning method\nthat replaces traditional full model fine-tuning with lightweight cache model\nfine-tuning. The cache model is initialized using a class-balanced dataset\ngenerated by a generative pre-trained model, effectively mitigating the impact\nof non-IID data. This cache model is then distributed to local clients for\nfine-tuning, and the updated parameters from each client are aggregated on the\nserver and redistributed. With the updated cache model, the classification\nperformance of CLIP is improved after just a few epochs. By limiting the\ntraining and communication to the cache model, CacheFL significantly reduces\nresource demands while ensuring data privacy and security. Extensive\nexperiments conducted on ImageNet and 10 additional datasets demonstrate that\nCacheFL outperforms traditional approaches in terms of classification accuracy,\nresource efficiency, and privacy preservation."
                },
                "authors": [
                    {
                        "name": "Mengjun Yi"
                    },
                    {
                        "name": "Hanwen Zhang"
                    },
                    {
                        "name": "Hui Dou"
                    },
                    {
                        "name": "Jian Zhao"
                    },
                    {
                        "name": "Furao Shen"
                    }
                ],
                "author_detail": {
                    "name": "Furao Shen"
                },
                "author": "Furao Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05130v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05130v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03762v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03762v2",
                "updated": "2025-05-08T09:05:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    9,
                    5,
                    51,
                    3,
                    128,
                    0
                ],
                "published": "2025-04-20T17:48:54Z",
                "published_parsed": [
                    2025,
                    4,
                    20,
                    17,
                    48,
                    54,
                    6,
                    110,
                    0
                ],
                "title": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CVA6S+: A Superscalar RISC-V Core with High-Throughput Memory\n  Architecture"
                },
                "summary": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Open-source RISC-V cores are increasingly adopted in high-end embedded\ndomains such as automotive, where maximizing instructions per cycle (IPC) is\nbecoming critical. Building on the industry-supported open-source CVA6 core and\nits superscalar variant, CVA6S, we introduce CVA6S+, an enhanced version\nincorporating improved branch prediction, register renaming and enhanced\noperand forwarding. These optimizations enable CVA6S+ to achieve a 43.5%\nperformance improvement over the scalar configuration and 10.9% over CVA6S,\nwith an area overhead of just 9.30% over the scalar core (CVA6). Furthermore,\nwe integrate CVA6S+ with the OpenHW Core-V High-Performance L1 Dcache\n(HPDCache) and report a 74.1% bandwidth improvement over the legacy CVA6 cache\nsubsystem."
                },
                "authors": [
                    {
                        "name": "Riccardo Tedeschi"
                    },
                    {
                        "name": "Gianmarco Ottavi"
                    },
                    {
                        "name": "Cme Allart"
                    },
                    {
                        "name": "Nils Wistoff"
                    },
                    {
                        "name": "Zexin Fu"
                    },
                    {
                        "name": "Filippo Grillotti"
                    },
                    {
                        "name": "Fabio De Ambroggi"
                    },
                    {
                        "name": "Elio Guidetti"
                    },
                    {
                        "name": "Jean-Baptiste Rigaud"
                    },
                    {
                        "name": "Olivier Potin"
                    },
                    {
                        "name": "Jean Roch Coulon"
                    },
                    {
                        "name": "Csar Fuguet"
                    },
                    {
                        "name": "Luca Benini"
                    },
                    {
                        "name": "Davide Rossi"
                    }
                ],
                "author_detail": {
                    "name": "Davide Rossi"
                },
                "author": "Davide Rossi",
                "arxiv_comment": "3 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03762v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03762v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.12110v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.12110v2",
                "updated": "2025-05-08T07:55:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    55,
                    38,
                    3,
                    128,
                    0
                ],
                "published": "2024-06-17T21:43:39Z",
                "published_parsed": [
                    2024,
                    6,
                    17,
                    21,
                    43,
                    39,
                    0,
                    169,
                    0
                ],
                "title": "CacheSquash: Making caches speculation-aware",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CacheSquash: Making caches speculation-aware"
                },
                "summary": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculation is key to achieving high CPU performance, yet it enables risks\nlike Spectre attacks which remain a significant challenge to mitigate without\nincurring substantial performance overheads. These attacks typically unfold in\nthree stages: access, transmit, and receive. Typically, they exploit a cache\ntiming side channel during the transmit and receive phases: speculatively\naccessing sensitive data (access), altering cache state (transmit), and then\nutilizing a cache timing attack (e.g., Flush+Reload) to extract the secret\n(receive). Our key observation is that Spectre attacks only require the\ntransmit instruction to execute and dispatch a request to the cache hierarchy.\nIt need not complete before a misprediction is detected (and mis-speculated\ninstructions squashed) because responses from memory that arrive at the cache\nafter squashing still alter cache state. We propose a novel mitigation,\nCacheSquash, that cancels mis-speculated memory accesses. Immediately upon\nsquashing, a cancellation is sent to the cache hierarchy, propagating\ndownstream and preventing any changes to caches that have not yet received a\nresponse. This minimizes cache state changes, thereby reducing the likelihood\nof Spectre attacks succeeding. We implement CacheSquash on gem5 and show that\nit thwarts practical Spectre attacks, with near-zero performance overheads."
                },
                "authors": [
                    {
                        "name": "Hossam ElAtali"
                    },
                    {
                        "name": "N. Asokan"
                    }
                ],
                "author_detail": {
                    "name": "N. Asokan"
                },
                "author": "N. Asokan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.12110v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.12110v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01658v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01658v2",
                "updated": "2025-05-08T07:08:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    7,
                    8,
                    40,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-03T02:47:43Z",
                "published_parsed": [
                    2025,
                    5,
                    3,
                    2,
                    47,
                    43,
                    5,
                    123,
                    0
                ],
                "title": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Survey on Inference Engines for Large Language Models: Perspectives on\n  Optimization and Efficiency"
                },
                "summary": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) are widely applied in chatbots, code generators,\nand search engines. Workloads such as chain-of-thought, complex reasoning, and\nagent services significantly increase the inference cost by invoking the model\nrepeatedly. Optimization methods such as parallelism, compression, and caching\nhave been adopted to reduce costs, but the diverse service requirements make it\nhard to select the right method. Recently, specialized LLM inference engines\nhave emerged as a key component for integrating the optimization methods into\nservice-oriented infrastructures. However, a systematic study on inference\nengines is still lacking. This paper provides a comprehensive evaluation of 25\nopen-source and commercial inference engines. We examine each inference engine\nin terms of ease-of-use, ease-of-deployment, general-purpose support,\nscalability, and suitability for throughput- and latency-aware computation.\nFurthermore, we explore the design goals of each inference engine by\ninvestigating the optimization techniques it supports. In addition, we assess\nthe ecosystem maturity of open source inference engines and handle the\nperformance and cost policy of commercial solutions. We outline future research\ndirections that include support for complex LLM-based services, support of\nvarious hardware, and enhanced security, offering practical guidance to\nresearchers and developers in selecting and designing optimized LLM inference\nengines. We also provide a public repository to continually track developments\nin this fast-evolving field:\nhttps://github.com/sihyeong/Awesome-LLM-Inference-Engine"
                },
                "authors": [
                    {
                        "name": "Sihyeong Park"
                    },
                    {
                        "name": "Sungryeol Jeon"
                    },
                    {
                        "name": "Chaelyn Lee"
                    },
                    {
                        "name": "Seokhun Jeon"
                    },
                    {
                        "name": "Byung-Soo Kim"
                    },
                    {
                        "name": "Jemin Lee"
                    }
                ],
                "author_detail": {
                    "name": "Jemin Lee"
                },
                "author": "Jemin Lee",
                "arxiv_comment": "Under review; 65 pages; 27 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01658v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01658v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04896v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04896v1",
                "updated": "2025-05-08T02:16:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "published": "2025-05-08T02:16:08Z",
                "published_parsed": [
                    2025,
                    5,
                    8,
                    2,
                    16,
                    8,
                    3,
                    128,
                    0
                ],
                "title": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Memory Under Siege: A Comprehensive Survey of Side-Channel Attacks on\n  Memory"
                },
                "summary": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on memory (SCAM) exploit unintended data leaks from\nmemory subsystems to infer sensitive information, posing significant threats to\nsystem security. These attacks exploit vulnerabilities in memory access\npatterns, cache behaviors, and other microarchitectural features to bypass\ntraditional security measures. The purpose of this research is to examine SCAM,\nclassify various attack techniques, and evaluate existing defense mechanisms.\nIt guides researchers and industry professionals in improving memory security\nand mitigating emerging threats. We begin by identifying the major\nvulnerabilities in the memory system that are frequently exploited in SCAM,\nsuch as cache timing, speculative execution, \\textit{Rowhammer}, and other\nsophisticated approaches. Next, we outline a comprehensive taxonomy that\nsystematically classifies these attacks based on their types, target systems,\nattack vectors, and adversarial capabilities required to execute them. In\naddition, we review the current landscape of mitigation strategies, emphasizing\ntheir strengths and limitations. This work aims to provide a comprehensive\noverview of memory-based side-channel attacks with the goal of providing\nsignificant insights for researchers and practitioners to better understand,\ndetect, and mitigate SCAM risks."
                },
                "authors": [
                    {
                        "name": "MD Mahady Hassan"
                    },
                    {
                        "name": "Shanto Roy"
                    },
                    {
                        "name": "Reza Rahaeimehr"
                    }
                ],
                "author_detail": {
                    "name": "Reza Rahaeimehr"
                },
                "author": "Reza Rahaeimehr",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04896v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04896v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04556v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04556v1",
                "updated": "2025-05-07T16:44:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T16:44:21Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    16,
                    44,
                    21,
                    2,
                    127,
                    0
                ],
                "title": "Comparing CPU and GPU compute of PERMANOVA on MI300A",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing CPU and GPU compute of PERMANOVA on MI300A"
                },
                "summary": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing the tradeoffs of CPU and GPU compute for memory-heavy algorithms is\noften challenging, due to the drastically different memory subsystems on host\nCPUs and discrete GPUs. The AMD MI300A is an exception, since it sports both\nCPU and GPU cores in a single package, all backed by the same type of HBM\nmemory. In this paper we analyze the performance of Permutational Multivariate\nAnalysis of Variance (PERMANOVA), a non-parametric method that tests whether\ntwo or more groups of objects are significantly different based on a\ncategorical factor. This method is memory-bound and has been recently optimized\nfor CPU cache locality. Our tests show that GPU cores on the MI300A prefer the\nbrute force approach instead, significantly outperforming the CPU-based\nimplementation. The significant benefit of Simultaneous Multithreading (SMT)\nwas also a pleasant surprise."
                },
                "authors": [
                    {
                        "name": "Igor Sfiligoi"
                    }
                ],
                "author_detail": {
                    "name": "Igor Sfiligoi"
                },
                "author": "Igor Sfiligoi",
                "arxiv_comment": "7 pages, 1 figure, Accepted at PEARC25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04556v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04556v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.QM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04466v1",
                "updated": "2025-05-07T14:37:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T14:37:13Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    14,
                    37,
                    13,
                    2,
                    127,
                    0
                ],
                "title": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Securing Immersive 360 Video Streams through Attribute-Based Selective\n  Encryption"
                },
                "summary": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF).",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Delivering high-quality, secure 360{\\deg} video content introduces unique\nchallenges, primarily due to the high bitrates and interactive demands of\nimmersive media. Traditional HTTPS-based methods, although widely used, face\nlimitations in computational efficiency and scalability when securing these\nhigh-resolution streams. To address these issues, this paper proposes a novel\nframework integrating Attribute-Based Encryption (ABE) with selective\nencryption techniques tailored specifically for tiled 360{\\deg} video\nstreaming. Our approach employs selective encryption of frames at varying\nlevels to reduce computational overhead while ensuring robust protection\nagainst unauthorized access.\n  Moreover, we explore viewport-adaptive encryption, dynamically encrypting\nmore frames within tiles occupying larger portions of the viewer's field of\nview. This targeted method significantly enhances security in critical viewing\nareas without unnecessary overhead in peripheral regions. We deploy and\nevaluate our proposed approach using the CloudLab testbed, comparing its\nperformance against traditional HTTPS streaming. Experimental results\ndemonstrate that our ABE-based model achieves reduced computational load on\nintermediate caches, improves cache hit rates, and maintains comparable visual\nquality to HTTPS, as assessed by Video Multimethod Assessment Fusion (VMAF)."
                },
                "authors": [
                    {
                        "name": "Mohammad Waquas Usmani"
                    },
                    {
                        "name": "Susmit Shannigrahi"
                    },
                    {
                        "name": "Michael Zink"
                    }
                ],
                "author_detail": {
                    "name": "Michael Zink"
                },
                "author": "Michael Zink",
                "arxiv_comment": "8 pages plus references, 10 figures, some with subfigures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.MM",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.MM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.IV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04421v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04421v1",
                "updated": "2025-05-07T13:54:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T13:54:26Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    13,
                    54,
                    26,
                    2,
                    127,
                    0
                ],
                "title": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LONGER: Scaling Up Long Sequence Modeling in Industrial Recommenders"
                },
                "summary": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modeling ultra-long user behavior sequences is critical for capturing both\nlong- and short-term preferences in industrial recommender systems. Existing\nsolutions typically rely on two-stage retrieval or indirect modeling paradigms,\nincuring upstream-downstream inconsistency and computational inefficiency. In\nthis paper, we present LONGER, a Long-sequence Optimized traNsformer for\nGPU-Efficient Recommenders. LONGER incorporates (i) a global token mechanism\nfor stabilizing attention over long contexts, (ii) a token merge module with\nlightweight InnerTransformers and hybrid attention strategy to reduce quadratic\ncomplexity, and (iii) a series of engineering optimizations, including training\nwith mixed-precision and activation recomputation, KV cache serving, and the\nfully synchronous model training and serving framework for unified GPU-based\ndense and sparse parameter updates. LONGER consistently outperforms strong\nbaselines in both offline metrics and online A/B testing in both advertising\nand e-commerce services at ByteDance, validating its consistent effectiveness\nand industrial-level scaling laws. Currently, LONGER has been fully deployed at\nmore than 10 influential scenarios at ByteDance, serving billion users."
                },
                "authors": [
                    {
                        "name": "Zheng Chai"
                    },
                    {
                        "name": "Qin Ren"
                    },
                    {
                        "name": "Xijun Xiao"
                    },
                    {
                        "name": "Huizhi Yang"
                    },
                    {
                        "name": "Bo Han"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Di Chen"
                    },
                    {
                        "name": "Hui Lu"
                    },
                    {
                        "name": "Wenlin Zhao"
                    },
                    {
                        "name": "Lele Yu"
                    },
                    {
                        "name": "Xionghang Xie"
                    },
                    {
                        "name": "Shiru Ren"
                    },
                    {
                        "name": "Xiang Sun"
                    },
                    {
                        "name": "Yaocheng Tan"
                    },
                    {
                        "name": "Peng Xu"
                    },
                    {
                        "name": "Yuchao Zheng"
                    },
                    {
                        "name": "Di Wu"
                    }
                ],
                "author_detail": {
                    "name": "Di Wu"
                },
                "author": "Di Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04421v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04421v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04326v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04326v1",
                "updated": "2025-05-07T11:21:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T11:21:12Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    11,
                    21,
                    12,
                    2,
                    127,
                    0
                ],
                "title": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Design and Evaluation of an NDN-Based Network for Distributed Digital\n  Twins"
                },
                "summary": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Digital twins (DT) have received significant attention due to their numerous\nbenefits, such as real-time data analytics and cost reduction in production. DT\nserves as a fundamental component of many applications, encompassing smart\nmanufacturing, intelligent vehicles, and smart cities. By using Machine\nLearning (ML) and Artificial Intelligence (AI) techniques, DTs can efficiently\nfacilitate decision-making and productivity by simulating the status and\nchanges of a physical entity. To handle the massive amount of data brought by\nDTs, it is challenging to achieve low response latency for data fetching over\nexisting IP-based networks. IP-based networks use host addresses for end-to-end\ncommunication, making data distribution between DTs inefficient. Thus, we\npropose to use DTs in a distributed manner over Named Data Networking (NDN)\nnetworks. NDN is data-centric where data is routed based on content names,\ndynamically adjusting paths to optimize latency. Popular data is cached in\nnetwork nodes, reducing data transmission and network congestion. Since data is\nfetched by content names, users and mobile devices can move freely without IP\naddress reassignment. By using in-network caching and adaptive routing, we\nreckon NDN is an ideal fit for Future G Networks in the context of Digital\nTwins. We compared DTs in edge scenarios with cloud scenarios over NDN and\nIP-based networks to validate our insights. Extensive simulation results show\nthat using DT in the edge reduces response latency by 10.2x. This position\npaper represents an initial investigation into the gap in distributed DTs over\nNDN, serving as an early-stage study."
                },
                "authors": [
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Zihan Jia"
                    },
                    {
                        "name": "Ze Wang"
                    },
                    {
                        "name": "Lin Cui"
                    },
                    {
                        "name": "Fung Po Tso"
                    }
                ],
                "author_detail": {
                    "name": "Fung Po Tso"
                },
                "author": "Fung Po Tso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04326v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04326v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04216v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04216v1",
                "updated": "2025-05-07T08:10:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T08:10:39Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    8,
                    10,
                    39,
                    2,
                    127,
                    0
                ],
                "title": "Computational Model for Photoionization in Pure SF6 Streamer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Computational Model for Photoionization in Pure SF6 Streamer"
                },
                "summary": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Photoionization plays a crucial role in achieving spatial numerical\nconvergence and accurate quantitative predictions in SF6 streamer simulations,\nbut accurate models for SF6 photoionization remains limited, motivating this\npaper. First, we develop a computational model for SF6 photoionization and\nprovide the detailed modeling process. Then, we perform comparative studies\nagainst simplified approaches. The results demonstrate that the proposed model\neffectively captures the non-local effects of SF6 photoionization, enhancing\nboth the spatial numerical convergence and the accuracy of the streamer\nstructure. Finally, we perform comparative studies by artificially increasing\nthe photoionization intensity through multiplying the photoionization source\nterm Sph by a factor of 10 (10*Sph) relative to the baseline intensity.\nRegarding breakdown voltage prediction, 10*Sph leads to a significant\nunderestimation of the breakdown voltage for positive streamers, introducing\nerrors greater than 0.5 kV, while exerting a relatively small impact on\nnegative streamers. Regarding streamer propagation dynamics, 10*Sph reduces the\ncontraction at the positive streamer head and significantly lowers the local\nfield by more than 700 Td, thereby slowing down its speed. In contrast, 10*Sph\nhas little impact on the morphology of the negative streamers and slightly\nenhances the local field by less than 200 Td, thereby consistently accelerating\nits propagation."
                },
                "authors": [
                    {
                        "name": "Zihao Feng"
                    }
                ],
                "author_detail": {
                    "name": "Zihao Feng"
                },
                "author": "Zihao Feng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04216v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04216v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.plasm-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.plasm-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.12224v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.12224v2",
                "updated": "2025-05-07T07:57:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    7,
                    57,
                    21,
                    2,
                    127,
                    0
                ],
                "published": "2025-02-17T14:54:14Z",
                "published_parsed": [
                    2025,
                    2,
                    17,
                    14,
                    54,
                    14,
                    0,
                    48,
                    0
                ],
                "title": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer\n  Gate"
                },
                "summary": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated impressive performance across\nvarious tasks, and their application in edge scenarios has attracted\nsignificant attention. However, sparse-activated Mixture-of-Experts (MoE)\nmodels, which are well suited for edge scenarios, have received relatively\nlittle attention due to their high memory demands. Offload-based methods have\nbeen proposed to address this challenge, but they face difficulties with expert\nprediction. Inaccurate expert predictions can result in prolonged inference\ndelays. To promote the application of MoE models in edge scenarios, we propose\nFate, an offloading system designed for MoE models to enable efficient\ninference in resource-constrained environments. The key insight behind Fate is\nthat gate inputs from adjacent layers can be effectively used for expert\nprefetching, achieving high prediction accuracy without additional GPU\noverhead. Furthermore, Fate employs a shallow-favoring expert caching strategy\nthat increases the expert hit rate to 99\\%. Additionally, Fate integrates\ntailored quantization strategies for cache optimization and IO efficiency.\nExperimental results show that, compared to Load on Demand and Expert\nActivation Path-based method, Fate achieves up to 4.5x and 1.9x speedups in\nprefill speed and up to 4.1x and 2.2x speedups in decoding speed, respectively,\nwhile maintaining inference quality. Moreover, Fate's performance improvements\nare scalable across different memory budgets."
                },
                "authors": [
                    {
                        "name": "Zhiyuan Fang"
                    },
                    {
                        "name": "Zicong Hong"
                    },
                    {
                        "name": "Yuegui Huang"
                    },
                    {
                        "name": "Yufeng Lyu"
                    },
                    {
                        "name": "Wuhui Chen"
                    },
                    {
                        "name": "Yue Yu"
                    },
                    {
                        "name": "Fan Yu"
                    },
                    {
                        "name": "Zibin Zheng"
                    }
                ],
                "author_detail": {
                    "name": "Zibin Zheng"
                },
                "author": "Zibin Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.12224v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.12224v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.04129v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.04129v1",
                "updated": "2025-05-07T05:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "published": "2025-05-07T05:00:10Z",
                "published_parsed": [
                    2025,
                    5,
                    7,
                    5,
                    0,
                    10,
                    2,
                    127,
                    0
                ],
                "title": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Maxing Out the SVM: Performance Impact of Memory and Program Cache Sizes\n  in the Agave Validator"
                },
                "summary": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper we analyze some of the bottlenecks in the execution pipeline of\nSolana's Agave validator client, focusing on RAM and program cache usage under\nmainnet conditions. Through a series of controlled experiments, we measure the\nvalidator's throughput and resource efficiency as RAM availability ranges\nbetween 128 GB to 1,536 GB (1.5 TB). We discover that the validator performance\ndegrades significantly below 256 GB, with transaction processing falling behind\nreal-time block production. Additionally, we study the program cache behavior,\nidentifying inefficiencies in program eviction and load latency. Our results\nprovide practical guidance for hardware provisioning and suggest improvements\nto the Solana execution and caching strategy, reducing latency due to the\nprogram cache by 90%."
                },
                "authors": [
                    {
                        "name": "Turan Vural"
                    },
                    {
                        "name": "Yuki Yuminaga"
                    },
                    {
                        "name": "Alex Petrosyan"
                    },
                    {
                        "name": "Ben Livshits"
                    }
                ],
                "author_detail": {
                    "name": "Ben Livshits"
                },
                "author": "Ben Livshits",
                "arxiv_comment": "15 pages, 13 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.04129v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.04129v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12240v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12240v3",
                "updated": "2025-05-06T15:23:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    6,
                    15,
                    23,
                    12,
                    1,
                    126,
                    0
                ],
                "published": "2025-04-16T16:45:19Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    16,
                    45,
                    19,
                    2,
                    106,
                    0
                ],
                "title": "Cobra: Efficient Line Art COlorization with BRoAder References",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cobra: Efficient Line Art COlorization with BRoAder References"
                },
                "summary": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The comic production industry requires reference-based line art colorization\nwith high accuracy, efficiency, contextual consistency, and flexible control. A\ncomic page often involves diverse characters, objects, and backgrounds, which\ncomplicates the coloring process. Despite advancements in diffusion models for\nimage generation, their application in line art colorization remains limited,\nfacing challenges related to handling extensive reference images,\ntime-consuming inference, and flexible control. We investigate the necessity of\nextensive contextual image guidance on the quality of line art colorization. To\naddress these challenges, we introduce Cobra, an efficient and versatile method\nthat supports color hints and utilizes over 200 reference images while\nmaintaining low latency. Central to Cobra is a Causal Sparse DiT architecture,\nwhich leverages specially designed positional encodings, causal sparse\nattention, and Key-Value Cache to effectively manage long-context references\nand ensure color identity consistency. Results demonstrate that Cobra achieves\naccurate line art colorization through extensive contextual reference,\nsignificantly enhancing inference speed and interactivity, thereby meeting\ncritical industrial demands. We release our codes and models on our project\npage: https://zhuang2002.github.io/Cobra/."
                },
                "authors": [
                    {
                        "name": "Junhao Zhuang"
                    },
                    {
                        "name": "Lingen Li"
                    },
                    {
                        "name": "Xuan Ju"
                    },
                    {
                        "name": "Zhaoyang Zhang"
                    },
                    {
                        "name": "Chun Yuan"
                    },
                    {
                        "name": "Ying Shan"
                    }
                ],
                "author_detail": {
                    "name": "Ying Shan"
                },
                "author": "Ying Shan",
                "arxiv_comment": "Project page with code: https://zhuang2002.github.io/Cobra/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12240v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12240v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02922v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02922v1",
                "updated": "2025-05-05T18:01:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T18:01:17Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    18,
                    1,
                    17,
                    0,
                    125,
                    0
                ],
                "title": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RetroInfer: A Vector-Storage Approach for Scalable Long-Context LLM\n  Inference"
                },
                "summary": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing context lengths of large language models (LLMs) pose significant\nchallenges for efficient inference, primarily due to GPU memory and bandwidth\nconstraints. We present RetroInfer, a novel system that reconceptualizes the\nkey-value (KV) cache as a vector storage system which exploits the inherent\nattention sparsity to accelerate long-context LLM inference. At its core is the\nwave index, an Attention-aWare VEctor index that enables efficient and accurate\nretrieval of critical tokens through techniques such as tripartite attention\napproximation, accuracy-bounded attention estimation, and segmented clustering.\nComplementing this is the wave buffer, which coordinates KV cache placement and\noverlaps computation and data transfer across GPU and CPU to sustain high\nthroughput. Unlike prior sparsity-based methods that struggle with token\nselection and hardware coordination, RetroInfer delivers robust performance\nwithout compromising model accuracy. Experiments on long-context benchmarks\nshow up to 4.5X speedup over full attention within GPU memory limits and up to\n10.5X over sparse attention baselines when KV cache is extended to CPU memory,\nall while preserving full-attention-level accuracy."
                },
                "authors": [
                    {
                        "name": "Yaoqi Chen"
                    },
                    {
                        "name": "Jinkai Zhang"
                    },
                    {
                        "name": "Baotong Lu"
                    },
                    {
                        "name": "Qianxi Zhang"
                    },
                    {
                        "name": "Chengruidong Zhang"
                    },
                    {
                        "name": "Jingjia Luo"
                    },
                    {
                        "name": "Di Liu"
                    },
                    {
                        "name": "Huiqiang Jiang"
                    },
                    {
                        "name": "Qi Chen"
                    },
                    {
                        "name": "Jing Liu"
                    },
                    {
                        "name": "Bailu Ding"
                    },
                    {
                        "name": "Xiao Yan"
                    },
                    {
                        "name": "Jiawei Jiang"
                    },
                    {
                        "name": "Chen Chen"
                    },
                    {
                        "name": "Mingxing Zhang"
                    },
                    {
                        "name": "Yuqing Yang"
                    },
                    {
                        "name": "Fan Yang"
                    },
                    {
                        "name": "Mao Yang"
                    }
                ],
                "author_detail": {
                    "name": "Mao Yang"
                },
                "author": "Mao Yang",
                "arxiv_comment": "16 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02922v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02922v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02533v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02533v1",
                "updated": "2025-05-05T10:16:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T10:16:16Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    10,
                    16,
                    16,
                    0,
                    125,
                    0
                ],
                "title": "Large Language Model Partitioning for Low-Latency Inference at the Edge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model Partitioning for Low-Latency Inference at the Edge"
                },
                "summary": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) based on autoregressive, decoder-only\nTransformers generate text one token at a time, where a token represents a\ndiscrete unit of text. As each newly produced token is appended to the partial\noutput sequence, the length grows and so does the memory and compute load, due\nto the expanding key-value caches, which store intermediate representations of\nall previously generated tokens in the multi-head attention (MHA) layer. As\nthis iterative process steadily increases memory and compute demands,\nlayer-based partitioning in resource-constrained edge environments often\nresults in memory overload or high inference latency. To address this and\nreduce inference latency, we propose a resource-aware Transformer architecture\npartitioning algorithm, where the partitioning decision is updated at regular\nintervals during token generation. The approach is myopic in that it is based\non instantaneous information about device resource availability and network\nlink bandwidths. When first executed, the algorithm places blocks on devices,\nand in later executions, it migrates these blocks among devices so that the sum\nof migration delay and inference delay remains low. Our approach partitions the\ndecoder at the attention head level, co-locating each attention head with its\nkey-value cache and allowing dynamic migrations whenever resources become\ntight. By allocating different attention heads to different devices, we exploit\nparallel execution of attention heads and thus achieve substantial reductions\nin inference delays. Our experiments show that in small-scale settings (3-5\ndevices), the proposed method achieves within 15 to 20 percent of an exact\noptimal solver's latency, while in larger-scale tests it achieves notable\nimprovements in inference speed and memory usage compared to state-of-the-art\nlayer-based partitioning approaches."
                },
                "authors": [
                    {
                        "name": "Dimitrios Kafetzis"
                    },
                    {
                        "name": "Ramin Khalili"
                    },
                    {
                        "name": "Iordanis Koutsopoulos"
                    }
                ],
                "author_detail": {
                    "name": "Iordanis Koutsopoulos"
                },
                "author": "Iordanis Koutsopoulos",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02533v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02533v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02346v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02346v1",
                "updated": "2025-05-05T04:01:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "published": "2025-05-05T04:01:56Z",
                "published_parsed": [
                    2025,
                    5,
                    5,
                    4,
                    1,
                    56,
                    0,
                    125,
                    0
                ],
                "title": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Empirical Study on the Performance and Energy Usage of Compiled\n  Python Code"
                },
                "summary": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Python is a popular programming language known for its ease of learning and\nextensive libraries. However, concerns about performance and energy consumption\nhave led to the development of compilers to enhance Python code efficiency.\nDespite the proven benefits of existing compilers on the efficiency of Python\ncode, there is limited analysis comparing their performance and energy\nefficiency, particularly considering code characteristics and factors like CPU\nfrequency and core count. Our study investigates how compilation impacts the\nperformance and energy consumption of Python code, using seven benchmarks\ncompiled with eight different tools: PyPy, Numba, Nuitka, Mypyc, Codon, Cython,\nPyston-lite, and the experimental Python 3.13 version, compared to CPython. The\nbenchmarks are single-threaded and executed on an NUC and a server, measuring\nenergy usage, execution time, memory usage, and Last-Level Cache (LLC) miss\nrates at a fixed frequency and on a single core. The results show that\ncompilation can significantly enhance execution time, energy and memory usage,\nwith Codon, PyPy, and Numba achieving over 90\\% speed and energy improvements.\nNuitka optimizes memory usage consistently on both testbeds. The impact of\ncompilation on LLC miss rate is not clear since it varies considerably across\nbenchmarks for each compiler. Our study is important for researchers and\npractitioners focused on improving Python code performance and energy\nefficiency. We outline future research directions, such as exploring caching\neffects on energy usage. Our findings help practitioners choose the best\ncompiler based on their efficiency benefits and accessibility."
                },
                "authors": [
                    {
                        "name": "Vincenzo Stoico"
                    },
                    {
                        "name": "Andrei Calin Dragomir"
                    },
                    {
                        "name": "Patricia Lago"
                    }
                ],
                "author_detail": {
                    "name": "Patricia Lago"
                },
                "author": "Patricia Lago",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02346v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02346v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.10375v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.10375v2",
                "updated": "2025-05-04T09:49:42Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    9,
                    49,
                    42,
                    6,
                    124,
                    0
                ],
                "published": "2024-12-16T07:59:21Z",
                "published_parsed": [
                    2024,
                    12,
                    16,
                    7,
                    59,
                    21,
                    0,
                    351,
                    0
                ],
                "title": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient\n  MoE Inference"
                },
                "summary": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture-of-Experts (MoE) models, though highly effective for various machine\nlearning tasks, face significant deployment challenges on memory-constrained\ndevices. While GPUs offer fast inference, their limited memory compared to CPUs\nmeans not all experts can be stored on the GPU simultaneously, necessitating\nfrequent, costly data transfers from CPU memory, often negating GPU speed\nadvantages. To address this, we present DAOP, an on-device MoE inference engine\nto optimize parallel GPU-CPU execution. DAOP dynamically allocates experts\nbetween CPU and GPU based on per-sequence activation patterns, and selectively\npre-calculates predicted experts on CPUs to minimize transfer latency. This\napproach enables efficient resource utilization across various expert cache\nratios while maintaining model accuracy through a novel graceful degradation\nmechanism. Comprehensive evaluations across various datasets show that DAOP\noutperforms traditional expert caching and prefetching methods by up to 8.20x\nand offloading techniques by 1.35x while maintaining accuracy."
                },
                "authors": [
                    {
                        "name": "Yujie Zhang"
                    },
                    {
                        "name": "Shivam Aggarwal"
                    },
                    {
                        "name": "Tulika Mitra"
                    }
                ],
                "author_detail": {
                    "name": "Tulika Mitra"
                },
                "author": "Tulika Mitra",
                "arxiv_comment": "7 pages, 10 figures, Accepted by DATE Conference 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.10375v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.10375v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.02027v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.02027v1",
                "updated": "2025-05-04T08:30:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "published": "2025-05-04T08:30:00Z",
                "published_parsed": [
                    2025,
                    5,
                    4,
                    8,
                    30,
                    0,
                    6,
                    124,
                    0
                ],
                "title": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GraphPrompter: Multi-stage Adaptive Prompt Optimization for Graph\n  In-Context Learning"
                },
                "summary": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph In-Context Learning, with the ability to adapt pre-trained graph models\nto novel and diverse downstream graphs without updating any parameters, has\ngained much attention in the community. The key to graph in-context learning is\nto perform downstream graphs conditioned on chosen prompt examples. Existing\nmethods randomly select subgraphs or edges as prompts, leading to noisy graph\nprompts and inferior model performance. Additionally, due to the gap between\npre-training and testing graphs, when the number of classes in the testing\ngraphs is much greater than that in the training, the in-context learning\nability will also significantly deteriorate. To tackle the aforementioned\nchallenges, we develop a multi-stage adaptive prompt optimization method\nGraphPrompter, which optimizes the entire process of generating, selecting, and\nusing graph prompts for better in-context learning capabilities. Firstly,\nPrompt Generator introduces a reconstruction layer to highlight the most\ninformative edges and reduce irrelevant noise for graph prompt construction.\nFurthermore, in the selection stage, Prompt Selector employs the $k$-nearest\nneighbors algorithm and pre-trained selection layers to dynamically choose\nappropriate samples and minimize the influence of irrelevant prompts. Finally,\nwe leverage a Prompt Augmenter with a cache replacement strategy to enhance the\ngeneralization capability of the pre-trained model on new datasets. Extensive\nexperiments show that GraphPrompter effectively enhances the in-context\nlearning ability of graph models. On average across all the settings, our\napproach surpasses the state-of-the-art baselines by over 8%. Our code is\nreleased at https://github.com/karin0018/GraphPrompter."
                },
                "authors": [
                    {
                        "name": "Rui Lv"
                    },
                    {
                        "name": "Zaixi Zhang"
                    },
                    {
                        "name": "Kai Zhang"
                    },
                    {
                        "name": "Qi Liu"
                    },
                    {
                        "name": "Weibo Gao"
                    },
                    {
                        "name": "Jiawei Liu"
                    },
                    {
                        "name": "Jiaxia Yan"
                    },
                    {
                        "name": "Linan Yue"
                    },
                    {
                        "name": "Fangzhou Yao"
                    }
                ],
                "author_detail": {
                    "name": "Fangzhou Yao"
                },
                "author": "Fangzhou Yao",
                "arxiv_comment": "14 pages. IEEE International Conference on Data Engineering\n  (ICDE'2025), accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.02027v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.02027v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.07578v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.07578v3",
                "updated": "2025-05-03T04:07:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    4,
                    7,
                    7,
                    5,
                    123,
                    0
                ],
                "published": "2025-02-11T14:25:20Z",
                "published_parsed": [
                    2025,
                    2,
                    11,
                    14,
                    25,
                    20,
                    1,
                    42,
                    0
                ],
                "title": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PIM Is All You Need: A CXL-Enabled GPU-Free System for Large Language\n  Model Inference"
                },
                "summary": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Model (LLM) inference uses an autoregressive manner to\ngenerate one token at a time, which exhibits notably lower operational\nintensity compared to earlier Machine Learning (ML) models such as encoder-only\ntransformers and Convolutional Neural Networks. At the same time, LLMs possess\nlarge parameter sizes and use key-value caches to store context information.\nModern LLMs support context windows with up to 1 million tokens to generate\nversatile text, audio, and video content. A large key-value cache unique to\neach prompt requires a large memory capacity, limiting the inference batch\nsize. Both low operational intensity and limited batch size necessitate a high\nmemory bandwidth. However, contemporary hardware systems for ML model\ndeployment, such as GPUs and TPUs, are primarily optimized for compute\nthroughput. This mismatch challenges the efficient deployment of advanced LLMs\nand makes users pay for expensive compute resources that are poorly utilized\nfor the memory-bound LLM inference tasks.\n  We propose CENT, a CXL-ENabled GPU-Free sysTem for LLM inference, which\nharnesses CXL memory expansion capabilities to accommodate substantial LLM\nsizes, and utilizes near-bank processing units to deliver high memory\nbandwidth, eliminating the need for expensive GPUs. CENT exploits a scalable\nCXL network to support peer-to-peer and collective communication primitives\nacross CXL devices. We implement various parallelism strategies to distribute\nLLMs across these devices. Compared to GPU baselines with maximum supported\nbatch sizes and similar average power, CENT achieves 2.3$\\times$ higher\nthroughput and consumes 2.9$\\times$ less energy. CENT enhances the Total Cost\nof Ownership (TCO), generating 5.2$\\times$ more tokens per dollar than GPUs."
                },
                "authors": [
                    {
                        "name": "Yufeng Gu"
                    },
                    {
                        "name": "Alireza Khadem"
                    },
                    {
                        "name": "Sumanth Umesh"
                    },
                    {
                        "name": "Ning Liang"
                    },
                    {
                        "name": "Xavier Servot"
                    },
                    {
                        "name": "Onur Mutlu"
                    },
                    {
                        "name": "Ravi Iyer"
                    },
                    {
                        "name": "Reetuparna Das"
                    }
                ],
                "author_detail": {
                    "name": "Reetuparna Das"
                },
                "author": "Reetuparna Das",
                "arxiv_doi": "10.1145/3676641.3716267",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3676641.3716267",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2502.07578v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.07578v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "In Proceedings of the 30th ACM International Conference on\n  Architectural Support for Programming Languages and Operating Systems, Volume\n  2 (ASPLOS'25)",
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20335v2",
                "updated": "2025-05-03T01:10:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    3,
                    1,
                    10,
                    30,
                    5,
                    123,
                    0
                ],
                "published": "2025-04-29T00:58:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    0,
                    58,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with\n  Delayed Hits"
                },
                "summary": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Caches are fundamental to latency-sensitive systems like Content Delivery\nNetworks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit\nphenomenon where multiple requests for an object occur during its fetch from\nthe remote server after a miss significantly inflates user-perceived latency.\nWhile recent algorithms acknowledge delayed hits by estimating the resulting\naggregate delay, they predominantly focus on its mean value. We identify and\ndemonstrate that such approaches are insufficient, as the real aggregate delay\nfrequently exhibits substantial variance in the true production system, leading\nto suboptimal latency performance when ignored. Thus, we propose VA-CDH, a\nvariance-aware method to optimize latency for caching with delayed hits. It\nemploys a novel ranking function that explicitly incorporates both the\nempirically estimated mean and standard deviation of aggregate delay, allowing\ncaching decisions to account for its variation. We derive the analytical\ndistribution of aggregate delay under Poisson arrivals as a theoretical\ncontribution, offering more statistical insight beyond the mean value. Through\nthe simulations conducted on synthetic and real-world datasets, we show that\nVA-CDH reduces the total latency by 1%-6% approximately compared to\nstate-of-the-art algorithms."
                },
                "authors": [
                    {
                        "name": "Bowen Jiang"
                    },
                    {
                        "name": "Chaofan Ma"
                    },
                    {
                        "name": "Duo Wang"
                    }
                ],
                "author_detail": {
                    "name": "Duo Wang"
                },
                "author": "Duo Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13298v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13298v3",
                "updated": "2025-05-02T13:55:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    13,
                    55,
                    21,
                    4,
                    122,
                    0
                ],
                "published": "2025-01-23T00:57:01Z",
                "published_parsed": [
                    2025,
                    1,
                    23,
                    0,
                    57,
                    1,
                    3,
                    23,
                    0
                ],
                "title": "Collaborative Coded Caching for Partially Connected Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Coded Caching for Partially Connected Networks"
                },
                "summary": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching leverages the differences in user cache memories to achieve\ngains that scale with the total cache size, alleviating network congestion due\nto high-quality content requests. Additionally, distributing transmitters over\na wide area can mitigate the adverse effects of path loss. In this work, we\nconsider a partially connected network where the channel between distributed\ntransmitters (helpers) and users is modeled as a distributed\nmultiple-input-multiple-output (MIMO) Gaussian broadcast channel. We propose a\nnovel delivery scheme consisting of two phases: partitioning and transmission.\nIn the partitioning phase, users with identical cache profiles are partitioned\ninto the minimum number of sets, such that users within each set can\nsuccessfully decode their desired message from a joint transmission enabled by\nMIMO precoding. To optimally partition the users, we employ the branch and\nbound method. In the transmission phase, each partition is treated as a single\nentity, and codewords are multicast to partitions with distinct cache profiles.\nThe proposed delivery scheme is applicable to any partially connected network,\nand while the partitioning is optimal, the overall delivery scheme, including\ntransmission, is heuristic. Interestingly, simulation results show that its\nperformance closely approximates that of the fully connected optimal solution."
                },
                "authors": [
                    {
                        "name": "Kagan Akcay"
                    },
                    {
                        "name": "Eleftherios Lampiris"
                    },
                    {
                        "name": "MohammadJavad Salehi"
                    },
                    {
                        "name": "Giuseppe Caire"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Caire"
                },
                "author": "Giuseppe Caire",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13298v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13298v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01164v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01164v1",
                "updated": "2025-05-02T10:13:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T10:13:12Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    10,
                    13,
                    12,
                    4,
                    122,
                    0
                ],
                "title": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CaGR-RAG: Context-aware Query Grouping for Disk-based Vector Search in\n  RAG Systems"
                },
                "summary": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern embedding models capture both semantic and syntactic structures of\nqueries, often mapping different queries to similar regions in vector space.\nThis results in non-uniform cluster access patterns in disk-based vector search\nsystems, particularly in Retrieval Augmented Generation (RAG) framework. While\nexisting approaches optimize individual queries, they overlook the impact of\ncluster access patterns, failing to account for the locality effects of queries\nthat access similar clusters. This oversight reduces cache efficiency and\nincreases search latency due to excessive disk I/O. To address this, we\nintroduce CaGR-RAG, a context-aware query grouping mechanism that organizes\nqueries based on shared cluster access patterns. Additionally, it incorporates\nopportunistic cluster prefetching to minimize cache misses during transitions\nbetween query groups, further optimizing retrieval performance. Experimental\nresults show that CaGR-RAG reduces 99th percentile tail latency by up to 51.55%\nwhile consistently maintaining a higher cache hit ratio than the baseline."
                },
                "authors": [
                    {
                        "name": "Yeonwoo Jeong"
                    },
                    {
                        "name": "Kyuli Park"
                    },
                    {
                        "name": "Hyunji Cho"
                    },
                    {
                        "name": "Sungyong Park"
                    }
                ],
                "author_detail": {
                    "name": "Sungyong Park"
                },
                "author": "Sungyong Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01164v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01164v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01002v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01002v1",
                "updated": "2025-05-02T04:57:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T04:57:06Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    4,
                    57,
                    6,
                    4,
                    122,
                    0
                ],
                "title": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High Voltage Delivery and Distribution for the NEXT-100 Time Projection\n  Chamber"
                },
                "summary": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A critical element in the realization of large liquid and gas time projection\nchambers (TPCs) is the delivery and distribution of high voltages into and\naround the detector. Such experiments require of order tens of kilovolts to\nenable electron drift over meter-scale distances. This paper describes the\ndesign and operation of the cathode feedthrough and high voltage distribution\nthrough the field cage of the NEXT-100 experiment, an underground TPC that will\nsearch for neutrinoless double beta decay $0\\nu\\beta\\beta$. The feedthrough has\nbeen demonstrated to hold pressures up to 20~bar and sustain voltages as high\nas -65~kV, and the TPC is operating stably at its design high voltages. The\nsystem has been realized within the constraints of a stringent radiopurity\nbudget and is now being used to execute a suite of sensitive double beta decay\nanalyses."
                },
                "authors": [
                    {
                        "name": "NEXT Collaboration"
                    },
                    {
                        "name": "C. Adams"
                    },
                    {
                        "name": "H. Almazn"
                    },
                    {
                        "name": "V. lvarez"
                    },
                    {
                        "name": "K. Bailey"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "B. J. P. Jones"
                    },
                    {
                        "name": "S. Johnston"
                    },
                    {
                        "name": "K. Mistry"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "D. R. Nygren"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "L. Rogers"
                    },
                    {
                        "name": "J. Waldschmidt"
                    },
                    {
                        "name": "B. Aparicio"
                    },
                    {
                        "name": "A. I. Aranburu"
                    },
                    {
                        "name": "L. Arazi"
                    },
                    {
                        "name": "I. J. Arnquist"
                    },
                    {
                        "name": "F. Auria-Luna"
                    },
                    {
                        "name": "S. Ayet"
                    },
                    {
                        "name": "C. D. R. Azevedo"
                    },
                    {
                        "name": "F. Ballester"
                    },
                    {
                        "name": "M. del Barrio-Torregrosa"
                    },
                    {
                        "name": "A. Bayo"
                    },
                    {
                        "name": "J. M. Benlloch-Rodrguez"
                    },
                    {
                        "name": "F. I. G. M. Borges"
                    },
                    {
                        "name": "A. Brodolin"
                    },
                    {
                        "name": "S. Crcel"
                    },
                    {
                        "name": "A. Castillo"
                    },
                    {
                        "name": "L. Cid"
                    },
                    {
                        "name": "C. A. N. Conde"
                    },
                    {
                        "name": "T. Contreras"
                    },
                    {
                        "name": "F. P. Cosso"
                    },
                    {
                        "name": "R. Coupe"
                    },
                    {
                        "name": "E. Dey"
                    },
                    {
                        "name": "G. Daz"
                    },
                    {
                        "name": "C. Echevarria"
                    },
                    {
                        "name": "M. Elorza"
                    },
                    {
                        "name": "J. Escada"
                    },
                    {
                        "name": "R. Esteve"
                    },
                    {
                        "name": "R. Felkai"
                    },
                    {
                        "name": "L. M. P. Fernandes"
                    },
                    {
                        "name": "P. Ferrario"
                    },
                    {
                        "name": "A. L. Ferreira"
                    },
                    {
                        "name": "F. W. Foss"
                    },
                    {
                        "name": "Z. Freixa"
                    },
                    {
                        "name": "J. Garca-Barrena"
                    },
                    {
                        "name": "J. J. Gmez-Cadenas"
                    },
                    {
                        "name": "J. W. R. Grocott"
                    },
                    {
                        "name": "R. Guenette"
                    },
                    {
                        "name": "J. Hauptman"
                    },
                    {
                        "name": "C. A. O. Henriques"
                    },
                    {
                        "name": "J. A. Hernando Morata"
                    },
                    {
                        "name": "P. Herrero-Gmez"
                    },
                    {
                        "name": "V. Herrero"
                    },
                    {
                        "name": "C. Hervs Carrete"
                    },
                    {
                        "name": "Y. Ifergan"
                    },
                    {
                        "name": "F. Kellerer"
                    },
                    {
                        "name": "L. Larizgoitia"
                    },
                    {
                        "name": "A. Larumbe"
                    },
                    {
                        "name": "P. Lebrun"
                    },
                    {
                        "name": "F. Lopez"
                    },
                    {
                        "name": "N. Lpez-March"
                    },
                    {
                        "name": "R. Madigan"
                    },
                    {
                        "name": "R. D. P. Mano"
                    },
                    {
                        "name": "A. P. Marques"
                    },
                    {
                        "name": "J. Martn-Albo"
                    },
                    {
                        "name": "G. Martnez-Lema"
                    },
                    {
                        "name": "M. Martnez-Vara"
                    },
                    {
                        "name": "R. L. Miller"
                    },
                    {
                        "name": "J. Molina-Canteras"
                    },
                    {
                        "name": "F. Monrabal"
                    },
                    {
                        "name": "C. M. B. Monteiro"
                    },
                    {
                        "name": "F. J. Mora"
                    },
                    {
                        "name": "P. Novella"
                    },
                    {
                        "name": "A. Nuez"
                    },
                    {
                        "name": "E. Oblak"
                    },
                    {
                        "name": "J. Palacio"
                    },
                    {
                        "name": "B. Palmeiro"
                    },
                    {
                        "name": "A. Para"
                    },
                    {
                        "name": "A. Pazos"
                    },
                    {
                        "name": "J. Pelegrin"
                    },
                    {
                        "name": "M. Prez Maneiro"
                    },
                    {
                        "name": "M. Querol"
                    },
                    {
                        "name": "J. Renner"
                    },
                    {
                        "name": "I. Rivilla"
                    },
                    {
                        "name": "C. Rogero"
                    },
                    {
                        "name": "B. Romeo"
                    },
                    {
                        "name": "C. Romo-Luque"
                    },
                    {
                        "name": "V. San Nacienciano"
                    },
                    {
                        "name": "F. P. Santos"
                    },
                    {
                        "name": "J. M. F. dos Santos"
                    },
                    {
                        "name": "M. Seemann"
                    },
                    {
                        "name": "I. Shomroni"
                    },
                    {
                        "name": "P. A. O. C. Silva"
                    },
                    {
                        "name": "A. Simn"
                    },
                    {
                        "name": "S. R. Soleti"
                    },
                    {
                        "name": "M. Sorel"
                    },
                    {
                        "name": "J. Soto-Oton"
                    },
                    {
                        "name": "J. M. R. Teixeira"
                    },
                    {
                        "name": "S. Teruel-Pardo"
                    },
                    {
                        "name": "J. F. Toledo"
                    },
                    {
                        "name": "C. Tonnel"
                    },
                    {
                        "name": "S. Torelli"
                    },
                    {
                        "name": "J. Torrent"
                    },
                    {
                        "name": "A. Trettin"
                    },
                    {
                        "name": "A. Usn"
                    },
                    {
                        "name": "P. R. G. Valle"
                    },
                    {
                        "name": "J. F. C. A. Veloso"
                    },
                    {
                        "name": "J. Waiton"
                    },
                    {
                        "name": "A. Yubero-Navarro"
                    }
                ],
                "author_detail": {
                    "name": "A. Yubero-Navarro"
                },
                "author": "A. Yubero-Navarro",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01002v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01002v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "physics.ins-det",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "physics.ins-det",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00962v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00962v1",
                "updated": "2025-05-02T02:36:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "published": "2025-05-02T02:36:23Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    2,
                    36,
                    23,
                    4,
                    122,
                    0
                ],
                "title": "The Open-Source BlackParrot-BedRock Cache Coherence System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Open-Source BlackParrot-BedRock Cache Coherence System"
                },
                "summary": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This dissertation revisits the topic of programmable cache coherence engines\nin the context of modern shared-memory multicore processors. First, the\nopen-source BedRock cache coherence protocol is described. BedRock employs the\ncanonical MOESIF coherence states and reduces implementation burden by\neliminating transient coherence states from the protocol. The protocol's design\ncomplexity, concurrency, and verification effort are analyzed and compared to a\ncanonical directory-based invalidate coherence protocol. Second, the\narchitecture and microarchitecture of three separate cache coherence\ndirectories implementing the BedRock protocol within the BlackParrot 64-bit\nRISC-V multicore processor, collectively called BlackParrot-BedRock\n(BP-BedRock), are described. A fixed-function coherence directory engine\nimplementation provides a baseline design for performance and area comparisons.\nA microcode-programmable coherence directory implementation demonstrates the\nfeasibility of implementing a programmable coherence engine capable of\nmaintaining sufficient protocol processing performance. A hybrid fixed-function\nand programmable coherence directory blends the protocol processing performance\nof the fixed-function design with the programmable flexibility of the\nmicrocode-programmable design. Collectively, the BedRock coherence protocol and\nits three BP-BedRock implementations demonstrate the feasibility and challenges\nof including programmable logic within the coherence system of modern\nshared-memory multicore processors, paving the way for future research into the\napplication- and system-level benefits of programmable coherence engines."
                },
                "authors": [
                    {
                        "name": "Mark Unruh Wyse"
                    }
                ],
                "author_detail": {
                    "name": "Mark Unruh Wyse"
                },
                "author": "Mark Unruh Wyse",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00962v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00962v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00901v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00901v1",
                "updated": "2025-05-01T22:32:29Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T22:32:29Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    22,
                    32,
                    29,
                    3,
                    121,
                    0
                ],
                "title": "Heterogeneous Memory Benchmarking Toolkit",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Heterogeneous Memory Benchmarking Toolkit"
                },
                "summary": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents an open-source kernel-level heterogeneous memory\ncharacterization framework (MemScope) for embedded systems that enables users\nto understand and precisely characterize the temporal behavior of all available\nmemory modules under configurable contention stress scenarios. Since\nkernel-level provides a high degree of control over allocation, cache\nmaintenance, $CPUs$, interrupts, and I/O device activity, seeking the most\naccurate way to benchmark heterogeneous memory subsystems, would be achieved by\nimplementing it in the kernel. This gives us the privilege to directly map\npieces of contiguous physical memory and instantiate allocators, allowing us to\nfinely control cores to create and eliminate interference. Additionally, we can\nminimize noise and interruptions, guaranteeing more consistent and precise\nresults compared to equivalent user-space solutions. Running our Framework on a\nXilinx Zynq UltraScale+ ZCU102 CPU_FPGA platform, demonstrates its capability\nto precisely benchmark bandwidth and latency across various memory types,\nincluding PL-side DRAM and BRAM, in a multi-core system."
                },
                "authors": [
                    {
                        "name": "Golsana Ghaemi"
                    },
                    {
                        "name": "Kazem Taram"
                    },
                    {
                        "name": "Renato Mancuso"
                    }
                ],
                "author_detail": {
                    "name": "Renato Mancuso"
                },
                "author": "Renato Mancuso",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00901v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00901v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00817v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00817v1",
                "updated": "2025-05-01T19:18:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T19:18:56Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    19,
                    18,
                    56,
                    3,
                    121,
                    0
                ],
                "title": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Spill The Beans: Exploiting CPU Cache Side-Channels to Leak Tokens from\n  Large Language Models"
                },
                "summary": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Side-channel attacks on shared hardware resources increasingly threaten\nconfidentiality, especially with the rise of Large Language Models (LLMs). In\nthis work, we introduce Spill The Beans, a novel application of cache\nside-channels to leak tokens generated by an LLM. By co-locating an attack\nprocess on the same hardware as the victim model, we flush and reload embedding\nvectors from the embedding layer, where each token corresponds to a unique\nembedding vector. When accessed during token generation, it results in a cache\nhit detectable by our attack on shared lower-level caches.\n  A significant challenge is the massive size of LLMs, which, by nature of\ntheir compute intensive operation, quickly evicts embedding vectors from the\ncache. We address this by balancing the number of tokens monitored against the\namount of information leaked. Monitoring more tokens increases potential\nvocabulary leakage but raises the chance of missing cache hits due to eviction;\nmonitoring fewer tokens improves detection reliability but limits vocabulary\ncoverage.\n  Through extensive experimentation, we demonstrate the feasibility of leaking\ntokens from LLMs via cache side-channels. Our findings reveal a new\nvulnerability in LLM deployments, highlighting that even sophisticated models\nare susceptible to traditional side-channel attacks. We discuss the\nimplications for privacy and security in LLM-serving infrastructures and\nsuggest considerations for mitigating such threats. For proof of concept we\nconsider two concrete attack scenarios: Our experiments show that an attacker\ncan recover as much as 80%-90% of a high entropy API key with single shot\nmonitoring. As for English text we can reach a 40% recovery rate with a single\nshot. We should note that the rate highly depends on the monitored token set\nand these rates can be improved by targeting more specialized output domains."
                },
                "authors": [
                    {
                        "name": "Andrew Adiletta"
                    },
                    {
                        "name": "Berk Sunar"
                    }
                ],
                "author_detail": {
                    "name": "Berk Sunar"
                },
                "author": "Berk Sunar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00817v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00817v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "K.6.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00768v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00768v1",
                "updated": "2025-05-01T18:00:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T18:00:40Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    18,
                    0,
                    40,
                    3,
                    121,
                    0
                ],
                "title": "Optomechanical resource for fault-tolerant quantum computing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optomechanical resource for fault-tolerant quantum computing"
                },
                "summary": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fusion-based quantum computing with dual-rail qubits is a leading candidate\nfor scalable quantum computing using linear optics. This paradigm requires\nsingle photons which are entangled into small resource states before being fed\ninto a fusion network. The most common sources for single optical photons and\nfor small entangled states are probabilistic and heralded. The realization of a\nsingle reliable deterministic source requires many redundant probabilistic\nsources and a complex optical network for rerouting and retiming probabilistic\noutputs. In this work, we show how optomechanics enables reliable production of\nresources for photonic quantum computing without the redundancy of the\nall-optical approach. This is achieved by using acoustic modes as caches of\nquantum resources, ranging from single-particle states to small entangled\nstates, with on-demand read-out. The advantages of acoustic modes as optical\nquantum memories, compared to other technologies, include their intrinsically\nlong lifetimes and that they are solid state, highly tailorable, and\ninsensitive to electromagnetic noise. We show how the resource states can be\nprepared directly in the acoustic modes using optical controls. This is still\nprobabilistic and heralded, as in the all-optical approach, but the acoustic\nmodes act as a quantum memory which is integrated into the production of the\nstates. The quantum states may be deterministically transferred from acoustic\nmodes to optical modes, on demand, with another optical drive."
                },
                "authors": [
                    {
                        "name": "Margaret Pavlovich"
                    },
                    {
                        "name": "Peter Rakich"
                    },
                    {
                        "name": "Shruti Puri"
                    }
                ],
                "author_detail": {
                    "name": "Shruti Puri"
                },
                "author": "Shruti Puri",
                "arxiv_comment": "19 pages, 9 figures. Supplement 29 pages, 7 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00768v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00768v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00570v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00570v1",
                "updated": "2025-05-01T14:53:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T14:53:12Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    14,
                    53,
                    12,
                    3,
                    121,
                    0
                ],
                "title": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FreqKV: Frequency Domain Key-Value Compression for Efficient Context\n  Window Extension"
                },
                "summary": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Extending the context window in large language models (LLMs) is essential for\napplications involving long-form content generation. However, the linear\nincrease in key-value (KV) cache memory requirements and the quadratic\ncomplexity of self-attention with respect to sequence length present\nsignificant challenges during fine-tuning and inference. Existing methods\nsuffer from performance degradation when extending to longer contexts. In this\nwork, we introduce a novel context extension method that optimizes both\nfine-tuning and inference efficiency. Our method exploits a key observation: in\nthe frequency domain, the energy distribution of the KV cache is primarily\nconcentrated in low-frequency components. By filtering out the high-frequency\ncomponents, the KV cache can be effectively compressed with minimal information\nloss. Building on this insight, we propose an efficient compression technique,\nFreqKV, that iteratively compresses the increasing KV cache to a fixed size in\nthe frequency domain, applicable to both fine-tuning and inference. FreqKV\nintroduces no additional parameters or architectural modifications. With\nminimal fine-tuning, LLMs can learn to leverage the limited cache that is\ncompressed in the frequency domain and extend the context window efficiently.\nExperiments on various long context language modeling and understanding tasks\ndemonstrate the efficiency and efficacy of the proposed method."
                },
                "authors": [
                    {
                        "name": "Jushi Kai"
                    },
                    {
                        "name": "Boyi Zeng"
                    },
                    {
                        "name": "Yixuan Wang"
                    },
                    {
                        "name": "Haoli Bai"
                    },
                    {
                        "name": "Bo Jiang"
                    },
                    {
                        "name": "Zhouhan Lin"
                    }
                ],
                "author_detail": {
                    "name": "Zhouhan Lin"
                },
                "author": "Zhouhan Lin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00570v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00570v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00315v1",
                "updated": "2025-05-01T05:22:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "published": "2025-05-01T05:22:11Z",
                "published_parsed": [
                    2025,
                    5,
                    1,
                    5,
                    22,
                    11,
                    3,
                    121,
                    0
                ],
                "title": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mixture of Sparse Attention: Content-Based Learnable Sparse Attention\n  via Expert-Choice Routing"
                },
                "summary": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models highlighted the excessive quadratic\ncost of self-attention. Despite the significant research efforts, subquadratic\nattention methods still suffer from inferior performance in practice. We\nhypothesize that dynamic, learned content-based sparsity can lead to more\nefficient attention mechanisms. We present Mixture of Sparse Attention (MoSA),\na novel approach inspired by Mixture of Experts (MoE) with expert choice\nrouting. MoSA dynamically selects tokens for each attention head, allowing\narbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of\nlength $T$, MoSA reduces the computational complexity of each attention head\nfrom $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same\ncomputational budget, allowing higher specialization. We show that among the\ntested sparse attention variants, MoSA is the only one that can outperform the\ndense baseline, sometimes with up to 27% better perplexity for an identical\ncompute budget. MoSA can also reduce the resource usage compared to dense\nself-attention. Despite using torch implementation without an optimized kernel,\nperplexity-matched MoSA models are simultaneously faster in wall-clock time,\nrequire less memory for training, and drastically reduce the size of the\nKV-cache compared to the dense transformer baselines."
                },
                "authors": [
                    {
                        "name": "Piotr Pikos"
                    },
                    {
                        "name": "Rbert Csords"
                    },
                    {
                        "name": "Jrgen Schmidhuber"
                    }
                ],
                "author_detail": {
                    "name": "Jrgen Schmidhuber"
                },
                "author": "Jrgen Schmidhuber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04532v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04532v3",
                "updated": "2025-05-01T02:14:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    2,
                    14,
                    5,
                    3,
                    121,
                    0
                ],
                "published": "2024-05-07T17:59:30Z",
                "published_parsed": [
                    2024,
                    5,
                    7,
                    17,
                    59,
                    30,
                    1,
                    128,
                    0
                ],
                "title": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM\n  Serving"
                },
                "summary": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantization can accelerate large language model (LLM) inference. Going\nbeyond INT8 quantization, the research community is actively exploring even\nlower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization\ntechniques only accelerate low-batch, edge LLM inference, failing to deliver\nperformance gains in large-batch, cloud-based LLM serving. We uncover a\ncritical issue: existing INT4 quantization methods suffer from significant\nruntime overhead (20-90%) when dequantizing either weights or partial sums on\nGPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization\nalgorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands\nfor quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented\nby the QServe inference library that achieves measured speedup. The key insight\ndriving QServe is that the efficiency of LLM serving on GPUs is critically\ninfluenced by operations on low-throughput CUDA cores. Building upon this\ninsight, in QoQ algorithm, we introduce progressive quantization that can allow\nlow dequantization overhead in W4A8 GEMM. Additionally, we develop\nSmoothAttention to effectively mitigate the accuracy degradation incurred by\n4-bit KV quantization. In the QServe system, we perform compute-aware weight\nreordering and take advantage of register-level parallelism to reduce\ndequantization latency. We also make fused attention memory-bound, harnessing\nthe performance gain brought by KV4 quantization. As a result, QServe improves\nthe maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x\non L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to\nTensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput\nthan TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of\nLLM serving by 3x. Code is available at\nhttps://github.com/mit-han-lab/omniserve."
                },
                "authors": [
                    {
                        "name": "Yujun Lin"
                    },
                    {
                        "name": "Haotian Tang"
                    },
                    {
                        "name": "Shang Yang"
                    },
                    {
                        "name": "Zhekai Zhang"
                    },
                    {
                        "name": "Guangxuan Xiao"
                    },
                    {
                        "name": "Chuang Gan"
                    },
                    {
                        "name": "Song Han"
                    }
                ],
                "author_detail": {
                    "name": "Song Han"
                },
                "author": "Song Han",
                "arxiv_comment": "The first three authors contribute equally to this project and are\n  listed in the alphabetical order. Yujun Lin leads the quantization algorithm,\n  Haotian Tang and Shang Yang lead the GPU kernels and the serving system. Code\n  is available at https://github.com/mit-han-lab/omniserve",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04532v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04532v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19602v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19602v2",
                "updated": "2025-05-01T00:13:06Z",
                "updated_parsed": [
                    2025,
                    5,
                    1,
                    0,
                    13,
                    6,
                    3,
                    121,
                    0
                ],
                "published": "2025-04-28T09:04:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    4,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Soft-Label Caching and Sharpening for Communication-Efficient Federated\n  Distillation"
                },
                "summary": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated Learning (FL) enables collaborative model training across\ndecentralized clients, enhancing privacy by keeping data local. Yet\nconventional FL, relying on frequent parameter-sharing, suffers from high\ncommunication overhead and limited model heterogeneity. Distillation-based FL\napproaches address these issues by sharing predictions (soft-labels) instead,\nbut they often involve redundant transmissions across communication rounds,\nreducing efficiency. We propose SCARLET, a novel framework integrating\nsynchronized soft-label caching and an enhanced Entropy Reduction Aggregation\n(Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing\ncached soft-labels, achieving up to 50% reduction in communication costs\ncompared to existing methods while maintaining accuracy. Enhanced ERA can be\ntuned to adapt to non-IID data variations, ensuring robust aggregation and\nperformance in diverse client scenarios. Experimental evaluations demonstrate\nthat SCARLET consistently outperforms state-of-the-art distillation-based FL\nmethods in terms of accuracy and communication efficiency. The implementation\nof SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET."
                },
                "authors": [
                    {
                        "name": "Kitsuya Azuma"
                    },
                    {
                        "name": "Takayuki Nishio"
                    },
                    {
                        "name": "Yuichi Kitagawa"
                    },
                    {
                        "name": "Wakako Nakano"
                    },
                    {
                        "name": "Takahito Tanimura"
                    }
                ],
                "author_detail": {
                    "name": "Takahito Tanimura"
                },
                "author": "Takahito Tanimura",
                "arxiv_comment": "15 pages, 12 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19602v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19602v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.19243v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.19243v2",
                "updated": "2025-04-30T19:48:41Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    19,
                    48,
                    41,
                    2,
                    120,
                    0
                ],
                "published": "2025-01-31T15:58:15Z",
                "published_parsed": [
                    2025,
                    1,
                    31,
                    15,
                    58,
                    15,
                    4,
                    31,
                    0
                ],
                "title": "Accelerating Diffusion Transformer via Error-Optimized Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accelerating Diffusion Transformer via Error-Optimized Cache"
                },
                "summary": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Diffusion Transformer (DiT) is a crucial method for content generation.\nHowever, it needs a lot of time to sample. Many studies have attempted to use\ncaching to reduce the time consumption of sampling. Existing caching methods\naccelerate generation by reusing DiT features from the previous time step and\nskipping calculations in the next, but they tend to locate and cache low-error\nmodules without focusing on reducing caching-induced errors, resulting in a\nsharp decline in generated content quality when increasing caching intensity.\nTo solve this problem, we propose the Error-Optimized Cache (EOC). This method\nintroduces three key improvements: (1) Prior knowledge extraction: Extract and\nprocess the caching differences; (2) A judgment method for cache optimization:\nDetermine whether certain caching steps need to be optimized; (3) Cache\noptimization: reduce caching errors. Experiments show that this algorithm\nsignificantly reduces the error accumulation caused by caching, especially\nexcessive caching. On the ImageNet dataset, without substantially increasing\nthe computational load, this method improves the FID of the generated images\nwhen the rule-based model FORA has a caching level of 75%, 50%, and 25%, and\nthe training-based model Learning-to-cache has a caching level of 22%.\nSpecifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857\nto 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%)\nrespectively."
                },
                "authors": [
                    {
                        "name": "Junxiang Qiu"
                    },
                    {
                        "name": "Shuo Wang"
                    },
                    {
                        "name": "Jinda Lu"
                    },
                    {
                        "name": "Lin Liu"
                    },
                    {
                        "name": "Houcheng Jiang"
                    },
                    {
                        "name": "Xingyu Zhu"
                    },
                    {
                        "name": "Yanbin Hao"
                    }
                ],
                "author_detail": {
                    "name": "Yanbin Hao"
                },
                "author": "Yanbin Hao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.19243v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.19243v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00074v1",
                "updated": "2025-04-30T18:00:02Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T18:00:02Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    18,
                    0,
                    2,
                    2,
                    120,
                    0
                ],
                "title": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SDW driven \"magnetic breakdown\" in a d-wave altermagnet KV$_2$Se$_2$O"
                },
                "summary": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Altermagnets, combining zero net magnetization with intrinsic spin splitting,\ndemonstrate unique quantum phenomena crucial for spintronic applications.\nKV$_2$Se$_2$O is proven to be a d-wave altermagnet with phase transition from a\ncheckerboard-type (C-type) antiferromagnetic (AFM) state to a spin density wave\n(SDW) state as the temperature decreases. After phase transition, the apparent\nparadox emerges where angle-resolved photoemission spectroscopy (ARPES) reveals\nnegligible Fermi surface modifications, while physical property measurement\nsystem (PPMS) measurements uncover substantial changes in transport properties.\nOur study explores the microscopic mechanisms governing phase-dependent\ntransport properties of KV$_2$Se$_2$O base on first-principles calculations.\nThe spin canting driven by periodic spin modulation in the SDW phase reduces\nthe magnetic symmetry of KV$_2$Se$_2$O. The resultant band degeneracy lifting\nand Fermi surface reconstruction induce the ``magnetic breakdown\" phenomenon,\nwhich alters carrier trajectories, modifies carrier concentration, strengthens\nelectron-hole compensation, and ultimately accounts for the contrasting\nmagnetic-field-dependent Hall resistivity relative to the C-type AFM state. Our\nwork proposes an innovative method for identifying the electronic structure\nevolution across phase transitions from transport signatures, providing a novel\nparadigm for altermagnets research."
                },
                "authors": [
                    {
                        "name": "Xu Yan"
                    },
                    {
                        "name": "Ziyin Song"
                    },
                    {
                        "name": "Juntao Song"
                    },
                    {
                        "name": "Zhong Fang"
                    },
                    {
                        "name": "Hongming Weng"
                    },
                    {
                        "name": "Quansheng Wu"
                    }
                ],
                "author_detail": {
                    "name": "Quansheng Wu"
                },
                "author": "Quansheng Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.mtrl-sci",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21594v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21594v1",
                "updated": "2025-04-30T12:51:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T12:51:59Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    51,
                    59,
                    2,
                    120,
                    0
                ],
                "title": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Switching Transients in Constrained Transformer-Line/Cable\n  Configurations"
                },
                "summary": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper investigates the transient phenomena that occur in two special\ncases in the Netherlands: (A) during the energization of a power transformer\nvia a cable feeder and (B) the energization of a power transformer together\nwith an overhead line (OHL). In Case A a 7 km long 150 kV cable and a 150/50 kV\ntransformer are connected and energized at the same time. In Case B a 150/50 kV\ntransformer and a short 50 kV OHL are connected and energized simultaneously.\nThe reason behind this kind of situations is related to space restrictions and\ncost efficiency."
                },
                "authors": [
                    {
                        "name": "Y. Xiang"
                    },
                    {
                        "name": "L. Wu"
                    },
                    {
                        "name": "K. Velitsikakis"
                    },
                    {
                        "name": "A. L. J. Janssen"
                    }
                ],
                "author_detail": {
                    "name": "A. L. J. Janssen"
                },
                "author": "A. L. J. Janssen",
                "arxiv_comment": "11 pages, 17 figures, CIGRE conference 2016",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21594v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21594v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.00745v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.00745v1",
                "updated": "2025-04-30T08:08:15Z",
                "updated_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "published": "2025-04-30T08:08:15Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    8,
                    8,
                    15,
                    2,
                    120,
                    0
                ],
                "title": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsive DNN Adaptation for Video Analytics against Environment Shift\n  via Hierarchical Mobile-Cloud Collaborations"
                },
                "summary": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile video analysis systems often encounter various deploying environments,\nwhere environment shifts present greater demands for responsiveness in\nadaptations of deployed \"expert DNN models\". Existing model adaptation\nframeworks primarily operate in a cloud-centric way, exhibiting degraded\nperformance during adaptation and delayed reactions to environment shifts.\nInstead, this paper proposes MOCHA, a novel framework optimizing the\nresponsiveness of continuous model adaptation through hierarchical\ncollaborations between mobile and cloud resources. Specifically, MOCHA (1)\nreduces adaptation response delays by performing on-device model reuse and fast\nfine-tuning before requesting cloud model retrieval and end-to-end retraining;\n(2) accelerates history expert model retrieval by organizing them into a\nstructured taxonomy utilizing domain semantics analyzed by a cloud foundation\nmodel as indices; (3) enables efficient local model reuse by maintaining\nonboard expert model caches for frequent scenes, which proactively prefetch\nmodel weights from the cloud model database. Extensive evaluations with\nreal-world videos on three DNN tasks show MOCHA improves the model accuracy\nduring adaptation by up to 6.8% while saving the response delay and retraining\ntime by up to 35.5x and 3.0x respectively."
                },
                "authors": [
                    {
                        "name": "Maozhe Zhao"
                    },
                    {
                        "name": "Shengzhong Liu"
                    },
                    {
                        "name": "Fan Wu"
                    },
                    {
                        "name": "Guihai Chen"
                    }
                ],
                "author_detail": {
                    "name": "Guihai Chen"
                },
                "author": "Guihai Chen",
                "arxiv_comment": "Sensys 2025 final version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.00745v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.00745v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21230v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21230v1",
                "updated": "2025-04-29T23:43:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:43:59Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    43,
                    59,
                    1,
                    119,
                    0
                ],
                "title": "Kimina Lean Server: Technical Report",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Kimina Lean Server: Technical Report"
                },
                "summary": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We introduce the Kimina Lean Server, an open-source project that enables fast\nand scalable interaction with Lean 4 via a unified REST API, designed as a\nsimple verifier for reinforcement learning pipelines. Built on top of the Lean\nFRO's LeanREPL, it combines server-side parallelization by managing multiple\nLean REPL processes in parallel, with an LRU caching strategy that reuses Lean\nimports across multiple requests. These features help reduce initialization\noverhead and allow large-scale batch processing of Lean code. The client-side\ninterface allows users to submit batches of proofs and receive Lean feedback,\nincluding extracted tactics and tactic states via infotree processing. These\nfeatures enable a high-performance, scalable workflow for both interaction and\nextraction of proofs, tactics, and tactic states. We open source our\nimplementation on GitHub."
                },
                "authors": [
                    {
                        "name": "Marco Dos Santos"
                    },
                    {
                        "name": "Haiming Wang"
                    },
                    {
                        "name": "Hugues de Saxc"
                    },
                    {
                        "name": "Ran Wang"
                    },
                    {
                        "name": "Mantas Baksys"
                    },
                    {
                        "name": "Mert Unsal"
                    },
                    {
                        "name": "Junqi Liu"
                    },
                    {
                        "name": "Zhengying Liu"
                    },
                    {
                        "name": "Jia Li"
                    }
                ],
                "author_detail": {
                    "name": "Jia Li"
                },
                "author": "Jia Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21230v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21230v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.21228v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.21228v1",
                "updated": "2025-04-29T23:42:21Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-29T23:42:21Z",
                "published_parsed": [
                    2025,
                    4,
                    29,
                    23,
                    42,
                    21,
                    1,
                    119,
                    0
                ],
                "title": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CachePrune: Neural-Based Attribution Defense Against Indirect Prompt\n  Injection Attacks"
                },
                "summary": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) are identified as being susceptible to indirect\nprompt injection attack, where the model undesirably deviates from\nuser-provided instructions by executing tasks injected in the prompt context.\nThis vulnerability stems from LLMs' inability to distinguish between data and\ninstructions within a prompt. In this paper, we propose CachePrune that defends\nagainst this attack by identifying and pruning task-triggering neurons from the\nKV cache of the input prompt context. By pruning such neurons, we encourage the\nLLM to treat the text spans of input prompt context as only pure data, instead\nof any indicator of instruction following. These neurons are identified via\nfeature attribution with a loss function induced from an upperbound of the\nDirect Preference Optimization (DPO) objective. We show that such a loss\nfunction enables effective feature attribution with only a few samples. We\nfurther improve on the quality of feature attribution, by exploiting an\nobserved triggering effect in instruction following. Our approach does not\nimpose any formatting on the original prompt or introduce extra test-time LLM\ncalls. Experiments show that CachePrune significantly reduces attack success\nrates without compromising the response quality. Note: This paper aims to\ndefend against indirect prompt injection attacks, with the goal of developing\nmore secure and robust AI systems."
                },
                "authors": [
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Junda Wu"
                    },
                    {
                        "name": "Yu Xia"
                    },
                    {
                        "name": "Tong Yu"
                    },
                    {
                        "name": "Ruiyi Zhang"
                    },
                    {
                        "name": "Ryan Rossi"
                    },
                    {
                        "name": "Lina Yao"
                    },
                    {
                        "name": "Julian McAuley"
                    }
                ],
                "author_detail": {
                    "name": "Julian McAuley"
                },
                "author": "Julian McAuley",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.21228v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.21228v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.12322v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.12322v2",
                "updated": "2025-04-29T17:54:42Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    17,
                    54,
                    42,
                    1,
                    119,
                    0
                ],
                "published": "2025-01-21T17:41:54Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    17,
                    41,
                    54,
                    1,
                    21,
                    0
                ],
                "title": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "An Achievable Scheme for the K-user Linear Computation Broadcast Channel"
                },
                "summary": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a new achievable scheme for the K-user Linear Computation\nBroadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K\nusers, each aiming to retrieve a desired linear function of the data by\nleveraging their prior locally available side information in the form of\nanother linear function of the data. The proposed scheme is based on a subspace\ndecomposition derived from representable polymatroid spaces. This decomposition\nenables the server to effectively design multicast messages that simultaneously\nbenefit multiple users and allow users to eliminate interference using their\navailable side information. This work extends existing results for the 3-LCBC\nby introducing a linear programming framework to optimize multicast\nopportunities across an arbitrary number of users. The proposed approach can be\nused to derive achievable scheme for the K-user coded caching problem with\nlinear coded placement and scalar linear function retrieval, which was our\noriginal motivation to investigate the K-LCBC."
                },
                "authors": [
                    {
                        "name": "Yinbin Ma"
                    },
                    {
                        "name": "Daniela Tuninetti"
                    }
                ],
                "author_detail": {
                    "name": "Daniela Tuninetti"
                },
                "author": "Daniela Tuninetti",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.12322v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.12322v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12397v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12397v2",
                "updated": "2025-04-29T14:25:08Z",
                "updated_parsed": [
                    2025,
                    4,
                    29,
                    14,
                    25,
                    8,
                    1,
                    119,
                    0
                ],
                "published": "2025-04-16T18:03:21Z",
                "published_parsed": [
                    2025,
                    4,
                    16,
                    18,
                    3,
                    21,
                    2,
                    106,
                    0
                ],
                "title": "Activated LoRA: Fine-tuned LLMs for Intrinsics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Activated LoRA: Fine-tuned LLMs for Intrinsics"
                },
                "summary": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for\nfinetuning the weights of large foundation models, and has become the go-to\nmethod for data-driven customization of LLMs. Despite the promise of highly\ncustomized behaviors and capabilities, switching between relevant LoRAs in a\nmultiturn setting is highly inefficient, as the key-value (KV) cache of the\nentire turn history must be recomputed with the LoRA weights before generation\ncan begin. To address this problem, we propose Activated LoRA (aLoRA), which\nmodifies the LoRA framework to only adapt weights for the tokens in the\nsequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA\nto accept the base model's KV cache of the input string, meaning that aLoRA can\nbe instantly activated whenever needed in a chain without recomputing the\ncache. This enables building what we call \\emph{intrinsics}, i.e. highly\nspecialized models invoked to perform well-defined operations on portions of an\ninput chain or conversation that otherwise uses the base model by default. We\nuse aLoRA to train a set of intrinsics models, demonstrating competitive\naccuracy with standard LoRA while achieving significant inference benefits."
                },
                "authors": [
                    {
                        "name": "Kristjan Greenewald"
                    },
                    {
                        "name": "Luis Lastras"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Vraj Shah"
                    },
                    {
                        "name": "Lucian Popa"
                    },
                    {
                        "name": "Giulio Zizzo"
                    },
                    {
                        "name": "Chulaka Gunasekara"
                    },
                    {
                        "name": "Ambrish Rawat"
                    },
                    {
                        "name": "David Cox"
                    }
                ],
                "author_detail": {
                    "name": "David Cox"
                },
                "author": "David Cox",
                "arxiv_comment": "arXiv admin note: text overlap with arXiv:2504.11704",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12397v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12397v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.20246v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.20246v1",
                "updated": "2025-04-28T20:30:59Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T20:30:59Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    20,
                    30,
                    59,
                    0,
                    118,
                    0
                ],
                "title": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tree embedding based mapping system for low-latency mobile applications\n  in multi-access networks"
                },
                "summary": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Low-latency applications like AR/VR and online gaming need fast, stable\nconnections. New technologies such as V2X, LEO satellites, and 6G bring unique\nchallenges in mobility management. Traditional solutions based on centralized\nor distributed anchors often fall short in supporting rapid mobility due to\ninefficient routing, low versatility, and insufficient multi-access support. In\nthis paper, we design a new end-to-end system for tracking multi-connected\nmobile devices at scale and optimizing performance for latency-sensitive,\nhighly dynamic applications. Our system, based on the locator/ID separation\nprinciple, extends to multi-access networks without requiring specialized\nrouters or caching. Using a novel tree embedding-based overlay, we enable fast\nsession setup while allowing endpoints to directly handle mobility between\nthem. Evaluation with real network data shows our solution cuts connection\nlatency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due\nto cache misses. It also significantly reduces location update overhead and\ndisruption time during mobility."
                },
                "authors": [
                    {
                        "name": "Yu Mi"
                    },
                    {
                        "name": "Randeep Bhatia"
                    },
                    {
                        "name": "Fang Hao"
                    },
                    {
                        "name": "An Wang"
                    },
                    {
                        "name": "Steve Benno"
                    },
                    {
                        "name": "Tv Lakshman"
                    }
                ],
                "author_detail": {
                    "name": "Tv Lakshman"
                },
                "author": "Tv Lakshman",
                "arxiv_comment": "Accepted by IEEE INFOCOM 2025-IEEE Conference on Computer\n  Communications",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.20246v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.20246v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.12747v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.12747v3",
                "updated": "2025-04-28T17:17:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    17,
                    17,
                    53,
                    0,
                    118,
                    0
                ],
                "published": "2024-05-21T12:59:59Z",
                "published_parsed": [
                    2024,
                    5,
                    21,
                    12,
                    59,
                    59,
                    1,
                    142,
                    0
                ],
                "title": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Coded Caching with Low Subpacketization and Coding Delay\n  using Combinatorial t-Designs"
                },
                "summary": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Coded caching scheme originally proposed by Maddah-Ali and Niesen (MN)\nconsidered a broadcast network consisting of a single server connected to a set\nof users each having a cache memory. Motivated by practical scenarios,\nKaramchandani \\textit{et al.} in [16] proposed a coded caching scheme for a\ntwo-layer hierarchical network consisting of a single server connected to\nmultiple mirror sites and each mirror site connected to a distinct set of\nusers, in which both mirror sites and users having cache memories. Low\nsubpacketization level coded caching schemes are desirable for practical\nimplementations. Placement delivery array (PDA) was proposed as a tool to\ndesign coded caching schemes with reduced subpacketization level by Yan\n\\textit{et al.} in [4]. Schemes with reduced subpacketization levels are\nstudied extensively in the literature for single-layer networks. Kong\n\\textit{et al.} in [17] proposed a structure called hierarchical placement\ndelivery arrays (HPDA), which characterizes a hierarchical coded caching system\nand also proposed a class of HPDAs that gives low subpacketization level\nschemes by using two PDAs. Low subpacketization level hierarchical schemes\nusing combinatorial $t$-designs is proposed in [20]. Apart from that there is\nno other existing work that discusses the subpacketization problem in a\nhierarchical network. This paper proposes a class of HPDA construction that\ngives low subpacketization level hierarchical coded caching schemes, by first\nconstructing a new class of PDAs. Compared with the existing schemes, in cases\nwhere the system parameters and subpacketization level are the same, the\nproposed hierarchical scheme has a better coding delay. Further, the new class\nof PDAs constructed either subsumes several known PDA constructions or achieves\nbetter transmission load for the same system parameters."
                },
                "authors": [
                    {
                        "name": "Rashid Ummer N. T."
                    },
                    {
                        "name": "B. Sundar Rajan"
                    }
                ],
                "author_detail": {
                    "name": "B. Sundar Rajan"
                },
                "author": "B. Sundar Rajan",
                "arxiv_comment": "IEEE Internet of Things Journal (Accepted for publication). The\n  Hierarchical coded caching scheme in this updated version unifies the scheme\n  in the previous version and the schemes in arxiv:2402.07188. This version\n  includes a more comprehensive performance analysis. To reflect these the\n  title has been updated",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.12747v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.12747v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19984v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19984v1",
                "updated": "2025-04-28T16:59:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T16:59:13Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    16,
                    59,
                    13,
                    0,
                    118,
                    0
                ],
                "title": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D MPSoC with On-Chip Cache Support -- Design and Exploitation"
                },
                "summary": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing density of transistors in Integrated Circuits (ICs) has\nenabled the development of highly integrated Systems-on-Chip (SoCs) and, more\nrecently, Multiprocessor Systems-on-Chip (MPSoCs). To address scalability\nchallenges in communication and memory performance, three-dimensional (3D)\nNetwork-on-Chip (NoC) architectures have emerged, offering improvements in\ncommunication latency and throughput. However, memory system efficiency remains\na critical bottleneck in NoC-based designs. This work proposes the design and\nexperimental exploration of 3D MPSoCs with on-chip cache support by employing\ndistinct communication infrastructures for inter-processor and memory\ninteractions. Specifically, packet-based NoCs are adopted for inter-processor\ncommunication, while a crossbar-based infrastructure supports a cache coherence\nhierarchy for memory access. A two-layer system architecture is introduced,\ncombining a Uniform Memory Access (UMA) model within clusters and a No Remote\nMemory Access (NORMA) model between clusters, aiming to balance scalability and\ncoherence requirements. Emerging memory technologies such as PCRAM and MRAM are\nexplored to optimize performance, energy consumption, and area usage.\nExperimental evaluations are conducted using the Gem5 simulator, targeting a\nmodel based on the ARM Versatile Express platform. The outcomes of this study\naim to enhance MPSoC scalability while meeting the stringent demands of\nmemory-centric applications."
                },
                "authors": [
                    {
                        "name": "Rodrigo Cataldo"
                    },
                    {
                        "name": "Cesar Marcon"
                    },
                    {
                        "name": "Debora Matos"
                    }
                ],
                "author_detail": {
                    "name": "Debora Matos"
                },
                "author": "Debora Matos",
                "arxiv_comment": "Progress Seminar",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19984v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19984v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19874v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19874v1",
                "updated": "2025-04-28T15:05:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:05:35Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    5,
                    35,
                    0,
                    118,
                    0
                ],
                "title": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TurboQuant: Online Vector Quantization with Near-optimal Distortion Rate"
                },
                "summary": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vector quantization, a problem rooted in Shannon's source coding theory, aims\nto quantize high-dimensional Euclidean vectors while minimizing distortion in\ntheir geometric structure. We propose TurboQuant to address both mean-squared\nerror (MSE) and inner product distortion, overcoming limitations of existing\nmethods that fail to achieve optimal distortion rates. Our data-oblivious\nalgorithms, suitable for online applications, achieve near-optimal distortion\nrates (within a small constant factor) across all bit-widths and dimensions.\nTurboQuant achieves this by randomly rotating input vectors, inducing a\nconcentrated Beta distribution on coordinates, and leveraging the\nnear-independence property of distinct coordinates in high dimensions to simply\napply optimal scalar quantizers per each coordinate. Recognizing that\nMSE-optimal quantizers introduce bias in inner product estimation, we propose a\ntwo-stage approach: applying an MSE quantizer followed by a 1-bit Quantized JL\n(QJL) transform on the residual, resulting in an unbiased inner product\nquantizer. We also provide a formal proof of the information-theoretic lower\nbounds on best achievable distortion rate by any vector quantizer,\ndemonstrating that TurboQuant closely matches these bounds, differing only by a\nsmall constant ($\\approx 2.7$) factor. Experimental results validate our\ntheoretical findings, showing that for KV cache quantization, we achieve\nabsolute quality neutrality with 3.5 bits per channel and marginal quality\ndegradation with 2.5 bits per channel. Furthermore, in nearest neighbor search\ntasks, our method outperforms existing product quantization techniques in\nrecall while reducing indexing time to virtually zero."
                },
                "authors": [
                    {
                        "name": "Amir Zandieh"
                    },
                    {
                        "name": "Majid Daliri"
                    },
                    {
                        "name": "Majid Hadian"
                    },
                    {
                        "name": "Vahab Mirrokni"
                    }
                ],
                "author_detail": {
                    "name": "Vahab Mirrokni"
                },
                "author": "Vahab Mirrokni",
                "arxiv_comment": "25 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19874v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19874v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DB",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19867v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19867v1",
                "updated": "2025-04-28T15:00:03Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T15:00:03Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    15,
                    0,
                    3,
                    0,
                    118,
                    0
                ],
                "title": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated\n  Computation and Unified Storage"
                },
                "summary": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Existing large language model (LLM) serving systems fall into two categories:\n1) a unified system where prefill phase and decode phase are co-located on the\nsame GPU, sharing the unified computational resource and storage, and 2) a\ndisaggregated system where the two phases are disaggregated to different GPUs.\nThe design of the disaggregated system addresses the latency interference and\nsophisticated scheduling issues in the unified system but leads to storage\nchallenges including 1) replicated weights for both phases that prevent\nflexible deployment, 2) KV cache transfer overhead between the two phases, 3)\nstorage imbalance that causes substantial wasted space of the GPU capacity, and\n4) suboptimal resource adjustment arising from the difficulties in migrating KV\ncache. Such storage inefficiency delivers poor serving performance under high\nrequest rates.\n  In this paper, we identify that the advantage of the disaggregated system\nlies in the disaggregated computation, i.e., partitioning the computational\nresource to enable the asynchronous computation of two phases. Thus, we propose\na novel LLM serving system, semi-PD, characterized by disaggregated computation\nand unified storage. In semi-PD, we introduce a computation resource controller\nto achieve disaggregated computation at the streaming multi-processor (SM)\nlevel, and a unified memory manager to manage the asynchronous memory access\nfrom both phases. semi-PD has a low-overhead resource adjustment mechanism\nbetween the two phases, and a service-level objective (SLO) aware dynamic\npartitioning algorithm to optimize the SLO attainment. Compared to\nstate-of-the-art systems, semi-PD maintains lower latency at higher request\nrates, reducing the average end-to-end latency per request by 1.27-2.58x on\nDeepSeek series models, and serves 1.55-1.72x more requests adhering to latency\nconstraints on Llama series models."
                },
                "authors": [
                    {
                        "name": "Ke Hong"
                    },
                    {
                        "name": "Lufang Chen"
                    },
                    {
                        "name": "Zhong Wang"
                    },
                    {
                        "name": "Xiuhong Li"
                    },
                    {
                        "name": "Qiuli Mao"
                    },
                    {
                        "name": "Jianping Ma"
                    },
                    {
                        "name": "Chao Xiong"
                    },
                    {
                        "name": "Guanyu Wu"
                    },
                    {
                        "name": "Buhe Han"
                    },
                    {
                        "name": "Guohao Dai"
                    },
                    {
                        "name": "Yun Liang"
                    },
                    {
                        "name": "Yu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yu Wang"
                },
                "author": "Yu Wang",
                "arxiv_comment": "18 pages, 16 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19867v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19867v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19601v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19601v1",
                "updated": "2025-04-28T09:03:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T09:03:45Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    9,
                    3,
                    45,
                    0,
                    118,
                    0
                ],
                "title": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Characterizing the Optimal Memory-Rate Tradeoff in Secure Coded Caching\n  for Small Buffer or Small Rate"
                },
                "summary": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider the secure coded caching problem proposed by Ravindrakumar et. al\nwhere no user can obtain information about files other than the one requested.\nWe first propose three new schemes for the three cases of cache size $M=1$,\n$N=2$ files and arbitrary $K$ users, delivery rate $ R=1$, arbitrary $N$ files\nand $K$ users, and the general case for arbitrary $N$ files and $K$ users,\nrespectively. Then we derive converse results by characterizing new properties\nof secure coded caching schemes. As a result, we characterize the two\nend-points of the optimal memory-rate tradeoff curve for arbitrary number of\nusers and files. Furthermore, for the case of $N=2$ files and arbitrary number\nof users, we also characterize a segment of the optimal memory-rate tradeoff\ncurve, where the cache size is relatively small."
                },
                "authors": [
                    {
                        "name": "Han Fang"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    }
                ],
                "author_detail": {
                    "name": "Wei Kang"
                },
                "author": "Wei Kang",
                "arxiv_comment": "Submitted to IEEE Transactions on Information Theory",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19601v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19601v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19561v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19561v1",
                "updated": "2025-04-28T08:12:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T08:12:30Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    8,
                    12,
                    30,
                    0,
                    118,
                    0
                ],
                "title": "Quantifying Memory Utilization with Effective State-Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Quantifying Memory Utilization with Effective State-Size"
                },
                "summary": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need to develop a general framework for architecture analysis is becoming\nincreasingly important, given the expanding design space of sequence models. To\nthis end, we draw insights from classical signal processing and control theory,\nto develop a quantitative measure of \\textit{memory utilization}: the internal\nmechanisms through which a model stores past information to produce future\noutputs. This metric, which we call \\textbf{\\textit{effective state-size}}\n(ESS), is tailored to the fundamental class of systems with\n\\textit{input-invariant} and \\textit{input-varying linear operators},\nencompassing a variety of computational units such as variants of attention,\nconvolutions, and recurrences. Unlike prior work on memory utilization, which\neither relies on raw operator visualizations (e.g. attention maps), or simply\nthe total \\textit{memory capacity} (i.e. cache size) of a model, our metrics\nprovide highly interpretable and actionable measurements. In particular, we\nshow how ESS can be leveraged to improve initialization strategies, inform\nnovel regularizers and advance the performance-efficiency frontier through\nmodel distillation. Furthermore, we demonstrate that the effect of context\ndelimiters (such as end-of-speech tokens) on ESS highlights cross-architectural\ndifferences in how large language models utilize their available memory to\nrecall information. Overall, we find that ESS provides valuable insights into\nthe dynamics that dictate memory utilization, enabling the design of more\nefficient and effective sequence models."
                },
                "authors": [
                    {
                        "name": "Rom N. Parnichkun"
                    },
                    {
                        "name": "Neehal Tumma"
                    },
                    {
                        "name": "Armin W. Thomas"
                    },
                    {
                        "name": "Alessandro Moro"
                    },
                    {
                        "name": "Qi An"
                    },
                    {
                        "name": "Taiji Suzuki"
                    },
                    {
                        "name": "Atsushi Yamashita"
                    },
                    {
                        "name": "Michael Poli"
                    },
                    {
                        "name": "Stefano Massaroli"
                    }
                ],
                "author_detail": {
                    "name": "Stefano Massaroli"
                },
                "author": "Stefano Massaroli",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19561v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19561v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19475v1",
                "updated": "2025-04-28T04:31:24Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-28T04:31:24Z",
                "published_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    31,
                    24,
                    0,
                    118,
                    0
                ],
                "title": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Prisma: An Open Source Toolkit for Mechanistic Interpretability in\n  Vision and Video"
                },
                "summary": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Robust tooling and publicly available pre-trained models have helped drive\nrecent advances in mechanistic interpretability for language models. However,\nsimilar progress in vision mechanistic interpretability has been hindered by\nthe lack of accessible frameworks and pre-trained weights. We present Prisma\n(Access the codebase here: https://github.com/Prisma-Multimodal/ViT-Prisma), an\nopen-source framework designed to accelerate vision mechanistic\ninterpretability research, providing a unified toolkit for accessing 75+ vision\nand video transformers; support for sparse autoencoder (SAE), transcoder, and\ncrosscoder training; a suite of 80+ pre-trained SAE weights; activation\ncaching, circuit analysis tools, and visualization tools; and educational\nresources. Our analysis reveals surprising findings, including that effective\nvision SAEs can exhibit substantially lower sparsity patterns than language\nSAEs, and that in some instances, SAE reconstructions can decrease model loss.\nPrisma enables new research directions for understanding vision model internals\nwhile lowering barriers to entry in this emerging field."
                },
                "authors": [
                    {
                        "name": "Sonia Joseph"
                    },
                    {
                        "name": "Praneet Suresh"
                    },
                    {
                        "name": "Lorenz Hufe"
                    },
                    {
                        "name": "Edward Stevinson"
                    },
                    {
                        "name": "Robert Graham"
                    },
                    {
                        "name": "Yash Vadi"
                    },
                    {
                        "name": "Danilo Bzdok"
                    },
                    {
                        "name": "Sebastian Lapuschkin"
                    },
                    {
                        "name": "Lee Sharkey"
                    },
                    {
                        "name": "Blake Aaron Richards"
                    }
                ],
                "author_detail": {
                    "name": "Blake Aaron Richards"
                },
                "author": "Blake Aaron Richards",
                "arxiv_comment": "4 pages, 3 figures, 9 tables. Oral and Tutorial at the CVPR\n  Mechanistic Interpretability for Vision (MIV) Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18001v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18001v2",
                "updated": "2025-04-28T04:02:30Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    4,
                    2,
                    30,
                    0,
                    118,
                    0
                ],
                "published": "2025-04-25T01:10:49Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    1,
                    10,
                    49,
                    4,
                    115,
                    0
                ],
                "title": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Cluster to Desktop: A Cache-Accelerated INR framework for\n  Interactive Visualization of Tera-Scale Data"
                },
                "summary": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Machine learning has enabled the use of implicit neural representations\n(INRs) to efficiently compress and reconstruct massive scientific datasets.\nHowever, despite advances in fast INR rendering algorithms, INR-based rendering\nremains computationally expensive, as computing data values from an INR is\nsignificantly slower than reading them from GPU memory. This bottleneck\ncurrently restricts interactive INR visualization to professional workstations.\nTo address this challenge, we introduce an INR rendering framework accelerated\nby a scalable, multi-resolution GPU cache capable of efficiently representing\ntera-scale datasets. By minimizing redundant data queries and prioritizing\nnovel volume regions, our method reduces the number of INR computations per\nframe, achieving an average 5x speedup over the state-of-the-art INR rendering\nmethod while still maintaining high visualization quality. Coupled with\nexisting hardware-accelerated INR compressors, our framework enables scientists\nto generate and compress massive datasets in situ on high-performance computing\nplatforms and then interactively explore them on consumer-grade hardware post\nhoc."
                },
                "authors": [
                    {
                        "name": "Daniel Zavorotny"
                    },
                    {
                        "name": "Qi Wu"
                    },
                    {
                        "name": "David Bauer"
                    },
                    {
                        "name": "Kwan-Liu Ma"
                    }
                ],
                "author_detail": {
                    "name": "Kwan-Liu Ma"
                },
                "author": "Kwan-Liu Ma",
                "arxiv_comment": "11 pages, 11 figures, EGPGV25",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18001v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18001v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.GR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.GR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12150v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12150v3",
                "updated": "2025-04-28T02:58:27Z",
                "updated_parsed": [
                    2025,
                    4,
                    28,
                    2,
                    58,
                    27,
                    0,
                    118,
                    0
                ],
                "published": "2025-03-15T14:13:23Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    14,
                    13,
                    23,
                    5,
                    74,
                    0
                ],
                "title": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Point-Cache: Test-time Dynamic and Hierarchical Cache for Robust and\n  Generalizable Point Cloud Analysis"
                },
                "summary": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper proposes a general solution to enable point cloud recognition\nmodels to handle distribution shifts at test time. Unlike prior methods, which\nrely heavily on training data (often inaccessible during online inference) and\nare limited to recognizing a fixed set of point cloud classes predefined during\ntraining, we explore a more practical and challenging scenario: adapting the\nmodel solely based on online test data to recognize both previously seen\nclasses and novel, unseen classes at test time. To this end, we develop\n\\textbf{Point-Cache}, a hierarchical cache model that captures essential clues\nof online test samples, particularly focusing on the global structure of point\nclouds and their local-part details. Point-Cache, which serves as a rich 3D\nknowledge base, is dynamically managed to prioritize the inclusion of\nhigh-quality samples. Designed as a plug-and-play module, our method can be\nflexibly integrated into large multimodal 3D models to support open-vocabulary\npoint cloud recognition. Notably, our solution operates with efficiency\ncomparable to zero-shot inference, as it is entirely training-free. Point-Cache\ndemonstrates substantial gains across 8 challenging benchmarks and 4\nrepresentative large 3D models, highlighting its effectiveness. Code is\navailable at https://github.com/auniquesun/Point-Cache."
                },
                "authors": [
                    {
                        "name": "Hongyu Sun"
                    },
                    {
                        "name": "Qiuhong Ke"
                    },
                    {
                        "name": "Ming Cheng"
                    },
                    {
                        "name": "Yongcai Wang"
                    },
                    {
                        "name": "Deying Li"
                    },
                    {
                        "name": "Chenhui Gou"
                    },
                    {
                        "name": "Jianfei Cai"
                    }
                ],
                "author_detail": {
                    "name": "Jianfei Cai"
                },
                "author": "Jianfei Cai",
                "arxiv_comment": "Accepted by CVPR 2025; 24 pages, 14 figures, 18 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.12150v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12150v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19365v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19365v1",
                "updated": "2025-04-27T22:05:14Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T22:05:14Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    22,
                    5,
                    14,
                    6,
                    117,
                    0
                ],
                "title": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AGILE: Lightweight and Efficient Asynchronous GPU-SSD Integration"
                },
                "summary": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graphics Processing Units (GPUs) have become essential for computationally\nintensive applications. However, emerging workloads such as recommender\nsystems, graph analytics, and data analytics often involve processing data\nexceeding GPU on-chip memory capacity. To mitigate this issue, existing\nsolutions enable GPUs to use CPU DRAM or SSDs as external memory. Among them,\nthe GPU-centric approach lets GPU threads directly initiate NVMe requests,\neliminating CPU intervention overhead over traditional methods. However, the\nSOTA GPU-centric approach adopts a synchronous IO model, and threads must\ntolerate the long latency in communication before starting any tasks.\n  In this work, we propose AGILE, a lightweight and efficient asynchronous\nlibrary allowing GPU threads to access SSDs asynchronously while eliminating\ndeadlock risks. AGILE also integrates a flexible software cache using GPU\nHigh-Bandwidth Mamory (HBM). We demonstrate that the asynchronous GPU-centric\nIO achieves up to 1.88$\\times$ improvement in workloads with different\ncomputation-to-communication (CTC) ratios. We also compare AGILE with the SOTA\nwork BaM on Deep Learning Recommendation Models (DLRM) with various settings,\nand the results show that AGILE achieves 1.75$\\times$ performance improvement\ndue to its efficient design and the overlapping strategy enabled by an\nasynchronous IO model. We further evaluate AGILE's API overhead on graph\napplications, and the results demonstrate AGILE reduces software cache overhead\nby up to 3.12$\\times$ and overhead in NVMe IO requests by up to 2.85$\\times$.\nCompared with BaM, AGILE consumes fewer registers and exhibits up to\n1.32$\\times$ reduction in the usage of registers."
                },
                "authors": [
                    {
                        "name": "Zhuoping Yang"
                    },
                    {
                        "name": "Jinming Zhuang"
                    },
                    {
                        "name": "Xingzhen Chen"
                    },
                    {
                        "name": "Alex K. Jones"
                    },
                    {
                        "name": "Peipei Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Peipei Zhou"
                },
                "author": "Peipei Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19365v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19365v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19266v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19266v1",
                "updated": "2025-04-27T14:46:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T14:46:43Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    14,
                    46,
                    43,
                    6,
                    117,
                    0
                ],
                "title": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "OpenFusion++: An Open-vocabulary Real-time Scene Understanding System"
                },
                "summary": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-time open-vocabulary scene understanding is essential for efficient 3D\nperception in applications such as vision-language navigation, embodied\nintelligence, and augmented reality. However, existing methods suffer from\nimprecise instance segmentation, static semantic updates, and limited handling\nof complex queries. To address these issues, we present OpenFusion++, a\nTSDF-based real-time 3D semantic-geometric reconstruction system. Our approach\nrefines 3D point clouds by fusing confidence maps from foundational models,\ndynamically updates global semantic labels via an adaptive cache based on\ninstance area, and employs a dual-path encoding framework that integrates\nobject attributes with environmental context for precise query responses.\nExperiments on the ICL, Replica, ScanNet, and ScanNet++ datasets demonstrate\nthat OpenFusion++ significantly outperforms the baseline in both semantic\naccuracy and query responsiveness."
                },
                "authors": [
                    {
                        "name": "Xiaofeng Jin"
                    },
                    {
                        "name": "Matteo Frosi"
                    },
                    {
                        "name": "Matteo Matteucci"
                    }
                ],
                "author_detail": {
                    "name": "Matteo Matteucci"
                },
                "author": "Matteo Matteucci",
                "arxiv_comment": "8 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19266v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19266v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T45, 68U05",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.10; I.4.8",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.19191v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.19191v1",
                "updated": "2025-04-27T10:48:56Z",
                "updated_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "published": "2025-04-27T10:48:56Z",
                "published_parsed": [
                    2025,
                    4,
                    27,
                    10,
                    48,
                    56,
                    6,
                    117,
                    0
                ],
                "title": "WuNeng: Hybrid State with Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WuNeng: Hybrid State with Attention"
                },
                "summary": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The WuNeng architecture introduces a novel approach to enhancing the\nexpressivity and power of large language models by integrating recurrent neural\nnetwork (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing\nheightened contextual coherence over reducing KV cache size. Building upon the\nhybrid-head concept from Hymba, WuNeng augments standard multi-head attention\nwith additional RWKV-7 state-driven heads, rather than replacing existing\nheads, to enrich the model's representational capacity. A cross-head\ninteraction technique fosters dynamic synergy among standard, state-driven, and\nnewly introduced middle heads, leveraging concatenation, additive modulation,\nand gated fusion for robust information integration. Furthermore, a multi-token\nstate processing mechanism harnesses the continuous RWKV-7 state to capture\nintricate, sequence-wide dependencies, significantly boosting expressivity.\nRemarkably, these enhancements are achieved with minimal additional parameters,\nensuring efficiency while empowering the model to excel in complex reasoning\nand sequence generation tasks. WuNeng sets a new standard for balancing\nexpressivity and computational efficiency in modern neural architectures."
                },
                "authors": [
                    {
                        "name": "Liu Xiao"
                    },
                    {
                        "name": "Li Zhiyuan"
                    },
                    {
                        "name": "Lin Yueyu"
                    }
                ],
                "author_detail": {
                    "name": "Lin Yueyu"
                },
                "author": "Lin Yueyu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.19191v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.19191v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.10883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.10883v2",
                "updated": "2025-04-26T12:07:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    26,
                    12,
                    7,
                    35,
                    5,
                    116,
                    0
                ],
                "published": "2024-11-16T20:40:08Z",
                "published_parsed": [
                    2024,
                    11,
                    16,
                    20,
                    40,
                    8,
                    5,
                    321,
                    0
                ],
                "title": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "I Know What You Sync: Covert and Side Channel Attacks on File Systems\n  via syncfs"
                },
                "summary": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Operating Systems enforce logical isolation using abstractions such as\nprocesses, containers, and isolation technologies to protect a system from\nmalicious or buggy code. In this paper, we show new types of side channels\nthrough the file system that break this logical isolation. The file system\nplays a critical role in the operating system, managing all I/O activities\nbetween the application layer and the physical storage device. We observe that\nthe file system implementation is shared, leading to timing leakage when using\ncommon I/O system calls. Specifically, we found that modern operating systems\ntake advantage of any flush operation (which saves cached blocks in memory to\nthe SSD or disk) to flush all of the I/O buffers, even those used by other\nisolation domains. Thus, by measuring the delay of syncfs, the attacker can\ninfer the I/O behavior of victim programs. We then demonstrate a syncfs covert\nchannel attack on multiple file systems, including both Linux native file\nsystems and the Windows file system, achieving a maximum bandwidth of 5 Kbps\nwith an error rate of 0.15% on Linux and 7.6 Kbps with an error rate of 1.9% on\nWindows. In addition, we construct three side-channel attacks targeting both\nLinux and Android devices. On Linux devices, we implement a website\nfingerprinting attack and a video fingerprinting attack by tracking the write\npatterns of temporary buffering files. On Android devices, we design an\napplication fingerprinting attack that leaks application write patterns during\nboot-up. The attacks achieve over 90% F1 score, precision, and recall. Finally,\nwe demonstrate that these attacks can be exploited across containers\nimplementing a container detection technique and a cross-container covert\nchannel attack."
                },
                "authors": [
                    {
                        "name": "Cheng Gu"
                    },
                    {
                        "name": "Yicheng Zhang"
                    },
                    {
                        "name": "Nael Abu-Ghazaleh"
                    }
                ],
                "author_detail": {
                    "name": "Nael Abu-Ghazaleh"
                },
                "author": "Nael Abu-Ghazaleh",
                "arxiv_comment": "Accepted to IEEE S&P 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.10883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.10883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21465v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21465v3",
                "updated": "2025-04-25T19:40:54Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    19,
                    40,
                    54,
                    4,
                    115,
                    0
                ],
                "published": "2024-10-28T19:08:12Z",
                "published_parsed": [
                    2024,
                    10,
                    28,
                    19,
                    8,
                    12,
                    0,
                    302,
                    0
                ],
                "title": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM\n  Inference"
                },
                "summary": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread deployment of long-context large language models (LLMs),\nthere has been a growing demand for efficient support of high-throughput\ninference. However, as the key-value (KV) cache expands with the sequence\nlength, the increasing memory footprint and the need to access it for each\ntoken generation both result in low throughput when serving long-context LLMs.\nWhile various dynamic sparse attention methods have been proposed to speed up\ninference while maintaining generation quality, they either fail to\nsufficiently reduce GPU memory consumption or introduce significant decoding\nlatency by offloading the KV cache to the CPU. We present ShadowKV, a\nhigh-throughput long-context LLM inference system that stores the low-rank key\ncache and offloads the value cache to reduce the memory footprint for larger\nbatch sizes and longer sequences. To minimize decoding latency, ShadowKV\nemploys an accurate KV selection strategy that reconstructs minimal sparse KV\npairs on-the-fly. By evaluating ShadowKV on a broad range of benchmarks,\nincluding RULER, LongBench, and Needle In A Haystack, and models like\nLlama-3.1-8B, Llama-3-8B-1M, GLM-4-9B-1M, Yi-9B-200K, Phi-3-Mini-128K, and\nQwen2-7B-128K, we demonstrate that it can support up to 6$\\times$ larger batch\nsizes and boost throughput by up to 3.04$\\times$ on an A100 GPU without\nsacrificing accuracy, even surpassing the performance achievable with infinite\nbatch size under the assumption of infinite GPU memory. The code is available\nat https://github.com/bytedance/ShadowKV."
                },
                "authors": [
                    {
                        "name": "Hanshi Sun"
                    },
                    {
                        "name": "Li-Wen Chang"
                    },
                    {
                        "name": "Wenlei Bao"
                    },
                    {
                        "name": "Size Zheng"
                    },
                    {
                        "name": "Ningxin Zheng"
                    },
                    {
                        "name": "Xin Liu"
                    },
                    {
                        "name": "Harry Dong"
                    },
                    {
                        "name": "Yuejie Chi"
                    },
                    {
                        "name": "Beidi Chen"
                    }
                ],
                "author_detail": {
                    "name": "Beidi Chen"
                },
                "author": "Beidi Chen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21465v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21465v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18434v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18434v1",
                "updated": "2025-04-25T15:45:36Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:45:36Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    45,
                    36,
                    4,
                    115,
                    0
                ],
                "title": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Constructing Hamiltonian Decompositions of Complete $k$-Uniform\n  Hypergraphs"
                },
                "summary": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Motivated by the wide-ranging applications of Hamiltonian decompositions in\ndistributed computing, coded caching, routing, resource allocation, load\nbalancing, and fault tolerance, our work presents a comprehensive design for\nHamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$.\nBuilding upon the resolution of the long-standing conjecture of the existence\nof Hamiltonian decompositions of complete hypergraphs, a problem that was\nresolved using existence-based methods, our contribution goes beyond the\nprevious explicit designs, which were confined to the specific cases of $k=2$\nand $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing\nfor a broad applicability of Hamiltonian decompositions in various settings."
                },
                "authors": [
                    {
                        "name": "Javad Maheri"
                    },
                    {
                        "name": "Petros Elia"
                    }
                ],
                "author_detail": {
                    "name": "Petros Elia"
                },
                "author": "Petros Elia",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18434v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18434v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18432v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18432v1",
                "updated": "2025-04-25T15:44:38Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T15:44:38Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    15,
                    44,
                    38,
                    4,
                    115,
                    0
                ],
                "title": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack"
                },
                "summary": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the gap between network and CPU speeds rapidly increases, the CPU-centric\nnetwork stack proves inadequate due to excessive CPU and memory overhead. While\nhardware-offloaded network stacks alleviate these issues, they suffer from\nlimited flexibility in both control and data planes. Offloading network stack\nto off-path SmartNIC seems promising to provide high flexibility; however,\nthroughput remains constrained by inherent SmartNIC architectural limitations.\n  To this end, we design FlexiNS, a SmartNIC-centric network stack with\nsoftware transport programmability and line-rate packet processing\ncapabilities. To grapple with the limitation of SmartNIC-induced challenges,\nFlexiNS introduces: (a) a header-only offloading TX path; (b) an\nunlimited-working-set in-cache processing RX path; (c) a high-performance\nDMA-only notification pipe; and (d) a programmable offloading engine. We\nprototype FlexiNS using Nvidia BlueField-3 SmartNIC and provide out-of-the-box\nRDMA IBV verbs compatibility to users. FlexiNS achieves 2.2$\\times$ higher\nthroughput than the microkernel-based baseline in block storage disaggregation\nand 1.3$\\times$ higher throughput than the hardware-offloaded baseline in\nKVCache transfer."
                },
                "authors": [
                    {
                        "name": "Xuzheng Chen"
                    },
                    {
                        "name": "Jie Zhang"
                    },
                    {
                        "name": "Baolin Zhu"
                    },
                    {
                        "name": "Xueying Zhu"
                    },
                    {
                        "name": "Zhongqing Chen"
                    },
                    {
                        "name": "Shu Ma"
                    },
                    {
                        "name": "Lingjun Zhu"
                    },
                    {
                        "name": "Chao Shi"
                    },
                    {
                        "name": "Yin Zhang"
                    },
                    {
                        "name": "Zeke Wang"
                    }
                ],
                "author_detail": {
                    "name": "Zeke Wang"
                },
                "author": "Zeke Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18432v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18432v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18242v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18242v1",
                "updated": "2025-04-25T10:43:23Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T10:43:23Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    10,
                    43,
                    23,
                    4,
                    115,
                    0
                ],
                "title": "Demand Private Coded Caching: Small Cache Size",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demand Private Coded Caching: Small Cache Size"
                },
                "summary": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We investigate the demand private coded caching problem, which is an $(N,K)$\ncoded caching problem with $N$ files, $K$ users, each equipped with a cache of\nsize $M$, and an additional privacy constraint on user demands, i.e., each user\ncan not gain any information about the demands of other users. We focus on\nscenarios where the size of users' caches is small, aiming to further\ncharacterize the fundamental limits of this problem. We first present a new\nvirtual-user-based achievable scheme for arbitrary number of users and files,\nand two MDS-code-based achievable schemes for the case $N \\le K$. With a newly\nderived converse bound for the case $N \\le K$, these proposed schemes lead to\nthe optimal memory-rate tradeoff of the demand private coded caching problem\nfor $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and\nthe optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where\n$ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users,\nby deriving another new converse bound, the optimal memory-rate tradeoff is\ncharacterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup\n\\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate\ntradeoff of the demand private coded caching problem for 2 files and 3 users."
                },
                "authors": [
                    {
                        "name": "Qinyi Lu"
                    },
                    {
                        "name": "Nan Liu"
                    },
                    {
                        "name": "Wei Kang"
                    },
                    {
                        "name": "Chunguo Li"
                    }
                ],
                "author_detail": {
                    "name": "Chunguo Li"
                },
                "author": "Chunguo Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18242v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18242v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IT",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.18082v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.18082v1",
                "updated": "2025-04-25T05:16:53Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T05:16:53Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    16,
                    53,
                    4,
                    115,
                    0
                ],
                "title": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient GNN Training Through Structure-Aware Randomized Mini-Batching"
                },
                "summary": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) enable learning on realworld graphs and\nmini-batch training has emerged as the de facto standard for training GNNs\nbecause it can scale to very large graphs and improve convergence. Current\nmini-batch construction policies largely ignore efficiency considerations of\nGNN training. Specifically, existing mini-batching techniques employ\nrandomization schemes to improve accuracy and convergence. However, these\nrandomization schemes are often agnostic to the structural properties of the\ngraph (for eg. community structure), resulting in highly irregular memory\naccess patterns during GNN training that make suboptimal use of on-chip GPU\ncaches. On the other hand, while deterministic mini-batching based solely on\ngraph structure delivers fast runtime performance, the lack of randomness\ncompromises both the final model accuracy and training convergence speed. In\nthis paper, we present Community-structure-aware Randomized Mini-batching\n(COMM-RAND), a novel methodology that bridges the gap between the above\nextremes. COMM-RAND allows practitioners to explore the space between pure\nrandomness and pure graph structural awareness during mini-batch construction,\nleading to significantly more efficient GNN training with similar accuracy. We\nevaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND\ncuts down GNN training time by up to 2.76x (1.8x on average) while achieving an\naccuracy that is within 1.79% points (0.42% on average) compared to popular\nrandom mini-batching approaches."
                },
                "authors": [
                    {
                        "name": "Vignesh Balaji"
                    },
                    {
                        "name": "Christos Kozyrakis"
                    },
                    {
                        "name": "Gal Chechik"
                    },
                    {
                        "name": "Haggai Maron"
                    }
                ],
                "author_detail": {
                    "name": "Haggai Maron"
                },
                "author": "Haggai Maron",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.18082v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.18082v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2412.14335v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2412.14335v2",
                "updated": "2025-04-25T05:08:45Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    8,
                    45,
                    4,
                    115,
                    0
                ],
                "published": "2024-12-18T21:09:08Z",
                "published_parsed": [
                    2024,
                    12,
                    18,
                    21,
                    9,
                    8,
                    2,
                    353,
                    0
                ],
                "title": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Optimizing ML Concurrent Computation and Communication with GPU DMA\n  Engines"
                },
                "summary": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Concurrent computation and communication (C3) is a pervasive paradigm in ML\nand other domains, making its performance optimization crucial. In this paper,\nwe carefully characterize C3 in ML on GPUs, which are most widely deployed for\nML training and inference. We observe that while C3 leads to performance\nuplifts, the uplifts are far lower than ideal speedups (serial computation and\ncommunication versus maximum of computation or communication; all times from\nisolated executions). That is, C3 on average achieves only 21% of ideal\nspeedup. This is so, due to known challenges of compute and memory interference\nbetween concurrent GPU kernels (that is, sharing of GPU's compute units, caches\nand HBM).\n  To attain better performance for C3, first, we evaluate dual strategies of\nschedule prioritization and careful resource partitioning of compute units on\nGPUs to push performance attained with C3 (on average 42% of ideal speedup). We\nalso provide heuristics that can guide a runtime while employing these\nstrategies. To further enhance C3 performance, we propose to mitigate C3\ninterference by offloading communication tasks to the GPU's DMA engines. To\nthis end, we build concurrent communication collectives (ConCCL)\nproof-of-concepts that harness DMA engines for communication. We show how\nConCCL considerably closes the gap between realized and ideal speedup for C3\n(on average 72% of ideal speedup is realized, up to 1.67x speedup). Overall,\nour work makes a strong case for GPU DMA engine advancements to better support\nC3 on GPUs."
                },
                "authors": [
                    {
                        "name": "Anirudha Agrawal"
                    },
                    {
                        "name": "Shaizeen Aga"
                    },
                    {
                        "name": "Suchita Pati"
                    },
                    {
                        "name": "Mahzabeen Islam"
                    }
                ],
                "author_detail": {
                    "name": "Mahzabeen Islam"
                },
                "author": "Mahzabeen Islam",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2412.14335v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2412.14335v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.16620v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.16620v2",
                "updated": "2025-04-25T05:05:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    5,
                    5,
                    49,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-23T11:18:34Z",
                "published_parsed": [
                    2025,
                    4,
                    23,
                    11,
                    18,
                    34,
                    2,
                    113,
                    0
                ],
                "title": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Fluctuated lattice-driven charge density wave far above the condensation\n  temperature in kagome superconductor KV$_3$Sb$_5$"
                },
                "summary": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The kagome material AV$_3$Sb$_5$ exhibits multiple exotic orders, including\nan unconventional charge density wave (CDW). Elucidating the underlying\nmechanism behind the CDW transition is crucial for unraveling the complex\ninteractions among these phases. However, the driving force of the CDW remains\na topic of debate due to the intertwined interactions among the system's\nvarious excitations. Here we investigated the CDW transition in KV$_3$Sb$_5$ by\nisolating the ultrafast electronic phase transition using time- and\nangleresolved photoemission spectroscopy. An ultrafast electronic phase\ntransition was observed at a critical photoexcitation fluence, F$_c$, without\nreduction in CDW lattice-distortion-induced band folding. This folded band\npersisted up to 150 K under equilibrium heating, well above the CDW\ncondensation temperature of T$_c$ = 78 K. Notably, the pump-induced band shifts\nat F$_c$ were comparable to those caused by thermal effects at T$_c$. These\nfindings suggest that in KV$_3$Sb$_5$, a fluctuating lattice-driven in-plane\nCDW emerges above 150 K, with out-of-plane electronic correlations leading to\nthe $2\\times2 \\times 2$ CDW near T$_c$, offering key insights into the\ninterplay between the electronic and structural dynamics in AV$_3$Sb$_5$."
                },
                "authors": [
                    {
                        "name": "Haoran Liu"
                    },
                    {
                        "name": "Shaofeng Duan"
                    },
                    {
                        "name": "Xiangqi Liu"
                    },
                    {
                        "name": "Zhihua Liu"
                    },
                    {
                        "name": "Shichong Wang"
                    },
                    {
                        "name": "Lingxiao Gu"
                    },
                    {
                        "name": "Jiongyu Huang"
                    },
                    {
                        "name": "Wenxuan Yang"
                    },
                    {
                        "name": "Jianzhe Liu"
                    },
                    {
                        "name": "Dong Qian"
                    },
                    {
                        "name": "Yanfeng Guo"
                    },
                    {
                        "name": "Wentao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wentao Zhang"
                },
                "author": "Wentao Zhang",
                "arxiv_doi": "10.1016/j.scib.2025.02.018",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.scib.2025.02.018",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.16620v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.16620v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "21 pages, 4 figures",
                "arxiv_journal_ref": "Science Bulletin 70, 1211-1214 (2025)",
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.mtrl-sci",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.supr-con",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17995v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17995v1",
                "updated": "2025-04-25T00:41:43Z",
                "updated_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "published": "2025-04-25T00:41:43Z",
                "published_parsed": [
                    2025,
                    4,
                    25,
                    0,
                    41,
                    43,
                    4,
                    115,
                    0
                ],
                "title": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role of On-site and Inter-site Coulomb Interactions in KV$_3$Sb$_5$: A\n  first-principles DFT+$U$+$V$ study"
                },
                "summary": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nonlocal Coulomb interactions play a crucial role in stabilizing distinct\nelectronic phases in kagome materials. In this work, we systematically\ninvestigate the effects of on-site ($U$) and inter-site ($V$) Coulomb\ninteractions on the electronic structure and stability of charge-density-wave\n(CDW) phases in the kagome metal KV$_3$Sb$_5$ using density functional theory\n(DFT+$U$+$V$) calculations. We demonstrate that $V$ promotes the formation and\nstability of CDW phases, whereas $U$ suppresses these phases, highlighting a\nfundamental competition between local and nonlocal electronic correlations. By\ndirectly comparing our theoretical results with angle-resolved photoemission\nspectroscopy (ARPES) data, we identify realistic values of $U$ and $V$ that\naccurately describe the electronic band structure of KV$_3$Sb$_5$. Our findings\nestablish a detailed $U$-$V$ phase diagram for KV$_3$Sb$_5$, offering valuable\ninsights into the correlated electronic states in kagome metals and serving as\na foundation for future explorations of correlation-driven phenomena in related\nmaterials."
                },
                "authors": [
                    {
                        "name": "Indukuru Ramesh Reddy"
                    },
                    {
                        "name": "Sayandeep Ghosh"
                    },
                    {
                        "name": "Bongjae Kim"
                    },
                    {
                        "name": "Chang-Jong Kang"
                    }
                ],
                "author_detail": {
                    "name": "Chang-Jong Kang"
                },
                "author": "Chang-Jong Kang",
                "arxiv_comment": "8 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17995v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17995v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.str-el",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17866v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17866v1",
                "updated": "2025-04-24T18:09:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T18:09:25Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    18,
                    9,
                    25,
                    3,
                    114,
                    0
                ],
                "title": "Updated parameters of the LArQL model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Updated parameters of the LArQL model"
                },
                "summary": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The need for a microscopic description of scintillation light generation in\nliquid argon becomes increasingly desirable with the upcoming operation of\nlarge scale LArTPCs in the next decade. While a detailed mathematical account\nof the process is still to be achieved, a phenomenological model for\nsimultaneously treating ionization and scintillation, LArQL, has been\nsuccessfully employed to describe the range of electric fields from 0 to 0.75\nkV/cm and dE/dx from 2 to 40 MeV/cm providing the anti-correlation between the\nfree ionization charge and scintillation light. A reanalysis of the original\nmodel parameter values has been performed within a global fit procedure and is\npresented."
                },
                "authors": [
                    {
                        "name": "L. Paulucci"
                    },
                    {
                        "name": "F. Cavanna"
                    },
                    {
                        "name": "V. Vale"
                    },
                    {
                        "name": "F. Marinho"
                    }
                ],
                "author_detail": {
                    "name": "F. Marinho"
                },
                "author": "F. Marinho",
                "arxiv_comment": "Part of the proceedings of LIDINE 2024",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17866v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17866v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "hep-ex",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "hep-ex",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17584v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17584v1",
                "updated": "2025-04-24T14:14:07Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T14:14:07Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    14,
                    14,
                    7,
                    3,
                    114,
                    0
                ],
                "title": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "L3: DIMM-PIM Integrated Architecture and Coordination for Scalable\n  Long-Context LLM Inference"
                },
                "summary": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) increasingly require processing long text\nsequences, but GPU memory limitations force difficult trade-offs between memory\ncapacity and bandwidth. While HBM-based acceleration offers high bandwidth, its\ncapacity remains constrained. Offloading data to host-side DIMMs improves\ncapacity but introduces costly data swapping overhead. We identify that the\ncritical memory bottleneck lies in the decoding phase of multi-head attention\n(MHA) exclusively, which demands substantial capacity for storing KV caches and\nhigh bandwidth for attention computation. Our key insight reveals this\noperation uniquely aligns with modern DIMM-based processing-in-memory (PIM)\narchitectures, which offers scalability of both capacity and bandwidth.\n  Based on this observation and insight, we propose L3, a hardware-software\nco-designed system integrating DIMM-PIM and GPU devices. L3 introduces three\ninnovations: First, hardware redesigns resolve data layout mismatches and\ncomputational element mismatches in DIMM-PIM, enhancing LLM inference\nutilization. Second, communication optimization enables hiding the data\ntransfer overhead with the computation. Third, an adaptive scheduler\ncoordinates GPU-DIMM-PIM operations to maximize parallelism between devices.\nEvaluations using real-world traces show L3 achieves up to 6.1$\\times$ speedup\nover state-of-the-art HBM-PIM solutions while significantly improving batch\nsizes."
                },
                "authors": [
                    {
                        "name": "Qingyuan Liu"
                    },
                    {
                        "name": "Liyan Chen"
                    },
                    {
                        "name": "Yanning Yang"
                    },
                    {
                        "name": "Haocheng Wang"
                    },
                    {
                        "name": "Dong Du"
                    },
                    {
                        "name": "Zhigang Mao"
                    },
                    {
                        "name": "Naifeng Jing"
                    },
                    {
                        "name": "Yubin Xia"
                    },
                    {
                        "name": "Haibo Chen"
                    }
                ],
                "author_detail": {
                    "name": "Haibo Chen"
                },
                "author": "Haibo Chen",
                "arxiv_comment": "16 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17584v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17584v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17554v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17554v1",
                "updated": "2025-04-24T13:47:35Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-24T13:47:35Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    13,
                    47,
                    35,
                    3,
                    114,
                    0
                ],
                "title": "Rethinking PM Crash Consistency in the CXL Era",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking PM Crash Consistency in the CXL Era"
                },
                "summary": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Persistent Memory (PM) introduces new opportunities for designing\ncrash-consistent applications without the traditional storage overheads.\nHowever, ensuring crash consistency in PM demands intricate knowledge of CPU,\ncache, and memory interactions. Hardware and software mechanisms have been\nproposed to ease this burden, but neither proved sufficient, prompting a\nvariety of bug detection tools.\n  With the sunset of Intel Optane comes the rise of Compute Express Link (CXL)\nfor PM. In this position paper, we discuss the impact of CXL's disaggregated\nand heterogeneous nature in the development of crash-consistent PM\napplications, and outline three research directions: hardware primitives,\npersistency frameworks, and bug detection tools."
                },
                "authors": [
                    {
                        "name": "Joo Oliveira"
                    },
                    {
                        "name": "Joo Gonalves"
                    },
                    {
                        "name": "Miguel Matos"
                    }
                ],
                "author_detail": {
                    "name": "Miguel Matos"
                },
                "author": "Miguel Matos",
                "arxiv_comment": "5 pages (2 extra pages for references), 1 figure, 2 algorithms",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17554v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17554v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.08141v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.08141v3",
                "updated": "2025-04-24T08:39:13Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    8,
                    39,
                    13,
                    3,
                    114,
                    0
                ],
                "published": "2024-09-12T15:34:23Z",
                "published_parsed": [
                    2024,
                    9,
                    12,
                    15,
                    34,
                    23,
                    3,
                    256,
                    0
                ],
                "title": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Programmed I/O for Fast Devices, Cheap Cores, and Coherent\n  Interconnects"
                },
                "summary": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional wisdom holds that an efficient interface between an OS running\non a CPU and a high-bandwidth I/O device should use Direct Memory Access (DMA)\nto offload data transfer, descriptor rings for buffering and queuing, and\ninterrupts for asynchrony between cores and device.\n  In this paper we question this wisdom in the light of two trends: modern and\nemerging cache-coherent interconnects like CXL3.0, and workloads, particularly\nmicroservices and serverless computing. Like some others before us, we argue\nthat the assumptions of the DMA-based model are obsolete, and in many use-cases\nprogrammed I/O, where the CPU explicitly transfers data and control information\nto and from a device via loads and stores, delivers a more efficient system.\n  However, we push this idea much further. We show, in a real hardware\nimplementation, the gains in latency for fine-grained communication achievable\nusing an open cache-coherence protocol which exposes cache transitions to a\nsmart device, and that throughput is competitive with DMA over modern\ninterconnects. We also demonstrate three use-cases: fine-grained RPC-style\ninvocation of functions on an accelerator, offloading of operators in a\nstreaming dataflow engine, and a network interface targeting serverless\nfunctions, comparing our use of coherence with both traditional DMA-style\ninteraction and a highly-optimized implementation using memory-mapped\nprogrammed I/O over PCIe."
                },
                "authors": [
                    {
                        "name": "Anastasiia Ruzhanskaia"
                    },
                    {
                        "name": "Pengcheng Xu"
                    },
                    {
                        "name": "David Cock"
                    },
                    {
                        "name": "Timothy Roscoe"
                    }
                ],
                "author_detail": {
                    "name": "Timothy Roscoe"
                },
                "author": "Timothy Roscoe",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.08141v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.08141v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.OS",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.15192v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.15192v2",
                "updated": "2025-04-24T04:36:20Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    36,
                    20,
                    3,
                    114,
                    0
                ],
                "published": "2025-02-21T04:07:00Z",
                "published_parsed": [
                    2025,
                    2,
                    21,
                    4,
                    7,
                    0,
                    4,
                    52,
                    0
                ],
                "title": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SPAARC: Spatial Proximity and Association based prefetching for\n  Augmented Reality in edge Cache"
                },
                "summary": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mobile Augmented Reality (MAR) applications face performance challenges due\nto their high computational demands and need for low-latency responses.\nTraditional approaches like on-device storage or reactive data fetching from\nthe cloud often result in limited AR experiences or unacceptable lag. Edge\ncaching, which caches AR objects closer to the user, provides a promising\nsolution. However, existing edge caching approaches do not consider AR-specific\nfeatures such as AR object sizes, user interactions, and physical location.\nThis paper investigates how to further optimize edge caching by employing\nAR-aware prefetching techniques. We present SPAARC, a Spatial Proximity and\nAssociation-based Prefetching policy specifically designed for MAR Caches.\nSPAARC intelligently prioritizes the caching of virtual objects based on their\nassociation with other similar objects and the user's proximity to them. It\nalso considers the recency of associations and uses a lazy fetching strategy to\nefficiently manage edge resources and maximize Quality of Experience (QoE).\n  Through extensive evaluation using both synthetic and real-world workloads,\nwe demonstrate that SPAARC significantly improves cache hit rates compared to\nstandard caching algorithms, achieving gains ranging from 3% to 40% while\nreducing the need for on-demand data retrieval from the cloud. Further, we\npresent an adaptive tuning algorithm that automatically tunes SPAARC parameters\nto achieve optimal performance. Our findings demonstrate the potential of\nSPAARC to substantially enhance the user experience in MAR applications by\nensuring the timely availability of virtual objects."
                },
                "authors": [
                    {
                        "name": "Nikhil Sreekumar"
                    },
                    {
                        "name": "Abhishek Chandra"
                    },
                    {
                        "name": "Jon Weissman"
                    }
                ],
                "author_detail": {
                    "name": "Jon Weissman"
                },
                "author": "Jon Weissman",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.15192v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.15192v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.ET",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.ET",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.14992v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.14992v2",
                "updated": "2025-04-24T04:13:49Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    4,
                    13,
                    49,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-21T09:41:26Z",
                "published_parsed": [
                    2025,
                    4,
                    21,
                    9,
                    41,
                    26,
                    0,
                    111,
                    0
                ],
                "title": "Efficient Pretraining Length Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Pretraining Length Scaling"
                },
                "summary": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in large language models have demonstrated the effectiveness\nof length scaling during post-training, yet its potential in pre-training\nremains underexplored. We present the Parallel Hidden Decoding Transformer\n(\\textit{PHD}-Transformer), a novel framework that enables efficient length\nscaling during pre-training while maintaining inference efficiency.\n\\textit{PHD}-Transformer achieves this through an innovative KV cache\nmanagement strategy that distinguishes between original tokens and hidden\ndecoding tokens. By retaining only the KV cache of original tokens for\nlong-range dependencies while immediately discarding hidden decoding tokens\nafter use, our approach maintains the same KV cache size as the vanilla\ntransformer while enabling effective length scaling. To further enhance\nperformance, we introduce two optimized variants: \\textit{PHD-SWA} employs\nsliding window attention to preserve local dependencies, while\n\\textit{PHD-CSWA} implements chunk-wise sliding window attention to eliminate\nlinear growth in pre-filling time. Extensive experiments demonstrate consistent\nimprovements across multiple benchmarks."
                },
                "authors": [
                    {
                        "name": "Bohong Wu"
                    },
                    {
                        "name": "Shen Yan"
                    },
                    {
                        "name": "Sijun Zhang"
                    },
                    {
                        "name": "Jianqiao Lu"
                    },
                    {
                        "name": "Yutao Zeng"
                    },
                    {
                        "name": "Ya Wang"
                    },
                    {
                        "name": "Xun Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Xun Zhou"
                },
                "author": "Xun Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.14992v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.14992v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.02441v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.02441v2",
                "updated": "2025-04-24T01:47:25Z",
                "updated_parsed": [
                    2025,
                    4,
                    24,
                    1,
                    47,
                    25,
                    3,
                    114,
                    0
                ],
                "published": "2025-04-03T09:58:19Z",
                "published_parsed": [
                    2025,
                    4,
                    3,
                    9,
                    58,
                    19,
                    3,
                    93,
                    0
                ],
                "title": "Cognitive Memory in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cognitive Memory in Large Language Models"
                },
                "summary": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper examines memory mechanisms in Large Language Models (LLMs),\nemphasizing their importance for context-rich responses, reduced\nhallucinations, and improved efficiency. It categorizes memory into sensory,\nshort-term, and long-term, with sensory memory corresponding to input prompts,\nshort-term memory processing immediate context, and long-term memory\nimplemented via external databases or structures. The text-based memory section\ncovers acquisition (selection and summarization), management (updating,\naccessing, storing, and resolving conflicts), and utilization (full-text\nsearch, SQL queries, semantic search). The KV cache-based memory section\ndiscusses selection methods (regularity-based summarization, score-based\napproaches, special token embeddings) and compression techniques (low-rank\ncompression, KV merging, multimodal compression), along with management\nstrategies like offloading and shared attention mechanisms. Parameter-based\nmemory methods (LoRA, TTT, MoE) transform memories into model parameters to\nenhance efficiency, while hidden-state-based memory approaches (chunk\nmechanisms, recurrent transformers, Mamba model) improve long-text processing\nby combining RNN hidden states with current methods. Overall, the paper offers\na comprehensive analysis of LLM memory mechanisms, highlighting their\nsignificance and future research directions."
                },
                "authors": [
                    {
                        "name": "Lianlei Shan"
                    },
                    {
                        "name": "Shixian Luo"
                    },
                    {
                        "name": "Zezhou Zhu"
                    },
                    {
                        "name": "Yu Yuan"
                    },
                    {
                        "name": "Yong Wu"
                    }
                ],
                "author_detail": {
                    "name": "Yong Wu"
                },
                "author": "Yong Wu",
                "arxiv_comment": "37 pages, 9 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.02441v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.02441v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Inference",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.10566v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10566v1",
                "updated": "2025-05-15T17:59:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:59:51Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    51,
                    3,
                    135,
                    0
                ],
                "title": "3D-Fixup: Advancing Photo Editing with 3D Priors",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "3D-Fixup: Advancing Photo Editing with 3D Priors"
                },
                "summary": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Despite significant advances in modeling image priors via diffusion models,\n3D-aware image editing remains challenging, in part because the object is only\nspecified via a single image. To tackle this challenge, we propose 3D-Fixup, a\nnew framework for editing 2D images guided by learned 3D priors. The framework\nsupports difficult editing situations such as object translation and 3D\nrotation. To achieve this, we leverage a training-based approach that harnesses\nthe generative power of diffusion models. As video data naturally encodes\nreal-world physical dynamics, we turn to video data for generating training\ndata pairs, i.e., a source and a target frame. Rather than relying solely on a\nsingle trained model to infer transformations between source and target frames,\nwe incorporate 3D guidance from an Image-to-3D model, which bridges this\nchallenging task by explicitly projecting 2D information into 3D space. We\ndesign a data generation pipeline to ensure high-quality 3D guidance throughout\ntraining. Results show that by integrating these 3D priors, 3D-Fixup\neffectively supports complex, identity coherent 3D-aware edits, achieving\nhigh-quality results and advancing the application of diffusion models in\nrealistic image manipulation. The code is provided at\nhttps://3dfixup.github.io/"
                },
                "authors": [
                    {
                        "name": "Yen-Chi Cheng"
                    },
                    {
                        "name": "Krishna Kumar Singh"
                    },
                    {
                        "name": "Jae Shin Yoon"
                    },
                    {
                        "name": "Alex Schwing"
                    },
                    {
                        "name": "Liangyan Gui"
                    },
                    {
                        "name": "Matheus Gadelha"
                    },
                    {
                        "name": "Paul Guerrero"
                    },
                    {
                        "name": "Nanxuan Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Nanxuan Zhao"
                },
                "author": "Nanxuan Zhao",
                "arxiv_doi": "10.1145/3721238.3730695",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3721238.3730695",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.10566v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10566v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "SIGGRAPH 2025. Project page: https://3dfixup.github.io/",
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10559v1",
                "updated": "2025-05-15T17:59:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    22,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:59:22Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    22,
                    3,
                    135,
                    0
                ],
                "title": "Neural Thermodynamic Laws for Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Thermodynamic Laws for Large Language Model Training"
                },
                "summary": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yizhou Liu"
                    },
                    {
                        "name": "Jeff Gore"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10547v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10547v1",
                "updated": "2025-05-15T17:55:28Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    55,
                    28,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:55:28Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    55,
                    28,
                    3,
                    135,
                    0
                ],
                "title": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal\n  Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-Time Out-of-Distribution Failure Prevention via Multi-Modal\n  Reasoning"
                },
                "summary": "Foundation models can provide robust high-level reasoning on appropriate\nsafety interventions in hazardous scenarios beyond a robot's training data,\ni.e. out-of-distribution (OOD) failures. However, due to the high inference\nlatency of Large Vision and Language Models, current methods rely on manually\ndefined intervention policies to enact fallbacks, thereby lacking the ability\nto plan generalizable, semantically safe motions. To overcome these challenges\nwe present FORTRESS, a framework that generates and reasons about semantically\nsafe fallback strategies in real time to prevent OOD failures. At a low\nfrequency in nominal operations, FORTRESS uses multi-modal reasoners to\nidentify goals and anticipate failure modes. When a runtime monitor triggers a\nfallback response, FORTRESS rapidly synthesizes plans to fallback goals while\ninferring and avoiding semantically unsafe regions in real time. By bridging\nopen-world, multi-modal reasoning with dynamics-aware planning, we eliminate\nthe need for hard-coded fallbacks and human safety interventions. FORTRESS\noutperforms on-the-fly prompting of slow reasoning models in safety\nclassification accuracy on synthetic benchmarks and real-world ANYmal robot\ndata, and further improves system safety and planning success in simulation and\non quadrotor hardware for urban navigation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Foundation models can provide robust high-level reasoning on appropriate\nsafety interventions in hazardous scenarios beyond a robot's training data,\ni.e. out-of-distribution (OOD) failures. However, due to the high inference\nlatency of Large Vision and Language Models, current methods rely on manually\ndefined intervention policies to enact fallbacks, thereby lacking the ability\nto plan generalizable, semantically safe motions. To overcome these challenges\nwe present FORTRESS, a framework that generates and reasons about semantically\nsafe fallback strategies in real time to prevent OOD failures. At a low\nfrequency in nominal operations, FORTRESS uses multi-modal reasoners to\nidentify goals and anticipate failure modes. When a runtime monitor triggers a\nfallback response, FORTRESS rapidly synthesizes plans to fallback goals while\ninferring and avoiding semantically unsafe regions in real time. By bridging\nopen-world, multi-modal reasoning with dynamics-aware planning, we eliminate\nthe need for hard-coded fallbacks and human safety interventions. FORTRESS\noutperforms on-the-fly prompting of slow reasoning models in safety\nclassification accuracy on synthetic benchmarks and real-world ANYmal robot\ndata, and further improves system safety and planning success in simulation and\non quadrotor hardware for urban navigation."
                },
                "authors": [
                    {
                        "name": "Milan Ganai"
                    },
                    {
                        "name": "Rohan Sinha"
                    },
                    {
                        "name": "Christopher Agia"
                    },
                    {
                        "name": "Daniel Morton"
                    },
                    {
                        "name": "Marco Pavone"
                    }
                ],
                "author_detail": {
                    "name": "Marco Pavone"
                },
                "author": "Marco Pavone",
                "arxiv_comment": "Website: https://milanganai.github.io/fortress/",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10547v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10547v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04689v3",
                "updated": "2025-05-15T17:52:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    52,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-07T06:30:33Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    30,
                    33,
                    4,
                    38,
                    0
                ],
                "title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities on\ncomplex evaluation benchmarks, many of which are formulated as\nquestion-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts\nis becoming increasingly vital for advancing their development and\napplicability. This paper introduces ARR, an intuitive, effective, and general\nQA solving method that explicitly incorporates three key steps: analyzing the\nintent of the question, retrieving relevant information, and reasoning step by\nstep. Notably, this paper is the first to introduce intent analysis in QA,\nwhich plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA\ntasks demonstrate that ARR consistently outperforms the baseline methods.\nAblation and case studies further validate the positive contributions of each\nARR component. Furthermore, experiments involving variations in prompt design\nindicate that ARR maintains its effectiveness regardless of the specific prompt\nformulation. Additionally, extensive evaluations across various model sizes,\nLLM series, and generation settings solidify the effectiveness, robustness, and\ngeneralizability of ARR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities on\ncomplex evaluation benchmarks, many of which are formulated as\nquestion-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts\nis becoming increasingly vital for advancing their development and\napplicability. This paper introduces ARR, an intuitive, effective, and general\nQA solving method that explicitly incorporates three key steps: analyzing the\nintent of the question, retrieving relevant information, and reasoning step by\nstep. Notably, this paper is the first to introduce intent analysis in QA,\nwhich plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA\ntasks demonstrate that ARR consistently outperforms the baseline methods.\nAblation and case studies further validate the positive contributions of each\nARR component. Furthermore, experiments involving variations in prompt design\nindicate that ARR maintains its effectiveness regardless of the specific prompt\nformulation. Additionally, extensive evaluations across various model sizes,\nLLM series, and generation settings solidify the effectiveness, robustness, and\ngeneralizability of ARR."
                },
                "authors": [
                    {
                        "name": "Yuwei Yin"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Carenini"
                },
                "author": "Giuseppe Carenini",
                "arxiv_comment": "21 pages. Code: https://github.com/YuweiYin/ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.20312v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.20312v2",
                "updated": "2025-05-15T17:50:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    50,
                    39,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-27T17:38:38Z",
                "published_parsed": [
                    2025,
                    2,
                    27,
                    17,
                    38,
                    38,
                    3,
                    58,
                    0
                ],
                "title": "Inferring Black Hole Spin from Interferometric Measurements of the First\n  Photon Ring: A Geometric Approach",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Black Hole Spin from Interferometric Measurements of the First\n  Photon Ring: A Geometric Approach"
                },
                "summary": "Accurately inferring black hole spin is crucial for understanding black hole\ndynamics and their astrophysical environments. In this work, we outline a\ngeometric method for spin estimation by using the interferometric shape of the\nfirst photon ring ($n=1$) as an approximation to the critical curve, which,\ngiven an assumed value of the black hole inclination, is then mapped to a spin\nvalue. While future space-based missions will capture a wealth of data on the\nfirst photon ring--including the full angle-dependent diameter, angular\nbrightness profile, and astrometric offset from the $n=0$ ring--our analysis is\nrestricted to using only two angle-dependent diameters to compute its shape\nasymmetry and infer spin. Focusing on low inclinations and moderate-to-high\nspins, we test the method across various emission models, baselines, and noise\nsources, including a mock space-based observation. Although the size of the\n$n=1$ ring depends on the emission model, its interferometric shape remains a\nrobust spin probe at low inclinations. We find that the inferred asymmetry of\nthe $n=1$ image may be biased by the critical curve morphology, and it can be\nheavily skewed by the presence of noise, whether astrophysical or instrumental.\nIn low-noise limits at low viewing inclination, significant contributions from\nthe $n=0$ image at short baselines may lead to a downward bias in asymmetry\nestimates. While our method can estimate high spins in noise-free time-averaged\nimages, increasing the noise and astrophysical variability degrades the\nresulting constraints, providing only lower bounds on the spin when applied to\nsynthetic observed data. Remarkably, even using only the ring's asymmetry, we\ncan establish lower bounds on the spin, underscoring the promise of photon\nring-based spin inference in future space-based very long baseline\ninterferometry missions, such as the proposed Black Hole Explorer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately inferring black hole spin is crucial for understanding black hole\ndynamics and their astrophysical environments. In this work, we outline a\ngeometric method for spin estimation by using the interferometric shape of the\nfirst photon ring ($n=1$) as an approximation to the critical curve, which,\ngiven an assumed value of the black hole inclination, is then mapped to a spin\nvalue. While future space-based missions will capture a wealth of data on the\nfirst photon ring--including the full angle-dependent diameter, angular\nbrightness profile, and astrometric offset from the $n=0$ ring--our analysis is\nrestricted to using only two angle-dependent diameters to compute its shape\nasymmetry and infer spin. Focusing on low inclinations and moderate-to-high\nspins, we test the method across various emission models, baselines, and noise\nsources, including a mock space-based observation. Although the size of the\n$n=1$ ring depends on the emission model, its interferometric shape remains a\nrobust spin probe at low inclinations. We find that the inferred asymmetry of\nthe $n=1$ image may be biased by the critical curve morphology, and it can be\nheavily skewed by the presence of noise, whether astrophysical or instrumental.\nIn low-noise limits at low viewing inclination, significant contributions from\nthe $n=0$ image at short baselines may lead to a downward bias in asymmetry\nestimates. While our method can estimate high spins in noise-free time-averaged\nimages, increasing the noise and astrophysical variability degrades the\nresulting constraints, providing only lower bounds on the spin when applied to\nsynthetic observed data. Remarkably, even using only the ring's asymmetry, we\ncan establish lower bounds on the spin, underscoring the promise of photon\nring-based spin inference in future space-based very long baseline\ninterferometry missions, such as the proposed Black Hole Explorer."
                },
                "authors": [
                    {
                        "name": "Lennox S. Keeble"
                    },
                    {
                        "name": "Alejandro Crdenas-Avendao"
                    },
                    {
                        "name": "Daniel C. M. Palumbo"
                    }
                ],
                "author_detail": {
                    "name": "Daniel C. M. Palumbo"
                },
                "author": "Daniel C. M. Palumbo",
                "arxiv_comment": "17+1 pages, 12 figures; V2: minor changes to the introduction, Sec.\n  IV and Figs. 1 and 11",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.20312v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.20312v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10539v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10539v1",
                "updated": "2025-05-15T17:48:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    48,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:48:51Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    48,
                    51,
                    3,
                    135,
                    0
                ],
                "title": "A Systematic Search for Trace Molecules in Exoplanet K2-18 b",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Systematic Search for Trace Molecules in Exoplanet K2-18 b"
                },
                "summary": "The first transmission spectrum of the habitable-zone sub-Neptune K2-18 b\nwith JWST has opened a new avenue for atmospheric characterisation of temperate\nlow-mass exoplanets. The observations led to inferences of methane and carbon\ndioxide, as well as of dimethyl sulfide (DMS) and/or dimethyl disulfide (DMDS),\nboth potential biosignatures. However, robust identification of DMS and/or DMDS\nrequires further observations to increase the detection significances. More\ntheoretical studies are also needed to identify potential false positives and\npossible abiotic sources for these molecules. In the present work we\ndemonstrate the next step in this direction with a comprehensive and agnostic\nsearch for other chemical species in the atmosphere of K2-18 b. Our exploration\nincludes 650 molecules, spanning a wide range of trace gases, including biotic,\nabiotic, and anthropogenic gases on Earth. We investigate possible evidence for\nany of these gases using three metrics: (a) evidence in the JWST mid-infrared\nspectrum, (b) evidence in the JWST near-infrared spectrum, and (c) plausible\nsources of production. We find three molecules, including DMS, which appear\npromising across the datasets considered. The two molecules besides DMS are\ndiethyl sulfide and methyl acrylonitrile, which are more complex than DMS,\nbiogenic on Earth, and have no significant sources known beyond Earth. A few\nother gases also provide comparable fits to a subset of the data considered but\nagain with limited known plausible sources. Our study highlights the need for\nfurther observations to distinguish between possible trace gases in K2-18 b and\ntheoretical work to establish their plausible sources if confirmed on this\nplanet.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The first transmission spectrum of the habitable-zone sub-Neptune K2-18 b\nwith JWST has opened a new avenue for atmospheric characterisation of temperate\nlow-mass exoplanets. The observations led to inferences of methane and carbon\ndioxide, as well as of dimethyl sulfide (DMS) and/or dimethyl disulfide (DMDS),\nboth potential biosignatures. However, robust identification of DMS and/or DMDS\nrequires further observations to increase the detection significances. More\ntheoretical studies are also needed to identify potential false positives and\npossible abiotic sources for these molecules. In the present work we\ndemonstrate the next step in this direction with a comprehensive and agnostic\nsearch for other chemical species in the atmosphere of K2-18 b. Our exploration\nincludes 650 molecules, spanning a wide range of trace gases, including biotic,\nabiotic, and anthropogenic gases on Earth. We investigate possible evidence for\nany of these gases using three metrics: (a) evidence in the JWST mid-infrared\nspectrum, (b) evidence in the JWST near-infrared spectrum, and (c) plausible\nsources of production. We find three molecules, including DMS, which appear\npromising across the datasets considered. The two molecules besides DMS are\ndiethyl sulfide and methyl acrylonitrile, which are more complex than DMS,\nbiogenic on Earth, and have no significant sources known beyond Earth. A few\nother gases also provide comparable fits to a subset of the data considered but\nagain with limited known plausible sources. Our study highlights the need for\nfurther observations to distinguish between possible trace gases in K2-18 b and\ntheoretical work to establish their plausible sources if confirmed on this\nplanet."
                },
                "authors": [
                    {
                        "name": "Lorenzo Pica-Ciamarra"
                    },
                    {
                        "name": "Nikku Madhusudhan"
                    },
                    {
                        "name": "Gregory J. Cooke"
                    },
                    {
                        "name": "Savvas Constantinou"
                    },
                    {
                        "name": "Martin Binet"
                    }
                ],
                "author_detail": {
                    "name": "Martin Binet"
                },
                "author": "Martin Binet",
                "arxiv_comment": "Submitted for publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10539v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10539v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.EP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.EP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08493v2",
                "updated": "2025-05-15T17:38:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    38,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-13T12:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    23,
                    11,
                    1,
                    133,
                    0
                ],
                "title": "BizChat: Scaffolding AI-Powered Business Planning for Small Business\n  Owners Across Digital Skill Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BizChat: Scaffolding AI-Powered Business Planning for Small Business\n  Owners Across Digital Skill Levels"
                },
                "summary": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment."
                },
                "authors": [
                    {
                        "name": "Quentin Romero Lauro"
                    },
                    {
                        "name": "Aakash Gautam"
                    },
                    {
                        "name": "Yasmine Kotturi"
                    }
                ],
                "author_detail": {
                    "name": "Yasmine Kotturi"
                },
                "author": "Yasmine Kotturi",
                "arxiv_doi": "10.1145/3707640.3731928",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3707640.3731928",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.08493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages, 1 figure, CHIWORK '25 Adjunct, June 23-25, 2025, Amsterdam,\n  Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10526v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10526v1",
                "updated": "2025-05-15T17:37:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    37,
                    0,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:37:00Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    37,
                    0,
                    3,
                    135,
                    0
                ],
                "title": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative\n  Decoding of Vision-Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASSV: Multimodal Adaptation and Self-Data Distillation for Speculative\n  Decoding of Vision-Language Models"
                },
                "summary": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative decoding significantly accelerates language model inference by\nenabling a lightweight draft model to propose multiple tokens that a larger\ntarget model verifies simultaneously. However, applying this technique to\nvision-language models (VLMs) presents two fundamental challenges: small\nlanguage models that could serve as efficient drafters lack the architectural\ncomponents to process visual inputs, and their token predictions fail to match\nthose of VLM target models that consider visual context. We introduce\nMultimodal Adaptation and Self-Data Distillation for Speculative Decoding of\nVision-Language Models (MASSV), which transforms existing small language models\ninto effective multimodal drafters through a two-phase approach. MASSV first\nconnects the target VLM's vision encoder to the draft model via a lightweight\ntrainable projector, then applies self-distilled visual instruction tuning\nusing responses generated by the target VLM to align token predictions.\nComprehensive experiments across the Qwen2.5-VL and Gemma3 model families\ndemonstrate that MASSV increases accepted length by up to 30% and delivers\nend-to-end inference speedups of up to 1.46x on visually-grounded tasks. MASSV\nprovides a scalable, architecture-compatible method for accelerating both\ncurrent and future VLMs."
                },
                "authors": [
                    {
                        "name": "Mugilan Ganesan"
                    },
                    {
                        "name": "Shane Segal"
                    },
                    {
                        "name": "Ankur Aggarwal"
                    },
                    {
                        "name": "Nish Sinnadurai"
                    },
                    {
                        "name": "Sean Lie"
                    },
                    {
                        "name": "Vithursan Thangarasa"
                    }
                ],
                "author_detail": {
                    "name": "Vithursan Thangarasa"
                },
                "author": "Vithursan Thangarasa",
                "arxiv_comment": "Main paper: 11 pp., 4 figs., 3 tabs.; Supplementary: 2 pp",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10526v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10526v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v4",
                "updated": "2025-05-15T17:18:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    18,
                    12,
                    3,
                    135,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10510v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10510v1",
                "updated": "2025-05-15T17:14:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    14,
                    21,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:14:21Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    14,
                    21,
                    3,
                    135,
                    0
                ],
                "title": "Efficient Uncertainty Propagation in Bayesian Two-Step Procedures",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Uncertainty Propagation in Bayesian Two-Step Procedures"
                },
                "summary": "Bayesian inference provides a principled framework for probabilistic\nreasoning. If inference is performed in two steps, uncertainty propagation\nplays a crucial role in accounting for all sources of uncertainty and\nvariability. This becomes particularly important when both aleatoric\nuncertainty, caused by data variability, and epistemic uncertainty, arising\nfrom incomplete knowledge or missing data, are present. Examples include\nsurrogate models and missing data problems. In surrogate modeling, the\nsurrogate is used as a simplified approximation of a resource-heavy and costly\nsimulation. The uncertainty from the surrogate-fitting process can be\npropagated using a two-step procedure. For modeling with missing data, methods\nlike Multivariate Imputation by Chained Equations (MICE) generate multiple\ndatasets to account for imputation uncertainty. These approaches, however, are\ncomputationally expensive, as multiple models must be fitted separately to\nsurrogate parameters respectively imputed datasets.\n  To address these challenges, we propose an efficient two-step approach that\nreduces computational overhead while maintaining accuracy. By selecting a\nrepresentative subset of draws or imputations, we construct a mixture\ndistribution to approximate the desired posteriors using Pareto smoothed\nimportance sampling. For more complex scenarios, this is further refined with\nimportance weighted moment matching and an iterative procedure that broadens\nthe mixture distribution to better capture diverse posterior distributions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference provides a principled framework for probabilistic\nreasoning. If inference is performed in two steps, uncertainty propagation\nplays a crucial role in accounting for all sources of uncertainty and\nvariability. This becomes particularly important when both aleatoric\nuncertainty, caused by data variability, and epistemic uncertainty, arising\nfrom incomplete knowledge or missing data, are present. Examples include\nsurrogate models and missing data problems. In surrogate modeling, the\nsurrogate is used as a simplified approximation of a resource-heavy and costly\nsimulation. The uncertainty from the surrogate-fitting process can be\npropagated using a two-step procedure. For modeling with missing data, methods\nlike Multivariate Imputation by Chained Equations (MICE) generate multiple\ndatasets to account for imputation uncertainty. These approaches, however, are\ncomputationally expensive, as multiple models must be fitted separately to\nsurrogate parameters respectively imputed datasets.\n  To address these challenges, we propose an efficient two-step approach that\nreduces computational overhead while maintaining accuracy. By selecting a\nrepresentative subset of draws or imputations, we construct a mixture\ndistribution to approximate the desired posteriors using Pareto smoothed\nimportance sampling. For more complex scenarios, this is further refined with\nimportance weighted moment matching and an iterative procedure that broadens\nthe mixture distribution to better capture diverse posterior distributions."
                },
                "authors": [
                    {
                        "name": "Svenja Jedhoff"
                    },
                    {
                        "name": "Hadi Kutabi"
                    },
                    {
                        "name": "Anne Meyer"
                    },
                    {
                        "name": "Paul-Christian Brkner"
                    }
                ],
                "author_detail": {
                    "name": "Paul-Christian Brkner"
                },
                "author": "Paul-Christian Brkner",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10510v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10510v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13957v2",
                "updated": "2025-05-15T17:09:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    9,
                    21,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-21T04:05:45Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    5,
                    45,
                    1,
                    21,
                    0
                ],
                "title": "Benchmarking Generative AI for Scoring Medical Student Interviews in\n  Objective Structured Clinical Examinations (OSCEs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Generative AI for Scoring Medical Student Interviews in\n  Objective Structured Clinical Examinations (OSCEs)"
                },
                "summary": "Objective Structured Clinical Examinations (OSCEs) are widely used to assess\nmedical students' communication skills, but scoring interview-based assessments\nis time-consuming and potentially subject to human bias. This study explored\nthe potential of large language models (LLMs) to automate OSCE evaluations\nusing the Master Interview Rating Scale (MIRS). We compared the performance of\nfour state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)\nin evaluating OSCE transcripts across all 28 items of the MIRS under the\nconditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step\nprompting. The models were benchmarked against a dataset of 10 OSCE cases with\n174 expert consensus scores available. Model performance was measured using\nthree accuracy metrics (exact, off-by-one, thresholded). Averaging across all\nMIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to\n0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded\naccuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater\nreliability ({\\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step\ntechniques proved valuable when tailored to specific assessment items. The\nperformance was consistent across MIRS items, independent of encounter phases\nand communication domains. We demonstrated the feasibility of AI-assisted OSCE\nevaluation and provided benchmarking of multiple LLMs across multiple prompt\ntechniques. Our work provides a baseline performance assessment for LLMs that\nlays a foundation for future research into automated assessment of clinical\ncommunication skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective Structured Clinical Examinations (OSCEs) are widely used to assess\nmedical students' communication skills, but scoring interview-based assessments\nis time-consuming and potentially subject to human bias. This study explored\nthe potential of large language models (LLMs) to automate OSCE evaluations\nusing the Master Interview Rating Scale (MIRS). We compared the performance of\nfour state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)\nin evaluating OSCE transcripts across all 28 items of the MIRS under the\nconditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step\nprompting. The models were benchmarked against a dataset of 10 OSCE cases with\n174 expert consensus scores available. Model performance was measured using\nthree accuracy metrics (exact, off-by-one, thresholded). Averaging across all\nMIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to\n0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded\naccuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater\nreliability ({\\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step\ntechniques proved valuable when tailored to specific assessment items. The\nperformance was consistent across MIRS items, independent of encounter phases\nand communication domains. We demonstrated the feasibility of AI-assisted OSCE\nevaluation and provided benchmarking of multiple LLMs across multiple prompt\ntechniques. Our work provides a baseline performance assessment for LLMs that\nlays a foundation for future research into automated assessment of clinical\ncommunication skills."
                },
                "authors": [
                    {
                        "name": "Jadon Geathers"
                    },
                    {
                        "name": "Yann Hicke"
                    },
                    {
                        "name": "Colleen Chan"
                    },
                    {
                        "name": "Niroop Rajashekar"
                    },
                    {
                        "name": "Justin Sewell"
                    },
                    {
                        "name": "Susannah Cornes"
                    },
                    {
                        "name": "Rene F. Kizilcec"
                    },
                    {
                        "name": "Dennis Shung"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Shung"
                },
                "author": "Dennis Shung",
                "arxiv_comment": "12 pages + 3 pages of references, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13504v3",
                "updated": "2025-05-15T17:05:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    5,
                    43,
                    3,
                    135,
                    0
                ],
                "published": "2024-11-20T17:55:38Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    55,
                    38,
                    2,
                    325,
                    0
                ],
                "title": "Disentangling Memory and Reasoning Ability in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Memory and Reasoning Ability in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Ruixiang Tang"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10495v1",
                "updated": "2025-05-15T16:53:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:53:45Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    45,
                    3,
                    135,
                    0
                ],
                "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating\n  Synthetic Training Data for Function Calling LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RouteNator: A Router-Based Multi-Modal Architecture for Generating\n  Synthetic Training Data for Function Calling LLMs"
                },
                "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks."
                },
                "authors": [
                    {
                        "name": "Vibha Belavadi"
                    },
                    {
                        "name": "Tushar Vatsa"
                    },
                    {
                        "name": "Dewang Sultania"
                    },
                    {
                        "name": "Suhas Suresha"
                    },
                    {
                        "name": "Ishita Verma"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Tracy Holloway King"
                    },
                    {
                        "name": "Michael Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Friedrich"
                },
                "author": "Michael Friedrich",
                "arxiv_comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing",
                "arxiv_journal_ref": "https://aclanthology.org/2025.knowledgenlp-1.10/ KnowledgeNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10494v1",
                "updated": "2025-05-15T16:53:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    41,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:53:41Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    41,
                    3,
                    135,
                    0
                ],
                "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models\n  from a Code Security Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Really Trust Code Copilots? Evaluating Large Language Models\n  from a Code Security Perspective"
                },
                "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security."
                },
                "authors": [
                    {
                        "name": "Yutao Mou"
                    },
                    {
                        "name": "Xiao Deng"
                    },
                    {
                        "name": "Yuxiao Luo"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "arxiv_comment": "Accepted by ACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10493v1",
                "updated": "2025-05-15T16:53:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    4,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:53:04Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    4,
                    3,
                    135,
                    0
                ],
                "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with\n  Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with\n  Curriculum Learning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods."
                },
                "authors": [
                    {
                        "name": "Shaohan Wang"
                    },
                    {
                        "name": "Licheng Zhang"
                    },
                    {
                        "name": "Zheren Fu"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10490v1",
                "updated": "2025-05-15T16:45:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    45,
                    33,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:45:33Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    45,
                    33,
                    3,
                    135,
                    0
                ],
                "title": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM\n  As-A-Service Customizations Shape Trust and Usage Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM\n  As-A-Service Customizations Shape Trust and Usage Patterns"
                },
                "summary": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment."
                },
                "authors": [
                    {
                        "name": "Leon Hannig"
                    },
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Meltem Aksoy"
                    },
                    {
                        "name": "Steffen Becker"
                    },
                    {
                        "name": "Greta Ontrup"
                    }
                ],
                "author_detail": {
                    "name": "Greta Ontrup"
                },
                "author": "Greta Ontrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21909v2",
                "updated": "2025-05-15T16:40:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    39,
                    3,
                    135,
                    0
                ],
                "published": "2024-10-29T10:01:40Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    1,
                    40,
                    1,
                    303,
                    0
                ],
                "title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent"
                },
                "summary": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent ."
                },
                "authors": [
                    {
                        "name": "Xiao Xia"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Zibo Liao"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Tianrui Sun"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ling Fu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12293v2",
                "updated": "2025-05-15T16:29:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    29,
                    38,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-15T23:20:26Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    23,
                    20,
                    26,
                    5,
                    74,
                    0
                ],
                "title": "Unified Modeling Language Code Generation from Diagram Images Using\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Modeling Language Code Generation from Diagram Images Using\n  Multimodal Large Language Models"
                },
                "summary": "The Unified Modeling Language is a standardized visual language widely used\nfor modeling and documenting the design of software systems. Although many\ntools generate UML diagrams from UML code, generating executable UML code from\nimage-based UML diagrams remains challenging. This paper proposes a new\napproach to generate UML code using a large multimodal language model\nautomatically. Synthetic UML activity and sequence diagram datasets were\ncreated to train and test the model. We compared standard fine-tuning with LoRA\ntechniques to optimize base models. The experiments measured code generation\naccuracy across different model sizes and training strategies. These results\ndemonstrated that domain-adapted MM-LLMs perform for UML code generation\nautomation, whereby, at the best model, it achieved BLEU and SSIM scores of\n0.779 and 0.942 on sequence diagrams. This will enable the modernization of\nlegacy systems and decrease the manual effort in software development\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unified Modeling Language is a standardized visual language widely used\nfor modeling and documenting the design of software systems. Although many\ntools generate UML diagrams from UML code, generating executable UML code from\nimage-based UML diagrams remains challenging. This paper proposes a new\napproach to generate UML code using a large multimodal language model\nautomatically. Synthetic UML activity and sequence diagram datasets were\ncreated to train and test the model. We compared standard fine-tuning with LoRA\ntechniques to optimize base models. The experiments measured code generation\naccuracy across different model sizes and training strategies. These results\ndemonstrated that domain-adapted MM-LLMs perform for UML code generation\nautomation, whereby, at the best model, it achieved BLEU and SSIM scores of\n0.779 and 0.942 on sequence diagrams. This will enable the modernization of\nlegacy systems and decrease the manual effort in software development\nworkflows."
                },
                "authors": [
                    {
                        "name": "Averi Bates"
                    },
                    {
                        "name": "Ryan Vavricka"
                    },
                    {
                        "name": "Shane Carleton"
                    },
                    {
                        "name": "Ruosi Shao"
                    },
                    {
                        "name": "Chongle Pan"
                    }
                ],
                "author_detail": {
                    "name": "Chongle Pan"
                },
                "author": "Chongle Pan",
                "arxiv_doi": "10.1016/j.mlwa.2025.100660",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.mlwa.2025.100660",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.12293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Journal of Machine Learning with Applications,\n  Author Contributions: Averi Bates: Methodology, Development, Analysis, Data\n  Curation, Drafting, Review. Ryan Vavricka: Data Curation, Development,\n  Review. Shane Carleton: Supervision, Funding. Ruosi Shao: Review. Chongle\n  Pan: Supervision, Review",
                "arxiv_journal_ref": "Mach. Learn. Appl. 20 (2025) 100660",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2; D.2.3; I.2.7; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10475v1",
                "updated": "2025-05-15T16:24:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    24,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:24:45Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    24,
                    45,
                    3,
                    135,
                    0
                ],
                "title": "Parallel Scaling Law for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Scaling Law for Language Models"
                },
                "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning."
                },
                "authors": [
                    {
                        "name": "Mouxiang Chen"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jianling Sun"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10472v1",
                "updated": "2025-05-15T16:23:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    23,
                    21,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:23:21Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    23,
                    21,
                    3,
                    135,
                    0
                ],
                "title": "Large Language Models for Cancer Communication: Evaluating Linguistic\n  Quality, Safety, and Accessibility in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cancer Communication: Evaluating Linguistic\n  Quality, Safety, and Accessibility in Generative AI"
                },
                "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools."
                },
                "authors": [
                    {
                        "name": "Agnik Saha"
                    },
                    {
                        "name": "Victoria Churchill"
                    },
                    {
                        "name": "Anny D. Rodriguez"
                    },
                    {
                        "name": "Ugur Kursuncu"
                    },
                    {
                        "name": "Muhammed Y. Idris"
                    }
                ],
                "author_detail": {
                    "name": "Muhammed Y. Idris"
                },
                "author": "Muhammed Y. Idris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10468v1",
                "updated": "2025-05-15T16:21:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    21,
                    33,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:21:33Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    21,
                    33,
                    3,
                    135,
                    0
                ],
                "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge"
                },
                "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications"
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Konstantinos I. Roumeliotis"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "arxiv_comment": "32 pages, 14 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10466v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10466v1",
                "updated": "2025-05-15T16:20:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    20,
                    36,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:20:36Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    20,
                    36,
                    3,
                    135,
                    0
                ],
                "title": "FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant\n  Tempering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FlowVAT: Normalizing Flow Variational Inference with Affine-Invariant\n  Tempering"
                },
                "summary": "Multi-modal and high-dimensional posteriors present significant challenges\nfor variational inference, causing mode-seeking behavior and collapse despite\nthe theoretical expressiveness of normalizing flows. Traditional annealing\nmethods require temperature schedules and hyperparameter tuning, falling short\nof the goal of truly black-box variational inference. We introduce FlowVAT, a\nconditional tempering approach for normalizing flow variational inference that\naddresses these limitations. Our method tempers both the base and target\ndistributions simultaneously, maintaining affine-invariance under tempering. By\nconditioning the normalizing flow on temperature, we leverage overparameterized\nneural networks' generalization capabilities to train a single flow\nrepresenting the posterior across a range of temperatures. This preserves modes\nidentified at higher temperatures when sampling from the variational posterior\nat $T = 1$, mitigating standard variational methods' mode-seeking behavior. In\nexperiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT\noutperforms traditional and adaptive annealing methods, finding more modes and\nachieving better ELBO values, particularly in higher dimensions where existing\napproaches fail. Our method requires minimal hyperparameter tuning and does not\nrequire an annealing schedule, advancing toward fully-automatic black-box\nvariational inference for complicated posteriors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-modal and high-dimensional posteriors present significant challenges\nfor variational inference, causing mode-seeking behavior and collapse despite\nthe theoretical expressiveness of normalizing flows. Traditional annealing\nmethods require temperature schedules and hyperparameter tuning, falling short\nof the goal of truly black-box variational inference. We introduce FlowVAT, a\nconditional tempering approach for normalizing flow variational inference that\naddresses these limitations. Our method tempers both the base and target\ndistributions simultaneously, maintaining affine-invariance under tempering. By\nconditioning the normalizing flow on temperature, we leverage overparameterized\nneural networks' generalization capabilities to train a single flow\nrepresenting the posterior across a range of temperatures. This preserves modes\nidentified at higher temperatures when sampling from the variational posterior\nat $T = 1$, mitigating standard variational methods' mode-seeking behavior. In\nexperiments with 2, 10, and 20 dimensional multi-modal distributions, FlowVAT\noutperforms traditional and adaptive annealing methods, finding more modes and\nachieving better ELBO values, particularly in higher dimensions where existing\napproaches fail. Our method requires minimal hyperparameter tuning and does not\nrequire an annealing schedule, advancing toward fully-automatic black-box\nvariational inference for complicated posteriors."
                },
                "authors": [
                    {
                        "name": "Juehang Qin"
                    },
                    {
                        "name": "Shixiao Liang"
                    },
                    {
                        "name": "Christopher Tunnell"
                    }
                ],
                "author_detail": {
                    "name": "Christopher Tunnell"
                },
                "author": "Christopher Tunnell",
                "arxiv_comment": "10 pages, 5 figures, and 2 tables in main text, two appendices",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10466v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10466v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10465v1",
                "updated": "2025-05-15T16:18:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    18,
                    13,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:18:13Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    18,
                    13,
                    3,
                    135,
                    0
                ],
                "title": "Superposition Yields Robust Neural Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposition Yields Robust Neural Scaling"
                },
                "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters."
                },
                "authors": [
                    {
                        "name": "Yizhou liu"
                    },
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Jeff Gore"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Gore"
                },
                "author": "Jeff Gore",
                "arxiv_comment": "30 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18036v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18036v4",
                "updated": "2025-05-15T16:08:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    8,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-25T09:48:53Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    48,
                    53,
                    1,
                    56,
                    0
                ],
                "title": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble"
                },
                "summary": "LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble."
                },
                "authors": [
                    {
                        "name": "Zhijun Chen"
                    },
                    {
                        "name": "Jingzheng Li"
                    },
                    {
                        "name": "Pengpeng Chen"
                    },
                    {
                        "name": "Zhuoran Li"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Yuankai Luo"
                    },
                    {
                        "name": "Qianren Mao"
                    },
                    {
                        "name": "Dingqi Yang"
                    },
                    {
                        "name": "Hailong Sun"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "9 pages, 2 figures, codebase:\n  https://github.com/junchenzhi/Awesome-LLM-Ensemble",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18036v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18036v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10448v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10448v1",
                "updated": "2025-05-15T16:06:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    6,
                    44,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:06:44Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    6,
                    44,
                    3,
                    135,
                    0
                ],
                "title": "Efficient MCMC Sampling with Expensive-to-Compute and Irregular\n  Likelihoods",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient MCMC Sampling with Expensive-to-Compute and Irregular\n  Likelihoods"
                },
                "summary": "Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when\nthe likelihood function is irregular and expensive to compute. We explore\nseveral sampling algorithms that make use of subset evaluations to reduce\ncomputational overhead. We adapt the subset samplers for this setting where\ngradient information is not available or is unreliable. To achieve this, we\nintroduce data-driven proxies in place of Taylor expansions and define a novel\ncomputation-cost aware adaptive controller. We undertake an extensive\nevaluation for a challenging disease modelling task and a configurable task\nwith similar irregularity in the likelihood surface. We find our improved\nversion of Hierarchical Importance with Nested Training Samples (HINTS), with\nadaptive proposals and a data-driven proxy, obtains the best sampling error in\na fixed computational budget. We conclude that subset evaluations can provide\ncheap and naturally-tempered exploration, while a data-driven proxy can\npre-screen proposals successfully in explored regions of the state space. These\ntwo elements combine through hierarchical delayed acceptance to achieve\nefficient, exact sampling.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Bayesian inference with Markov Chain Monte Carlo (MCMC) is challenging when\nthe likelihood function is irregular and expensive to compute. We explore\nseveral sampling algorithms that make use of subset evaluations to reduce\ncomputational overhead. We adapt the subset samplers for this setting where\ngradient information is not available or is unreliable. To achieve this, we\nintroduce data-driven proxies in place of Taylor expansions and define a novel\ncomputation-cost aware adaptive controller. We undertake an extensive\nevaluation for a challenging disease modelling task and a configurable task\nwith similar irregularity in the likelihood surface. We find our improved\nversion of Hierarchical Importance with Nested Training Samples (HINTS), with\nadaptive proposals and a data-driven proxy, obtains the best sampling error in\na fixed computational budget. We conclude that subset evaluations can provide\ncheap and naturally-tempered exploration, while a data-driven proxy can\npre-screen proposals successfully in explored regions of the state space. These\ntwo elements combine through hierarchical delayed acceptance to achieve\nefficient, exact sampling."
                },
                "authors": [
                    {
                        "name": "Conor Rosato"
                    },
                    {
                        "name": "Harvinder Lehal"
                    },
                    {
                        "name": "Simon Maskell"
                    },
                    {
                        "name": "Lee Devlin"
                    },
                    {
                        "name": "Malcolm Strens"
                    }
                ],
                "author_detail": {
                    "name": "Malcolm Strens"
                },
                "author": "Malcolm Strens",
                "arxiv_comment": "45 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10448v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10448v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ML",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10444v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10444v1",
                "updated": "2025-05-15T16:05:50Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    5,
                    50,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:05:50Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    5,
                    50,
                    3,
                    135,
                    0
                ],
                "title": "Inferring entropy production in many-body systems using nonequilibrium\n  MaxEnt",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring entropy production in many-body systems using nonequilibrium\n  MaxEnt"
                },
                "summary": "We propose a method for inferring entropy production (EP) in high-dimensional\nstochastic systems, including many-body systems and non-Markovian systems with\nlong memory. Standard techniques for estimating EP become intractable in such\nsystems due to computational and statistical limitations. We infer\ntrajectory-level EP and lower bounds on average EP by exploiting a\nnonequilibrium analogue of the Maximum Entropy principle, along with convex\nduality. Our approach uses only samples of trajectory observables (such as\nspatiotemporal correlation functions). It does not require reconstruction of\nhigh-dimensional probability distributions or rate matrices, nor any special\nassumptions such as discrete states or multipartite dynamics. It may be used to\ncompute a hierarchical decomposition of EP, reflecting contributions from\ndifferent kinds of interactions, and it has an intuitive physical\ninterpretation as a thermodynamic uncertainty relation. We demonstrate its\nnumerical performance on a disordered nonequilibrium spin model with 1000 spins\nand a large neural spike-train dataset.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose a method for inferring entropy production (EP) in high-dimensional\nstochastic systems, including many-body systems and non-Markovian systems with\nlong memory. Standard techniques for estimating EP become intractable in such\nsystems due to computational and statistical limitations. We infer\ntrajectory-level EP and lower bounds on average EP by exploiting a\nnonequilibrium analogue of the Maximum Entropy principle, along with convex\nduality. Our approach uses only samples of trajectory observables (such as\nspatiotemporal correlation functions). It does not require reconstruction of\nhigh-dimensional probability distributions or rate matrices, nor any special\nassumptions such as discrete states or multipartite dynamics. It may be used to\ncompute a hierarchical decomposition of EP, reflecting contributions from\ndifferent kinds of interactions, and it has an intuitive physical\ninterpretation as a thermodynamic uncertainty relation. We demonstrate its\nnumerical performance on a disordered nonequilibrium spin model with 1000 spins\nand a large neural spike-train dataset."
                },
                "authors": [
                    {
                        "name": "Miguel Aguilera"
                    },
                    {
                        "name": "Sosuke Ito"
                    },
                    {
                        "name": "Artemy Kolchinsky"
                    }
                ],
                "author_detail": {
                    "name": "Artemy Kolchinsky"
                },
                "author": "Artemy Kolchinsky",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10444v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10444v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "nlin.AO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10443v1",
                "updated": "2025-05-15T16:04:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    4,
                    25,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    4,
                    25,
                    3,
                    135,
                    0
                ],
                "title": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?"
                },
                "summary": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Pedro Orvalho"
                    },
                    {
                        "name": "Marta Kwiatkowska"
                    }
                ],
                "author_detail": {
                    "name": "Marta Kwiatkowska"
                },
                "author": "Marta Kwiatkowska",
                "arxiv_comment": "10 pages, 5 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17067v2",
                "updated": "2025-05-15T15:57:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    57,
                    32,
                    3,
                    135,
                    0
                ],
                "published": "2024-05-27T11:39:59Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    39,
                    59,
                    0,
                    148,
                    0
                ],
                "title": "Tokenization Matters! Degrading Large Language Models through\n  Challenging Their Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization Matters! Degrading Large Language Models through\n  Challenging Their Tokenization"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. This defect is more obvious in Chinese\nscenarios. To demonstrate this flaw of LLMs, we construct an adversarial\ndataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which\ndraws upon the vocabularies of various open-source LLMs to challenge LLMs'\ntokenization. ADT consists of two subsets: the manually constructed ADT-Human\nand the automatically generated ADT-Auto. Our empirical results reveal that our\nADT is highly effective on challenging the tokenization of leading LLMs,\nincluding GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'\ncapabilities. Moreover, our method of automatic data generation has been proven\nefficient and robust, which can be applied to any open-source LLMs. In this\npaper, we substantially investigate LLMs' vulnerability in terms of challenging\ntheir token segmentation, which will shed light on the subsequent research of\nimproving LLMs' capabilities through optimizing their tokenization process and\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. This defect is more obvious in Chinese\nscenarios. To demonstrate this flaw of LLMs, we construct an adversarial\ndataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which\ndraws upon the vocabularies of various open-source LLMs to challenge LLMs'\ntokenization. ADT consists of two subsets: the manually constructed ADT-Human\nand the automatically generated ADT-Auto. Our empirical results reveal that our\nADT is highly effective on challenging the tokenization of leading LLMs,\nincluding GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'\ncapabilities. Moreover, our method of automatic data generation has been proven\nefficient and robust, which can be applied to any open-source LLMs. In this\npaper, we substantially investigate LLMs' vulnerability in terms of challenging\ntheir token segmentation, which will shed light on the subsequent research of\nimproving LLMs' capabilities through optimizing their tokenization process and\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Dixuan Wang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Junyuan Jiang"
                    },
                    {
                        "name": "Zepeng Ding"
                    },
                    {
                        "name": "Ziqin Luo"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05901v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05901v2",
                "updated": "2025-05-15T15:46:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    46,
                    43,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-09T09:09:08Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    9,
                    8,
                    4,
                    129,
                    0
                ],
                "title": "Examining the Source of Defects from a Mechanical Perspective for 3D\n  Anomaly Detection",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Examining the Source of Defects from a Mechanical Perspective for 3D\n  Anomaly Detection"
                },
                "summary": "In this paper, we explore a novel approach to 3D anomaly detection (AD) that\ngoes beyond merely identifying anomalies based on structural characteristics.\nOur primary perspective is that most anomalies arise from unpredictable\ndefective forces originating from both internal and external sources. To\naddress these anomalies, we seek out opposing forces that can help correct\nthem. Therefore, we introduce the Mechanics Complementary Model-based Framework\nfor the 3D-AD task (MC4AD), which generates internal and external corrective\nforces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)\nmodule designed to simulate various types of anomalies. Next, we present the\nCorrective Force Prediction Network (CFP-Net), which uses complementary\nrepresentations for point-level analysis to simulate the different\ncontributions from internal and external corrective forces. To ensure the\ncorrective forces are constrained effectively, we have developed a combined\nloss function that includes a new symmetric loss and an overall loss. Notably,\nwe implement a Hierarchical Quality Control (HQC) strategy based on a three-way\ndecision process and contribute a dataset titled Anomaly-IntraVariance, which\nincorporates intraclass variance to evaluate our model. As a result, the\nproposed MC4AD has been proven effective through theory and experimentation.\nThe experimental results demonstrate that our approach yields nine\nstate-of-the-art performances, achieving optimal results with minimal\nparameters and the fastest inference speed across five existing datasets, in\naddition to the proposed Anomaly-IntraVariance dataset. The source is available\nat https://github.com/hzzzzzhappy/MC4AD",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we explore a novel approach to 3D anomaly detection (AD) that\ngoes beyond merely identifying anomalies based on structural characteristics.\nOur primary perspective is that most anomalies arise from unpredictable\ndefective forces originating from both internal and external sources. To\naddress these anomalies, we seek out opposing forces that can help correct\nthem. Therefore, we introduce the Mechanics Complementary Model-based Framework\nfor the 3D-AD task (MC4AD), which generates internal and external corrective\nforces for each point. We first propose a Diverse Anomaly-Generation (DA-Gen)\nmodule designed to simulate various types of anomalies. Next, we present the\nCorrective Force Prediction Network (CFP-Net), which uses complementary\nrepresentations for point-level analysis to simulate the different\ncontributions from internal and external corrective forces. To ensure the\ncorrective forces are constrained effectively, we have developed a combined\nloss function that includes a new symmetric loss and an overall loss. Notably,\nwe implement a Hierarchical Quality Control (HQC) strategy based on a three-way\ndecision process and contribute a dataset titled Anomaly-IntraVariance, which\nincorporates intraclass variance to evaluate our model. As a result, the\nproposed MC4AD has been proven effective through theory and experimentation.\nThe experimental results demonstrate that our approach yields nine\nstate-of-the-art performances, achieving optimal results with minimal\nparameters and the fastest inference speed across five existing datasets, in\naddition to the proposed Anomaly-IntraVariance dataset. The source is available\nat https://github.com/hzzzzzhappy/MC4AD"
                },
                "authors": [
                    {
                        "name": "Hanzhe Liang"
                    },
                    {
                        "name": "Aoran Wang"
                    },
                    {
                        "name": "Jie Zhou"
                    },
                    {
                        "name": "Xin Jin"
                    },
                    {
                        "name": "Can Gao"
                    },
                    {
                        "name": "Jinbao Wang"
                    }
                ],
                "author_detail": {
                    "name": "Jinbao Wang"
                },
                "author": "Jinbao Wang",
                "arxiv_comment": "26 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05901v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05901v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10425v1",
                "updated": "2025-05-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    40,
                    25,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:40:25Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    40,
                    25,
                    3,
                    135,
                    0
                ],
                "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for\n  LLMs"
                },
                "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jingyao Wang"
                    },
                    {
                        "name": "Wenwen Qiang"
                    },
                    {
                        "name": "Zeen Song"
                    },
                    {
                        "name": "Changwen Zheng"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10416v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10416v1",
                "updated": "2025-05-15T15:35:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    35,
                    1,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:35:01Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    35,
                    1,
                    3,
                    135,
                    0
                ],
                "title": "Euclid: Photometric redshift calibration with the clustering redshifts\n  technique",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Euclid: Photometric redshift calibration with the clustering redshifts\n  technique"
                },
                "summary": "Aims: The precision of cosmological constraints from imaging surveys hinges\non accurately estimating the redshift distribution $ n(z) $ of tomographic\nbins, especially their mean redshifts. We assess the effectiveness of the\nclustering redshifts technique in constraining Euclid tomographic redshift bins\nto meet the target uncertainty of $ \\sigma ( \\langle z \\rangle ) < 0.002 (1 +\nz) $. In this work, these mean redshifts are inferred from the small-scale\nangular clustering of Euclid galaxies, which are distributed into bins with\nspectroscopic samples localised in narrow redshift slices.\n  Methods: We generate spectroscopic mocks from the Flagship2 simulation for\nthe Baryon Oscillation Spectroscopic Survey (BOSS), the Dark Energy\nSpectroscopic Instrument (DESI), and Euclid's Near-Infrared Spectrometer and\nPhotometer (NISP) spectroscopic survey. We evaluate and optimise the clustering\nredshifts pipeline, introducing a new method for measuring photometric galaxy\nbias (clustering), which is the primary limitation of this technique.\n  Results: We have successfully constrained the means and standard deviations\nof the redshift distributions for all of the tomographic bins (with a maximum\nphotometric redshift of 1.6), achieving precision beyond the required\nthresholds. We have identified the main sources of bias, particularly the\nimpact of the 1-halo galaxy distribution, which imposed a minimal separation\nscale of 1.5 Mpc for evaluating cross-correlations. These results demonstrate\nthe potential of clustering redshifts to meet the precision requirements for\nEuclid, and we highlight several avenues for future improvements.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Aims: The precision of cosmological constraints from imaging surveys hinges\non accurately estimating the redshift distribution $ n(z) $ of tomographic\nbins, especially their mean redshifts. We assess the effectiveness of the\nclustering redshifts technique in constraining Euclid tomographic redshift bins\nto meet the target uncertainty of $ \\sigma ( \\langle z \\rangle ) < 0.002 (1 +\nz) $. In this work, these mean redshifts are inferred from the small-scale\nangular clustering of Euclid galaxies, which are distributed into bins with\nspectroscopic samples localised in narrow redshift slices.\n  Methods: We generate spectroscopic mocks from the Flagship2 simulation for\nthe Baryon Oscillation Spectroscopic Survey (BOSS), the Dark Energy\nSpectroscopic Instrument (DESI), and Euclid's Near-Infrared Spectrometer and\nPhotometer (NISP) spectroscopic survey. We evaluate and optimise the clustering\nredshifts pipeline, introducing a new method for measuring photometric galaxy\nbias (clustering), which is the primary limitation of this technique.\n  Results: We have successfully constrained the means and standard deviations\nof the redshift distributions for all of the tomographic bins (with a maximum\nphotometric redshift of 1.6), achieving precision beyond the required\nthresholds. We have identified the main sources of bias, particularly the\nimpact of the 1-halo galaxy distribution, which imposed a minimal separation\nscale of 1.5 Mpc for evaluating cross-correlations. These results demonstrate\nthe potential of clustering redshifts to meet the precision requirements for\nEuclid, and we highlight several avenues for future improvements."
                },
                "authors": [
                    {
                        "name": "W. d'Assignies Doumerg"
                    },
                    {
                        "name": "M. Manera"
                    },
                    {
                        "name": "C. Padilla"
                    },
                    {
                        "name": "O. Ilbert"
                    },
                    {
                        "name": "H. Hildebrandt"
                    },
                    {
                        "name": "L. Reynolds"
                    },
                    {
                        "name": "J. Chaves-Montero"
                    },
                    {
                        "name": "A. H. Wright"
                    },
                    {
                        "name": "P. Tallada-Cresp"
                    },
                    {
                        "name": "M. Eriksen"
                    },
                    {
                        "name": "J. Carretero"
                    },
                    {
                        "name": "W. Roster"
                    },
                    {
                        "name": "Y. Kang"
                    },
                    {
                        "name": "K. Naidoo"
                    },
                    {
                        "name": "R. Miquel"
                    },
                    {
                        "name": "B. Altieri"
                    },
                    {
                        "name": "A. Amara"
                    },
                    {
                        "name": "S. Andreon"
                    },
                    {
                        "name": "N. Auricchio"
                    },
                    {
                        "name": "C. Baccigalupi"
                    },
                    {
                        "name": "D. Bagot"
                    },
                    {
                        "name": "M. Baldi"
                    },
                    {
                        "name": "A. Balestra"
                    },
                    {
                        "name": "S. Bardelli"
                    },
                    {
                        "name": "P. Battaglia"
                    },
                    {
                        "name": "A. Biviano"
                    },
                    {
                        "name": "E. Branchini"
                    },
                    {
                        "name": "M. Brescia"
                    },
                    {
                        "name": "S. Camera"
                    },
                    {
                        "name": "V. Capobianco"
                    },
                    {
                        "name": "C. Carbone"
                    },
                    {
                        "name": "V. F. Cardone"
                    },
                    {
                        "name": "S. Casas"
                    },
                    {
                        "name": "F. J. Castander"
                    },
                    {
                        "name": "M. Castellano"
                    },
                    {
                        "name": "G. Castignani"
                    },
                    {
                        "name": "S. Cavuoti"
                    },
                    {
                        "name": "K. C. Chambers"
                    },
                    {
                        "name": "A. Cimatti"
                    },
                    {
                        "name": "C. Colodro-Conde"
                    },
                    {
                        "name": "G. Congedo"
                    },
                    {
                        "name": "C. J. Conselice"
                    },
                    {
                        "name": "L. Conversi"
                    },
                    {
                        "name": "Y. Copin"
                    },
                    {
                        "name": "F. Courbin"
                    },
                    {
                        "name": "H. M. Courtois"
                    },
                    {
                        "name": "M. Crocce"
                    },
                    {
                        "name": "A. Da Silva"
                    },
                    {
                        "name": "H. Degaudenzi"
                    },
                    {
                        "name": "S. de la Torre"
                    },
                    {
                        "name": "G. De Lucia"
                    },
                    {
                        "name": "M. Douspis"
                    },
                    {
                        "name": "X. Dupac"
                    },
                    {
                        "name": "A. Ealet"
                    },
                    {
                        "name": "S. Escoffier"
                    },
                    {
                        "name": "M. Farina"
                    },
                    {
                        "name": "F. Faustini"
                    },
                    {
                        "name": "S. Ferriol"
                    },
                    {
                        "name": "F. Finelli"
                    },
                    {
                        "name": "P. Fosalba"
                    },
                    {
                        "name": "S. Fotopoulou"
                    },
                    {
                        "name": "M. Frailis"
                    },
                    {
                        "name": "E. Franceschi"
                    },
                    {
                        "name": "M. Fumana"
                    },
                    {
                        "name": "S. Galeotta"
                    },
                    {
                        "name": "K. George"
                    },
                    {
                        "name": "B. Gillis"
                    },
                    {
                        "name": "C. Giocoli"
                    },
                    {
                        "name": "P. Gmez-Alvarez"
                    },
                    {
                        "name": "J. Gracia-Carpio"
                    },
                    {
                        "name": "A. Grazian"
                    },
                    {
                        "name": "F. Grupp"
                    },
                    {
                        "name": "W. Holmes"
                    },
                    {
                        "name": "I. M. Hook"
                    },
                    {
                        "name": "A. Hornstrup"
                    },
                    {
                        "name": "K. Jahnke"
                    },
                    {
                        "name": "M. Jhabvala"
                    },
                    {
                        "name": "B. Joachimi"
                    },
                    {
                        "name": "E. Keihnen"
                    },
                    {
                        "name": "S. Kermiche"
                    },
                    {
                        "name": "A. Kiessling"
                    },
                    {
                        "name": "B. Kubik"
                    },
                    {
                        "name": "M. Kmmel"
                    },
                    {
                        "name": "M. Kunz"
                    },
                    {
                        "name": "H. Kurki-Suonio"
                    },
                    {
                        "name": "O. Lahav"
                    },
                    {
                        "name": "A. M. C. Le Brun"
                    },
                    {
                        "name": "S. Ligori"
                    },
                    {
                        "name": "P. B. Lilje"
                    },
                    {
                        "name": "V. Lindholm"
                    },
                    {
                        "name": "I. Lloro"
                    },
                    {
                        "name": "G. Mainetti"
                    },
                    {
                        "name": "D. Maino"
                    },
                    {
                        "name": "E. Maiorano"
                    },
                    {
                        "name": "O. Mansutti"
                    },
                    {
                        "name": "S. Marcin"
                    },
                    {
                        "name": "O. Marggraf"
                    },
                    {
                        "name": "K. Markovic"
                    },
                    {
                        "name": "M. Martinelli"
                    },
                    {
                        "name": "N. Martinet"
                    },
                    {
                        "name": "F. Marulli"
                    },
                    {
                        "name": "R. Massey"
                    },
                    {
                        "name": "D. C. Masters"
                    },
                    {
                        "name": "E. Medinaceli"
                    },
                    {
                        "name": "S. Mei"
                    },
                    {
                        "name": "M. Melchior"
                    },
                    {
                        "name": "Y. Mellier"
                    },
                    {
                        "name": "M. Meneghetti"
                    },
                    {
                        "name": "E. Merlin"
                    },
                    {
                        "name": "G. Meylan"
                    },
                    {
                        "name": "A. Mora"
                    },
                    {
                        "name": "M. Moresco"
                    },
                    {
                        "name": "L. Moscardini"
                    },
                    {
                        "name": "C. Neissner"
                    },
                    {
                        "name": "S. -M. Niemi"
                    },
                    {
                        "name": "S. Paltani"
                    },
                    {
                        "name": "F. Pasian"
                    },
                    {
                        "name": "K. Pedersen"
                    },
                    {
                        "name": "V. Pettorino"
                    },
                    {
                        "name": "S. Pires"
                    },
                    {
                        "name": "G. Polenta"
                    },
                    {
                        "name": "M. Poncet"
                    },
                    {
                        "name": "L. A. Popa"
                    },
                    {
                        "name": "L. Pozzetti"
                    },
                    {
                        "name": "F. Raison"
                    },
                    {
                        "name": "R. Rebolo"
                    },
                    {
                        "name": "A. Renzi"
                    },
                    {
                        "name": "J. Rhodes"
                    },
                    {
                        "name": "G. Riccio"
                    },
                    {
                        "name": "E. Romelli"
                    },
                    {
                        "name": "M. Roncarelli"
                    },
                    {
                        "name": "E. Rossetti"
                    },
                    {
                        "name": "R. Saglia"
                    },
                    {
                        "name": "Z. Sakr"
                    },
                    {
                        "name": "D. Sapone"
                    },
                    {
                        "name": "B. Sartoris"
                    },
                    {
                        "name": "J. A. Schewtschenko"
                    },
                    {
                        "name": "P. Schneider"
                    },
                    {
                        "name": "T. Schrabback"
                    },
                    {
                        "name": "A. Secroun"
                    },
                    {
                        "name": "E. Sefusatti"
                    },
                    {
                        "name": "G. Seidel"
                    },
                    {
                        "name": "M. Seiffert"
                    },
                    {
                        "name": "S. Serrano"
                    },
                    {
                        "name": "P. Simon"
                    },
                    {
                        "name": "C. Sirignano"
                    },
                    {
                        "name": "G. Sirri"
                    },
                    {
                        "name": "A. Spurio Mancini"
                    },
                    {
                        "name": "L. Stanco"
                    },
                    {
                        "name": "J. Steinwagner"
                    },
                    {
                        "name": "D. Tavagnacco"
                    },
                    {
                        "name": "A. N. Taylor"
                    },
                    {
                        "name": "H. I. Teplitz"
                    },
                    {
                        "name": "I. Tereno"
                    },
                    {
                        "name": "N. Tessore"
                    },
                    {
                        "name": "S. Toft"
                    },
                    {
                        "name": "R. Toledo-Moreo"
                    },
                    {
                        "name": "F. Torradeflot"
                    },
                    {
                        "name": "A. Tsyganov"
                    },
                    {
                        "name": "I. Tutusaus"
                    },
                    {
                        "name": "L. Valenziano"
                    },
                    {
                        "name": "J. Valiviita"
                    },
                    {
                        "name": "T. Vassallo"
                    },
                    {
                        "name": "G. Verdoes Kleijn"
                    },
                    {
                        "name": "Y. Wang"
                    },
                    {
                        "name": "J. Weller"
                    },
                    {
                        "name": "G. Zamorani"
                    },
                    {
                        "name": "E. Zucca"
                    },
                    {
                        "name": "M. Bolzonella"
                    },
                    {
                        "name": "C. Burigana"
                    },
                    {
                        "name": "L. Gabarra"
                    },
                    {
                        "name": "J. Martn-Fleitas"
                    },
                    {
                        "name": "I. Risso"
                    },
                    {
                        "name": "V. Scottez"
                    },
                    {
                        "name": "M. Viel"
                    }
                ],
                "author_detail": {
                    "name": "M. Viel"
                },
                "arxiv_affiliation": "ICSC - Centro Nazionale di Ricerca in High Performance Computing, Big Data e Quantum Computing, Via Magnanelli 2, Bologna, Italy",
                "author": "M. Viel",
                "arxiv_comment": "32 pages, 26 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10416v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10416v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10415v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10415v1",
                "updated": "2025-05-15T15:35:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    35,
                    0,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:35:00Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    35,
                    0,
                    3,
                    135,
                    0
                ],
                "title": "Internal State Estimation in Groups via Active Information Gathering",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Internal State Estimation in Groups via Active Information Gathering"
                },
                "summary": "Accurately estimating human internal states, such as personality traits or\nbehavioral patterns, is critical for enhancing the effectiveness of human-robot\ninteraction, particularly in group settings. These insights are key in\napplications ranging from social navigation to autism diagnosis. However, prior\nmethods are limited by scalability and passive observation, making real-time\nestimation in complex, multi-human settings difficult. In this work, we propose\na practical method for active human personality estimation in groups, with a\nfocus on applications related to Autism Spectrum Disorder (ASD). Our method\ncombines a personality-conditioned behavior model, based on the Eysenck\n3-Factor theory, with an active robot information gathering policy that\ntriggers human behaviors through a receding-horizon planner. The robot's belief\nabout human personality is then updated via Bayesian inference. We demonstrate\nthe effectiveness of our approach through simulations, user studies with\ntypical adults, and preliminary experiments involving participants with ASD.\nOur results show that our method can scale to tens of humans and reduce\npersonality prediction error by 29.2% and uncertainty by 79.9% in simulation.\nUser studies with typical adults confirm the method's ability to generalize\nacross complex personality distributions. Additionally, we explore its\napplication in autism-related scenarios, demonstrating that the method can\nidentify the difference between neurotypical and autistic behavior,\nhighlighting its potential for diagnosing ASD. The results suggest that our\nframework could serve as a foundation for future ASD-specific interventions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurately estimating human internal states, such as personality traits or\nbehavioral patterns, is critical for enhancing the effectiveness of human-robot\ninteraction, particularly in group settings. These insights are key in\napplications ranging from social navigation to autism diagnosis. However, prior\nmethods are limited by scalability and passive observation, making real-time\nestimation in complex, multi-human settings difficult. In this work, we propose\na practical method for active human personality estimation in groups, with a\nfocus on applications related to Autism Spectrum Disorder (ASD). Our method\ncombines a personality-conditioned behavior model, based on the Eysenck\n3-Factor theory, with an active robot information gathering policy that\ntriggers human behaviors through a receding-horizon planner. The robot's belief\nabout human personality is then updated via Bayesian inference. We demonstrate\nthe effectiveness of our approach through simulations, user studies with\ntypical adults, and preliminary experiments involving participants with ASD.\nOur results show that our method can scale to tens of humans and reduce\npersonality prediction error by 29.2% and uncertainty by 79.9% in simulation.\nUser studies with typical adults confirm the method's ability to generalize\nacross complex personality distributions. Additionally, we explore its\napplication in autism-related scenarios, demonstrating that the method can\nidentify the difference between neurotypical and autistic behavior,\nhighlighting its potential for diagnosing ASD. The results suggest that our\nframework could serve as a foundation for future ASD-specific interventions."
                },
                "authors": [
                    {
                        "name": "Xuebo Ji"
                    },
                    {
                        "name": "Zherong Pan"
                    },
                    {
                        "name": "Xifeng Gao"
                    },
                    {
                        "name": "Lei Yang"
                    },
                    {
                        "name": "Xinxin Du"
                    },
                    {
                        "name": "Kaiyun Li"
                    },
                    {
                        "name": "Yongjin Liu"
                    },
                    {
                        "name": "Wenping Wang"
                    },
                    {
                        "name": "Changhe Tu"
                    },
                    {
                        "name": "Jia Pan"
                    }
                ],
                "author_detail": {
                    "name": "Jia Pan"
                },
                "author": "Jia Pan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10415v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10415v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10413v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10413v1",
                "updated": "2025-05-15T15:34:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    34,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:34:15Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    34,
                    15,
                    3,
                    135,
                    0
                ],
                "title": "Hierarchical Document Refinement for Long-context Retrieval-augmented\n  Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Hierarchical Document Refinement for Long-context Retrieval-augmented\n  Generation"
                },
                "summary": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world RAG applications often encounter long-context input scenarios,\nwhere redundant information and noise results in higher inference costs and\nreduced performance. To address these challenges, we propose LongRefiner, an\nefficient plug-and-play refiner that leverages the inherent structural\ncharacteristics of long documents. LongRefiner employs dual-level query\nanalysis, hierarchical document structuring, and adaptive refinement through\nmulti-task learning on a single foundation model. Experiments on seven QA\ndatasets demonstrate that LongRefiner achieves competitive performance in\nvarious scenarios while using 10x fewer computational costs and latency\ncompared to the best baseline. Further analysis validates that LongRefiner is\nscalable, efficient, and effective, providing practical insights for real-world\nlong-text RAG applications. Our code is available at\nhttps://github.com/ignorejjj/LongRefiner."
                },
                "authors": [
                    {
                        "name": "Jiajie Jin"
                    },
                    {
                        "name": "Xiaoxi Li"
                    },
                    {
                        "name": "Guanting Dong"
                    },
                    {
                        "name": "Yuyao Zhang"
                    },
                    {
                        "name": "Yutao Zhu"
                    },
                    {
                        "name": "Yongkang Wu"
                    },
                    {
                        "name": "Zhonghua Li"
                    },
                    {
                        "name": "Qi Ye"
                    },
                    {
                        "name": "Zhicheng Dou"
                    }
                ],
                "author_detail": {
                    "name": "Zhicheng Dou"
                },
                "author": "Zhicheng Dou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10413v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10413v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10409v1",
                "updated": "2025-05-15T15:31:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    31,
                    17,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:31:17Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    31,
                    17,
                    3,
                    135,
                    0
                ],
                "title": "Are LLM-generated plain language summaries truly understandable? A\n  large-scale crowdsourced evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-generated plain language summaries truly understandable? A\n  large-scale crowdsourced evaluation"
                },
                "summary": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension."
                },
                "authors": [
                    {
                        "name": "Yue Guo"
                    },
                    {
                        "name": "Jae Ho Sohn"
                    },
                    {
                        "name": "Gondy Leroy"
                    },
                    {
                        "name": "Trevor Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Cohen"
                },
                "author": "Trevor Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10402v1",
                "updated": "2025-05-15T15:26:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    26,
                    32,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:26:32Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    26,
                    32,
                    3,
                    135,
                    0
                ],
                "title": "Rethinking Repetition Problems of LLMs in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Repetition Problems of LLMs in Code Generation"
                },
                "summary": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "arxiv_comment": "Accepted to ACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06046v2",
                "updated": "2025-05-15T15:14:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    14,
                    47,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-09T13:42:59Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    42,
                    59,
                    4,
                    129,
                    0
                ],
                "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information"
                },
                "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries. To create PubHealthBench we extract free\ntext from 687 current UK government guidance documents and implement an\nautomated pipeline for generating MCQA samples. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% accuracy in the MCQA setup, and\noutperform humans with cursory search engine use. However, in the free form\nsetup we see lower performance with no model scoring >75%. Importantly we find\nin both setups LLMs have higher accuracy on guidance intended for the general\npublic. Therefore, there are promising signs that state of the art (SOTA) LLMs\nare an increasingly accurate source of public health information, but\nadditional safeguards or tools may still be needed when providing free form\nresponses on public health topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries. To create PubHealthBench we extract free\ntext from 687 current UK government guidance documents and implement an\nautomated pipeline for generating MCQA samples. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% accuracy in the MCQA setup, and\noutperform humans with cursory search engine use. However, in the free form\nsetup we see lower performance with no model scoring >75%. Importantly we find\nin both setups LLMs have higher accuracy on guidance intended for the general\npublic. Therefore, there are promising signs that state of the art (SOTA) LLMs\nare an increasingly accurate source of public health information, but\nadditional safeguards or tools may still be needed when providing free form\nresponses on public health topics."
                },
                "authors": [
                    {
                        "name": "Joshua Harris"
                    },
                    {
                        "name": "Fan Grayson"
                    },
                    {
                        "name": "Felix Feldman"
                    },
                    {
                        "name": "Timothy Laurence"
                    },
                    {
                        "name": "Toby Nonnenmacher"
                    },
                    {
                        "name": "Oliver Higgins"
                    },
                    {
                        "name": "Leo Loman"
                    },
                    {
                        "name": "Selina Patel"
                    },
                    {
                        "name": "Thomas Finnie"
                    },
                    {
                        "name": "Samuel Collins"
                    },
                    {
                        "name": "Michael Borowitz"
                    }
                ],
                "author_detail": {
                    "name": "Michael Borowitz"
                },
                "author": "Michael Borowitz",
                "arxiv_comment": "24 pages, 10 pages main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10392v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10392v1",
                "updated": "2025-05-15T15:14:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    14,
                    2,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:14:02Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    14,
                    2,
                    3,
                    135,
                    0
                ],
                "title": "Schreier-Coset Graph Propagation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Schreier-Coset Graph Propagation"
                },
                "summary": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Graph Neural Networks (GNNs) offer a principled framework for learning over\ngraph-structured data, yet their expressive capacity is often hindered by\nover-squashing, wherein information from distant nodes is compressed into\nfixed-size vectors. Existing solutions, including graph rewiring and\nbottleneck-resistant architectures such as Cayley and expander graphs, avoid\nthis problem but introduce scalability bottlenecks. In particular, the Cayley\ngraphs constructed over $SL(2,\\mathbb{Z}_n)$ exhibit strong theoretical\nproperties, yet suffer from cubic node growth $O(n^3)$, leading to high memory\nusage. To address this, this work introduces Schrier-Coset Graph Propagation\n(SCGP), a group-theoretic augmentation method that enriches node features\nthrough Schreier-coset embeddings without altering the input graph topology.\nSCGP embeds bottleneck-free connectivity patterns into a compact feature space,\nimproving long-range message passing while maintaining computational\nefficiency. Empirical evaluations across standard node and graph classification\nbenchmarks demonstrate that SCGP achieves performance comparable to, or\nexceeding, expander graph and rewired GNN baselines. Furthermore, SCGP exhibits\nparticular advantages in processing hierarchical and modular graph structures,\noffering reduced inference latency, improved scalability, and a low memory\nfootprint, making it suitable for real-time and resource-constrained\napplications."
                },
                "authors": [
                    {
                        "name": "Aryan Mishra"
                    },
                    {
                        "name": "Lizhen Lin"
                    }
                ],
                "author_detail": {
                    "name": "Lizhen Lin"
                },
                "author": "Lizhen Lin",
                "arxiv_comment": "9 pages, 1 figure , preprint",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10392v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10392v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10389v1",
                "updated": "2025-05-15T15:11:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    11,
                    48,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:11:48Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    11,
                    48,
                    3,
                    135,
                    0
                ],
                "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting\n  Aspect-based Opinion Quadruples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting\n  Aspect-based Opinion Quadruples"
                },
                "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks."
                },
                "authors": [
                    {
                        "name": "Benjamin White"
                    },
                    {
                        "name": "Anastasia Shimorina"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Shimorina"
                },
                "author": "Anastasia Shimorina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07119v2",
                "updated": "2025-05-15T15:05:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    5,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-11T21:05:33Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    21,
                    5,
                    33,
                    6,
                    131,
                    0
                ],
                "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression"
                },
                "summary": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation."
                },
                "authors": [
                    {
                        "name": "Arianna Stropeni"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Marco Fabris"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02205v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02205v2",
                "updated": "2025-05-15T15:00:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    0,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-04T10:42:30Z",
                "published_parsed": [
                    2025,
                    2,
                    4,
                    10,
                    42,
                    30,
                    1,
                    35,
                    0
                ],
                "title": "From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for\n  Safe PDE Control",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Uncertain to Safe: Conformal Fine-Tuning of Diffusion Models for\n  Safe PDE Control"
                },
                "summary": "The application of deep learning for partial differential equation\n(PDE)-constrained control is gaining increasing attention. However, existing\nmethods rarely consider safety requirements crucial in real-world applications.\nTo address this limitation, we propose Safe Diffusion Models for PDE Control\n(SafeDiffCon), which introduce the uncertainty quantile as model uncertainty\nquantification to achieve optimal control under safety constraints through both\npost-training and inference phases. Firstly, our approach post-trains a\npre-trained diffusion model to generate control sequences that better satisfy\nsafety constraints while achieving improved control objectives via a reweighted\ndiffusion loss, which incorporates the uncertainty quantile estimated using\nconformal prediction. Secondly, during inference, the diffusion model\ndynamically adjusts both its generation process and parameters through\niterative guidance and fine-tuning, conditioned on control targets while\nsimultaneously integrating the estimated uncertainty quantile. We evaluate\nSafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible\nfluid, and controlled nuclear fusion problem. Results demonstrate that\nSafeDiffCon is the only method that satisfies all safety constraints, whereas\nother classical and deep learning baselines fail. Furthermore, while adhering\nto safety constraints, SafeDiffCon achieves the best control performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The application of deep learning for partial differential equation\n(PDE)-constrained control is gaining increasing attention. However, existing\nmethods rarely consider safety requirements crucial in real-world applications.\nTo address this limitation, we propose Safe Diffusion Models for PDE Control\n(SafeDiffCon), which introduce the uncertainty quantile as model uncertainty\nquantification to achieve optimal control under safety constraints through both\npost-training and inference phases. Firstly, our approach post-trains a\npre-trained diffusion model to generate control sequences that better satisfy\nsafety constraints while achieving improved control objectives via a reweighted\ndiffusion loss, which incorporates the uncertainty quantile estimated using\nconformal prediction. Secondly, during inference, the diffusion model\ndynamically adjusts both its generation process and parameters through\niterative guidance and fine-tuning, conditioned on control targets while\nsimultaneously integrating the estimated uncertainty quantile. We evaluate\nSafeDiffCon on three control tasks: 1D Burgers' equation, 2D incompressible\nfluid, and controlled nuclear fusion problem. Results demonstrate that\nSafeDiffCon is the only method that satisfies all safety constraints, whereas\nother classical and deep learning baselines fail. Furthermore, while adhering\nto safety constraints, SafeDiffCon achieves the best control performance."
                },
                "authors": [
                    {
                        "name": "Peiyan Hu"
                    },
                    {
                        "name": "Xiaowei Qian"
                    },
                    {
                        "name": "Wenhao Deng"
                    },
                    {
                        "name": "Rui Wang"
                    },
                    {
                        "name": "Haodong Feng"
                    },
                    {
                        "name": "Ruiqi Feng"
                    },
                    {
                        "name": "Tao Zhang"
                    },
                    {
                        "name": "Long Wei"
                    },
                    {
                        "name": "Yue Wang"
                    },
                    {
                        "name": "Zhi-Ming Ma"
                    },
                    {
                        "name": "Tailin Wu"
                    }
                ],
                "author_detail": {
                    "name": "Tailin Wu"
                },
                "author": "Tailin Wu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02205v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02205v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10376v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10376v1",
                "updated": "2025-05-15T14:59:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    59,
                    36,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:59:36Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    59,
                    36,
                    3,
                    135,
                    0
                ],
                "title": "Measuring Long Stellar Rotation Periods (>10 days) from TESS FFI Light\n  Curves is Possible: An Investigation Using TESS and ZTF",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Measuring Long Stellar Rotation Periods (>10 days) from TESS FFI Light\n  Curves is Possible: An Investigation Using TESS and ZTF"
                },
                "summary": "The rotation period of a star is an important quantity that provides insight\ninto its structure and state. For stars with surface features like starspots,\ntheir periods can be inferred from brightness variations as these features move\nacross the stellar surface. TESS, with its all-sky coverage, is providing the\nlargest sample of stars for obtaining rotation periods. However, most of the\nperiods have been limited to shorter than the 13.7-day TESS orbital period due\nto strong background signals (e.g., scattered light) on those timescales. In\nthis study, we investigated the viability of measuring longer periods (> 10\ndays) from TESS light curves for stars in the Northern Continuous Viewing Zone\n(NCVZ). We first created a reference set of 272 period measurements longer than\n10 days for K & M dwarfs in the NCVZ using data from the Zwicky Transient\nFacility (ZTF) that we consider as the \"ground truth\" given ZTF's long temporal\nbaseline of 6+ years. We then used the unpopular pipeline to de-trend TESS\nlight curves and implemented a modified Lomb-Scargle (LS) periodogram that\naccounts for flux offsets between observing sectors. For 179 out of the 272\nsources (66%), the TESS-derived periods match the ZTF-derived periods to within\n10%. The match rate increases to 81% (137 out of 170) when restricting to\nsources with a TESS LS power that exceeds a threshold. Our results confirm the\ncapability of measuring periods longer than 10 days from TESS data,\nhighlighting the dataset's potential for studying slow rotators.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rotation period of a star is an important quantity that provides insight\ninto its structure and state. For stars with surface features like starspots,\ntheir periods can be inferred from brightness variations as these features move\nacross the stellar surface. TESS, with its all-sky coverage, is providing the\nlargest sample of stars for obtaining rotation periods. However, most of the\nperiods have been limited to shorter than the 13.7-day TESS orbital period due\nto strong background signals (e.g., scattered light) on those timescales. In\nthis study, we investigated the viability of measuring longer periods (> 10\ndays) from TESS light curves for stars in the Northern Continuous Viewing Zone\n(NCVZ). We first created a reference set of 272 period measurements longer than\n10 days for K & M dwarfs in the NCVZ using data from the Zwicky Transient\nFacility (ZTF) that we consider as the \"ground truth\" given ZTF's long temporal\nbaseline of 6+ years. We then used the unpopular pipeline to de-trend TESS\nlight curves and implemented a modified Lomb-Scargle (LS) periodogram that\naccounts for flux offsets between observing sectors. For 179 out of the 272\nsources (66%), the TESS-derived periods match the ZTF-derived periods to within\n10%. The match rate increases to 81% (137 out of 170) when restricting to\nsources with a TESS LS power that exceeds a threshold. Our results confirm the\ncapability of measuring periods longer than 10 days from TESS data,\nhighlighting the dataset's potential for studying slow rotators."
                },
                "authors": [
                    {
                        "name": "Soichiro Hattori"
                    },
                    {
                        "name": "Ruth Angus"
                    },
                    {
                        "name": "Daniel Foreman-Mackey"
                    },
                    {
                        "name": "Yuxi"
                    },
                    {
                        "name": "Lu"
                    },
                    {
                        "name": "Isabel Colman"
                    }
                ],
                "author_detail": {
                    "name": "Isabel Colman"
                },
                "arxiv_affiliation": "Lucy",
                "author": "Isabel Colman",
                "arxiv_doi": "10.3847/1538-3881/add0ab",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.3847/1538-3881/add0ab",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.10376v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10376v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "27 pages, 11 figures, accepted for publication in The Astronomical\n  Journal",
                "arxiv_primary_category": {
                    "term": "astro-ph.SR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.SR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.IM",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10375v1",
                "updated": "2025-05-15T14:59:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    59,
                    17,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:59:17Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    59,
                    17,
                    3,
                    135,
                    0
                ],
                "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Sparse Autoencoders Useful for Java Function Bug Detection?"
                },
                "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision."
                },
                "authors": [
                    {
                        "name": "Rui Melo"
                    },
                    {
                        "name": "Claudia Mamede"
                    },
                    {
                        "name": "Andre Catarino"
                    },
                    {
                        "name": "Rui Abreu"
                    },
                    {
                        "name": "Henrique Lopes Cardoso"
                    }
                ],
                "author_detail": {
                    "name": "Henrique Lopes Cardoso"
                },
                "author": "Henrique Lopes Cardoso",
                "arxiv_comment": "10 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10351v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10351v1",
                "updated": "2025-05-15T14:43:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    43,
                    34,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:43:34Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    43,
                    34,
                    3,
                    135,
                    0
                ],
                "title": "A Unified and Scalable Membership Inference Method for Visual\n  Self-supervised Encoder via Part-aware Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Unified and Scalable Membership Inference Method for Visual\n  Self-supervised Encoder via Part-aware Capability"
                },
                "summary": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Self-supervised learning shows promise in harnessing extensive unlabeled\ndata, but it also confronts significant privacy concerns, especially in vision.\nIn this paper, we perform membership inference on visual self-supervised models\nin a more realistic setting: self-supervised training method and details are\nunknown for an adversary when attacking as he usually faces a black-box system\nin practice. In this setting, considering that self-supervised model could be\ntrained by completely different self-supervised paradigms, e.g., masked image\nmodeling and contrastive learning, with complex training details, we propose a\nunified membership inference method called PartCrop. It is motivated by the\nshared part-aware capability among models and stronger part response on the\ntraining data. Specifically, PartCrop crops parts of objects in an image to\nquery responses within the image in representation space. We conduct extensive\nattacks on self-supervised models with different training protocols and\nstructures using three widely used image datasets. The results verify the\neffectiveness and generalization of PartCrop. Moreover, to defend against\nPartCrop, we evaluate two common approaches, i.e., early stop and differential\nprivacy, and propose a tailored method called shrinking crop scale range. The\ndefense experiments indicate that all of them are effective. Finally, besides\nprototype testing on toy visual encoders and small-scale image datasets, we\nquantitatively study the impacts of scaling from both data and model aspects in\na realistic scenario and propose a scalable PartCrop-v2 by introducing two\nstructural improvements to PartCrop. Our code is at\nhttps://github.com/JiePKU/PartCrop."
                },
                "authors": [
                    {
                        "name": "Jie Zhu"
                    },
                    {
                        "name": "Jirong Zha"
                    },
                    {
                        "name": "Ding Li"
                    },
                    {
                        "name": "Leye Wang"
                    }
                ],
                "author_detail": {
                    "name": "Leye Wang"
                },
                "author": "Leye Wang",
                "arxiv_comment": "An extension of our ACM CCS2024 conference paper (arXiv:2404.02462).\n  We show the impacts of scaling from both data and model aspects on membership\n  inference for self-supervised visual encoders",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10351v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10351v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10349v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10349v1",
                "updated": "2025-05-15T14:41:03Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    41,
                    3,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:41:03Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    41,
                    3,
                    3,
                    135,
                    0
                ],
                "title": "Locally Differentially Private Frequency Estimation via Joint Randomized\n  Response",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Locally Differentially Private Frequency Estimation via Joint Randomized\n  Response"
                },
                "summary": "Local Differential Privacy (LDP) has been widely recognized as a powerful\ntool for providing a strong theoretical guarantee of data privacy to data\ncontributors against an untrusted data collector. Under a typical LDP scheme,\neach data contributor independently randomly perturbs their data before\nsubmitting them to the data collector, which in turn infers valuable statistics\nabout the original data from received perturbed data. Common to existing LDP\nmechanisms is an inherent trade-off between the level of privacy protection and\ndata utility in the sense that strong data privacy often comes at the cost of\nreduced data utility. Frequency estimation based on Randomized Response (RR) is\na fundamental building block of many LDP mechanisms. In this paper, we propose\na novel Joint Randomized Response (JRR) mechanism based on correlated data\nperturbations to achieve locally differentially private frequency estimation.\nJRR divides data contributors into disjoint groups of two members and lets\nthose in the same group jointly perturb their binary data to improve\nfrequency-estimation accuracy and achieve the same level of data privacy by\nhiding the group membership information in contrast to the classical RR\nmechanism. Theoretical analysis and detailed simulation studies using both real\nand synthetic datasets show that JRR achieves the same level of data privacy as\nthe classical RR mechanism while improving the frequency-estimation accuracy in\nthe overwhelming majority of the cases by up to two orders of magnitude.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Local Differential Privacy (LDP) has been widely recognized as a powerful\ntool for providing a strong theoretical guarantee of data privacy to data\ncontributors against an untrusted data collector. Under a typical LDP scheme,\neach data contributor independently randomly perturbs their data before\nsubmitting them to the data collector, which in turn infers valuable statistics\nabout the original data from received perturbed data. Common to existing LDP\nmechanisms is an inherent trade-off between the level of privacy protection and\ndata utility in the sense that strong data privacy often comes at the cost of\nreduced data utility. Frequency estimation based on Randomized Response (RR) is\na fundamental building block of many LDP mechanisms. In this paper, we propose\na novel Joint Randomized Response (JRR) mechanism based on correlated data\nperturbations to achieve locally differentially private frequency estimation.\nJRR divides data contributors into disjoint groups of two members and lets\nthose in the same group jointly perturb their binary data to improve\nfrequency-estimation accuracy and achieve the same level of data privacy by\nhiding the group membership information in contrast to the classical RR\nmechanism. Theoretical analysis and detailed simulation studies using both real\nand synthetic datasets show that JRR achieves the same level of data privacy as\nthe classical RR mechanism while improving the frequency-estimation accuracy in\nthe overwhelming majority of the cases by up to two orders of magnitude."
                },
                "authors": [
                    {
                        "name": "Ye Zheng"
                    },
                    {
                        "name": "Shafizur Rahman Seeam"
                    },
                    {
                        "name": "Yidan Hu"
                    },
                    {
                        "name": "Rui Zhang"
                    },
                    {
                        "name": "Yanchao Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yanchao Zhang"
                },
                "author": "Yanchao Zhang",
                "arxiv_comment": "Accepted by PETS'25 (issue 3)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10349v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10349v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "E.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.11721v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.11721v2",
                "updated": "2025-05-15T14:37:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    37,
                    56,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-13T21:24:14Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    21,
                    24,
                    14,
                    3,
                    72,
                    0
                ],
                "title": "Identifying intermediate mass binary black hole mergers in AGN disks\n  using LISA",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Identifying intermediate mass binary black hole mergers in AGN disks\n  using LISA"
                },
                "summary": "We show that Laser Interferometer Space Antenna can uniquely identify the\nsites of intermediate-mass binary black hole (IMBBH) mergers if they occur in\nActive Galactic Nuclei (AGN) disks with a gas density $\\rho\\geq10^{-12} \\, {\\rm\ng/cc}$ via measurement of dynamical friction effect in the gravitational\nwaveform. We find that even a single observation of a gravitational wave source\nwith a total mass of $10^3 M_{\\odot}$ and a mass ratio of 2 at a luminosity\ndistance of 3 Gpc is sufficient to confidently associate the merger to be in an\nAGN disk with a density $\\sim 10^{-12} \\, {\\rm g/cc}$, as it allows estimation\nof the density with an error bar $O(100\\%)$. This provides a new way of\ninferring AGN disk densities that complement traditional X-ray observations.\nFurther, we find that neglecting the presence of environmental effects in the\nwaveform models used for parameter estimation can bias the chirp mass, mass\nratio and arrival time of a merger. If not corrected, this can significantly\nimpact our ability to carry out multiband data analysis of IMBBHs that combines\ninformation from LISA and ground-based gravitational wave detectors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We show that Laser Interferometer Space Antenna can uniquely identify the\nsites of intermediate-mass binary black hole (IMBBH) mergers if they occur in\nActive Galactic Nuclei (AGN) disks with a gas density $\\rho\\geq10^{-12} \\, {\\rm\ng/cc}$ via measurement of dynamical friction effect in the gravitational\nwaveform. We find that even a single observation of a gravitational wave source\nwith a total mass of $10^3 M_{\\odot}$ and a mass ratio of 2 at a luminosity\ndistance of 3 Gpc is sufficient to confidently associate the merger to be in an\nAGN disk with a density $\\sim 10^{-12} \\, {\\rm g/cc}$, as it allows estimation\nof the density with an error bar $O(100\\%)$. This provides a new way of\ninferring AGN disk densities that complement traditional X-ray observations.\nFurther, we find that neglecting the presence of environmental effects in the\nwaveform models used for parameter estimation can bias the chirp mass, mass\nratio and arrival time of a merger. If not corrected, this can significantly\nimpact our ability to carry out multiband data analysis of IMBBHs that combines\ninformation from LISA and ground-based gravitational wave detectors."
                },
                "authors": [
                    {
                        "name": "Poulami Dutta Roy"
                    },
                    {
                        "name": "Parthapratim Mahapatra"
                    },
                    {
                        "name": "Anuradha Samajdar"
                    },
                    {
                        "name": "K. G. Arun"
                    }
                ],
                "author_detail": {
                    "name": "K. G. Arun"
                },
                "author": "K. G. Arun",
                "arxiv_doi": "10.1103/PhysRevD.111.104047",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1103/PhysRevD.111.104047",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.11721v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.11721v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "9 pages, 2 figures. Matches published version",
                "arxiv_primary_category": {
                    "term": "astro-ph.HE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.HE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "astro-ph.GA",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06899v2",
                "updated": "2025-05-15T14:37:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    37,
                    1,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-10T04:05:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    4,
                    5,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue\n  Corpus"
                },
                "summary": "Video-based dialogue systems, such as education assistants, have compelling\napplication value, thereby garnering growing interest. However, the current\nvideo-based dialogue systems are limited by their reliance on a single dialogue\ntype, which hinders their versatility in practical applications across a range\nof scenarios, including question-answering, emotional dialog, etc. In this\npaper, we identify this challenge as how to generate video-driven multilingual\nmixed-type dialogues. To mitigate this challenge, we propose a novel task and\ncreate a human-to-human video-driven multilingual mixed-type dialogue corpus,\ntermed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues,\nacross 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally,\nwe establish baseline models on KwaiChat. An extensive analysis of 7 distinct\nLLMs on KwaiChat reveals that GPT-4o achieves the best performance but still\ncannot perform well in this situation even with the help of in-context learning\nand fine-tuning, which indicates that the task is not trivial and needs further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based dialogue systems, such as education assistants, have compelling\napplication value, thereby garnering growing interest. However, the current\nvideo-based dialogue systems are limited by their reliance on a single dialogue\ntype, which hinders their versatility in practical applications across a range\nof scenarios, including question-answering, emotional dialog, etc. In this\npaper, we identify this challenge as how to generate video-driven multilingual\nmixed-type dialogues. To mitigate this challenge, we propose a novel task and\ncreate a human-to-human video-driven multilingual mixed-type dialogue corpus,\ntermed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues,\nacross 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally,\nwe establish baseline models on KwaiChat. An extensive analysis of 7 distinct\nLLMs on KwaiChat reveals that GPT-4o achieves the best performance but still\ncannot perform well in this situation even with the help of in-context learning\nand fine-tuning, which indicates that the task is not trivial and needs further\nresearch."
                },
                "authors": [
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Yiming Lei"
                    },
                    {
                        "name": "Chenkai Zhang"
                    },
                    {
                        "name": "Haitao Leng"
                    },
                    {
                        "name": "Chuan Wang"
                    },
                    {
                        "name": "Qingjie Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Shaoguo Liu"
                    },
                    {
                        "name": "Size Li"
                    },
                    {
                        "name": "Yunhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhong Wang"
                },
                "author": "Yunhong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03780v2",
                "updated": "2025-05-15T14:26:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    26,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-30T12:57:21Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    57,
                    21,
                    2,
                    120,
                    0
                ],
                "title": "GPU Performance Portability needs Autotuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU Performance Portability needs Autotuning"
                },
                "summary": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with kernel parameter autotuning to\nenable portable LLM inference with state-of-the-art performance without code\nchanges. Focusing on flash attention -- a widespread performance critical LLM\nkernel -- we demonstrate that this approach explores up to 15x more kernel\nparameter configurations, produces significantly more diverse code across\nmultiple dimensions, and even outperforms vendor-optimized implementations by\nup to 230%, all while reducing kernel code size by 70x and eliminating manual\ncode optimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with kernel parameter autotuning to\nenable portable LLM inference with state-of-the-art performance without code\nchanges. Focusing on flash attention -- a widespread performance critical LLM\nkernel -- we demonstrate that this approach explores up to 15x more kernel\nparameter configurations, produces significantly more diverse code across\nmultiple dimensions, and even outperforms vendor-optimized implementations by\nup to 230%, all while reducing kernel code size by 70x and eliminating manual\ncode optimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors."
                },
                "authors": [
                    {
                        "name": "Burkhard Ringlein"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Radu Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Radu Stoica"
                },
                "author": "Radu Stoica",
                "arxiv_comment": "typos, fix grammatical mistakes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05760v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05760v3",
                "updated": "2025-05-15T14:23:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    23,
                    5,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-23T18:47:14Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    18,
                    47,
                    14,
                    6,
                    54,
                    0
                ],
                "title": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own"
                },
                "summary": "This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io."
                },
                "authors": [
                    {
                        "name": "Gokul Puthumanaillam"
                    },
                    {
                        "name": "Melkior Ornik"
                    }
                ],
                "author_detail": {
                    "name": "Melkior Ornik"
                },
                "author": "Melkior Ornik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05760v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05760v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00777v2",
                "updated": "2025-05-15T14:18:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    18,
                    58,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-01T09:00:10Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    9,
                    0,
                    10,
                    2,
                    1,
                    0
                ],
                "title": "FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation"
                },
                "summary": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Luis Felipe Villa-Arenas"
                    },
                    {
                        "name": "Sebastian Mller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "ACL 2025 Findings; camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13338v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13338v3",
                "updated": "2025-05-15T14:13:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    13,
                    36,
                    3,
                    135,
                    0
                ],
                "published": "2024-09-20T08:57:20Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    8,
                    57,
                    20,
                    4,
                    264,
                    0
                ],
                "title": "Time Awareness in Large Language Models: Benchmarking Fact Recall Across\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Awareness in Large Language Models: Benchmarking Fact Recall Across\n  Time"
                },
                "summary": "Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge."
                },
                "authors": [
                    {
                        "name": "David Herel"
                    },
                    {
                        "name": "Vojtech Bartek"
                    },
                    {
                        "name": "Jiri Jirak"
                    },
                    {
                        "name": "Tomas Mikolov"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Mikolov"
                },
                "author": "Tomas Mikolov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13338v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13338v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19477v3",
                "updated": "2025-05-15T14:06:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    6,
                    27,
                    3,
                    135,
                    0
                ],
                "published": "2024-11-29T05:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models"
                },
                "summary": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10321v1",
                "updated": "2025-05-15T14:06:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    6,
                    0,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:06:00Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    6,
                    0,
                    3,
                    135,
                    0
                ],
                "title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM\n  Agents"
                },
                "summary": "A recent area of increasing research is the use of Large Language Models\n(LLMs) in penetration testing, which promises to reduce costs and thus allow\nfor higher frequency. We conduct a review of related work, identifying best\npractices and common evaluation issues. We then present AutoPentest, an\napplication for performing black-box penetration tests with a high degree of\nautonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent\nframework LangChain. It can perform complex multi-step tasks, augmented by\nexternal tools and knowledge bases. We conduct a study on three\ncapture-the-flag style Hack The Box (HTB) machines, comparing our\nimplementation AutoPentest with the baseline approach of manually using the\nChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the\nsubtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.\nWe measure a total cost of \\$96.20 US when using AutoPentest across all\nexperiments, while a one-month subscription to ChatGPT Plus costs \\$20. The\nresults show that further implementation efforts and the use of more powerful\nLLMs released in the future are likely to make this a viable part of\nvulnerability management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent area of increasing research is the use of Large Language Models\n(LLMs) in penetration testing, which promises to reduce costs and thus allow\nfor higher frequency. We conduct a review of related work, identifying best\npractices and common evaluation issues. We then present AutoPentest, an\napplication for performing black-box penetration tests with a high degree of\nautonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent\nframework LangChain. It can perform complex multi-step tasks, augmented by\nexternal tools and knowledge bases. We conduct a study on three\ncapture-the-flag style Hack The Box (HTB) machines, comparing our\nimplementation AutoPentest with the baseline approach of manually using the\nChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the\nsubtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.\nWe measure a total cost of \\$96.20 US when using AutoPentest across all\nexperiments, while a one-month subscription to ChatGPT Plus costs \\$20. The\nresults show that further implementation efforts and the use of more powerful\nLLMs released in the future are likely to make this a viable part of\nvulnerability management."
                },
                "authors": [
                    {
                        "name": "Julius Henke"
                    }
                ],
                "author_detail": {
                    "name": "Julius Henke"
                },
                "author": "Julius Henke",
                "arxiv_comment": "24 pages, 1 figure, for implementation, see\n  https://github.com/JuliusHenke/autopentest",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10320v1",
                "updated": "2025-05-15T14:05:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    5,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:05:15Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    5,
                    15,
                    3,
                    135,
                    0
                ],
                "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning"
                },
                "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses."
                },
                "authors": [
                    {
                        "name": "Chenxi Whitehouse"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    }
                ],
                "author_detail": {
                    "name": "Swarnadeep Saha"
                },
                "author": "Swarnadeep Saha",
                "arxiv_comment": "10 pages, 8 tables, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09583v2",
                "updated": "2025-05-15T14:03:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    3,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T17:31:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    31,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media"
                },
                "summary": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments."
                },
                "authors": [
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Mingduo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Mingduo Zhao"
                },
                "author": "Mingduo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10315v1",
                "updated": "2025-05-15T14:00:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    0,
                    19,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:00:19Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    0,
                    19,
                    3,
                    135,
                    0
                ],
                "title": "Private Transformer Inference in MLaaS: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Transformer Inference in MLaaS: A Survey"
                },
                "summary": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Yitong Wang"
                    },
                    {
                        "name": "Liangxin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10309v1",
                "updated": "2025-05-15T13:55:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    55,
                    27,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:55:27Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    55,
                    27,
                    3,
                    135,
                    0
                ],
                "title": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments"
                },
                "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge."
                },
                "authors": [
                    {
                        "name": "Tuan Dung Nguyen"
                    },
                    {
                        "name": "Duncan J. Watts"
                    },
                    {
                        "name": "Mark E. Whiting"
                    }
                ],
                "author_detail": {
                    "name": "Mark E. Whiting"
                },
                "author": "Mark E. Whiting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10300v1",
                "updated": "2025-05-15T13:49:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    49,
                    2,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:49:02Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    49,
                    2,
                    3,
                    135,
                    0
                ],
                "title": "AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial\n  Responsible AI Practices during Early Design Stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial\n  Responsible AI Practices during Early Design Stages"
                },
                "summary": "Responsible AI (RAI) efforts increasingly emphasize the importance of\naddressing potential harms early in the AI development lifecycle through\nsocial-technical lenses. However, in cross-functional industry teams, this work\nis often stalled by a persistent knowledge handoff challenge: the difficulty of\ntransferring high-level, early-stage technical design rationales from technical\nexperts to non-technical or user-facing roles for ethical evaluation and harm\nidentification. Through literature review and a co-design study with 8\npractitioners, we unpack how this challenge manifests -- technical design\nchoices are rarely handed off in ways that support meaningful engagement by\nnon-technical roles; collaborative workflows lack shared, visual structures to\nsupport mutual understanding; and non-technical practitioners are left without\nscaffolds for systematic harm evaluation. Existing tools like JIRA or Google\nDocs, while useful for product tracking, are ill-suited for supporting joint\nharm identification across roles, often requiring significant extra effort to\nalign understanding. To address this, we developed AI LEGO, a web-based\nprototype that supports cross-functional AI practitioners in effectively\nfacilitating knowledge handoff and identifying harmful design choices in the\nearly design stages. Technical roles use interactive blocks to draft\ndevelopment plans, while non-technical roles engage with those blocks through\nstage-specific checklists and LLM-driven persona simulations to surface\npotential harms. In a study with 18 cross-functional practitioners, AI LEGO\nincreased the volume and likelihood of harms identified compared to baseline\nworksheets. Participants found that its modular structure and persona prompts\nmade harm identification more accessible, fostering clearer and more\ncollaborative RAI practices in early design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsible AI (RAI) efforts increasingly emphasize the importance of\naddressing potential harms early in the AI development lifecycle through\nsocial-technical lenses. However, in cross-functional industry teams, this work\nis often stalled by a persistent knowledge handoff challenge: the difficulty of\ntransferring high-level, early-stage technical design rationales from technical\nexperts to non-technical or user-facing roles for ethical evaluation and harm\nidentification. Through literature review and a co-design study with 8\npractitioners, we unpack how this challenge manifests -- technical design\nchoices are rarely handed off in ways that support meaningful engagement by\nnon-technical roles; collaborative workflows lack shared, visual structures to\nsupport mutual understanding; and non-technical practitioners are left without\nscaffolds for systematic harm evaluation. Existing tools like JIRA or Google\nDocs, while useful for product tracking, are ill-suited for supporting joint\nharm identification across roles, often requiring significant extra effort to\nalign understanding. To address this, we developed AI LEGO, a web-based\nprototype that supports cross-functional AI practitioners in effectively\nfacilitating knowledge handoff and identifying harmful design choices in the\nearly design stages. Technical roles use interactive blocks to draft\ndevelopment plans, while non-technical roles engage with those blocks through\nstage-specific checklists and LLM-driven persona simulations to surface\npotential harms. In a study with 18 cross-functional practitioners, AI LEGO\nincreased the volume and likelihood of harms identified compared to baseline\nworksheets. Participants found that its modular structure and persona prompts\nmade harm identification more accessible, fostering clearer and more\ncollaborative RAI practices in early design."
                },
                "authors": [
                    {
                        "name": "Muzhe Wu"
                    },
                    {
                        "name": "Yanzhi Zhao"
                    },
                    {
                        "name": "Shuyi Han"
                    },
                    {
                        "name": "Michael Xieyang Liu"
                    },
                    {
                        "name": "Hong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Shen"
                },
                "author": "Hong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10299v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10299v1",
                "updated": "2025-05-15T13:48:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    48,
                    14,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:48:14Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    48,
                    14,
                    3,
                    135,
                    0
                ],
                "title": "Nature-inspired optimization, the Philippine Eagle, and cosmological\n  parameter estimation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Nature-inspired optimization, the Philippine Eagle, and cosmological\n  parameter estimation"
                },
                "summary": "Precise and accurate estimation of cosmological parameters is crucial for\nunderstanding the Universe's dynamics and addressing cosmological tensions. In\nthis methods paper, we explore bio-inspired metaheuristic algorithms, including\nthe Improved Multi-Operator Differential Evolution scheme and the Philippine\nEagle Optimization Algorithm (PEOA), alongside the relatively known genetic\nalgorithm, for cosmological parameter estimation. Using mock data that underlay\na true fiducial cosmology, we test the viability of each optimization method to\nrecover the input cosmological parameters with confidence regions generated by\nbootstrapping on top of optimization. We compare the results with Markov chain\nMonte Carlo (MCMC) in terms of accuracy and precision, and show that PEOA\nperforms comparably well under the specific circumstances provided.\nUnderstandably, Bayesian inference and optimization serve distinct purposes,\nbut comparing them highlights the potential of nature-inspired algorithms in\ncosmological analysis, offering alternative pathways to explore parameter\nspaces and validate standard results.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Precise and accurate estimation of cosmological parameters is crucial for\nunderstanding the Universe's dynamics and addressing cosmological tensions. In\nthis methods paper, we explore bio-inspired metaheuristic algorithms, including\nthe Improved Multi-Operator Differential Evolution scheme and the Philippine\nEagle Optimization Algorithm (PEOA), alongside the relatively known genetic\nalgorithm, for cosmological parameter estimation. Using mock data that underlay\na true fiducial cosmology, we test the viability of each optimization method to\nrecover the input cosmological parameters with confidence regions generated by\nbootstrapping on top of optimization. We compare the results with Markov chain\nMonte Carlo (MCMC) in terms of accuracy and precision, and show that PEOA\nperforms comparably well under the specific circumstances provided.\nUnderstandably, Bayesian inference and optimization serve distinct purposes,\nbut comparing them highlights the potential of nature-inspired algorithms in\ncosmological analysis, offering alternative pathways to explore parameter\nspaces and validate standard results."
                },
                "authors": [
                    {
                        "name": "Reginald Christian Bernardo"
                    },
                    {
                        "name": "Erika Antonette Enriquez"
                    },
                    {
                        "name": "Renier Mendoza"
                    },
                    {
                        "name": "Reinabelle Reyes"
                    },
                    {
                        "name": "Arrianne Crystal Velasco"
                    }
                ],
                "author_detail": {
                    "name": "Arrianne Crystal Velasco"
                },
                "author": "Arrianne Crystal Velasco",
                "arxiv_comment": "24 pages + refs, 13 figures, comments welcome",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10299v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10299v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "astro-ph.CO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "astro-ph.CO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.comp-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13295v2",
                "updated": "2025-05-15T13:42:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    42,
                    18,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-18T21:32:24Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    21,
                    32,
                    24,
                    1,
                    49,
                    0
                ],
                "title": "Demonstrating specification gaming in reasoning models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstrating specification gaming in reasoning models"
                },
                "summary": "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1\nwill often hack the benchmark by default, while language models like GPT-4o and\nClaude 3.5 Sonnet need to be told that normal play won't work to hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1\nwill often hack the benchmark by default, while language models like GPT-4o and\nClaude 3.5 Sonnet need to be told that normal play won't work to hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing."
                },
                "authors": [
                    {
                        "name": "Alexander Bondarenko"
                    },
                    {
                        "name": "Denis Volk"
                    },
                    {
                        "name": "Dmitrii Volkov"
                    },
                    {
                        "name": "Jeffrey Ladish"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Ladish"
                },
                "author": "Jeffrey Ladish",
                "arxiv_comment": "Updated with o3 results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10282v1",
                "updated": "2025-05-15T13:30:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    30,
                    39,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:30:39Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    30,
                    39,
                    3,
                    135,
                    0
                ],
                "title": "From Questions to Clinical Recommendations: Large Language Models\n  Driving Evidence-Based Clinical Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Questions to Clinical Recommendations: Large Language Models\n  Driving Evidence-Based Clinical Decision Making"
                },
                "summary": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions."
                },
                "authors": [
                    {
                        "name": "Dubai Li"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Kangping Huang"
                    },
                    {
                        "name": "Ruiqi Tu"
                    },
                    {
                        "name": "Shuyu Ouyang"
                    },
                    {
                        "name": "Huayu Yu"
                    },
                    {
                        "name": "Lin Qiao"
                    },
                    {
                        "name": "Chen Yu"
                    },
                    {
                        "name": "Tianshu Zhou"
                    },
                    {
                        "name": "Danyang Tong"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Mengtao Li"
                    },
                    {
                        "name": "Xiaofeng Zeng"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Xinping Tian"
                    },
                    {
                        "name": "Jingsong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jingsong Li"
                },
                "author": "Jingsong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04877v2",
                "updated": "2025-05-15T13:27:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    27,
                    26,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-07T09:41:04Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    41,
                    4,
                    0,
                    97,
                    0
                ],
                "title": "System Log Parsing with Large Language Models: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System Log Parsing with Large Language Models: A Review"
                },
                "summary": "Log data provides crucial insights for tasks like monitoring, root cause\nanalysis, and anomaly detection. Due to the vast volume of logs, automated log\nparsing is essential to transform semi-structured log messages into structured\nrepresentations. Recent advances in large language models (LLMs) have\nintroduced the new research field of LLM-based log parsing. Despite promising\nresults, there is no structured overview of the approaches in this relatively\nnew research field with the earliest advances published in late 2023. This work\nsystematically reviews 29 LLM-based log parsing methods. We benchmark seven of\nthem on public datasets and critically assess their comparability and the\nreproducibility of their reported results. Our findings summarize the advances\nof this new research field, with insights on how to report results, which data\nsets, metrics and which terminology to use, and which inconsistencies to avoid,\nwith code and results made publicly available for transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log data provides crucial insights for tasks like monitoring, root cause\nanalysis, and anomaly detection. Due to the vast volume of logs, automated log\nparsing is essential to transform semi-structured log messages into structured\nrepresentations. Recent advances in large language models (LLMs) have\nintroduced the new research field of LLM-based log parsing. Despite promising\nresults, there is no structured overview of the approaches in this relatively\nnew research field with the earliest advances published in late 2023. This work\nsystematically reviews 29 LLM-based log parsing methods. We benchmark seven of\nthem on public datasets and critically assess their comparability and the\nreproducibility of their reported results. Our findings summarize the advances\nof this new research field, with insights on how to report results, which data\nsets, metrics and which terminology to use, and which inconsistencies to avoid,\nwith code and results made publicly available for transparency."
                },
                "authors": [
                    {
                        "name": "Viktor Beck"
                    },
                    {
                        "name": "Max Landauer"
                    },
                    {
                        "name": "Markus Wurzenberger"
                    },
                    {
                        "name": "Florian Skopik"
                    },
                    {
                        "name": "Andreas Rauber"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rauber"
                },
                "author": "Andreas Rauber",
                "arxiv_comment": "36 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10278v1",
                "updated": "2025-05-15T13:27:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    27,
                    18,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:27:18Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    27,
                    18,
                    3,
                    135,
                    0
                ],
                "title": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction"
                },
                "summary": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS."
                },
                "authors": [
                    {
                        "name": "Taian Guo"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Jinsheng Huang"
                    },
                    {
                        "name": "Zhengyang Mao"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Zhuoru Chen"
                    },
                    {
                        "name": "Xuhui Liu"
                    },
                    {
                        "name": "Bingyu Xia"
                    },
                    {
                        "name": "Luchen Liu"
                    },
                    {
                        "name": "Yun Ma"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10271v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10271v1",
                "updated": "2025-05-15T13:22:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    22,
                    20,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:22:20Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    22,
                    20,
                    3,
                    135,
                    0
                ],
                "title": "RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall\n  Probabilities Over 8 Hours",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RainPro-8: An Efficient Deep Learning Model to Estimate Rainfall\n  Probabilities Over 8 Hours"
                },
                "summary": "We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We present a deep learning model for high-resolution probabilistic\nprecipitation forecasting over an 8-hour horizon in Europe, overcoming the\nlimitations of radar-only deep learning models with short forecast lead times.\nOur model efficiently integrates multiple data sources - including radar,\nsatellite, and physics-based numerical weather prediction (NWP) - while\ncapturing long-range interactions, resulting in accurate forecasts with robust\nuncertainty quantification through consistent probabilistic maps. Featuring a\ncompact architecture, it enables more efficient training and faster inference\nthan existing models. Extensive experiments demonstrate that our model\nsurpasses current operational NWP systems, extrapolation-based methods, and\ndeep-learning nowcasting models, setting a new standard for high-resolution\nprecipitation forecasting in Europe, ensuring a balance between accuracy,\ninterpretability, and computational efficiency."
                },
                "authors": [
                    {
                        "name": "Rafael Pablos Sarabia"
                    },
                    {
                        "name": "Joachim Nyborg"
                    },
                    {
                        "name": "Morten Birk"
                    },
                    {
                        "name": "Jeppe Liborius Sjrup"
                    },
                    {
                        "name": "Anders Lillevang Vesterholt"
                    },
                    {
                        "name": "Ira Assent"
                    }
                ],
                "author_detail": {
                    "name": "Ira Assent"
                },
                "author": "Ira Assent",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10271v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10271v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10270v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10270v1",
                "updated": "2025-05-15T13:21:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    21,
                    49,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:21:49Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    21,
                    49,
                    3,
                    135,
                    0
                ],
                "title": "Inferring activity from the flow field around active colloidal particles\n  using deep learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring activity from the flow field around active colloidal particles\n  using deep learning"
                },
                "summary": "Active colloidal particles create flow around them due to non-equilibrium\nprocess on their surfaces. In this paper, we infer the activity of such\ncolloidal particles from the flow field created by them via deep learning. We\nfirst explain our method for one active particle, inferring the $2s$ mode (or\nthe stresslet) and the $3t$ mode (or the source dipole) from the flow field\ndata, along with the position and orientation of the particle. We then apply\nthe method to a system of many active particles. We find excellent agreements\nbetween the predictions and the true values of activity. Our method presents a\nprincipled way to predict arbitrary activity from the flow field created by\nactive particles.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Active colloidal particles create flow around them due to non-equilibrium\nprocess on their surfaces. In this paper, we infer the activity of such\ncolloidal particles from the flow field created by them via deep learning. We\nfirst explain our method for one active particle, inferring the $2s$ mode (or\nthe stresslet) and the $3t$ mode (or the source dipole) from the flow field\ndata, along with the position and orientation of the particle. We then apply\nthe method to a system of many active particles. We find excellent agreements\nbetween the predictions and the true values of activity. Our method presents a\nprincipled way to predict arbitrary activity from the flow field created by\nactive particles."
                },
                "authors": [
                    {
                        "name": "Aditya Mohapatra"
                    },
                    {
                        "name": "Aditya Kumar"
                    },
                    {
                        "name": "Mayurakshi Deb"
                    },
                    {
                        "name": "Siddharth Dhomkar"
                    },
                    {
                        "name": "Rajesh Singh"
                    }
                ],
                "author_detail": {
                    "name": "Rajesh Singh"
                },
                "author": "Rajesh Singh",
                "arxiv_comment": "10 pages, 4 Figures, and 1 Algorithm",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10270v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10270v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.flu-dyn",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10261v1",
                "updated": "2025-05-15T13:11:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    11,
                    14,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:11:14Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    11,
                    14,
                    3,
                    135,
                    0
                ],
                "title": "The Evolving Landscape of Generative Large Language Models and\n  Traditional Natural Language Processing in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Evolving Landscape of Generative Large Language Models and\n  Traditional Natural Language Processing in Medicine"
                },
                "summary": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Huitao Li"
                    },
                    {
                        "name": "Matthew Yu Heng Wong"
                    },
                    {
                        "name": "Yuhe Ke"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Kunyu Yu"
                    },
                    {
                        "name": "Jingchi Liao"
                    },
                    {
                        "name": "Jonathan Chong Kai Liew"
                    },
                    {
                        "name": "Sabarinath Vinod Nair"
                    },
                    {
                        "name": "Jasmine Chiat Ling Ong"
                    },
                    {
                        "name": "Irene Li"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Chuan Hong"
                    },
                    {
                        "name": "Daniel Shu Wei Ting"
                    },
                    {
                        "name": "Nan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Liu"
                },
                "author": "Nan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10260v1",
                "updated": "2025-05-15T13:10:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    47,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:10:47Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    47,
                    3,
                    135,
                    0
                ],
                "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations\n  in Social Media Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations\n  in Social Media Data"
                },
                "summary": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Poli Apollinaire Nemkova"
                    },
                    {
                        "name": "Solomon Ubani"
                    },
                    {
                        "name": "Mark V. Albert"
                    }
                ],
                "author_detail": {
                    "name": "Mark V. Albert"
                },
                "author": "Mark V. Albert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10259v1",
                "updated": "2025-05-15T13:10:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    31,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:10:31Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    31,
                    3,
                    135,
                    0
                ],
                "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices"
                },
                "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload ."
                },
                "authors": [
                    {
                        "name": "Xiangwen Zhuge"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Fan Dang"
                    },
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Yahui Han"
                    },
                    {
                        "name": "Tianxiang Hao"
                    },
                    {
                        "name": "Zheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yang"
                },
                "author": "Zheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10258v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10258v1",
                "updated": "2025-05-15T13:09:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    9,
                    19,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:09:19Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    9,
                    19,
                    3,
                    135,
                    0
                ],
                "title": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Inferring Driving Maps by Deep Learning-based Trail Map Extraction"
                },
                "summary": "High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "High-definition (HD) maps offer extensive and accurate environmental\ninformation about the driving scene, making them a crucial and essential\nelement for planning within autonomous driving systems. To avoid extensive\nefforts from manual labeling, methods for automating the map creation have\nemerged. Recent trends have moved from offline mapping to online mapping,\nensuring availability and actuality of the utilized maps. While the performance\nhas increased in recent years, online mapping still faces challenges regarding\ntemporal consistency, sensor occlusion, runtime, and generalization. We propose\na novel offline mapping approach that integrates trails - informal routes used\nby drivers - into the map creation process. Our method aggregates trail data\nfrom the ego vehicle and other traffic participants to construct a\ncomprehensive global map using transformer-based deep learning models. Unlike\ntraditional offline mapping, our approach enables continuous updates while\nremaining sensor-agnostic, facilitating efficient data transfer. Our method\ndemonstrates superior performance compared to state-of-the-art online mapping\napproaches, achieving improved generalization to previously unseen environments\nand sensor configurations. We validate our approach on two benchmark datasets,\nhighlighting its robustness and applicability in autonomous driving systems."
                },
                "authors": [
                    {
                        "name": "Michael Hubbertz"
                    },
                    {
                        "name": "Pascal Colling"
                    },
                    {
                        "name": "Qi Han"
                    },
                    {
                        "name": "Tobias Meisen"
                    }
                ],
                "author_detail": {
                    "name": "Tobias Meisen"
                },
                "author": "Tobias Meisen",
                "arxiv_comment": "This paper was accepted at the CVPR WAD 2025 Workshop",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10258v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10258v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06085v2",
                "updated": "2025-05-15T13:07:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    7,
                    31,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-09T14:29:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    29,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities"
                },
                "summary": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16."
                },
                "authors": [
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Daniele Cesarini"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.10769v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.10769v2",
                "updated": "2025-05-15T12:54:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    54,
                    41,
                    3,
                    135,
                    0
                ],
                "published": "2024-05-17T13:28:55Z",
                "published_parsed": [
                    2024,
                    5,
                    17,
                    13,
                    28,
                    55,
                    4,
                    138,
                    0
                ],
                "title": "Efficient estimation of the target population average treatment effect\n  from multi-source data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient estimation of the target population average treatment effect\n  from multi-source data"
                },
                "summary": "We consider estimation of the target population average treatment effect\n(TATE) when outcome information is unavailable. Instead, we observe the outcome\nin multiple source populations and wish to combine the treatment effects\ntherein to make inference on the TATE. In contrast to existing works that\nassume transportability on the conditional distribution of potential outcomes\nor conditional treatment-specific means, we work under a weaker form of effect\ntransportability. Following the framework for causally interpretable\nmeta-analysis, we assume transportability of conditional average treatment\neffects across multiple populations, which may hold with fewer standardization\nvariables. Under this assumption, we derive the semiparametric efficiency bound\nof the TATE and characterize a class of doubly robust and asymptotically linear\nestimators. Within this class, an efficient estimator assigns optimal weights\nto observations from different data sources. Additionally, we suggest\nestimators of a low-dimensional summary of effect heterogeneity in the target\npopulation. We illustrate the use of the proposed estimators on a multicentre\nweight management clinical trial for semaglutide, a glucagon-like peptide-1\nreceptor agonist, on overweight or obese patients. Using outcome information\nfrom other regions, we estimate the weight loss effect of semaglutide in the\nUnited States subgroup.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We consider estimation of the target population average treatment effect\n(TATE) when outcome information is unavailable. Instead, we observe the outcome\nin multiple source populations and wish to combine the treatment effects\ntherein to make inference on the TATE. In contrast to existing works that\nassume transportability on the conditional distribution of potential outcomes\nor conditional treatment-specific means, we work under a weaker form of effect\ntransportability. Following the framework for causally interpretable\nmeta-analysis, we assume transportability of conditional average treatment\neffects across multiple populations, which may hold with fewer standardization\nvariables. Under this assumption, we derive the semiparametric efficiency bound\nof the TATE and characterize a class of doubly robust and asymptotically linear\nestimators. Within this class, an efficient estimator assigns optimal weights\nto observations from different data sources. Additionally, we suggest\nestimators of a low-dimensional summary of effect heterogeneity in the target\npopulation. We illustrate the use of the proposed estimators on a multicentre\nweight management clinical trial for semaglutide, a glucagon-like peptide-1\nreceptor agonist, on overweight or obese patients. Using outcome information\nfrom other regions, we estimate the weight loss effect of semaglutide in the\nUnited States subgroup."
                },
                "authors": [
                    {
                        "name": "Zehao Su"
                    },
                    {
                        "name": "Helene Charlotte Rytgaard"
                    },
                    {
                        "name": "Henrik Ravn"
                    },
                    {
                        "name": "Frank Eriksson"
                    }
                ],
                "author_detail": {
                    "name": "Frank Eriksson"
                },
                "author": "Frank Eriksson",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.10769v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.10769v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "stat.ME",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "stat.ME",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15674v2",
                "updated": "2025-05-15T12:42:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    42,
                    44,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-26T21:05:16Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    21,
                    5,
                    16,
                    6,
                    26,
                    0
                ],
                "title": "TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and\n  Compression in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and\n  Compression in LLMs"
                },
                "summary": "The reasoning abilities of Large Language Models (LLMs) can be improved by\nstructurally denoising their weights, yet existing techniques primarily focus\non denoising the feed-forward network (FFN) of the transformer block, and can\nnot efficiently utilise the Multi-head Attention (MHA) block, which is the core\nof transformer architectures. To address this issue, we propose a novel\nintuitive framework that, at its very core, performs MHA compression through a\nmulti-head tensorisation process and the Tucker decomposition. This enables\nboth higher-dimensional structured denoising and compression of the MHA\nweights, by enforcing a shared higher-dimensional subspace across the weights\nof the multiple attention heads. We demonstrate that this approach consistently\nenhances the reasoning capabilities of LLMs across multiple benchmark datasets,\nand for both encoder-only and decoder-only architectures, while achieving\ncompression rates of up to $\\sim 250$ times in the MHA weights, all without\nrequiring any additional data, training, or fine-tuning. Furthermore, we show\nthat the proposed method can be seamlessly combined with existing\nFFN-only-based denoising techniques to achieve further improvements in LLM\nreasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of Large Language Models (LLMs) can be improved by\nstructurally denoising their weights, yet existing techniques primarily focus\non denoising the feed-forward network (FFN) of the transformer block, and can\nnot efficiently utilise the Multi-head Attention (MHA) block, which is the core\nof transformer architectures. To address this issue, we propose a novel\nintuitive framework that, at its very core, performs MHA compression through a\nmulti-head tensorisation process and the Tucker decomposition. This enables\nboth higher-dimensional structured denoising and compression of the MHA\nweights, by enforcing a shared higher-dimensional subspace across the weights\nof the multiple attention heads. We demonstrate that this approach consistently\nenhances the reasoning capabilities of LLMs across multiple benchmark datasets,\nand for both encoder-only and decoder-only architectures, while achieving\ncompression rates of up to $\\sim 250$ times in the MHA weights, all without\nrequiring any additional data, training, or fine-tuning. Furthermore, we show\nthat the proposed method can be seamlessly combined with existing\nFFN-only-based denoising techniques to achieve further improvements in LLM\nreasoning performance."
                },
                "authors": [
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "arxiv_comment": "Accpeted for IEEE International Joint Conference on Neural Networks\n  (IJCNN 2025). The code is available at https://github.com/guyuxuan9/TensorLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10226v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10226v1",
                "updated": "2025-05-15T12:33:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    33,
                    54,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:33:54Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    33,
                    54,
                    3,
                    135,
                    0
                ],
                "title": "Solar-CSK: Decoding Color Coded Visible Light Communications using Solar\n  Cells",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Solar-CSK: Decoding Color Coded Visible Light Communications using Solar\n  Cells"
                },
                "summary": "Visible Light Communication (VLC) provides an energy-efficient wireless\nsolution by using existing LED-based illumination for high-speed data\ntransmissions. Although solar cells offer the advantage of simultaneous energy\nharvesting and data reception, their broadband nature hinders accurate decoding\nof color-coded signals like Color Shift Keying (CSK). In this paper, we propose\na novel approach exploiting the concept of tandem solar cells, multi-layer\ndevices with partial wavelength selectivity, to capture coarse color\ninformation without resorting to energy-limiting color filters. To address the\nresidual spectral overlap, we develop a bidirectional LSTM-based machine\nlearning framework that infers channel characteristics by comparing solar\ncells' photovoltaic signals with pilot-based anchor data. Our commercial\noff-the-shelf (COTS) solar prototype achieves robust performance across varying\ndistances and ambient lighting levels, significantly reducing bit error rates\ncompared to conventional channel estimation methods. These findings mark a step\ntoward sustainable, high-performance VLC systems powered by the multi-layer\nsolar technologies.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visible Light Communication (VLC) provides an energy-efficient wireless\nsolution by using existing LED-based illumination for high-speed data\ntransmissions. Although solar cells offer the advantage of simultaneous energy\nharvesting and data reception, their broadband nature hinders accurate decoding\nof color-coded signals like Color Shift Keying (CSK). In this paper, we propose\na novel approach exploiting the concept of tandem solar cells, multi-layer\ndevices with partial wavelength selectivity, to capture coarse color\ninformation without resorting to energy-limiting color filters. To address the\nresidual spectral overlap, we develop a bidirectional LSTM-based machine\nlearning framework that infers channel characteristics by comparing solar\ncells' photovoltaic signals with pilot-based anchor data. Our commercial\noff-the-shelf (COTS) solar prototype achieves robust performance across varying\ndistances and ambient lighting levels, significantly reducing bit error rates\ncompared to conventional channel estimation methods. These findings mark a step\ntoward sustainable, high-performance VLC systems powered by the multi-layer\nsolar technologies."
                },
                "authors": [
                    {
                        "name": "Yanxiang Wang"
                    },
                    {
                        "name": "Yihe Yan"
                    },
                    {
                        "name": "Jiawei Hu"
                    },
                    {
                        "name": "Cheng Jiang"
                    },
                    {
                        "name": "Brano Kusy"
                    },
                    {
                        "name": "Ashraf Uddin"
                    },
                    {
                        "name": "Mahbub Hassan"
                    },
                    {
                        "name": "Wen Hu"
                    }
                ],
                "author_detail": {
                    "name": "Wen Hu"
                },
                "author": "Wen Hu",
                "arxiv_comment": "14 pages, 25 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10226v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10226v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10222v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10222v1",
                "updated": "2025-05-15T12:30:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    30,
                    33,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:30:33Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    30,
                    33,
                    3,
                    135,
                    0
                ],
                "title": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via\n  Head-Specific Complex Vector Attention",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ComplexFormer: Disruptively Advancing Transformer Inference Ability via\n  Head-Specific Complex Vector Attention"
                },
                "summary": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models rely on self-attention to capture token dependencies but\nface challenges in effectively integrating positional information while\nallowing multi-head attention (MHA) flexibility. Prior methods often model\nsemantic and positional differences disparately or apply uniform positional\nadjustments across heads, potentially limiting representational capacity. This\npaper introduces ComplexFormer, featuring Complex Multi-Head Attention-CMHA.\nCMHA empowers each head to independently model semantic and positional\ndifferences unified within the complex plane, representing interactions as\nrotations and scaling. ComplexFormer incorporates two key improvements: (1) a\nper-head Euler transformation, converting real-valued query/key projections\ninto polar-form complex vectors for head-specific complex subspace operation;\nand (2) a per-head adaptive differential rotation mechanism,\nexp[i(Adapt(ASmn,i) + Delta(Pmn),i)], allowing each head to learn distinct\nstrategies for integrating semantic angle differences (ASmn,i) with relative\npositional encodings (Delta(Pmn),i). Extensive experiments on language\nmodeling, text generation, code generation, and mathematical reasoning show\nComplexFormer achieves superior performance, significantly lower generation\nperplexity , and improved long-context coherence compared to strong baselines\nlike RoPE-Transformers. ComplexFormer demonstrates strong parameter efficiency,\noffering a more expressive, adaptable attention mechanism."
                },
                "authors": [
                    {
                        "name": "Jintian Shao"
                    },
                    {
                        "name": "Hongyi Huang"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "Beiwen Zhang"
                    },
                    {
                        "name": "ZhiYu Wu"
                    },
                    {
                        "name": "You Shan"
                    },
                    {
                        "name": "MingKai Zheng"
                    }
                ],
                "author_detail": {
                    "name": "MingKai Zheng"
                },
                "author": "MingKai Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10222v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10222v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.15577v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.15577v3",
                "updated": "2025-05-15T12:24:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    24,
                    34,
                    3,
                    135,
                    0
                ],
                "published": "2024-06-21T18:25:12Z",
                "published_parsed": [
                    2024,
                    6,
                    21,
                    18,
                    25,
                    12,
                    4,
                    173,
                    0
                ],
                "title": "Decoherence of Histories: Chaotic Versus Integrable Systems",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decoherence of Histories: Chaotic Versus Integrable Systems"
                },
                "summary": "We study the emergence of decoherent histories in isolated systems based on\nexact numerical integration of the Schr\\\"odinger equation for a Heisenberg\nchain. We reveal that the nature of the system, which we switch from (i)\nchaotic to (ii) interacting integrable to (iii) non-interacting integrable,\nstrongly impacts decoherence \\new{of coarse spin observables}. From a finite\nsize scaling law we infer a strong exponential suppression of coherences for\n(i), a weak exponential suppression for (ii) and no exponential suppression for\n(iii) on a relevant short (nonequilibrium) time scale. Moreover, for longer\ntimes we find stronger decoherence for (i) but the opposite for (ii), hinting\neven at a possible power-law decay for (ii) at equilibrium time scales. This\nbehaviour is encoded in the multi-time properties of the quantum histories and\nit can not be explained by environmentally induced decoherence. Our results\nsuggest that chaoticity plays a crucial role in the emergence of classicality\nin finite size systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We study the emergence of decoherent histories in isolated systems based on\nexact numerical integration of the Schr\\\"odinger equation for a Heisenberg\nchain. We reveal that the nature of the system, which we switch from (i)\nchaotic to (ii) interacting integrable to (iii) non-interacting integrable,\nstrongly impacts decoherence \\new{of coarse spin observables}. From a finite\nsize scaling law we infer a strong exponential suppression of coherences for\n(i), a weak exponential suppression for (ii) and no exponential suppression for\n(iii) on a relevant short (nonequilibrium) time scale. Moreover, for longer\ntimes we find stronger decoherence for (i) but the opposite for (ii), hinting\neven at a possible power-law decay for (ii) at equilibrium time scales. This\nbehaviour is encoded in the multi-time properties of the quantum histories and\nit can not be explained by environmentally induced decoherence. Our results\nsuggest that chaoticity plays a crucial role in the emergence of classicality\nin finite size systems."
                },
                "authors": [
                    {
                        "name": "Jiaozi Wang"
                    },
                    {
                        "name": "Philipp Strasberg"
                    }
                ],
                "author_detail": {
                    "name": "Philipp Strasberg"
                },
                "author": "Philipp Strasberg",
                "arxiv_comment": "14 pages, 22 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.15577v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.15577v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.stat-mech",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.stat-mech",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "gr-qc",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10218v1",
                "updated": "2025-05-15T12:22:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    22,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:22:10Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    22,
                    10,
                    3,
                    135,
                    0
                ],
                "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable\n  Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable\n  Reward"
                },
                "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs."
                },
                "authors": [
                    {
                        "name": "Zongsheng Wang"
                    },
                    {
                        "name": "Kaili Sun"
                    },
                    {
                        "name": "Bowen Wu"
                    },
                    {
                        "name": "Qun Yu"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Baoxun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baoxun Wang"
                },
                "author": "Baoxun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10213v1",
                "updated": "2025-05-15T12:17:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    17,
                    52,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:17:52Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    17,
                    52,
                    3,
                    135,
                    0
                ],
                "title": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM\n  Performance on Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM\n  Performance on Time Series Forecasting"
                },
                "summary": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks."
                },
                "authors": [
                    {
                        "name": "Mohammadmahdi Ghasemloo"
                    },
                    {
                        "name": "Alireza Moradi"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Moradi"
                },
                "author": "Alireza Moradi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10212v1",
                "updated": "2025-05-15T12:16:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    16,
                    36,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:16:36Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    16,
                    36,
                    3,
                    135,
                    0
                ],
                "title": "Do LLMs Memorize Recommendation Datasets? A Preliminary Study on\n  MovieLens-1M",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Memorize Recommendation Datasets? A Preliminary Study on\n  MovieLens-1M"
                },
                "summary": "Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector"
                },
                "authors": [
                    {
                        "name": "Dario Di Palma"
                    },
                    {
                        "name": "Felice Antonio Merra"
                    },
                    {
                        "name": "Maurizio Sfilio"
                    },
                    {
                        "name": "Vito Walter Anelli"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Di Noia"
                },
                "author": "Tommaso Di Noia",
                "arxiv_doi": "10.1145/3726302.3730178",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730178",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.10212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10325v2",
                "updated": "2025-05-15T12:02:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    2,
                    56,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-13T13:03:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    3,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "Collaborative Speculative Inference for Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Speculative Inference for Efficient LLM Inference Serving"
                },
                "summary": "Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Luyao Gao"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xichong Zhang"
                    },
                    {
                        "name": "Yunming Liao"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10202v1",
                "updated": "2025-05-15T11:58:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    58,
                    4,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T11:58:04Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    58,
                    4,
                    3,
                    135,
                    0
                ],
                "title": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models\n  via Vector Quantized Logits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models\n  via Vector Quantized Logits"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Jintian Shao"
                    },
                    {
                        "name": "Hongyi Huang"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "YiMing Cheng"
                    },
                    {
                        "name": "ZhiYu Wu"
                    },
                    {
                        "name": "You Shan"
                    },
                    {
                        "name": "MingKai Zheng"
                    }
                ],
                "author_detail": {
                    "name": "MingKai Zheng"
                },
                "author": "MingKai Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10186v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10186v1",
                "updated": "2025-05-15T11:37:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    37,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T11:37:15Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    37,
                    15,
                    3,
                    135,
                    0
                ],
                "title": "Closure and Complexity of Temporal Causality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Closure and Complexity of Temporal Causality"
                },
                "summary": "Temporal causality defines what property causes some observed temporal\nbehavior (the effect) in a given computation, based on a counterfactual\nanalysis of similar computations. In this paper, we study its closure\nproperties and the complexity of computing causes. For the former, we establish\nthat safety, reachability, and recurrence properties are all closed under\ncausal inference: If the effect is from one of these property classes, then the\ncause for this effect is from the same class. We also show that persistence and\nobligation properties are not closed in this way. These results rest on a\ntopological characterization of causes which makes them applicable to a wide\nrange of similarity relations between computations. Finally, our complexity\nanalysis establishes improved upper bounds for computing causes for safety,\nreachability, and recurrence properties. We also present the first lower bounds\nfor all of the classes.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Temporal causality defines what property causes some observed temporal\nbehavior (the effect) in a given computation, based on a counterfactual\nanalysis of similar computations. In this paper, we study its closure\nproperties and the complexity of computing causes. For the former, we establish\nthat safety, reachability, and recurrence properties are all closed under\ncausal inference: If the effect is from one of these property classes, then the\ncause for this effect is from the same class. We also show that persistence and\nobligation properties are not closed in this way. These results rest on a\ntopological characterization of causes which makes them applicable to a wide\nrange of similarity relations between computations. Finally, our complexity\nanalysis establishes improved upper bounds for computing causes for safety,\nreachability, and recurrence properties. We also present the first lower bounds\nfor all of the classes."
                },
                "authors": [
                    {
                        "name": "Mishel Carelli"
                    },
                    {
                        "name": "Bernd Finkbeiner"
                    },
                    {
                        "name": "Julian Siber"
                    }
                ],
                "author_detail": {
                    "name": "Julian Siber"
                },
                "author": "Julian Siber",
                "arxiv_comment": "Fortieth Annual ACM/IEEE Symposium on Logic in Computer Science (LICS\n  2025)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10186v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10186v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10182v1",
                "updated": "2025-05-15T11:29:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    29,
                    1,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T11:29:01Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    29,
                    1,
                    3,
                    135,
                    0
                ],
                "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with\n  Synthetic Data for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with\n  Synthetic Data for LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty."
                },
                "authors": [
                    {
                        "name": "Yoichi Ishibashi"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.00646v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.00646v2",
                "updated": "2025-05-15T11:25:54Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    25,
                    54,
                    3,
                    135,
                    0
                ],
                "published": "2024-11-01T15:04:37Z",
                "published_parsed": [
                    2024,
                    11,
                    1,
                    15,
                    4,
                    37,
                    4,
                    306,
                    0
                ],
                "title": "Phase Diagram of Vision Large Language Models Inference: A Perspective\n  from Interaction across Image and Instruction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Phase Diagram of Vision Large Language Models Inference: A Perspective\n  from Interaction across Image and Instruction"
                },
                "summary": "Vision Large Language Models (VLLMs) usually take input as a concatenation of\nimage token embeddings and text token embeddings and conduct causal modeling.\nHowever, their internal behaviors remain underexplored, raising the question of\ninteraction among two types of tokens. To investigate such multimodal\ninteraction during model inference, in this paper, we measure the\ncontextualization among the hidden state vectors of tokens from different\nmodalities. Our experiments uncover a four-phase inference dynamics of VLLMs\nagainst the depth of Transformer-based LMs, including (I) Alignment: In very\nearly layers, contextualization emerges between modalities, suggesting a\nfeature space alignment. (II) Intra-modal Encoding: In early layers,\nintra-modal contextualization is enhanced while inter-modal interaction is\nsuppressed, suggesting a local encoding within modalities. (III) Inter-modal\nEncoding: In later layers, contextualization across modalities is enhanced,\nsuggesting a deeper fusion across modalities. (IV) Output Preparation: In very\nlate layers, contextualization is reduced globally, and hidden states are\naligned towards the unembedding space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Vision Large Language Models (VLLMs) usually take input as a concatenation of\nimage token embeddings and text token embeddings and conduct causal modeling.\nHowever, their internal behaviors remain underexplored, raising the question of\ninteraction among two types of tokens. To investigate such multimodal\ninteraction during model inference, in this paper, we measure the\ncontextualization among the hidden state vectors of tokens from different\nmodalities. Our experiments uncover a four-phase inference dynamics of VLLMs\nagainst the depth of Transformer-based LMs, including (I) Alignment: In very\nearly layers, contextualization emerges between modalities, suggesting a\nfeature space alignment. (II) Intra-modal Encoding: In early layers,\nintra-modal contextualization is enhanced while inter-modal interaction is\nsuppressed, suggesting a local encoding within modalities. (III) Inter-modal\nEncoding: In later layers, contextualization across modalities is enhanced,\nsuggesting a deeper fusion across modalities. (IV) Output Preparation: In very\nlate layers, contextualization is reduced globally, and hidden states are\naligned towards the unembedding space."
                },
                "authors": [
                    {
                        "name": "Houjing Wei"
                    },
                    {
                        "name": "Yuting Shi"
                    },
                    {
                        "name": "Naoya Inoue"
                    }
                ],
                "author_detail": {
                    "name": "Naoya Inoue"
                },
                "author": "Naoya Inoue",
                "arxiv_comment": "6 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.00646v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.00646v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01797v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01797v3",
                "updated": "2025-05-15T11:25:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    25,
                    13,
                    3,
                    135,
                    0
                ],
                "published": "2023-12-04T10:37:58Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    10,
                    37,
                    58,
                    0,
                    338,
                    0
                ],
                "title": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for\n  Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for\n  Robotics"
                },
                "summary": "This research focuses on how Large Language Models (LLMs) can help with\n(path) planning for mobile embodied agents such as robots, in a\nhuman-in-the-loop and interactive manner. A novel framework named LLM A*, aims\nto leverage the commonsense of LLMs, and the utility-optimal A* is proposed to\nfacilitate few-shot near-optimal path planning. Prompts are used for two main\npurposes: 1) to provide LLMs with essential information like environments,\ncosts, heuristics, etc.; 2) to communicate human feedback on intermediate\nplanning results to LLMs. This approach takes human feedback on board and\nrenders the entire planning process transparent (akin to a `white box') to\nhumans. Moreover, it facilitates code-free path planning, thereby fostering the\naccessibility and inclusiveness of artificial intelligence techniques to\ncommunities less proficient in coding. Comparative analysis against A* and RL\ndemonstrates that LLM A* exhibits greater efficiency in terms of search space\nand achieves paths comparable to A* while outperforming RL. The interactive\nnature of LLM A* also makes it a promising tool for deployment in collaborative\nhuman-robot tasks. Codes and Supplemental Materials can be found at GitHub:\nhttps://github.com/speedhawk/LLM-A-.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research focuses on how Large Language Models (LLMs) can help with\n(path) planning for mobile embodied agents such as robots, in a\nhuman-in-the-loop and interactive manner. A novel framework named LLM A*, aims\nto leverage the commonsense of LLMs, and the utility-optimal A* is proposed to\nfacilitate few-shot near-optimal path planning. Prompts are used for two main\npurposes: 1) to provide LLMs with essential information like environments,\ncosts, heuristics, etc.; 2) to communicate human feedback on intermediate\nplanning results to LLMs. This approach takes human feedback on board and\nrenders the entire planning process transparent (akin to a `white box') to\nhumans. Moreover, it facilitates code-free path planning, thereby fostering the\naccessibility and inclusiveness of artificial intelligence techniques to\ncommunities less proficient in coding. Comparative analysis against A* and RL\ndemonstrates that LLM A* exhibits greater efficiency in terms of search space\nand achieves paths comparable to A* while outperforming RL. The interactive\nnature of LLM A* also makes it a promising tool for deployment in collaborative\nhuman-robot tasks. Codes and Supplemental Materials can be found at GitHub:\nhttps://github.com/speedhawk/LLM-A-."
                },
                "authors": [
                    {
                        "name": "Hengjia Xiao"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Mingzhe Yu"
                    },
                    {
                        "name": "Mattia Robbiani"
                    }
                ],
                "author_detail": {
                    "name": "Mattia Robbiani"
                },
                "author": "Mattia Robbiani",
                "arxiv_comment": "7 figures, 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01797v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01797v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.14191v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.14191v3",
                "updated": "2025-05-15T11:17:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    17,
                    59,
                    3,
                    135,
                    0
                ],
                "published": "2024-09-21T16:38:22Z",
                "published_parsed": [
                    2024,
                    9,
                    21,
                    16,
                    38,
                    22,
                    5,
                    265,
                    0
                ],
                "title": "Addressing and Visualizing Misalignments in Human Task-Solving\n  Trajectories",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Addressing and Visualizing Misalignments in Human Task-Solving\n  Trajectories"
                },
                "summary": "Understanding misalignments in human task-solving trajectories is critical\nfor improving AI models trained to mimic human reasoning. This study\ncategorizes such misalignments into three types: \\textbf{(1) Lack of functions\nto express intent}, \\textbf{(2) Inefficient action sequences}, and \\textbf{(3)\nIncorrect intentions that cannot solve the task}. To address these issues, we\nfirst formalize and define these three types of misalignments. We then propose\na heuristic algorithm to detect these misalignments in O2ARC trajectories and\nconduct a hierarchical and quantitative analysis of their impact. Furthermore,\nwe introduce an intention estimation algorithm that predicts missing alignment\ninformation between user actions and inferred intentions, leveraging our\nformalized framework. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the potential of intention learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding misalignments in human task-solving trajectories is critical\nfor improving AI models trained to mimic human reasoning. This study\ncategorizes such misalignments into three types: \\textbf{(1) Lack of functions\nto express intent}, \\textbf{(2) Inefficient action sequences}, and \\textbf{(3)\nIncorrect intentions that cannot solve the task}. To address these issues, we\nfirst formalize and define these three types of misalignments. We then propose\na heuristic algorithm to detect these misalignments in O2ARC trajectories and\nconduct a hierarchical and quantitative analysis of their impact. Furthermore,\nwe introduce an intention estimation algorithm that predicts missing alignment\ninformation between user actions and inferred intentions, leveraging our\nformalized framework. Through trajectory alignment, we experimentally\ndemonstrate that AI models trained on human task-solving trajectories improve\nperformance in mimicking human reasoning. Based on hierarchical analysis and\nexperiments, we highlight the importance of trajectory-intention alignment and\ndemonstrate the potential of intention learning."
                },
                "authors": [
                    {
                        "name": "Sejin Kim"
                    },
                    {
                        "name": "Hosung Lee"
                    },
                    {
                        "name": "Sundong Kim"
                    }
                ],
                "author_detail": {
                    "name": "Sundong Kim"
                },
                "author": "Sundong Kim",
                "arxiv_comment": "KDD 2025 accepted",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.14191v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.14191v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07247v2",
                "updated": "2025-05-15T11:01:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    1,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-12T05:43:21Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    43,
                    21,
                    0,
                    132,
                    0
                ],
                "title": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring\n  with Large Language Models"
                },
                "summary": "Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems."
                },
                "authors": [
                    {
                        "name": "Peichao Lai"
                    },
                    {
                        "name": "Kexuan Zhang"
                    },
                    {
                        "name": "Yi Lin"
                    },
                    {
                        "name": "Linyihan Zhang"
                    },
                    {
                        "name": "Feiyang Ye"
                    },
                    {
                        "name": "Jinhao Yan"
                    },
                    {
                        "name": "Yanwei Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Yilei Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04363v3",
                "updated": "2025-05-15T10:57:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    57,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2024-07-05T09:06:47Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    9,
                    6,
                    47,
                    4,
                    187,
                    0
                ],
                "title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents"
                },
                "summary": "Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering."
                },
                "authors": [
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Nikita Semenov"
                    },
                    {
                        "name": "Artyom Sorokin"
                    },
                    {
                        "name": "Dmitry Evseev"
                    },
                    {
                        "name": "Andrey Kravchenko"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Burnaev"
                },
                "author": "Evgeny Burnaev",
                "arxiv_comment": "Code for this work is avaliable at\n  https://github.com/AIRI-Institute/AriGraph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10167v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10167v1",
                "updated": "2025-05-15T10:51:34Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    51,
                    34,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T10:51:34Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    51,
                    34,
                    3,
                    135,
                    0
                ],
                "title": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "QuXAI: Explainers for Hybrid Quantum Machine Learning Models"
                },
                "summary": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The emergence of hybrid quantum-classical machine learning (HQML) models\nopens new horizons of computational intelligence but their fundamental\ncomplexity frequently leads to black box behavior that undermines transparency\nand reliability in their application. Although XAI for quantum systems still in\nits infancy, a major research gap is evident in robust global and local\nexplainability approaches that are designed for HQML architectures that employ\nquantized feature encoding followed by classical learning. The gap is the focus\nof this work, which introduces QuXAI, an framework based upon Q-MEDLEY, an\nexplainer for explaining feature importance in these hybrid systems. Our model\nentails the creation of HQML models incorporating quantum feature maps, the use\nof Q-MEDLEY, which combines feature based inferences, preserving the quantum\ntransformation stage and visualizing the resulting attributions. Our result\nshows that Q-MEDLEY delineates influential classical aspects in HQML models, as\nwell as separates their noise, and competes well against established XAI\ntechniques in classical validation settings. Ablation studies more\nsignificantly expose the virtues of the composite structure used in Q-MEDLEY.\nThe implications of this work are critically important, as it provides a route\nto improve the interpretability and reliability of HQML models, thus promoting\ngreater confidence and being able to engage in safer and more responsible use\nof quantum-enhanced AI technology."
                },
                "authors": [
                    {
                        "name": "Saikat Barua"
                    },
                    {
                        "name": "Mostafizur Rahman"
                    },
                    {
                        "name": "Shehenaz Khaled"
                    },
                    {
                        "name": "Md Jafor Sadek"
                    },
                    {
                        "name": "Rafiul Islam"
                    },
                    {
                        "name": "Shahnewaz Siddique"
                    }
                ],
                "author_detail": {
                    "name": "Shahnewaz Siddique"
                },
                "author": "Shahnewaz Siddique",
                "arxiv_comment": "16 pages, 6 figures, 7 equations",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10167v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10167v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13018v2",
                "updated": "2025-05-15T10:49:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    49,
                    9,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-22T17:05:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    5,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs"
                },
                "summary": "The selection of hyperparameters, such as prompt templates in large language\nmodels (LLMs), must often strike a balance between reliability and cost. In\nmany cases, structural relationships between the expected reliability levels of\nthe hyperparameters can be inferred from prior information and held-out data --\ne.g., longer prompt templates may be more detailed and thus more reliable.\nHowever, existing hyperparameter selection methods either do not provide formal\nreliability guarantees or are unable to incorporate structured knowledge in the\nhyperparameter space. This paper introduces reliability graph-based Pareto\ntesting (RG-PT), a novel multi-objective hyperparameter selection framework\nthat maintains formal reliability guarantees in terms of false discovery rate\n(FDR), while accounting for known relationships among hyperparameters via a\ndirected acyclic graph. Edges in the graph reflect expected reliability and\ncost trade-offs among hyperparameters, which are inferred via the Bradley-Terry\n(BT) ranking model from prior information and held-out data. Experimental\nevaluations demonstrate that RG-PT significantly outperforms existing methods\nsuch as learn-then-test (LTT) and Pareto testing (PT) through a more efficient\nexploration of the hyperparameter space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The selection of hyperparameters, such as prompt templates in large language\nmodels (LLMs), must often strike a balance between reliability and cost. In\nmany cases, structural relationships between the expected reliability levels of\nthe hyperparameters can be inferred from prior information and held-out data --\ne.g., longer prompt templates may be more detailed and thus more reliable.\nHowever, existing hyperparameter selection methods either do not provide formal\nreliability guarantees or are unable to incorporate structured knowledge in the\nhyperparameter space. This paper introduces reliability graph-based Pareto\ntesting (RG-PT), a novel multi-objective hyperparameter selection framework\nthat maintains formal reliability guarantees in terms of false discovery rate\n(FDR), while accounting for known relationships among hyperparameters via a\ndirected acyclic graph. Edges in the graph reflect expected reliability and\ncost trade-offs among hyperparameters, which are inferred via the Bradley-Terry\n(BT) ranking model from prior information and held-out data. Experimental\nevaluations demonstrate that RG-PT significantly outperforms existing methods\nsuch as learn-then-test (LTT) and Pareto testing (PT) through a more efficient\nexploration of the hyperparameter space."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Farzaneh"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.01047v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.01047v2",
                "updated": "2025-05-15T10:37:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    37,
                    58,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-02T06:43:09Z",
                "published_parsed": [
                    2025,
                    5,
                    2,
                    6,
                    43,
                    9,
                    4,
                    122,
                    0
                ],
                "title": "Transforming physics-informed machine learning to convex optimization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transforming physics-informed machine learning to convex optimization"
                },
                "summary": "Physics-Informed Machine Learning (PIML) offers a powerful paradigm of\nintegrating data with physical laws to address important scientific problems,\nsuch as parameter estimation, inferring hidden physics, equation discovery, and\nstate prediction, etc. However, PIML still faces many serious optimization\nchallenges that significantly restrict its applications. In this study, we\npropose a comprehensive framework that transforms PIML to convex optimization\nto overcome all these limitations, referred to as Convex-PIML. The linear\ncombination of B-splines is utilized to approximate the data, promoting the\nconvexity of the loss function. By replacing the non-convex components of the\nloss function with convex approximations, the problem is further converted into\na sequence of successively refined approximated convex optimization problems.\nThis conversion allows the use of well-established convex optimization\nalgorithms, obtaining solutions effectively and efficiently. Furthermore, an\nadaptive knot optimization method based on error estimate is introduced to\nmitigate the spectral bias issue of PIML, further improving the performance.\nThe proposed theoretically guaranteed framework is tested in scenarios with\ndistinct types of physical prior. The results indicate that optimization\nproblems are effectively solved in these scenarios, highlighting the potential\nof the framework for broad applications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Physics-Informed Machine Learning (PIML) offers a powerful paradigm of\nintegrating data with physical laws to address important scientific problems,\nsuch as parameter estimation, inferring hidden physics, equation discovery, and\nstate prediction, etc. However, PIML still faces many serious optimization\nchallenges that significantly restrict its applications. In this study, we\npropose a comprehensive framework that transforms PIML to convex optimization\nto overcome all these limitations, referred to as Convex-PIML. The linear\ncombination of B-splines is utilized to approximate the data, promoting the\nconvexity of the loss function. By replacing the non-convex components of the\nloss function with convex approximations, the problem is further converted into\na sequence of successively refined approximated convex optimization problems.\nThis conversion allows the use of well-established convex optimization\nalgorithms, obtaining solutions effectively and efficiently. Furthermore, an\nadaptive knot optimization method based on error estimate is introduced to\nmitigate the spectral bias issue of PIML, further improving the performance.\nThe proposed theoretically guaranteed framework is tested in scenarios with\ndistinct types of physical prior. The results indicate that optimization\nproblems are effectively solved in these scenarios, highlighting the potential\nof the framework for broad applications."
                },
                "authors": [
                    {
                        "name": "Letian Yi"
                    },
                    {
                        "name": "Siyuan Yang"
                    },
                    {
                        "name": "Ying Cui"
                    },
                    {
                        "name": "Zhilu Lai"
                    }
                ],
                "author_detail": {
                    "name": "Zhilu Lai"
                },
                "author": "Zhilu Lai",
                "arxiv_comment": "33 pages,14 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.01047v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.01047v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.app-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.12988v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.12988v3",
                "updated": "2025-05-15T10:25:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    25,
                    18,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-17T14:50:40Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    50,
                    40,
                    3,
                    107,
                    0
                ],
                "title": "Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the\n  Top-$k$ Experts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Why Ask One When You Can Ask $k$? Two-Stage Learning-to-Defer to the\n  Top-$k$ Experts"
                },
                "summary": "Although existing Learning-to-Defer (L2D) frameworks support multiple\nexperts, they allocate each query to a single expert, limiting their ability to\nleverage collective expertise in complex decision-making scenarios. To address\nthis, we introduce the first framework for Top-$k$ Learning-to-Defer, enabling\nsystems to defer each query to the $k$ most cost-effective experts. Our\nformulation strictly generalizes classical two-stage L2D by supporting\nmulti-expert deferral-a capability absent in prior work. We further propose\nTop-$k(x)$ Learning-to-Defer, an adaptive extension that learns the optimal\nnumber of experts per query based on input complexity, expert quality, and\nconsultation cost. We introduce a novel surrogate loss that is\nBayes-consistent, $(\\mathcal{R}, \\mathcal{G})$-consistent, and independent of\nthe cardinality parameter $k$, enabling efficient reuse across different values\nof $k$. We show that classical model cascades arise as a special case of our\nmethod, situating our framework as a strict generalization of both selective\ndeferral and cascaded inference. Experiments on classification and regression\ndemonstrate that Top-$k$ and Top-$k(x)$ yield improved accuracy--cost\ntrade-offs, establishing a new direction for multi-expert deferral in\nLearning-to-Defer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Although existing Learning-to-Defer (L2D) frameworks support multiple\nexperts, they allocate each query to a single expert, limiting their ability to\nleverage collective expertise in complex decision-making scenarios. To address\nthis, we introduce the first framework for Top-$k$ Learning-to-Defer, enabling\nsystems to defer each query to the $k$ most cost-effective experts. Our\nformulation strictly generalizes classical two-stage L2D by supporting\nmulti-expert deferral-a capability absent in prior work. We further propose\nTop-$k(x)$ Learning-to-Defer, an adaptive extension that learns the optimal\nnumber of experts per query based on input complexity, expert quality, and\nconsultation cost. We introduce a novel surrogate loss that is\nBayes-consistent, $(\\mathcal{R}, \\mathcal{G})$-consistent, and independent of\nthe cardinality parameter $k$, enabling efficient reuse across different values\nof $k$. We show that classical model cascades arise as a special case of our\nmethod, situating our framework as a strict generalization of both selective\ndeferral and cascaded inference. Experiments on classification and regression\ndemonstrate that Top-$k$ and Top-$k(x)$ yield improved accuracy--cost\ntrade-offs, establishing a new direction for multi-expert deferral in\nLearning-to-Defer."
                },
                "authors": [
                    {
                        "name": "Yannis Montreuil"
                    },
                    {
                        "name": "Axel Carlier"
                    },
                    {
                        "name": "Lai Xing Ng"
                    },
                    {
                        "name": "Wei Tsang Ooi"
                    }
                ],
                "author_detail": {
                    "name": "Wei Tsang Ooi"
                },
                "author": "Wei Tsang Ooi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.12988v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.12988v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07344v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07344v2",
                "updated": "2025-05-15T10:24:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    24,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-12T08:32:39Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    8,
                    32,
                    39,
                    0,
                    132,
                    0
                ],
                "title": "Generative Pre-trained Autoregressive Diffusion Transformer",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative Pre-trained Autoregressive Diffusion Transformer"
                },
                "summary": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this work, we present GPDiT, a Generative Pre-trained Autoregressive\nDiffusion Transformer that unifies the strengths of diffusion and\nautoregressive modeling for long-range video synthesis, within a continuous\nlatent space. Instead of predicting discrete tokens, GPDiT autoregressively\npredicts future latent frames using a diffusion loss, enabling natural modeling\nof motion dynamics and semantic consistency across frames. This continuous\nautoregressive framework not only enhances generation quality but also endows\nthe model with representation capabilities. Additionally, we introduce a\nlightweight causal attention variant and a parameter-free rotation-based\ntime-conditioning mechanism, improving both the training and inference\nefficiency. Extensive experiments demonstrate that GPDiT achieves strong\nperformance in video generation quality, video representation ability, and\nfew-shot learning tasks, highlighting its potential as an effective framework\nfor video modeling in continuous space."
                },
                "authors": [
                    {
                        "name": "Yuan Zhang"
                    },
                    {
                        "name": "Jiacheng Jiang"
                    },
                    {
                        "name": "Guoqing Ma"
                    },
                    {
                        "name": "Zhiying Lu"
                    },
                    {
                        "name": "Haoyang Huang"
                    },
                    {
                        "name": "Jianlong Yuan"
                    },
                    {
                        "name": "Nan Duan"
                    }
                ],
                "author_detail": {
                    "name": "Nan Duan"
                },
                "author": "Nan Duan",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07344v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07344v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15523v2",
                "updated": "2025-05-15T10:18:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    18,
                    58,
                    3,
                    135,
                    0
                ],
                "published": "2024-05-24T13:05:05Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    13,
                    5,
                    5,
                    4,
                    145,
                    0
                ],
                "title": "The Mosaic Memory of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mosaic Memory of Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) become widely adopted, understanding how they\nlearn from, and memorize, training data becomes crucial. Memorization in LLMs\nis widely assumed to only occur as a result of sequences being repeated in the\ntraining data. Instead, we show that LLMs memorize by assembling information\nfrom similar sequences, a phenomena we call mosaic memory. We show major LLMs\nto exhibit mosaic memory, with fuzzy duplicates contributing to memorization as\nmuch as 0.8 of an exact duplicate and even heavily modified sequences\ncontributing substantially to memorization. Despite models display reasoning\ncapabilities, we somewhat surprisingly show memorization to be predominantly\nsyntactic rather than semantic. We finally show fuzzy duplicates to be\nubiquitous in real-world data, untouched by deduplication techniques. Taken\ntogether, our results challenge widely held beliefs and show memorization to be\na more complex, mosaic process, with real-world implications for privacy,\nconfidentiality, model utility and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become widely adopted, understanding how they\nlearn from, and memorize, training data becomes crucial. Memorization in LLMs\nis widely assumed to only occur as a result of sequences being repeated in the\ntraining data. Instead, we show that LLMs memorize by assembling information\nfrom similar sequences, a phenomena we call mosaic memory. We show major LLMs\nto exhibit mosaic memory, with fuzzy duplicates contributing to memorization as\nmuch as 0.8 of an exact duplicate and even heavily modified sequences\ncontributing substantially to memorization. Despite models display reasoning\ncapabilities, we somewhat surprisingly show memorization to be predominantly\nsyntactic rather than semantic. We finally show fuzzy duplicates to be\nubiquitous in real-world data, untouched by deduplication techniques. Taken\ntogether, our results challenge widely held beliefs and show memorization to be\na more complex, mosaic process, with real-world implications for privacy,\nconfidentiality, model utility and evaluation."
                },
                "authors": [
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10143v1",
                "updated": "2025-05-15T10:17:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    17,
                    35,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T10:17:35Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    17,
                    35,
                    3,
                    135,
                    0
                ],
                "title": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response\n  Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response\n  Generation of LLMs"
                },
                "summary": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness."
                },
                "authors": [
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Parth Mitesh Shah"
                    },
                    {
                        "name": "Kuan-Ru Liou"
                    },
                    {
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "5 pages, 4 figures, accepted to IJCAI2025 demo track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10117v1",
                "updated": "2025-05-15T09:42:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    42,
                    11,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T09:42:11Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    42,
                    11,
                    3,
                    135,
                    0
                ],
                "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Virtual Machine Scheduling in Cloud Computing through Language\n  Agents"
                },
                "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments."
                },
                "authors": [
                    {
                        "name": "JieHao Wu"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Junjie Sheng"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Xiangfei Wang"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10113v1",
                "updated": "2025-05-15T09:35:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    35,
                    26,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T09:35:26Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    35,
                    26,
                    3,
                    135,
                    0
                ],
                "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical\n  Specialty Data in Medical LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical\n  Specialty Data in Medical LLMs"
                },
                "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community."
                },
                "authors": [
                    {
                        "name": "Xinlan Yan"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Yibin Lei"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Iacer Calixto"
                    }
                ],
                "author_detail": {
                    "name": "Iacer Calixto"
                },
                "author": "Iacer Calixto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05922v2",
                "updated": "2025-05-15T09:31:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    31,
                    11,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-09T09:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    54,
                    7,
                    4,
                    129,
                    0
                ],
                "title": "Cape: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cape: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy"
                },
                "summary": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "to be published in ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.20580v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.20580v2",
                "updated": "2025-05-15T09:12:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    12,
                    38,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-26T14:26:23Z",
                "published_parsed": [
                    2025,
                    3,
                    26,
                    14,
                    26,
                    23,
                    2,
                    85,
                    0
                ],
                "title": "Experiments and modeling of mechanically-soft, hard magnetorheological\n  foams with potential applications in haptic sensing",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Experiments and modeling of mechanically-soft, hard magnetorheological\n  foams with potential applications in haptic sensing"
                },
                "summary": "This study proposes a family of novel mechanically-soft and magnetically-hard\nmagnetorheological foams that, upon deformation, lead to robust and measurable\nmagnetic flux changes in their surroundings. This allows to infer qualitatively\nand even quantitatively the imposed deformation and, eventually from that, an\nestimation of the stiffness and average stress on the sample even in complex\nloading scenarios involving combinations of uniform or nonuniform\ncompression/tension with superposed shearing in different directions. The work\nprovides a complete experimental, theoretical and numerical framework on finite\nstrain, compressible magneto-elasticity, thereby allowing to measure and\npredict coupled magneto-mechanical properties of such materials with different\nparticle volume fractions and then use it to estimate and design potential\nhaptic sensing devices.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study proposes a family of novel mechanically-soft and magnetically-hard\nmagnetorheological foams that, upon deformation, lead to robust and measurable\nmagnetic flux changes in their surroundings. This allows to infer qualitatively\nand even quantitatively the imposed deformation and, eventually from that, an\nestimation of the stiffness and average stress on the sample even in complex\nloading scenarios involving combinations of uniform or nonuniform\ncompression/tension with superposed shearing in different directions. The work\nprovides a complete experimental, theoretical and numerical framework on finite\nstrain, compressible magneto-elasticity, thereby allowing to measure and\npredict coupled magneto-mechanical properties of such materials with different\nparticle volume fractions and then use it to estimate and design potential\nhaptic sensing devices."
                },
                "authors": [
                    {
                        "name": "Zehui Lin"
                    },
                    {
                        "name": "Zahra Hooshmand-Ahoor"
                    },
                    {
                        "name": "Laurence Bodelot"
                    },
                    {
                        "name": "Kostas Danas"
                    }
                ],
                "author_detail": {
                    "name": "Kostas Danas"
                },
                "author": "Kostas Danas",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.20580v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.20580v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cond-mat.soft",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cond-mat.soft",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10093v1",
                "updated": "2025-05-15T08:51:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    51,
                    53,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T08:51:53Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    51,
                    53,
                    3,
                    135,
                    0
                ],
                "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based\n  China Studies Using Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based\n  China Studies Using Generative AI"
                },
                "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems."
                },
                "authors": [
                    {
                        "name": "Hsuan-Lei Shao"
                    }
                ],
                "author_detail": {
                    "name": "Hsuan-Lei Shao"
                },
                "author": "Hsuan-Lei Shao",
                "arxiv_comment": "4 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; H.3.3; J.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12636v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12636v3",
                "updated": "2025-05-15T08:49:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    49,
                    58,
                    3,
                    135,
                    0
                ],
                "published": "2024-04-19T05:36:21Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    5,
                    36,
                    21,
                    4,
                    110,
                    0
                ],
                "title": "MORepair: Teaching LLMs to Repair Code via Multi-Objective Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MORepair: Teaching LLMs to Repair Code via Multi-Objective Fine-tuning"
                },
                "summary": "Within the realm of software engineering, specialized tasks on code, such as\nprogram repair, present unique challenges, necessitating fine-tuning Large\nlanguage models~(LLMs) to unlock state-of-the-art performance. Fine-tuning\napproaches proposed in the literature for LLMs on program repair tasks\ngenerally overlook the need to reason about the logic behind code changes,\nbeyond syntactic patterns in the data. High-performing fine-tuning experiments\nalso usually come at very high computational costs. With MORepair, we propose a\nnovel perspective on the learning focus of LLM fine-tuning for program repair:\nwe not only adapt the LLM parameters to the syntactic nuances of the task of\ncode transformation (objective 1), but we also specifically fine-tune the LLM\nwith respect to the logical reason behind the code change in the training data\n(objective 2). Such a multi-objective fine-tuning will instruct LLMs to\ngenerate high-quality patches. We apply MORepair to fine-tune four open-source\nLLMs with different sizes and architectures. Experimental results on\nfunction-level and repository-level repair benchmarks show that the implemented\nfine-tuning effectively boosts LLM repair performance by 11.4% to 56.0%. We\nfurther show that our fine-tuning strategy yields superior performance compared\nto the state-of-the-art approaches, including standard fine-tuning,\nFine-tune-CoT, and RepairLLaMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the realm of software engineering, specialized tasks on code, such as\nprogram repair, present unique challenges, necessitating fine-tuning Large\nlanguage models~(LLMs) to unlock state-of-the-art performance. Fine-tuning\napproaches proposed in the literature for LLMs on program repair tasks\ngenerally overlook the need to reason about the logic behind code changes,\nbeyond syntactic patterns in the data. High-performing fine-tuning experiments\nalso usually come at very high computational costs. With MORepair, we propose a\nnovel perspective on the learning focus of LLM fine-tuning for program repair:\nwe not only adapt the LLM parameters to the syntactic nuances of the task of\ncode transformation (objective 1), but we also specifically fine-tune the LLM\nwith respect to the logical reason behind the code change in the training data\n(objective 2). Such a multi-objective fine-tuning will instruct LLMs to\ngenerate high-quality patches. We apply MORepair to fine-tune four open-source\nLLMs with different sizes and architectures. Experimental results on\nfunction-level and repository-level repair benchmarks show that the implemented\nfine-tuning effectively boosts LLM repair performance by 11.4% to 56.0%. We\nfurther show that our fine-tuning strategy yields superior performance compared\nto the state-of-the-art approaches, including standard fine-tuning,\nFine-tune-CoT, and RepairLLaMA."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Jiadong Ren"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Claire Le Goues"
                    },
                    {
                        "name": "Shunfu Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shunfu Jin"
                },
                "author": "Shunfu Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12636v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12636v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10089v1",
                "updated": "2025-05-15T08:47:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    47,
                    55,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T08:47:55Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    47,
                    55,
                    3,
                    135,
                    0
                ],
                "title": "XRAG: Cross-lingual Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRAG: Cross-lingual Retrieval-Augmented Generation"
                },
                "summary": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Sony Trenous"
                    },
                    {
                        "name": "Leonardo F. R. Ribeiro"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Felix Hieber"
                    }
                ],
                "author_detail": {
                    "name": "Felix Hieber"
                },
                "author": "Felix Hieber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    },
    {
        "keyword": "LLM Deployment",
        "arxiv_results": [
            {
                "id": "http://arxiv.org/abs/2505.10559v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10559v1",
                "updated": "2025-05-15T17:59:22Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    22,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:59:22Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    59,
                    22,
                    3,
                    135,
                    0
                ],
                "title": "Neural Thermodynamic Laws for Large Language Model Training",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Neural Thermodynamic Laws for Large Language Model Training"
                },
                "summary": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond neural scaling laws, little is known about the laws underlying large\nlanguage models (LLMs). We introduce Neural Thermodynamic Laws (NTL) -- a new\nframework that offers fresh insights into LLM training dynamics. On the\ntheoretical side, we demonstrate that key thermodynamic quantities (e.g.,\ntemperature, entropy, heat capacity, thermal conduction) and classical\nthermodynamic principles (e.g., the three laws of thermodynamics and the\nequipartition theorem) naturally emerge under river-valley loss landscape\nassumptions. On the practical side, this scientific perspective yields\nintuitive guidelines for designing learning rate schedules."
                },
                "authors": [
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Yizhou Liu"
                    },
                    {
                        "name": "Jeff Gore"
                    },
                    {
                        "name": "Max Tegmark"
                    }
                ],
                "author_detail": {
                    "name": "Max Tegmark"
                },
                "author": "Max Tegmark",
                "arxiv_comment": "18 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10559v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10559v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.data-an",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.ML",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.04689v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.04689v3",
                "updated": "2025-05-15T17:52:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    52,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-07T06:30:33Z",
                "published_parsed": [
                    2025,
                    2,
                    7,
                    6,
                    30,
                    33,
                    4,
                    38,
                    0
                ],
                "title": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ARR: Question Answering with Large Language Models via Analyzing,\n  Retrieving, and Reasoning"
                },
                "summary": "Large language models (LLMs) have demonstrated impressive capabilities on\ncomplex evaluation benchmarks, many of which are formulated as\nquestion-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts\nis becoming increasingly vital for advancing their development and\napplicability. This paper introduces ARR, an intuitive, effective, and general\nQA solving method that explicitly incorporates three key steps: analyzing the\nintent of the question, retrieving relevant information, and reasoning step by\nstep. Notably, this paper is the first to introduce intent analysis in QA,\nwhich plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA\ntasks demonstrate that ARR consistently outperforms the baseline methods.\nAblation and case studies further validate the positive contributions of each\nARR component. Furthermore, experiments involving variations in prompt design\nindicate that ARR maintains its effectiveness regardless of the specific prompt\nformulation. Additionally, extensive evaluations across various model sizes,\nLLM series, and generation settings solidify the effectiveness, robustness, and\ngeneralizability of ARR.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) have demonstrated impressive capabilities on\ncomplex evaluation benchmarks, many of which are formulated as\nquestion-answering (QA) tasks. Enhancing the performance of LLMs in QA contexts\nis becoming increasingly vital for advancing their development and\napplicability. This paper introduces ARR, an intuitive, effective, and general\nQA solving method that explicitly incorporates three key steps: analyzing the\nintent of the question, retrieving relevant information, and reasoning step by\nstep. Notably, this paper is the first to introduce intent analysis in QA,\nwhich plays a vital role in ARR. Comprehensive evaluations across 10 diverse QA\ntasks demonstrate that ARR consistently outperforms the baseline methods.\nAblation and case studies further validate the positive contributions of each\nARR component. Furthermore, experiments involving variations in prompt design\nindicate that ARR maintains its effectiveness regardless of the specific prompt\nformulation. Additionally, extensive evaluations across various model sizes,\nLLM series, and generation settings solidify the effectiveness, robustness, and\ngeneralizability of ARR."
                },
                "authors": [
                    {
                        "name": "Yuwei Yin"
                    },
                    {
                        "name": "Giuseppe Carenini"
                    }
                ],
                "author_detail": {
                    "name": "Giuseppe Carenini"
                },
                "author": "Giuseppe Carenini",
                "arxiv_comment": "21 pages. Code: https://github.com/YuweiYin/ARR",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.04689v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.04689v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10537v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10537v1",
                "updated": "2025-05-15T17:47:30Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    47,
                    30,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:47:30Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    47,
                    30,
                    3,
                    135,
                    0
                ],
                "title": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LibIQ: Toward Real-Time Spectrum Classification in O-RAN dApps"
                },
                "summary": "The O-RAN architecture is transforming cellular networks by adopting RAN\nsoftwarization and disaggregation concepts to enable data-driven monitoring and\ncontrol of the network. Such management is enabled by RICs, which facilitate\nnear-real-time and non-real-time network control through xApps and rApps.\nHowever, they face limitations, including latency overhead in data exchange\nbetween the RAN and RIC, restricting real-time monitoring, and the inability to\naccess user plain data due to privacy and security constraints, hindering use\ncases like beamforming and spectrum classification. In this paper, we leverage\nthe dApps concept to enable real-time RF spectrum classification with LibIQ, a\nnovel library for RF signals that facilitates efficient spectrum monitoring and\nsignal classification by providing functionalities to read I/Q samples as\ntime-series, create datasets and visualize time-series data through plots and\nspectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to\ndetect external RF signals, which are subsequently classified using a CNN\ninside the library. To achieve accurate spectrum analysis, we created an\nextensive dataset of time-series-based I/Q samples, representing distinct\nsignal types captured using a custom dApp running on a 5G deployment over the\nColosseum network emulator and an OTA testbed. We evaluate our model by\ndeploying LibIQ in heterogeneous scenarios with varying center frequencies,\ntime windows, and external RF signals. In real-time analysis, the model\nclassifies the processed I/Q samples, achieving an average accuracy of\napproximately 97.8\\% in identifying signal types across all scenarios. We\npledge to release both LibIQ and the dataset created as a publicly available\nframework upon acceptance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The O-RAN architecture is transforming cellular networks by adopting RAN\nsoftwarization and disaggregation concepts to enable data-driven monitoring and\ncontrol of the network. Such management is enabled by RICs, which facilitate\nnear-real-time and non-real-time network control through xApps and rApps.\nHowever, they face limitations, including latency overhead in data exchange\nbetween the RAN and RIC, restricting real-time monitoring, and the inability to\naccess user plain data due to privacy and security constraints, hindering use\ncases like beamforming and spectrum classification. In this paper, we leverage\nthe dApps concept to enable real-time RF spectrum classification with LibIQ, a\nnovel library for RF signals that facilitates efficient spectrum monitoring and\nsignal classification by providing functionalities to read I/Q samples as\ntime-series, create datasets and visualize time-series data through plots and\nspectrograms. Thanks to LibIQ, I/Q samples can be efficiently processed to\ndetect external RF signals, which are subsequently classified using a CNN\ninside the library. To achieve accurate spectrum analysis, we created an\nextensive dataset of time-series-based I/Q samples, representing distinct\nsignal types captured using a custom dApp running on a 5G deployment over the\nColosseum network emulator and an OTA testbed. We evaluate our model by\ndeploying LibIQ in heterogeneous scenarios with varying center frequencies,\ntime windows, and external RF signals. In real-time analysis, the model\nclassifies the processed I/Q samples, achieving an average accuracy of\napproximately 97.8\\% in identifying signal types across all scenarios. We\npledge to release both LibIQ and the dataset created as a publicly available\nframework upon acceptance."
                },
                "authors": [
                    {
                        "name": "Filippo Olimpieri"
                    },
                    {
                        "name": "Noemi Giustini"
                    },
                    {
                        "name": "Andrea Lacava"
                    },
                    {
                        "name": "Salvatore D'Oro"
                    },
                    {
                        "name": "Tommaso Melodia"
                    },
                    {
                        "name": "Francesca Cuomo"
                    }
                ],
                "author_detail": {
                    "name": "Francesca Cuomo"
                },
                "author": "Francesca Cuomo",
                "arxiv_comment": "6 pages, 5 figures, 2 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10537v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10537v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10536v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10536v1",
                "updated": "2025-05-15T17:47:16Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    47,
                    16,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:47:16Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    47,
                    16,
                    3,
                    135,
                    0
                ],
                "title": "Real-World fNIRS-Based Brain-Computer Interfaces: Benchmarking Deep\n  Learning and Classical Models in Interactive Gaming",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-World fNIRS-Based Brain-Computer Interfaces: Benchmarking Deep\n  Learning and Classical Models in Interactive Gaming"
                },
                "summary": "Brain-Computer Interfaces enable direct communication between the brain and\nexternal systems, with functional Near-Infrared Spectroscopy emerging as a\nportable and non-invasive method for capturing cerebral hemodynamics. This\nstudy investigates the classification of rest and task states during a\nrealistic, interactive tennis simulation using fNIRS signals and a range of\nmachine learning approaches. We benchmarked traditional classifiers based on\nengineered features, Long Short-Term Memory networks on raw time-series data,\nand Convolutional Neural Networks applied to Gramian Angular Field-transformed\nimages. Ensemble models like Extra Trees and Gradient Boosting achieved\naccuracies above 97 percent, while the ResNet-based CNN reached 95.0 percent\naccuracy with a near-perfect AUC of 99.2 percent, outperforming both LSTM and\nEfficientNet architectures. A novel data augmentation strategy was employed to\nequalize trial durations while preserving physiological integrity. Feature\nimportance analyses revealed that both oxygenated and deoxygenated hemoglobin\nsignals, particularly slope and RMS metrics, were key contributors to\nclassification performance. These findings demonstrate the strong potential of\nfNIRS-based BCIs for deployment in dynamic, real-world environments and\nunderscore the advantages of deep learning models in decoding complex neural\nsignals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Brain-Computer Interfaces enable direct communication between the brain and\nexternal systems, with functional Near-Infrared Spectroscopy emerging as a\nportable and non-invasive method for capturing cerebral hemodynamics. This\nstudy investigates the classification of rest and task states during a\nrealistic, interactive tennis simulation using fNIRS signals and a range of\nmachine learning approaches. We benchmarked traditional classifiers based on\nengineered features, Long Short-Term Memory networks on raw time-series data,\nand Convolutional Neural Networks applied to Gramian Angular Field-transformed\nimages. Ensemble models like Extra Trees and Gradient Boosting achieved\naccuracies above 97 percent, while the ResNet-based CNN reached 95.0 percent\naccuracy with a near-perfect AUC of 99.2 percent, outperforming both LSTM and\nEfficientNet architectures. A novel data augmentation strategy was employed to\nequalize trial durations while preserving physiological integrity. Feature\nimportance analyses revealed that both oxygenated and deoxygenated hemoglobin\nsignals, particularly slope and RMS metrics, were key contributors to\nclassification performance. These findings demonstrate the strong potential of\nfNIRS-based BCIs for deployment in dynamic, real-world environments and\nunderscore the advantages of deep learning models in decoding complex neural\nsignals."
                },
                "authors": [
                    {
                        "name": "Mohammad Ghalavand"
                    },
                    {
                        "name": "Javad Hatami"
                    },
                    {
                        "name": "Seyed Kamaledin Setarehdan"
                    },
                    {
                        "name": "Hananeh Ghalavand"
                    }
                ],
                "author_detail": {
                    "name": "Hananeh Ghalavand"
                },
                "author": "Hananeh Ghalavand",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10536v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10536v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "q-bio.NC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "q-bio.NC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10530v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10530v1",
                "updated": "2025-05-15T17:41:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    41,
                    14,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T17:41:14Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    41,
                    14,
                    3,
                    135,
                    0
                ],
                "title": "Exploring Variational Entanglement Hamiltonians",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring Variational Entanglement Hamiltonians"
                },
                "summary": "Recent advances in analog and digital quantum-simulation platforms have\nenabled exploration of the spectrum of entanglement Hamiltonians via\nvariational algorithms. In this work we analyze the convergence properties of\nthe variationally obtained solutions and compare them to numerically exact\ncalculations in quantum critical systems. We demonstrate that interpreting the\ncost functional as an integral permits the deployment of iterative quadrature\nschemes, thereby reducing the required number of measurements by several orders\nof magnitude. We further show that a modified ansatz captures deviations from\nthe Bisognano-Wichmann form in lattice models, improves convergence, and\nprovides a cost-function-level diagnostic for quantum phase transitions.\nFinally, we establish that a low cost value does not by itself guarantee\nconvergence in trace distance. Nevertheless, it faithfully reproduces\ndegeneracies and spectral gaps, which are essential for applications to\ntopological phases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Recent advances in analog and digital quantum-simulation platforms have\nenabled exploration of the spectrum of entanglement Hamiltonians via\nvariational algorithms. In this work we analyze the convergence properties of\nthe variationally obtained solutions and compare them to numerically exact\ncalculations in quantum critical systems. We demonstrate that interpreting the\ncost functional as an integral permits the deployment of iterative quadrature\nschemes, thereby reducing the required number of measurements by several orders\nof magnitude. We further show that a modified ansatz captures deviations from\nthe Bisognano-Wichmann form in lattice models, improves convergence, and\nprovides a cost-function-level diagnostic for quantum phase transitions.\nFinally, we establish that a low cost value does not by itself guarantee\nconvergence in trace distance. Nevertheless, it faithfully reproduces\ndegeneracies and spectral gaps, which are essential for applications to\ntopological phases."
                },
                "authors": [
                    {
                        "name": "Yanick S. Kind"
                    },
                    {
                        "name": "Benedikt Fauseweh"
                    }
                ],
                "author_detail": {
                    "name": "Benedikt Fauseweh"
                },
                "author": "Benedikt Fauseweh",
                "arxiv_comment": "24 pages, 18 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10530v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10530v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "quant-ph",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "quant-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cond-mat.str-el",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.08493v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.08493v2",
                "updated": "2025-05-15T17:38:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    38,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-13T12:23:11Z",
                "published_parsed": [
                    2025,
                    5,
                    13,
                    12,
                    23,
                    11,
                    1,
                    133,
                    0
                ],
                "title": "BizChat: Scaffolding AI-Powered Business Planning for Small Business\n  Owners Across Digital Skill Levels",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "BizChat: Scaffolding AI-Powered Business Planning for Small Business\n  Owners Across Digital Skill Levels"
                },
                "summary": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Generative AI can help small business owners automate tasks, increase\nefficiency, and improve their bottom line. However, despite the seemingly\nintuitive design of systems like ChatGPT, significant barriers remain for those\nless comfortable with technology. To address these disparities, prior work\nhighlights accessory skills -- beyond prompt engineering -- users must master\nto successfully adopt generative AI including keyboard shortcuts, editing\nskills, file conversions, and browser literacy. Building on a design workshop\nseries and 15 interviews with small businesses, we introduce BizChat, a large\nlanguage model (LLM)-powered web application that helps business owners across\ndigital skills levels write their business plan -- an essential but often\nneglected document. To do so, BizChat's interface embodies three design\nconsiderations inspired by learning sciences: ensuring accessibility to users\nwith less digital skills while maintaining extensibility to power users\n(\"low-floor-high-ceiling\"), providing in situ micro-learning to support\nentrepreneurial education (\"just-in-time learning\"), and framing interaction\naround business activities (\"contextualized technology introduction\"). We\nconclude with plans for a future BizChat deployment."
                },
                "authors": [
                    {
                        "name": "Quentin Romero Lauro"
                    },
                    {
                        "name": "Aakash Gautam"
                    },
                    {
                        "name": "Yasmine Kotturi"
                    }
                ],
                "author_detail": {
                    "name": "Yasmine Kotturi"
                },
                "author": "Yasmine Kotturi",
                "arxiv_doi": "10.1145/3707640.3731928",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3707640.3731928",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.08493v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.08493v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "4 pages, 1 figure, CHIWORK '25 Adjunct, June 23-25, 2025, Amsterdam,\n  Netherlands",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "H.5.2",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2406.02069v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2406.02069v4",
                "updated": "2025-05-15T17:18:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    18,
                    12,
                    3,
                    135,
                    0
                ],
                "published": "2024-06-04T07:51:30Z",
                "published_parsed": [
                    2024,
                    6,
                    4,
                    7,
                    51,
                    30,
                    1,
                    156,
                    0
                ],
                "title": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information\n  Funneling"
                },
                "summary": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this study, we investigate whether attention-based information flow inside\nlarge language models (LLMs) is aggregated through noticeable patterns for long\ncontext processing. Our observations reveal that LLMs aggregate information\nthrough Pyramidal Information Funneling where attention is scattering widely in\nlower layers, progressively consolidating within specific contexts, and\nultimately focusing on critical tokens (a.k.a massive activation or attention\nsink) in higher layers. Motivated by these insights, we developed PyramidKV, a\nnovel and effective KV cache compression method. This approach dynamically\nadjusts the KV cache size across different layers, allocating more cache in\nlower layers and less in higher ones, diverging from traditional methods that\nmaintain a uniform KV cache size. Our experimental evaluations, utilizing the\nLongBench benchmark, show that PyramidKV matches the performance of models with\na full KV cache while retaining only 12% of the KV cache, thus significantly\nreducing memory usage. In scenarios emphasizing memory efficiency, where only\n0.7% of the KV cache is maintained, PyramidKV surpasses other KV cache\ncompression techniques, achieving up to a 20.5 absolute accuracy improvement on\nTREC dataset. In the Needle-in-a-Haystack experiment, PyramidKV outperforms\ncompeting methods in maintaining long-context comprehension in LLMs; notably,\nretaining just 128 KV cache entries enables the LLAMA-3-70B model to achieve\n100.0 Acc. performance."
                },
                "authors": [
                    {
                        "name": "Zefan Cai"
                    },
                    {
                        "name": "Yichi Zhang"
                    },
                    {
                        "name": "Bofei Gao"
                    },
                    {
                        "name": "Yuliang Liu"
                    },
                    {
                        "name": "Yucheng Li"
                    },
                    {
                        "name": "Tianyu Liu"
                    },
                    {
                        "name": "Keming Lu"
                    },
                    {
                        "name": "Wayne Xiong"
                    },
                    {
                        "name": "Yue Dong"
                    },
                    {
                        "name": "Junjie Hu"
                    },
                    {
                        "name": "Wen Xiao"
                    }
                ],
                "author_detail": {
                    "name": "Wen Xiao"
                },
                "author": "Wen Xiao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2406.02069v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2406.02069v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13957v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13957v2",
                "updated": "2025-05-15T17:09:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    9,
                    21,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-21T04:05:45Z",
                "published_parsed": [
                    2025,
                    1,
                    21,
                    4,
                    5,
                    45,
                    1,
                    21,
                    0
                ],
                "title": "Benchmarking Generative AI for Scoring Medical Student Interviews in\n  Objective Structured Clinical Examinations (OSCEs)",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Benchmarking Generative AI for Scoring Medical Student Interviews in\n  Objective Structured Clinical Examinations (OSCEs)"
                },
                "summary": "Objective Structured Clinical Examinations (OSCEs) are widely used to assess\nmedical students' communication skills, but scoring interview-based assessments\nis time-consuming and potentially subject to human bias. This study explored\nthe potential of large language models (LLMs) to automate OSCE evaluations\nusing the Master Interview Rating Scale (MIRS). We compared the performance of\nfour state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)\nin evaluating OSCE transcripts across all 28 items of the MIRS under the\nconditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step\nprompting. The models were benchmarked against a dataset of 10 OSCE cases with\n174 expert consensus scores available. Model performance was measured using\nthree accuracy metrics (exact, off-by-one, thresholded). Averaging across all\nMIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to\n0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded\naccuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater\nreliability ({\\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step\ntechniques proved valuable when tailored to specific assessment items. The\nperformance was consistent across MIRS items, independent of encounter phases\nand communication domains. We demonstrated the feasibility of AI-assisted OSCE\nevaluation and provided benchmarking of multiple LLMs across multiple prompt\ntechniques. Our work provides a baseline performance assessment for LLMs that\nlays a foundation for future research into automated assessment of clinical\ncommunication skills.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Objective Structured Clinical Examinations (OSCEs) are widely used to assess\nmedical students' communication skills, but scoring interview-based assessments\nis time-consuming and potentially subject to human bias. This study explored\nthe potential of large language models (LLMs) to automate OSCE evaluations\nusing the Master Interview Rating Scale (MIRS). We compared the performance of\nfour state-of-the-art LLMs (GPT-4o, Claude 3.5, Llama 3.1, and Gemini 1.5 Pro)\nin evaluating OSCE transcripts across all 28 items of the MIRS under the\nconditions of zero-shot, chain-of-thought (CoT), few-shot, and multi-step\nprompting. The models were benchmarked against a dataset of 10 OSCE cases with\n174 expert consensus scores available. Model performance was measured using\nthree accuracy metrics (exact, off-by-one, thresholded). Averaging across all\nMIRS items and OSCE cases, LLMs performed with low exact accuracy (0.27 to\n0.44), and moderate to high off-by-one accuracy (0.67 to 0.87) and thresholded\naccuracy (0.75 to 0.88). A zero temperature parameter ensured high intra-rater\nreliability ({\\alpha} = 0.98 for GPT-4o). CoT, few-shot, and multi-step\ntechniques proved valuable when tailored to specific assessment items. The\nperformance was consistent across MIRS items, independent of encounter phases\nand communication domains. We demonstrated the feasibility of AI-assisted OSCE\nevaluation and provided benchmarking of multiple LLMs across multiple prompt\ntechniques. Our work provides a baseline performance assessment for LLMs that\nlays a foundation for future research into automated assessment of clinical\ncommunication skills."
                },
                "authors": [
                    {
                        "name": "Jadon Geathers"
                    },
                    {
                        "name": "Yann Hicke"
                    },
                    {
                        "name": "Colleen Chan"
                    },
                    {
                        "name": "Niroop Rajashekar"
                    },
                    {
                        "name": "Justin Sewell"
                    },
                    {
                        "name": "Susannah Cornes"
                    },
                    {
                        "name": "Rene F. Kizilcec"
                    },
                    {
                        "name": "Dennis Shung"
                    }
                ],
                "author_detail": {
                    "name": "Dennis Shung"
                },
                "author": "Dennis Shung",
                "arxiv_comment": "12 pages + 3 pages of references, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13957v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13957v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.13504v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.13504v3",
                "updated": "2025-05-15T17:05:43Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    17,
                    5,
                    43,
                    3,
                    135,
                    0
                ],
                "published": "2024-11-20T17:55:38Z",
                "published_parsed": [
                    2024,
                    11,
                    20,
                    17,
                    55,
                    38,
                    2,
                    325,
                    0
                ],
                "title": "Disentangling Memory and Reasoning Ability in Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Disentangling Memory and Reasoning Ability in Large Language Models"
                },
                "summary": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated strong performance in handling\ncomplex tasks requiring both extensive knowledge and reasoning abilities.\nHowever, the existing LLM inference pipeline operates as an opaque process\nwithout explicit separation between knowledge retrieval and reasoning steps,\nmaking the model's decision-making process unclear and disorganized. This\nambiguity can lead to issues such as hallucinations and knowledge forgetting,\nwhich significantly impact the reliability of LLMs in high-stakes domains. In\nthis paper, we propose a new inference paradigm that decomposes the complex\ninference process into two distinct and clear actions: (1) memory recall: which\nretrieves relevant knowledge, and (2) reasoning: which performs logical steps\nbased on the recalled knowledge. To facilitate this decomposition, we introduce\ntwo special tokens memory and reason, guiding the model to distinguish between\nsteps that require knowledge retrieval and those that involve reasoning. Our\nexperiment results show that this decomposition not only improves model\nperformance but also enhances the interpretability of the inference process,\nenabling users to identify sources of error and refine model responses\neffectively. The code is available at\nhttps://github.com/MingyuJ666/Disentangling-Memory-and-Reasoning."
                },
                "authors": [
                    {
                        "name": "Mingyu Jin"
                    },
                    {
                        "name": "Weidi Luo"
                    },
                    {
                        "name": "Sitao Cheng"
                    },
                    {
                        "name": "Xinyi Wang"
                    },
                    {
                        "name": "Wenyue Hua"
                    },
                    {
                        "name": "Ruixiang Tang"
                    },
                    {
                        "name": "William Yang Wang"
                    },
                    {
                        "name": "Yongfeng Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Yongfeng Zhang"
                },
                "author": "Yongfeng Zhang",
                "arxiv_comment": "Accepted by ACL 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.13504v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.13504v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10495v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10495v1",
                "updated": "2025-05-15T16:53:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:53:45Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    45,
                    3,
                    135,
                    0
                ],
                "title": "RouteNator: A Router-Based Multi-Modal Architecture for Generating\n  Synthetic Training Data for Function Calling LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RouteNator: A Router-Based Multi-Modal Architecture for Generating\n  Synthetic Training Data for Function Calling LLMs"
                },
                "summary": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper addresses fine-tuning Large Language Models (LLMs) for function\ncalling tasks when real user interaction data is unavailable. In digital\ncontent creation tools, where users express their needs through natural\nlanguage queries that must be mapped to API calls, the lack of real-world\ntask-specific data and privacy constraints for training on it necessitate\nsynthetic data generation. Existing approaches to synthetic data generation\nfall short in diversity and complexity, failing to replicate real-world data\ndistributions and leading to suboptimal performance after LLM fine-tuning. We\npresent a novel router-based architecture that leverages domain resources like\ncontent metadata and structured knowledge graphs, along with text-to-text and\nvision-to-text language models to generate high-quality synthetic training\ndata. Our architecture's flexible routing mechanism enables synthetic data\ngeneration that matches observed real-world distributions, addressing a\nfundamental limitation of traditional approaches. Evaluation on a comprehensive\nset of real user queries demonstrates significant improvements in both function\nclassification accuracy and API parameter selection. Models fine-tuned with our\nsynthetic data consistently outperform traditional approaches, establishing new\nbenchmarks for function calling tasks."
                },
                "authors": [
                    {
                        "name": "Vibha Belavadi"
                    },
                    {
                        "name": "Tushar Vatsa"
                    },
                    {
                        "name": "Dewang Sultania"
                    },
                    {
                        "name": "Suhas Suresha"
                    },
                    {
                        "name": "Ishita Verma"
                    },
                    {
                        "name": "Cheng Chen"
                    },
                    {
                        "name": "Tracy Holloway King"
                    },
                    {
                        "name": "Michael Friedrich"
                    }
                ],
                "author_detail": {
                    "name": "Michael Friedrich"
                },
                "author": "Michael Friedrich",
                "arxiv_comment": "Proceedings of the 4th International Workshop on Knowledge-Augmented\n  Methods for Natural Language Processing",
                "arxiv_journal_ref": "https://aclanthology.org/2025.knowledgenlp-1.10/ KnowledgeNLP 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10495v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10495v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10494v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10494v1",
                "updated": "2025-05-15T16:53:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    41,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:53:41Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    41,
                    3,
                    135,
                    0
                ],
                "title": "Can You Really Trust Code Copilots? Evaluating Large Language Models\n  from a Code Security Perspective",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Can You Really Trust Code Copilots? Evaluating Large Language Models\n  from a Code Security Perspective"
                },
                "summary": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Code security and usability are both essential for various coding assistant\napplications driven by large language models (LLMs). Current code security\nbenchmarks focus solely on single evaluation task and paradigm, such as code\ncompletion and generation, lacking comprehensive assessment across dimensions\nlike secure code generation, vulnerability repair and discrimination. In this\npaper, we first propose CoV-Eval, a multi-task benchmark covering various tasks\nsuch as code completion, vulnerability repair, vulnerability detection and\nclassification, for comprehensive evaluation of LLM code security. Besides, we\ndeveloped VC-Judge, an improved judgment model that aligns closely with human\nexperts and can review LLM-generated programs for vulnerabilities in a more\nefficient and reliable way. We conduct a comprehensive evaluation of 20\nproprietary and open-source LLMs. Overall, while most LLMs identify vulnerable\ncodes well, they still tend to generate insecure codes and struggle with\nrecognizing specific vulnerability types and performing repairs. Extensive\nexperiments and qualitative analyses reveal key challenges and optimization\ndirections, offering insights for future research in LLM code security."
                },
                "authors": [
                    {
                        "name": "Yutao Mou"
                    },
                    {
                        "name": "Xiao Deng"
                    },
                    {
                        "name": "Yuxiao Luo"
                    },
                    {
                        "name": "Shikun Zhang"
                    },
                    {
                        "name": "Wei Ye"
                    }
                ],
                "author_detail": {
                    "name": "Wei Ye"
                },
                "author": "Wei Ye",
                "arxiv_comment": "Accepted by ACL2025 Main Conference",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10494v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10494v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10493v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10493v1",
                "updated": "2025-05-15T16:53:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    4,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:53:04Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    53,
                    4,
                    3,
                    135,
                    0
                ],
                "title": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with\n  Curriculum Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CL-RAG: Bridging the Gap in Retrieval-Augmented Generation with\n  Curriculum Learning"
                },
                "summary": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Retrieval-Augmented Generation (RAG) is an effective method to enhance the\ncapabilities of large language models (LLMs). Existing methods focus on\noptimizing the retriever or generator in the RAG system by directly utilizing\nthe top-k retrieved documents. However, the documents effectiveness are various\nsignificantly across user queries, i.e. some documents provide valuable\nknowledge while others totally lack critical information. It hinders the\nretriever and generator's adaptation during training. Inspired by human\ncognitive learning, curriculum learning trains models using samples progressing\nfrom easy to difficult, thus enhancing their generalization ability, and we\nintegrate this effective paradigm to the training of the RAG system. In this\npaper, we propose a multi-stage Curriculum Learning based RAG system training\nframework, named CL-RAG. We first construct training data with multiple\ndifficulty levels for the retriever and generator separately through sample\nevolution. Then, we train the model in stages based on the curriculum learning\napproach, thereby optimizing the overall performance and generalization of the\nRAG system more effectively. Our CL-RAG framework demonstrates consistent\neffectiveness across four open-domain QA datasets, achieving performance gains\nof 2% to 4% over multiple advanced methods."
                },
                "authors": [
                    {
                        "name": "Shaohan Wang"
                    },
                    {
                        "name": "Licheng Zhang"
                    },
                    {
                        "name": "Zheren Fu"
                    },
                    {
                        "name": "Zhendong Mao"
                    }
                ],
                "author_detail": {
                    "name": "Zhendong Mao"
                },
                "author": "Zhendong Mao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10493v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10493v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10490v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10490v1",
                "updated": "2025-05-15T16:45:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    45,
                    33,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:45:33Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    45,
                    33,
                    3,
                    135,
                    0
                ],
                "title": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM\n  As-A-Service Customizations Shape Trust and Usage Patterns",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Campus AI vs Commercial AI: A Late-Breaking Study on How LLM\n  As-A-Service Customizations Shape Trust and Usage Patterns"
                },
                "summary": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As the use of Large Language Models (LLMs) by students, lecturers and\nresearchers becomes more prevalent, universities - like other organizations -\nare pressed to develop coherent AI strategies. LLMs as-a-Service (LLMaaS) offer\naccessible pre-trained models, customizable to specific (business) needs. While\nmost studies prioritize data, model, or infrastructure adaptations (e.g., model\nfine-tuning), we focus on user-salient customizations, like interface changes\nand corporate branding, which we argue influence users' trust and usage\npatterns. This study serves as a functional prequel to a large-scale field\nstudy in which we examine how students and employees at a German university\nperceive and use their institution's customized LLMaaS compared to ChatGPT. The\ngoals of this prequel are to stimulate discussions on psychological effects of\nLLMaaS customizations and refine our research approach through feedback. Our\nforthcoming findings will deepen the understanding of trust dynamics in LLMs,\nproviding practical guidance for organizations considering LLMaaS deployment."
                },
                "authors": [
                    {
                        "name": "Leon Hannig"
                    },
                    {
                        "name": "Annika Bush"
                    },
                    {
                        "name": "Meltem Aksoy"
                    },
                    {
                        "name": "Steffen Becker"
                    },
                    {
                        "name": "Greta Ontrup"
                    }
                ],
                "author_detail": {
                    "name": "Greta Ontrup"
                },
                "author": "Greta Ontrup",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10490v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10490v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2410.21909v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2410.21909v2",
                "updated": "2025-05-15T16:40:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    40,
                    39,
                    3,
                    135,
                    0
                ],
                "published": "2024-10-29T10:01:40Z",
                "published_parsed": [
                    2024,
                    10,
                    29,
                    10,
                    1,
                    40,
                    1,
                    303,
                    0
                ],
                "title": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SceneGenAgent: Precise Industrial Scene Generation with Coding Agent"
                },
                "summary": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The modeling of industrial scenes is essential for simulations in industrial\nmanufacturing. While large language models (LLMs) have shown significant\nprogress in generating general 3D scenes from textual descriptions, generating\nindustrial scenes with LLMs poses a unique challenge due to their demand for\nprecise measurements and positioning, requiring complex planning over spatial\narrangement. To address this challenge, we introduce SceneGenAgent, an\nLLM-based agent for generating industrial scenes through C# code. SceneGenAgent\nensures precise layout planning through a structured and calculable format,\nlayout verification, and iterative refinement to meet the quantitative\nrequirements of industrial scenarios. Experiment results demonstrate that LLMs\npowered by SceneGenAgent exceed their original performance, reaching up to\n81.0% success rate in real-world industrial scene generation tasks and\neffectively meeting most scene generation requirements. To further enhance\naccessibility, we construct SceneInstruct, a dataset designed for fine-tuning\nopen-source LLMs to integrate into SceneGenAgent. Experiments show that\nfine-tuning open-source LLMs on SceneInstruct yields significant performance\nimprovements, with Llama3.1-70B approaching the capabilities of GPT-4o. Our\ncode and data are available at https://github.com/THUDM/SceneGenAgent ."
                },
                "authors": [
                    {
                        "name": "Xiao Xia"
                    },
                    {
                        "name": "Dan Zhang"
                    },
                    {
                        "name": "Zibo Liao"
                    },
                    {
                        "name": "Zhenyu Hou"
                    },
                    {
                        "name": "Tianrui Sun"
                    },
                    {
                        "name": "Jing Li"
                    },
                    {
                        "name": "Ling Fu"
                    },
                    {
                        "name": "Yuxiao Dong"
                    }
                ],
                "author_detail": {
                    "name": "Yuxiao Dong"
                },
                "author": "Yuxiao Dong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2410.21909v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2410.21909v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.12293v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.12293v2",
                "updated": "2025-05-15T16:29:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    29,
                    38,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-15T23:20:26Z",
                "published_parsed": [
                    2025,
                    3,
                    15,
                    23,
                    20,
                    26,
                    5,
                    74,
                    0
                ],
                "title": "Unified Modeling Language Code Generation from Diagram Images Using\n  Multimodal Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Unified Modeling Language Code Generation from Diagram Images Using\n  Multimodal Large Language Models"
                },
                "summary": "The Unified Modeling Language is a standardized visual language widely used\nfor modeling and documenting the design of software systems. Although many\ntools generate UML diagrams from UML code, generating executable UML code from\nimage-based UML diagrams remains challenging. This paper proposes a new\napproach to generate UML code using a large multimodal language model\nautomatically. Synthetic UML activity and sequence diagram datasets were\ncreated to train and test the model. We compared standard fine-tuning with LoRA\ntechniques to optimize base models. The experiments measured code generation\naccuracy across different model sizes and training strategies. These results\ndemonstrated that domain-adapted MM-LLMs perform for UML code generation\nautomation, whereby, at the best model, it achieved BLEU and SSIM scores of\n0.779 and 0.942 on sequence diagrams. This will enable the modernization of\nlegacy systems and decrease the manual effort in software development\nworkflows.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Unified Modeling Language is a standardized visual language widely used\nfor modeling and documenting the design of software systems. Although many\ntools generate UML diagrams from UML code, generating executable UML code from\nimage-based UML diagrams remains challenging. This paper proposes a new\napproach to generate UML code using a large multimodal language model\nautomatically. Synthetic UML activity and sequence diagram datasets were\ncreated to train and test the model. We compared standard fine-tuning with LoRA\ntechniques to optimize base models. The experiments measured code generation\naccuracy across different model sizes and training strategies. These results\ndemonstrated that domain-adapted MM-LLMs perform for UML code generation\nautomation, whereby, at the best model, it achieved BLEU and SSIM scores of\n0.779 and 0.942 on sequence diagrams. This will enable the modernization of\nlegacy systems and decrease the manual effort in software development\nworkflows."
                },
                "authors": [
                    {
                        "name": "Averi Bates"
                    },
                    {
                        "name": "Ryan Vavricka"
                    },
                    {
                        "name": "Shane Carleton"
                    },
                    {
                        "name": "Ruosi Shao"
                    },
                    {
                        "name": "Chongle Pan"
                    }
                ],
                "author_detail": {
                    "name": "Chongle Pan"
                },
                "author": "Chongle Pan",
                "arxiv_doi": "10.1016/j.mlwa.2025.100660",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1016/j.mlwa.2025.100660",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2503.12293v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.12293v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "Published in the Journal of Machine Learning with Applications,\n  Author Contributions: Averi Bates: Methodology, Development, Analysis, Data\n  Curation, Drafting, Review. Ryan Vavricka: Data Curation, Development,\n  Review. Shane Carleton: Supervision, Funding. Ruosi Shao: Review. Chongle\n  Pan: Supervision, Review",
                "arxiv_journal_ref": "Mach. Learn. Appl. 20 (2025) 100660",
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "D.2.2; D.2.3; I.2.7; I.4.9",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.17671v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.17671v3",
                "updated": "2025-05-15T16:24:49Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    24,
                    49,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-24T15:39:46Z",
                "published_parsed": [
                    2025,
                    4,
                    24,
                    15,
                    39,
                    46,
                    3,
                    114,
                    0
                ],
                "title": "Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Data-Driven Calibration of Prediction Sets in Large Vision-Language\n  Models Based on Inductive Conformal Prediction"
                },
                "summary": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study addresses the critical challenge of hallucination mitigation in\nLarge Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks\nthrough a Split Conformal Prediction (SCP) framework. While LVLMs excel in\nmulti-modal reasoning, their outputs often exhibit hallucinated content with\nhigh confidence, posing risks in safety-critical applications. We propose a\nmodel-agnostic uncertainty quantification method that integrates dynamic\nthreshold calibration and cross-modal consistency verification. By partitioning\ndata into calibration and test sets, the framework computes nonconformity\nscores to construct prediction sets with statistical guarantees under\nuser-defined risk levels ($\\alpha$). Key innovations include: (1) rigorous\ncontrol of \\textbf{marginal coverage} to ensure empirical error rates remain\nstrictly below $\\alpha$; (2) dynamic adjustment of prediction set sizes\ninversely with $\\alpha$, filtering low-confidence outputs; (3) elimination of\nprior distribution assumptions and retraining requirements. Evaluations on\nbenchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces\ntheoretical guarantees across all $\\alpha$ values. The framework achieves\nstable performance across varying calibration-to-test split ratios,\nunderscoring its robustness for real-world deployment in healthcare, autonomous\nsystems, and other safety-sensitive domains. This work bridges the gap between\ntheoretical reliability and practical applicability in multi-modal AI systems,\noffering a scalable solution for hallucination detection and uncertainty-aware\ndecision-making."
                },
                "authors": [
                    {
                        "name": "Yuanchang Ye"
                    },
                    {
                        "name": "Weiyan Wen"
                    }
                ],
                "author_detail": {
                    "name": "Weiyan Wen"
                },
                "author": "Weiyan Wen",
                "arxiv_comment": "Accepted by ICIPCA 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.17671v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.17671v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10475v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10475v1",
                "updated": "2025-05-15T16:24:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    24,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:24:45Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    24,
                    45,
                    3,
                    135,
                    0
                ],
                "title": "Parallel Scaling Law for Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Parallel Scaling Law for Language Models"
                },
                "summary": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "It is commonly believed that scaling language models should commit a\nsignificant space or time cost, by increasing the parameters (parameter\nscaling) or output tokens (inference-time scaling). We introduce the third and\nmore inference-efficient scaling paradigm: increasing the model's parallel\ncomputation during both training and inference time. We apply $P$ diverse and\nlearnable transformations to the input, execute forward passes of the model in\nparallel, and dynamically aggregate the $P$ outputs. This method, namely\nparallel scaling (ParScale), scales parallel computation by reusing existing\nparameters and can be applied to any model structure, optimization procedure,\ndata, or task. We theoretically propose a new scaling law and validate it\nthrough large-scale pre-training, which shows that a model with $P$ parallel\nstreams is similar to scaling the parameters by $O(\\log P)$ while showing\nsuperior inference efficiency. For example, ParScale can use up to 22$\\times$\nless memory increase and 6$\\times$ less latency increase compared to parameter\nscaling that achieves the same performance improvement. It can also recycle an\noff-the-shelf pre-trained model into a parallelly scaled one by post-training\non a small amount of tokens, further reducing the training budget. The new\nscaling law we discovered potentially facilitates the deployment of more\npowerful models in low-resource scenarios, and provides an alternative\nperspective for the role of computation in machine learning."
                },
                "authors": [
                    {
                        "name": "Mouxiang Chen"
                    },
                    {
                        "name": "Binyuan Hui"
                    },
                    {
                        "name": "Zeyu Cui"
                    },
                    {
                        "name": "Jiaxi Yang"
                    },
                    {
                        "name": "Dayiheng Liu"
                    },
                    {
                        "name": "Jianling Sun"
                    },
                    {
                        "name": "Junyang Lin"
                    },
                    {
                        "name": "Zhongxin Liu"
                    }
                ],
                "author_detail": {
                    "name": "Zhongxin Liu"
                },
                "author": "Zhongxin Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10475v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10475v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10473v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10473v1",
                "updated": "2025-05-15T16:23:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    23,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:23:51Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    23,
                    51,
                    3,
                    135,
                    0
                ],
                "title": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware\n  Gaussian Splatting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Consistent Quantity-Quality Control across Scenes for Deployment-Aware\n  Gaussian Splatting"
                },
                "summary": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To reduce storage and computational costs, 3D Gaussian splatting (3DGS) seeks\nto minimize the number of Gaussians used while preserving high rendering\nquality, introducing an inherent trade-off between Gaussian quantity and\nrendering quality. Existing methods strive for better quantity-quality\nperformance, but lack the ability for users to intuitively adjust this\ntrade-off to suit practical needs such as model deployment under diverse\nhardware and communication constraints. Here, we present ControlGS, a 3DGS\noptimization method that achieves semantically meaningful and cross-scene\nconsistent quantity-quality control while maintaining strong quantity-quality\nperformance. Through a single training run using a fixed setup and a\nuser-specified hyperparameter reflecting quantity-quality preference, ControlGS\ncan automatically find desirable quantity-quality trade-off points across\ndiverse scenes, from compact objects to large outdoor scenes. It also\noutperforms baselines by achieving higher rendering quality with fewer\nGaussians, and supports a broad adjustment range with stepless control over the\ntrade-off."
                },
                "authors": [
                    {
                        "name": "Fengdi Zhang"
                    },
                    {
                        "name": "Hongkun Cao"
                    },
                    {
                        "name": "Ruqi Huang"
                    }
                ],
                "author_detail": {
                    "name": "Ruqi Huang"
                },
                "author": "Ruqi Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10473v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10473v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10472v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10472v1",
                "updated": "2025-05-15T16:23:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    23,
                    21,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:23:21Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    23,
                    21,
                    3,
                    135,
                    0
                ],
                "title": "Large Language Models for Cancer Communication: Evaluating Linguistic\n  Quality, Safety, and Accessibility in Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cancer Communication: Evaluating Linguistic\n  Quality, Safety, and Accessibility in Generative AI"
                },
                "summary": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Effective communication about breast and cervical cancers remains a\npersistent health challenge, with significant gaps in public understanding of\ncancer prevention, screening, and treatment, potentially leading to delayed\ndiagnoses and inadequate treatments. This study evaluates the capabilities and\nlimitations of Large Language Models (LLMs) in generating accurate, safe, and\naccessible cancer-related information to support patient understanding. We\nevaluated five general-purpose and three medical LLMs using a mixed-methods\nevaluation framework across linguistic quality, safety and trustworthiness, and\ncommunication accessibility and affectiveness. Our approach utilized\nquantitative metrics, qualitative expert ratings, and statistical analysis\nusing Welch's ANOVA, Games-Howell, and Hedges' g. Our results show that\ngeneral-purpose LLMs produced outputs of higher linguistic quality and\naffectiveness, while medical LLMs demonstrate greater communication\naccessibility. However, medical LLMs tend to exhibit higher levels of potential\nharm, toxicity, and bias, reducing their performance in safety and\ntrustworthiness. Our findings indicate a duality between domain-specific\nknowledge and safety in health communications. The results highlight the need\nfor intentional model design with targeted improvements, particularly in\nmitigating harm and bias, and improving safety and affectiveness. This study\nprovides a comprehensive evaluation of LLMs for cancer communication, offering\ncritical insights for improving AI-generated health content and informing\nfuture development of accurate, safe, and accessible digital health tools."
                },
                "authors": [
                    {
                        "name": "Agnik Saha"
                    },
                    {
                        "name": "Victoria Churchill"
                    },
                    {
                        "name": "Anny D. Rodriguez"
                    },
                    {
                        "name": "Ugur Kursuncu"
                    },
                    {
                        "name": "Muhammed Y. Idris"
                    }
                ],
                "author_detail": {
                    "name": "Muhammed Y. Idris"
                },
                "author": "Muhammed Y. Idris",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10472v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10472v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10468v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10468v1",
                "updated": "2025-05-15T16:21:33Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    21,
                    33,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:21:33Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    21,
                    33,
                    3,
                    135,
                    0
                ],
                "title": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI Agents vs. Agentic AI: A Conceptual Taxonomy, Applications and\n  Challenge"
                },
                "summary": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This study critically distinguishes between AI Agents and Agentic AI,\noffering a structured conceptual taxonomy, application mapping, and challenge\nanalysis to clarify their divergent design philosophies and capabilities. We\nbegin by outlining the search strategy and foundational definitions,\ncharacterizing AI Agents as modular systems driven by Large Language Models\n(LLMs) and Large Image Models (LIMs) for narrow, task-specific automation.\nGenerative AI is positioned as a precursor, with AI Agents advancing through\ntool integration, prompt engineering, and reasoning enhancements. In contrast,\nAgentic AI systems represent a paradigmatic shift marked by multi-agent\ncollaboration, dynamic task decomposition, persistent memory, and orchestrated\nautonomy. Through a sequential evaluation of architectural evolution,\noperational mechanisms, interaction styles, and autonomy levels, we present a\ncomparative analysis across both paradigms. Application domains such as\ncustomer support, scheduling, and data summarization are contrasted with\nAgentic AI deployments in research automation, robotic coordination, and\nmedical decision support. We further examine unique challenges in each paradigm\nincluding hallucination, brittleness, emergent behavior, and coordination\nfailure and propose targeted solutions such as ReAct loops, RAG, orchestration\nlayers, and causal modeling. This work aims to provide a definitive roadmap for\ndeveloping robust, scalable, and explainable AI agent and Agentic AI-driven\nsystems. >AI Agents, Agent-driven, Vision-Language-Models, Agentic AI Decision\nSupport System, Agentic-AI Applications"
                },
                "authors": [
                    {
                        "name": "Ranjan Sapkota"
                    },
                    {
                        "name": "Konstantinos I. Roumeliotis"
                    },
                    {
                        "name": "Manoj Karkee"
                    }
                ],
                "author_detail": {
                    "name": "Manoj Karkee"
                },
                "author": "Manoj Karkee",
                "arxiv_comment": "32 pages, 14 figures, 11 tables",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10468v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10468v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10465v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10465v1",
                "updated": "2025-05-15T16:18:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    18,
                    13,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:18:13Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    18,
                    13,
                    3,
                    135,
                    0
                ],
                "title": "Superposition Yields Robust Neural Scaling",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Superposition Yields Robust Neural Scaling"
                },
                "summary": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The success of today's large language models (LLMs) depends on the\nobservation that larger models perform better. However, the origin of this\nneural scaling law -- the finding that loss decreases as a power law with model\nsize -- remains unclear. Starting from two empirical principles -- that LLMs\nrepresent more things than the model dimensions (widths) they have (i.e.,\nrepresentations are superposed), and that words or concepts in language occur\nwith varying frequencies -- we constructed a toy model to study the loss\nscaling with model size. We found that when superposition is weak, meaning only\nthe most frequent features are represented without interference, the scaling of\nloss with model size depends on the underlying feature frequency; if feature\nfrequencies follow a power law, so does the loss. In contrast, under strong\nsuperposition, where all features are represented but overlap with each other,\nthe loss becomes inversely proportional to the model dimension across a wide\nrange of feature frequency distributions. This robust scaling behavior is\nexplained geometrically: when many more vectors are packed into a lower\ndimensional space, the interference (squared overlaps) between vectors scales\ninversely with that dimension. We then analyzed four families of open-sourced\nLLMs and found that they exhibit strong superposition and quantitatively match\nthe predictions of our toy model. The Chinchilla scaling law turned out to also\nagree with our results. We conclude that representation superposition is an\nimportant mechanism underlying the observed neural scaling laws. We anticipate\nthat these insights will inspire new training strategies and model\narchitectures to achieve better performance with less computation and fewer\nparameters."
                },
                "authors": [
                    {
                        "name": "Yizhou liu"
                    },
                    {
                        "name": "Ziming Liu"
                    },
                    {
                        "name": "Jeff Gore"
                    }
                ],
                "author_detail": {
                    "name": "Jeff Gore"
                },
                "author": "Jeff Gore",
                "arxiv_comment": "30 pages, 23 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10465v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10465v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.18036v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.18036v4",
                "updated": "2025-05-15T16:08:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    8,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-25T09:48:53Z",
                "published_parsed": [
                    2025,
                    2,
                    25,
                    9,
                    48,
                    53,
                    1,
                    56,
                    0
                ],
                "title": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Harnessing Multiple Large Language Models: A Survey on LLM Ensemble"
                },
                "summary": "LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble."
                },
                "authors": [
                    {
                        "name": "Zhijun Chen"
                    },
                    {
                        "name": "Jingzheng Li"
                    },
                    {
                        "name": "Pengpeng Chen"
                    },
                    {
                        "name": "Zhuoran Li"
                    },
                    {
                        "name": "Kai Sun"
                    },
                    {
                        "name": "Yuankai Luo"
                    },
                    {
                        "name": "Qianren Mao"
                    },
                    {
                        "name": "Dingqi Yang"
                    },
                    {
                        "name": "Hailong Sun"
                    },
                    {
                        "name": "Philip S. Yu"
                    }
                ],
                "author_detail": {
                    "name": "Philip S. Yu"
                },
                "author": "Philip S. Yu",
                "arxiv_comment": "9 pages, 2 figures, codebase:\n  https://github.com/junchenzhi/Awesome-LLM-Ensemble",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.18036v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.18036v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10443v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10443v1",
                "updated": "2025-05-15T16:04:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    4,
                    25,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T16:04:25Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    16,
                    4,
                    25,
                    3,
                    135,
                    0
                ],
                "title": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Large Language Models Robust in Understanding Code Against\n  Semantics-Preserving Mutations?"
                },
                "summary": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Understanding the reasoning and robustness of Large Language Models (LLMs) is\ncritical for their reliable use in programming tasks. While recent studies have\nassessed LLMs' ability to predict program outputs, most focus solely on the\naccuracy of those predictions, without evaluating the reasoning behind them.\nMoreover, it has been observed on mathematical reasoning tasks that LLMs can\narrive at correct answers through flawed logic, raising concerns about similar\nissues in code understanding.\n  In this work, we evaluate whether state-of-the-art LLMs with up to 8B\nparameters can reason about Python programs or are simply guessing. We apply\nfive semantics-preserving code mutations: renaming variables, mirroring\ncomparison expressions, swapping if-else branches, converting for loops to\nwhile, and loop unrolling. These mutations maintain program semantics while\naltering its syntax. We evaluated six LLMs and performed a human expert\nanalysis using LiveCodeBench to assess whether the correct predictions are\nbased on sound reasoning. We also evaluated prediction stability across\ndifferent code mutations on LiveCodeBench and CruxEval. Our findings show that\nsome LLMs, such as Llama3.2, produce correct predictions based on flawed\nreasoning in up to 61% of cases. Furthermore, LLMs often change predictions in\nresponse to our code mutations, indicating limited robustness in their semantic\nunderstanding."
                },
                "authors": [
                    {
                        "name": "Pedro Orvalho"
                    },
                    {
                        "name": "Marta Kwiatkowska"
                    }
                ],
                "author_detail": {
                    "name": "Marta Kwiatkowska"
                },
                "author": "Marta Kwiatkowska",
                "arxiv_comment": "10 pages, 5 tables, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10443v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10443v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.17067v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.17067v2",
                "updated": "2025-05-15T15:57:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    57,
                    32,
                    3,
                    135,
                    0
                ],
                "published": "2024-05-27T11:39:59Z",
                "published_parsed": [
                    2024,
                    5,
                    27,
                    11,
                    39,
                    59,
                    0,
                    148,
                    0
                ],
                "title": "Tokenization Matters! Degrading Large Language Models through\n  Challenging Their Tokenization",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Tokenization Matters! Degrading Large Language Models through\n  Challenging Their Tokenization"
                },
                "summary": "Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. This defect is more obvious in Chinese\nscenarios. To demonstrate this flaw of LLMs, we construct an adversarial\ndataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which\ndraws upon the vocabularies of various open-source LLMs to challenge LLMs'\ntokenization. ADT consists of two subsets: the manually constructed ADT-Human\nand the automatically generated ADT-Auto. Our empirical results reveal that our\nADT is highly effective on challenging the tokenization of leading LLMs,\nincluding GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'\ncapabilities. Moreover, our method of automatic data generation has been proven\nefficient and robust, which can be applied to any open-source LLMs. In this\npaper, we substantially investigate LLMs' vulnerability in terms of challenging\ntheir token segmentation, which will shed light on the subsequent research of\nimproving LLMs' capabilities through optimizing their tokenization process and\nalgorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have shown remarkable capabilities in language\nunderstanding and generation. Nonetheless, it was also witnessed that LLMs tend\nto produce inaccurate responses to specific queries. This deficiency can be\ntraced to the tokenization step LLMs must undergo, which is an inevitable\nlimitation inherent to all LLMs. In fact, incorrect tokenization is the\ncritical point that hinders LLMs in understanding the input precisely, thus\nleading to unsatisfactory output. This defect is more obvious in Chinese\nscenarios. To demonstrate this flaw of LLMs, we construct an adversarial\ndataset, named as $\\textbf{ADT (Adversarial Dataset for Tokenizer)}$, which\ndraws upon the vocabularies of various open-source LLMs to challenge LLMs'\ntokenization. ADT consists of two subsets: the manually constructed ADT-Human\nand the automatically generated ADT-Auto. Our empirical results reveal that our\nADT is highly effective on challenging the tokenization of leading LLMs,\nincluding GPT-4o, Llama-3, Deepseek-R1 and so on, thus degrading these LLMs'\ncapabilities. Moreover, our method of automatic data generation has been proven\nefficient and robust, which can be applied to any open-source LLMs. In this\npaper, we substantially investigate LLMs' vulnerability in terms of challenging\ntheir token segmentation, which will shed light on the subsequent research of\nimproving LLMs' capabilities through optimizing their tokenization process and\nalgorithms."
                },
                "authors": [
                    {
                        "name": "Dixuan Wang"
                    },
                    {
                        "name": "Yanda Li"
                    },
                    {
                        "name": "Junyuan Jiang"
                    },
                    {
                        "name": "Zepeng Ding"
                    },
                    {
                        "name": "Ziqin Luo"
                    },
                    {
                        "name": "Guochao Jiang"
                    },
                    {
                        "name": "Jiaqing Liang"
                    },
                    {
                        "name": "Deqing Yang"
                    }
                ],
                "author_detail": {
                    "name": "Deqing Yang"
                },
                "author": "Deqing Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.17067v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.17067v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10425v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10425v1",
                "updated": "2025-05-15T15:40:25Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    40,
                    25,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:40:25Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    40,
                    25,
                    3,
                    135,
                    0
                ],
                "title": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for\n  LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning to Think: Information-Theoretic Reinforcement Fine-Tuning for\n  LLMs"
                },
                "summary": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) excel at complex tasks thanks to advances in\nreasoning abilities. However, existing methods overlook the trade-off between\nreasoning effectiveness and computational efficiency, often encouraging\nunnecessarily long reasoning chains and wasting tokens. To address this, we\npropose Learning to Think (L2T), an information-theoretic reinforcement\nfine-tuning framework for LLMs to make the models achieve optimal reasoning\nwith fewer tokens. Specifically, L2T treats each query-response interaction as\na hierarchical session of multiple episodes and proposes a universal dense\nprocess reward, i.e., quantifies the episode-wise information gain in\nparameters, requiring no extra annotations or task-specific evaluators. We\npropose a method to quickly estimate this reward based on PAC-Bayes bounds and\nthe Fisher information matrix. Theoretical analyses show that it significantly\nreduces computational complexity with high estimation accuracy. By immediately\nrewarding each episode's contribution and penalizing excessive updates, L2T\noptimizes the model via reinforcement learning to maximize the use of each\nepisode and achieve effective updates. Empirical results on various reasoning\nbenchmarks and base models demonstrate the advantage of L2T across different\ntasks, boosting both reasoning effectiveness and efficiency."
                },
                "authors": [
                    {
                        "name": "Jingyao Wang"
                    },
                    {
                        "name": "Wenwen Qiang"
                    },
                    {
                        "name": "Zeen Song"
                    },
                    {
                        "name": "Changwen Zheng"
                    },
                    {
                        "name": "Hui Xiong"
                    }
                ],
                "author_detail": {
                    "name": "Hui Xiong"
                },
                "author": "Hui Xiong",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10425v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10425v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10409v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10409v1",
                "updated": "2025-05-15T15:31:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    31,
                    17,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:31:17Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    31,
                    17,
                    3,
                    135,
                    0
                ],
                "title": "Are LLM-generated plain language summaries truly understandable? A\n  large-scale crowdsourced evaluation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are LLM-generated plain language summaries truly understandable? A\n  large-scale crowdsourced evaluation"
                },
                "summary": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Plain language summaries (PLSs) are essential for facilitating effective\ncommunication between clinicians and patients by making complex medical\ninformation easier for laypeople to understand and act upon. Large language\nmodels (LLMs) have recently shown promise in automating PLS generation, but\ntheir effectiveness in supporting health information comprehension remains\nunclear. Prior evaluations have generally relied on automated scores that do\nnot measure understandability directly, or subjective Likert-scale ratings from\nconvenience samples with limited generalizability. To address these gaps, we\nconducted a large-scale crowdsourced evaluation of LLM-generated PLSs using\nAmazon Mechanical Turk with 150 participants. We assessed PLS quality through\nsubjective Likert-scale ratings focusing on simplicity, informativeness,\ncoherence, and faithfulness; and objective multiple-choice comprehension and\nrecall measures of reader understanding. Additionally, we examined the\nalignment between 10 automated evaluation metrics and human judgments. Our\nfindings indicate that while LLMs can generate PLSs that appear\nindistinguishable from human-written ones in subjective evaluations,\nhuman-written PLSs lead to significantly better comprehension. Furthermore,\nautomated evaluation metrics fail to reflect human judgment, calling into\nquestion their suitability for evaluating PLSs. This is the first study to\nsystematically evaluate LLM-generated PLSs based on both reader preferences and\ncomprehension outcomes. Our findings highlight the need for evaluation\nframeworks that move beyond surface-level quality and for generation methods\nthat explicitly optimize for layperson comprehension."
                },
                "authors": [
                    {
                        "name": "Yue Guo"
                    },
                    {
                        "name": "Jae Ho Sohn"
                    },
                    {
                        "name": "Gondy Leroy"
                    },
                    {
                        "name": "Trevor Cohen"
                    }
                ],
                "author_detail": {
                    "name": "Trevor Cohen"
                },
                "author": "Trevor Cohen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10409v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10409v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10402v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10402v1",
                "updated": "2025-05-15T15:26:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    26,
                    32,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:26:32Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    26,
                    32,
                    3,
                    135,
                    0
                ],
                "title": "Rethinking Repetition Problems of LLMs in Code Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rethinking Repetition Problems of LLMs in Code Generation"
                },
                "summary": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the advent of neural language models, the performance of code generation\nhas been significantly boosted. However, the problem of repetitions during the\ngeneration process continues to linger. Previous work has primarily focused on\ncontent repetition, which is merely a fraction of the broader repetition\nproblem in code generation. A more prevalent and challenging problem is\nstructural repetition. In structural repetition, the repeated code appears in\nvarious patterns but possesses a fixed structure, which can be inherently\nreflected in grammar. In this paper, we formally define structural repetition\nand propose an efficient decoding approach called RPG, which stands for\nRepetition Penalization based on Grammar, to alleviate the repetition problems\nin code generation for LLMs. Specifically, RPG first leverages grammar rules to\nidentify repetition problems during code generation, and then strategically\ndecays the likelihood of critical tokens that contribute to repetitions,\nthereby mitigating them in code generation. To facilitate this study, we\nconstruct a new dataset CodeRepetEval to comprehensively evaluate approaches\nfor mitigating the repetition problems in code generation. Extensive\nexperimental results demonstrate that RPG substantially outperforms the\nbest-performing baselines on CodeRepetEval dataset as well as HumanEval and\nMBPP benchmarks, effectively reducing repetitions and enhancing the quality of\ngenerated code."
                },
                "authors": [
                    {
                        "name": "Yihong Dong"
                    },
                    {
                        "name": "Yuchen Liu"
                    },
                    {
                        "name": "Xue Jiang"
                    },
                    {
                        "name": "Zhi Jin"
                    },
                    {
                        "name": "Ge Li"
                    }
                ],
                "author_detail": {
                    "name": "Ge Li"
                },
                "author": "Ge Li",
                "arxiv_comment": "Accepted to ACL 2025 (main)",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10402v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10402v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06046v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06046v2",
                "updated": "2025-05-15T15:14:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    14,
                    47,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-09T13:42:59Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    13,
                    42,
                    59,
                    4,
                    129,
                    0
                ],
                "title": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health\n  Information"
                },
                "summary": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries. To create PubHealthBench we extract free\ntext from 687 current UK government guidance documents and implement an\nautomated pipeline for generating MCQA samples. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% accuracy in the MCQA setup, and\noutperform humans with cursory search engine use. However, in the free form\nsetup we see lower performance with no model scoring >75%. Importantly we find\nin both setups LLMs have higher accuracy on guidance intended for the general\npublic. Therefore, there are promising signs that state of the art (SOTA) LLMs\nare an increasingly accurate source of public health information, but\nadditional safeguards or tools may still be needed when providing free form\nresponses on public health topics.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become widely accessible, a detailed\nunderstanding of their knowledge within specific domains becomes necessary for\nsuccessful real world use. This is particularly critical in public health,\nwhere failure to retrieve relevant, accurate, and current information could\nsignificantly impact UK residents. However, currently little is known about LLM\nknowledge of UK Government public health information. To address this issue,\nthis paper introduces a new benchmark, PubHealthBench, with over 8000 questions\nfor evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form\nresponses to public health queries. To create PubHealthBench we extract free\ntext from 687 current UK government guidance documents and implement an\nautomated pipeline for generating MCQA samples. Assessing 24 LLMs on\nPubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a\nhigh degree of knowledge, achieving >90% accuracy in the MCQA setup, and\noutperform humans with cursory search engine use. However, in the free form\nsetup we see lower performance with no model scoring >75%. Importantly we find\nin both setups LLMs have higher accuracy on guidance intended for the general\npublic. Therefore, there are promising signs that state of the art (SOTA) LLMs\nare an increasingly accurate source of public health information, but\nadditional safeguards or tools may still be needed when providing free form\nresponses on public health topics."
                },
                "authors": [
                    {
                        "name": "Joshua Harris"
                    },
                    {
                        "name": "Fan Grayson"
                    },
                    {
                        "name": "Felix Feldman"
                    },
                    {
                        "name": "Timothy Laurence"
                    },
                    {
                        "name": "Toby Nonnenmacher"
                    },
                    {
                        "name": "Oliver Higgins"
                    },
                    {
                        "name": "Leo Loman"
                    },
                    {
                        "name": "Selina Patel"
                    },
                    {
                        "name": "Thomas Finnie"
                    },
                    {
                        "name": "Samuel Collins"
                    },
                    {
                        "name": "Michael Borowitz"
                    }
                ],
                "author_detail": {
                    "name": "Michael Borowitz"
                },
                "author": "Michael Borowitz",
                "arxiv_comment": "24 pages, 10 pages main text",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06046v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06046v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10389v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10389v1",
                "updated": "2025-05-15T15:11:48Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    11,
                    48,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T15:11:48Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    11,
                    48,
                    3,
                    135,
                    0
                ],
                "title": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting\n  Aspect-based Opinion Quadruples",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-domain Multilingual Sentiment Analysis in Industry: Predicting\n  Aspect-based Opinion Quadruples"
                },
                "summary": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper explores the design of an aspect-based sentiment analysis system\nusing large language models (LLMs) for real-world use. We focus on quadruple\nopinion extraction -- identifying aspect categories, sentiment polarity,\ntargets, and opinion expressions from text data across different domains and\nlanguages. Using internal datasets, we investigate whether a single fine-tuned\nmodel can effectively handle multiple domain-specific taxonomies\nsimultaneously. We demonstrate that a combined multi-domain model achieves\nperformance comparable to specialized single-domain models while reducing\noperational complexity. We also share lessons learned for handling\nnon-extractive predictions and evaluating various failure modes when developing\nLLM-based systems for structured prediction tasks."
                },
                "authors": [
                    {
                        "name": "Benjamin White"
                    },
                    {
                        "name": "Anastasia Shimorina"
                    }
                ],
                "author_detail": {
                    "name": "Anastasia Shimorina"
                },
                "author": "Anastasia Shimorina",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10389v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10389v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07119v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07119v2",
                "updated": "2025-05-15T15:05:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    15,
                    5,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-11T21:05:33Z",
                "published_parsed": [
                    2025,
                    5,
                    11,
                    21,
                    5,
                    33,
                    6,
                    131,
                    0
                ],
                "title": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Scalable IoT Deployment for Visual Anomaly Detection via\n  Efficient Compression"
                },
                "summary": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visual Anomaly Detection (VAD) is a key task in industrial settings, where\nminimizing operational costs is essential. Deploying deep learning models\nwithin Internet of Things (IoT) environments introduces specific challenges due\nto limited computational power and bandwidth of edge devices. This study\ninvestigates how to perform VAD effectively under such constraints by\nleveraging compact, efficient processing strategies. We evaluate several data\ncompression techniques, examining the tradeoff between system latency and\ndetection accuracy. Experiments on the MVTec AD benchmark demonstrate that\nsignificant compression can be achieved with minimal loss in anomaly detection\nperformance compared to uncompressed data. Current results show up to 80%\nreduction in end-to-end inference time, including edge processing,\ntransmission, and server computation."
                },
                "authors": [
                    {
                        "name": "Arianna Stropeni"
                    },
                    {
                        "name": "Francesco Borsatti"
                    },
                    {
                        "name": "Manuel Barusco"
                    },
                    {
                        "name": "Davide Dalle Pezze"
                    },
                    {
                        "name": "Marco Fabris"
                    },
                    {
                        "name": "Gian Antonio Susto"
                    }
                ],
                "author_detail": {
                    "name": "Gian Antonio Susto"
                },
                "author": "Gian Antonio Susto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07119v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07119v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10375v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10375v1",
                "updated": "2025-05-15T14:59:17Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    59,
                    17,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:59:17Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    59,
                    17,
                    3,
                    135,
                    0
                ],
                "title": "Are Sparse Autoencoders Useful for Java Function Bug Detection?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Are Sparse Autoencoders Useful for Java Function Bug Detection?"
                },
                "summary": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Software vulnerabilities such as buffer overflows and SQL injections are a\nmajor source of security breaches. Traditional methods for vulnerability\ndetection remain essential but are limited by high false positive rates,\nscalability issues, and reliance on manual effort. These constraints have\ndriven interest in AI-based approaches to automated vulnerability detection and\nsecure code generation. While Large Language Models (LLMs) have opened new\navenues for classification tasks, their complexity and opacity pose challenges\nfor interpretability and deployment. Sparse Autoencoder offer a promising\nsolution to this problem. We explore whether SAEs can serve as a lightweight,\ninterpretable alternative for bug detection in Java functions. We evaluate the\neffectiveness of SAEs when applied to representations from GPT-2 Small and\nGemma 2B, examining their capacity to highlight buggy behaviour without\nfine-tuning the underlying LLMs. We found that SAE-derived features enable bug\ndetection with an F1 score of up to 89%, consistently outperforming fine-tuned\ntransformer encoder baselines. Our work provides the first empirical evidence\nthat SAEs can be used to detect software bugs directly from the internal\nrepresentations of pretrained LLMs, without any fine-tuning or task-specific\nsupervision."
                },
                "authors": [
                    {
                        "name": "Rui Melo"
                    },
                    {
                        "name": "Claudia Mamede"
                    },
                    {
                        "name": "Andre Catarino"
                    },
                    {
                        "name": "Rui Abreu"
                    },
                    {
                        "name": "Henrique Lopes Cardoso"
                    }
                ],
                "author_detail": {
                    "name": "Henrique Lopes Cardoso"
                },
                "author": "Henrique Lopes Cardoso",
                "arxiv_comment": "10 pages, 10 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10375v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10375v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13231v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13231v2",
                "updated": "2025-05-15T14:47:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    47,
                    2,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-17T14:43:56Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    14,
                    43,
                    56,
                    3,
                    107,
                    0
                ],
                "title": "WildFireCan-MMD: A Multimodal Dataset for Classification of\n  User-Generated Content During Wildfires in Canada",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "WildFireCan-MMD: A Multimodal Dataset for Classification of\n  User-Generated Content During Wildfires in Canada"
                },
                "summary": "Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. We present WildFireCan-MMD, a\nnew multimodal dataset of X posts from recent Canadian wildfires, annotated\nacross twelve key themes. Evaluating both vision-language models and\ncustom-trained classifiers, we show that while zero-shot prompting offers quick\ndeployment, even simple trained models outperform them when labelled data is\navailable. Our best-performing transformer-based fine-tuned model reaches 83%\nf-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this\nmodel can be used to uncover trends during wildfires. Our findings highlight\nthe enduring importance of tailored datasets and task-specific training.\nImportantly, such datasets should be localized, as disaster response\nrequirements vary across regions and contexts.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Rapid information access is vital during wildfires, yet traditional data\nsources are slow and costly. Social media offers real-time updates, but\nextracting relevant insights remains a challenge. We present WildFireCan-MMD, a\nnew multimodal dataset of X posts from recent Canadian wildfires, annotated\nacross twelve key themes. Evaluating both vision-language models and\ncustom-trained classifiers, we show that while zero-shot prompting offers quick\ndeployment, even simple trained models outperform them when labelled data is\navailable. Our best-performing transformer-based fine-tuned model reaches 83%\nf-score, outperforming gpt4 by 23%. As a use case, we demonstrate how this\nmodel can be used to uncover trends during wildfires. Our findings highlight\nthe enduring importance of tailored datasets and task-specific training.\nImportantly, such datasets should be localized, as disaster response\nrequirements vary across regions and contexts."
                },
                "authors": [
                    {
                        "name": "Braeden Sherritt"
                    },
                    {
                        "name": "Isar Nejadgholi"
                    },
                    {
                        "name": "Marzieh Amini"
                    }
                ],
                "author_detail": {
                    "name": "Marzieh Amini"
                },
                "author": "Marzieh Amini",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.13231v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13231v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.06899v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.06899v2",
                "updated": "2025-05-15T14:37:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    37,
                    1,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-10T04:05:38Z",
                "published_parsed": [
                    2025,
                    3,
                    10,
                    4,
                    5,
                    38,
                    0,
                    69,
                    0
                ],
                "title": "KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue\n  Corpus",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "KwaiChat: A Large-Scale Video-Driven Multilingual Mixed-Type Dialogue\n  Corpus"
                },
                "summary": "Video-based dialogue systems, such as education assistants, have compelling\napplication value, thereby garnering growing interest. However, the current\nvideo-based dialogue systems are limited by their reliance on a single dialogue\ntype, which hinders their versatility in practical applications across a range\nof scenarios, including question-answering, emotional dialog, etc. In this\npaper, we identify this challenge as how to generate video-driven multilingual\nmixed-type dialogues. To mitigate this challenge, we propose a novel task and\ncreate a human-to-human video-driven multilingual mixed-type dialogue corpus,\ntermed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues,\nacross 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally,\nwe establish baseline models on KwaiChat. An extensive analysis of 7 distinct\nLLMs on KwaiChat reveals that GPT-4o achieves the best performance but still\ncannot perform well in this situation even with the help of in-context learning\nand fine-tuning, which indicates that the task is not trivial and needs further\nresearch.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Video-based dialogue systems, such as education assistants, have compelling\napplication value, thereby garnering growing interest. However, the current\nvideo-based dialogue systems are limited by their reliance on a single dialogue\ntype, which hinders their versatility in practical applications across a range\nof scenarios, including question-answering, emotional dialog, etc. In this\npaper, we identify this challenge as how to generate video-driven multilingual\nmixed-type dialogues. To mitigate this challenge, we propose a novel task and\ncreate a human-to-human video-driven multilingual mixed-type dialogue corpus,\ntermed KwaiChat, containing a total of 93,209 videos and 246,080 dialogues,\nacross 4 dialogue types, 30 domains, 4 languages, and 13 topics. Additionally,\nwe establish baseline models on KwaiChat. An extensive analysis of 7 distinct\nLLMs on KwaiChat reveals that GPT-4o achieves the best performance but still\ncannot perform well in this situation even with the help of in-context learning\nand fine-tuning, which indicates that the task is not trivial and needs further\nresearch."
                },
                "authors": [
                    {
                        "name": "Xiaoming Shi"
                    },
                    {
                        "name": "Zeming Liu"
                    },
                    {
                        "name": "Yiming Lei"
                    },
                    {
                        "name": "Chenkai Zhang"
                    },
                    {
                        "name": "Haitao Leng"
                    },
                    {
                        "name": "Chuan Wang"
                    },
                    {
                        "name": "Qingjie Liu"
                    },
                    {
                        "name": "Wanxiang Che"
                    },
                    {
                        "name": "Shaoguo Liu"
                    },
                    {
                        "name": "Size Li"
                    },
                    {
                        "name": "Yunhong Wang"
                    }
                ],
                "author_detail": {
                    "name": "Yunhong Wang"
                },
                "author": "Yunhong Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.06899v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.06899v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.03780v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.03780v2",
                "updated": "2025-05-15T14:26:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    26,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-30T12:57:21Z",
                "published_parsed": [
                    2025,
                    4,
                    30,
                    12,
                    57,
                    21,
                    2,
                    120,
                    0
                ],
                "title": "GPU Performance Portability needs Autotuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GPU Performance Portability needs Autotuning"
                },
                "summary": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with kernel parameter autotuning to\nenable portable LLM inference with state-of-the-art performance without code\nchanges. Focusing on flash attention -- a widespread performance critical LLM\nkernel -- we demonstrate that this approach explores up to 15x more kernel\nparameter configurations, produces significantly more diverse code across\nmultiple dimensions, and even outperforms vendor-optimized implementations by\nup to 230%, all while reducing kernel code size by 70x and eliminating manual\ncode optimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As LLMs grow in complexity, achieving state-of-the-art performance requires\ntight co-design across algorithms, software, and hardware. Today's reliance on\na single dominant platform limits portability, creates vendor lock-in, and\nraises barriers for new AI hardware. In this work, we make the case for\ncombining just-in-time (JIT) compilation with kernel parameter autotuning to\nenable portable LLM inference with state-of-the-art performance without code\nchanges. Focusing on flash attention -- a widespread performance critical LLM\nkernel -- we demonstrate that this approach explores up to 15x more kernel\nparameter configurations, produces significantly more diverse code across\nmultiple dimensions, and even outperforms vendor-optimized implementations by\nup to 230%, all while reducing kernel code size by 70x and eliminating manual\ncode optimizations. Our results highlight autotuning as a promising path to\nunlocking model portability across GPU vendors."
                },
                "authors": [
                    {
                        "name": "Burkhard Ringlein"
                    },
                    {
                        "name": "Thomas Parnell"
                    },
                    {
                        "name": "Radu Stoica"
                    }
                ],
                "author_detail": {
                    "name": "Radu Stoica"
                },
                "author": "Radu Stoica",
                "arxiv_comment": "typos, fix grammatical mistakes",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.03780v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.03780v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.PL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.05760v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.05760v3",
                "updated": "2025-05-15T14:23:05Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    23,
                    5,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-23T18:47:14Z",
                "published_parsed": [
                    2025,
                    2,
                    23,
                    18,
                    47,
                    14,
                    6,
                    54,
                    0
                ],
                "title": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Lazy Student's Dream: ChatGPT Passing an Engineering Course on Its\n  Own"
                },
                "summary": "This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper presents a comprehensive investigation into the capability of\nLarge Language Models (LLMs) to successfully complete a semester-long\nundergraduate control systems course. Through evaluation of 115 course\ndeliverables, we assess LLM performance using ChatGPT under a \"minimal effort\"\nprotocol that simulates realistic student usage patterns. The investigation\nemploys a rigorous testing methodology across multiple assessment formats, from\nauto-graded multiple choice questions to complex Python programming tasks and\nlong-form analytical writing. Our analysis provides quantitative insights into\nAI's strengths and limitations in handling mathematical formulations, coding\nchallenges, and theoretical concepts in control systems engineering. The LLM\nachieved a B-grade performance (82.24\\%), approaching but not exceeding the\nclass average (84.99\\%), with strongest results in structured assignments and\ngreatest limitations in open-ended projects. The findings inform discussions\nabout course design adaptation in response to AI advancement, moving beyond\nsimple prohibition towards thoughtful integration of these tools in engineering\neducation. Additional materials including syllabus, examination papers, design\nprojects, and example responses can be found at the project website:\nhttps://gradegpt.github.io."
                },
                "authors": [
                    {
                        "name": "Gokul Puthumanaillam"
                    },
                    {
                        "name": "Melkior Ornik"
                    }
                ],
                "author_detail": {
                    "name": "Melkior Ornik"
                },
                "author": "Melkior Ornik",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.05760v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.05760v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10330v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10330v1",
                "updated": "2025-05-15T14:19:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    19,
                    1,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:19:01Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    19,
                    1,
                    3,
                    135,
                    0
                ],
                "title": "Efficient Adaptation of Reinforcement Learning Agents to Sudden\n  Environmental Change",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient Adaptation of Reinforcement Learning Agents to Sudden\n  Environmental Change"
                },
                "summary": "Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Real-world autonomous decision-making systems, from robots to recommendation\nengines, must operate in environments that change over time. While deep\nreinforcement learning (RL) has shown an impressive ability to learn optimal\npolicies in stationary environments, most methods are data intensive and assume\na world that does not change between training and test time. As a result,\nconventional RL methods struggle to adapt when conditions change. This poses a\nfundamental challenge: how can RL agents efficiently adapt their behavior when\nencountering novel environmental changes during deployment without\ncatastrophically forgetting useful prior knowledge? This dissertation\ndemonstrates that efficient online adaptation requires two key capabilities:\n(1) prioritized exploration and sampling strategies that help identify and\nlearn from relevant experiences, and (2) selective preservation of prior\nknowledge through structured representations that can be updated without\ndisruption to reusable components."
                },
                "authors": [
                    {
                        "name": "Jonathan Clifford Balloch"
                    }
                ],
                "author_detail": {
                    "name": "Jonathan Clifford Balloch"
                },
                "author": "Jonathan Clifford Balloch",
                "arxiv_comment": "PhD Dissertation, 131 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10330v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10330v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.00777v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.00777v2",
                "updated": "2025-05-15T14:18:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    18,
                    58,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-01T09:00:10Z",
                "published_parsed": [
                    2025,
                    1,
                    1,
                    9,
                    0,
                    10,
                    2,
                    1,
                    0
                ],
                "title": "FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "FitCF: A Framework for Automatic Feature Importance-guided\n  Counterfactual Example Generation"
                },
                "summary": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Counterfactual examples are widely used in natural language processing (NLP)\nas valuable data to improve models, and in explainable artificial intelligence\n(XAI) to understand model behavior. The automated generation of counterfactual\nexamples remains a challenging task even for large language models (LLMs),\ndespite their impressive performance on many tasks. In this paper, we first\nintroduce ZeroCF, a faithful approach for leveraging important words derived\nfrom feature attribution methods to generate counterfactual examples in a\nzero-shot setting. Second, we present a new framework, FitCF, which further\nverifies aforementioned counterfactuals by label flip verification and then\ninserts them as demonstrations for few-shot prompting, outperforming two\nstate-of-the-art baselines. Through ablation studies, we identify the\nimportance of each of FitCF's core components in improving the quality of\ncounterfactuals, as assessed through flip rate, perplexity, and similarity\nmeasures. Furthermore, we show the effectiveness of LIME and Integrated\nGradients as backbone attribution methods for FitCF and find that the number of\ndemonstrations has the largest effect on performance. Finally, we reveal a\nstrong correlation between the faithfulness of feature attribution scores and\nthe quality of generated counterfactuals."
                },
                "authors": [
                    {
                        "name": "Qianli Wang"
                    },
                    {
                        "name": "Nils Feldhus"
                    },
                    {
                        "name": "Simon Ostermann"
                    },
                    {
                        "name": "Luis Felipe Villa-Arenas"
                    },
                    {
                        "name": "Sebastian Mller"
                    },
                    {
                        "name": "Vera Schmitt"
                    }
                ],
                "author_detail": {
                    "name": "Vera Schmitt"
                },
                "author": "Vera Schmitt",
                "arxiv_comment": "ACL 2025 Findings; camera-ready version",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.00777v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.00777v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2409.13338v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2409.13338v3",
                "updated": "2025-05-15T14:13:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    13,
                    36,
                    3,
                    135,
                    0
                ],
                "published": "2024-09-20T08:57:20Z",
                "published_parsed": [
                    2024,
                    9,
                    20,
                    8,
                    57,
                    20,
                    4,
                    264,
                    0
                ],
                "title": "Time Awareness in Large Language Models: Benchmarking Fact Recall Across\n  Time",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Time Awareness in Large Language Models: Benchmarking Fact Recall Across\n  Time"
                },
                "summary": "Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Who is the US President? The answer changes depending on when the question is\nasked. While large language models (LLMs) are evaluated on various reasoning\ntasks, they often miss a crucial dimension: time. In real-world scenarios, the\ncorrectness of answers is frequently tied to temporal context. To address this\ngap, we present a novel framework and dataset spanning over 8,000 events from\n2018 to 2024, annotated with day-level granularity and sourced globally across\ndomains such as politics, science, and business. Our TimeShift evaluation\nmethod systematically probes LLMs for temporal reasoning, revealing that base\nmodels often outperform instruction-tuned and synthetic-trained counterparts on\ntime-sensitive recall. Additionally, we find that even large-scale models\nexhibit brittleness in handling paraphrased facts, highlighting unresolved\nchallenges in temporal consistency. By identifying these limitations, our work\nprovides a significant step toward advancing time-aware language models capable\nof adapting to the dynamic nature of real-world knowledge."
                },
                "authors": [
                    {
                        "name": "David Herel"
                    },
                    {
                        "name": "Vojtech Bartek"
                    },
                    {
                        "name": "Jiri Jirak"
                    },
                    {
                        "name": "Tomas Mikolov"
                    }
                ],
                "author_detail": {
                    "name": "Tomas Mikolov"
                },
                "author": "Tomas Mikolov",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2409.13338v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2409.13338v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10325v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10325v1",
                "updated": "2025-05-15T14:08:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    8,
                    0,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:08:00Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    8,
                    0,
                    3,
                    135,
                    0
                ],
                "title": "A Representation Learning Approach to Feature Drift Detection in\n  Wireless Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Representation Learning Approach to Feature Drift Detection in\n  Wireless Networks"
                },
                "summary": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI is foreseen to be a centerpiece in next generation wireless networks\nenabling enabling ubiquitous communication as well as new services. However, in\nreal deployment, feature distribution changes may degrade the performance of AI\nmodels and lead to undesired behaviors. To counter for undetected model\ndegradation, we propose ALERT; a method that can detect feature distribution\nchanges and trigger model re-training that works well on two wireless network\nuse cases: wireless fingerprinting and link anomaly detection. ALERT includes\nthree components: representation learning, statistical testing and utility\nassessment. We rely on MLP for designing the representation learning component,\non Kolmogorov-Smirnov and Population Stability Index tests for designing the\nstatistical testing and a new function for utility assessment. We show the\nsuperiority of the proposed method against ten standard drift detection methods\navailable in the literature on two wireless network use cases."
                },
                "authors": [
                    {
                        "name": "Athanasios Tziouvaras"
                    },
                    {
                        "name": "Blaz Bertalanic"
                    },
                    {
                        "name": "George Floros"
                    },
                    {
                        "name": "Kostas Kolomvatsos"
                    },
                    {
                        "name": "Panagiotis Sarigiannidis"
                    },
                    {
                        "name": "Carolina Fortuna"
                    }
                ],
                "author_detail": {
                    "name": "Carolina Fortuna"
                },
                "author": "Carolina Fortuna",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10325v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10325v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10322v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10322v1",
                "updated": "2025-05-15T14:06:38Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    6,
                    38,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:06:38Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    6,
                    38,
                    3,
                    135,
                    0
                ],
                "title": "Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate\n  Descent Framework",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Asynchronous Decentralized SGD under Non-Convexity: A Block-Coordinate\n  Descent Framework"
                },
                "summary": "Decentralized optimization has become vital for leveraging distributed data\nwithout central control, enhancing scalability and privacy. However, practical\ndeployments face fundamental challenges due to heterogeneous computation speeds\nand unpredictable communication delays. This paper introduces a refined model\nof Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under\npractical assumptions of bounded computation and communication times. To\nunderstand the convergence of ADSGD, we first analyze Asynchronous Stochastic\nBlock Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges\nunder computation-delay-independent step sizes. The convergence result is\nestablished without assuming bounded data heterogeneity. Empirical experiments\nreveal that ADSGD outperforms existing methods in wall-clock convergence time\nacross various scenarios. With its simplicity, efficiency in memory and\ncommunication, and resilience to communication and computation delays, ADSGD is\nwell-suited for real-world decentralized learning tasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Decentralized optimization has become vital for leveraging distributed data\nwithout central control, enhancing scalability and privacy. However, practical\ndeployments face fundamental challenges due to heterogeneous computation speeds\nand unpredictable communication delays. This paper introduces a refined model\nof Asynchronous Decentralized Stochastic Gradient Descent (ADSGD) under\npractical assumptions of bounded computation and communication times. To\nunderstand the convergence of ADSGD, we first analyze Asynchronous Stochastic\nBlock Coordinate Descent (ASBCD) as a tool, and then show that ADSGD converges\nunder computation-delay-independent step sizes. The convergence result is\nestablished without assuming bounded data heterogeneity. Empirical experiments\nreveal that ADSGD outperforms existing methods in wall-clock convergence time\nacross various scenarios. With its simplicity, efficiency in memory and\ncommunication, and resilience to communication and computation delays, ADSGD is\nwell-suited for real-world decentralized learning tasks."
                },
                "authors": [
                    {
                        "name": "Yijie Zhou"
                    },
                    {
                        "name": "Shi Pu"
                    }
                ],
                "author_detail": {
                    "name": "Shi Pu"
                },
                "author": "Shi Pu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10322v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10322v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.OC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2411.19477v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2411.19477v3",
                "updated": "2025-05-15T14:06:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    6,
                    27,
                    3,
                    135,
                    0
                ],
                "published": "2024-11-29T05:29:47Z",
                "published_parsed": [
                    2024,
                    11,
                    29,
                    5,
                    29,
                    47,
                    4,
                    334,
                    0
                ],
                "title": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Simple and Provable Scaling Laws for the Test-Time Compute of Large\n  Language Models"
                },
                "summary": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose two simple, principled and practical algorithms that enjoy\nprovable scaling laws for the test-time compute of large language models\n(LLMs). The first one is a two-stage knockout-style algorithm: given an input\nproblem, it first generates multiple candidate solutions, and then aggregate\nthem via a knockout tournament for the final output. Assuming that the LLM can\ngenerate a correct solution with non-zero probability and do better than a\nrandom guess in comparing a pair of correct and incorrect solutions, we prove\ntheoretically that the failure probability of this algorithm decays to zero\nexponentially or by a power law (depending on the specific way of scaling) as\nits test-time compute grows. The second one is a two-stage league-style\nalgorithm, where each candidate is evaluated by its average win rate against\nmultiple opponents, rather than eliminated upon loss to a single opponent.\nUnder analogous but more robust assumptions, we prove that its failure\nprobability also decays to zero exponentially with more test-time compute. Both\nalgorithms require a black-box LLM and nothing else (e.g., no verifier or\nreward model) for a minimalistic implementation, which makes them appealing for\npractical applications and easy to adapt for different tasks. Through extensive\nexperiments with diverse models and datasets, we validate the proposed theories\nand demonstrate the outstanding scaling properties of both algorithms."
                },
                "authors": [
                    {
                        "name": "Yanxi Chen"
                    },
                    {
                        "name": "Xuchen Pan"
                    },
                    {
                        "name": "Yaliang Li"
                    },
                    {
                        "name": "Bolin Ding"
                    },
                    {
                        "name": "Jingren Zhou"
                    }
                ],
                "author_detail": {
                    "name": "Jingren Zhou"
                },
                "author": "Jingren Zhou",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2411.19477v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2411.19477v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10321v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10321v1",
                "updated": "2025-05-15T14:06:00Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    6,
                    0,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:06:00Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    6,
                    0,
                    3,
                    135,
                    0
                ],
                "title": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AutoPentest: Enhancing Vulnerability Management With Autonomous LLM\n  Agents"
                },
                "summary": "A recent area of increasing research is the use of Large Language Models\n(LLMs) in penetration testing, which promises to reduce costs and thus allow\nfor higher frequency. We conduct a review of related work, identifying best\npractices and common evaluation issues. We then present AutoPentest, an\napplication for performing black-box penetration tests with a high degree of\nautonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent\nframework LangChain. It can perform complex multi-step tasks, augmented by\nexternal tools and knowledge bases. We conduct a study on three\ncapture-the-flag style Hack The Box (HTB) machines, comparing our\nimplementation AutoPentest with the baseline approach of manually using the\nChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the\nsubtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.\nWe measure a total cost of \\$96.20 US when using AutoPentest across all\nexperiments, while a one-month subscription to ChatGPT Plus costs \\$20. The\nresults show that further implementation efforts and the use of more powerful\nLLMs released in the future are likely to make this a viable part of\nvulnerability management.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A recent area of increasing research is the use of Large Language Models\n(LLMs) in penetration testing, which promises to reduce costs and thus allow\nfor higher frequency. We conduct a review of related work, identifying best\npractices and common evaluation issues. We then present AutoPentest, an\napplication for performing black-box penetration tests with a high degree of\nautonomy. AutoPentest is based on the LLM GPT-4o from OpenAI and the LLM agent\nframework LangChain. It can perform complex multi-step tasks, augmented by\nexternal tools and knowledge bases. We conduct a study on three\ncapture-the-flag style Hack The Box (HTB) machines, comparing our\nimplementation AutoPentest with the baseline approach of manually using the\nChatGPT-4o user interface. Both approaches are able to complete 15-25 % of the\nsubtasks on the HTB machines, with AutoPentest slightly outperforming ChatGPT.\nWe measure a total cost of \\$96.20 US when using AutoPentest across all\nexperiments, while a one-month subscription to ChatGPT Plus costs \\$20. The\nresults show that further implementation efforts and the use of more powerful\nLLMs released in the future are likely to make this a viable part of\nvulnerability management."
                },
                "authors": [
                    {
                        "name": "Julius Henke"
                    }
                ],
                "author_detail": {
                    "name": "Julius Henke"
                },
                "author": "Julius Henke",
                "arxiv_comment": "24 pages, 1 figure, for implementation, see\n  https://github.com/JuliusHenke/autopentest",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10321v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10321v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10320v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10320v1",
                "updated": "2025-05-15T14:05:15Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    5,
                    15,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:05:15Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    5,
                    15,
                    3,
                    135,
                    0
                ],
                "title": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "J1: Incentivizing Thinking in LLM-as-a-Judge via Reinforcement Learning"
                },
                "summary": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The progress of AI is bottlenecked by the quality of evaluation, and powerful\nLLM-as-a-Judge models have proved to be a core solution. Improved judgment\nability is enabled by stronger chain-of-thought reasoning, motivating the need\nto find the best recipes for training such models to think. In this work we\nintroduce J1, a reinforcement learning approach to training such models. Our\nmethod converts both verifiable and non-verifiable prompts to judgment tasks\nwith verifiable rewards that incentivize thinking and mitigate judgment bias.\nIn particular, our approach outperforms all other existing 8B or 70B models\nwhen trained at those sizes, including models distilled from DeepSeek-R1. J1\nalso outperforms o1-mini, and even R1 on some benchmarks, despite training a\nsmaller model. We provide analysis and ablations comparing Pairwise-J1 vs\nPointwise-J1 models, offline vs online training recipes, reward strategies,\nseed prompts, and variations in thought length and content. We find that our\nmodels make better judgments by learning to outline evaluation criteria,\ncomparing against self-generated reference answers, and re-evaluating the\ncorrectness of model responses."
                },
                "authors": [
                    {
                        "name": "Chenxi Whitehouse"
                    },
                    {
                        "name": "Tianlu Wang"
                    },
                    {
                        "name": "Ping Yu"
                    },
                    {
                        "name": "Xian Li"
                    },
                    {
                        "name": "Jason Weston"
                    },
                    {
                        "name": "Ilia Kulikov"
                    },
                    {
                        "name": "Swarnadeep Saha"
                    }
                ],
                "author_detail": {
                    "name": "Swarnadeep Saha"
                },
                "author": "Swarnadeep Saha",
                "arxiv_comment": "10 pages, 8 tables, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10320v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10320v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09583v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09583v2",
                "updated": "2025-05-15T14:03:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    3,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T17:31:16Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    17,
                    31,
                    16,
                    2,
                    134,
                    0
                ],
                "title": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Beyond Likes: How Normative Feedback Complements Engagement Signals on\n  Social Media"
                },
                "summary": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Many online platforms incorporate engagement signals--such as likes and\nupvotes--into their content ranking systems and interface design. These signals\nare designed to boost user engagement. However, they can unintentionally\nelevate content that is less inclusive and may not support normatively\ndesirable behavior. This issue becomes especially concerning when toxic content\ncorrelates strongly with popularity indicators such as likes and upvotes. In\nthis study, we propose structured prosocial feedback as a complementary signal\nto likes and upvotes--one that highlights content quality based on normative\ncriteria to help address the limitations of conventional engagement signals. We\nbegin by designing and implementing a machine learning feedback system powered\nby a large language model (LLM), which evaluates user comments based on\nprinciples of positive psychology, such as individual well-being, constructive\nsocial media use, and character strengths. We then conduct a pre-registered\nuser study to examine how existing peer-based and the new expert-based feedback\ninteract to shape users' selection of comments in a social media setting.\nResults show that peer feedback increases conformity to popularity cues, while\nexpert feedback shifts preferences toward normatively higher-quality content.\nMoreover, incorporating expert feedback alongside peer evaluations improves\nalignment with expert assessments and contributes to a less toxic community\nenvironment. This illustrates the added value of normative cues--such as expert\nscores generated by LLMs using psychological rubrics--and underscores the\npotential benefits of incorporating such signals into platform feedback systems\nto foster healthier online environments."
                },
                "authors": [
                    {
                        "name": "Yuchen Wu"
                    },
                    {
                        "name": "Mingduo Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Mingduo Zhao"
                },
                "author": "Mingduo Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09583v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09583v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10315v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10315v1",
                "updated": "2025-05-15T14:00:19Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    0,
                    19,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T14:00:19Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    14,
                    0,
                    19,
                    3,
                    135,
                    0
                ],
                "title": "Private Transformer Inference in MLaaS: A Survey",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Private Transformer Inference in MLaaS: A Survey"
                },
                "summary": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Transformer models have revolutionized AI, powering applications like content\ngeneration and sentiment analysis. However, their deployment in Machine\nLearning as a Service (MLaaS) raises significant privacy concerns, primarily\ndue to the centralized processing of sensitive user data. Private Transformer\nInference (PTI) offers a solution by utilizing cryptographic techniques such as\nsecure multi-party computation and homomorphic encryption, enabling inference\nwhile preserving both user data and model privacy. This paper reviews recent\nPTI advancements, highlighting state-of-the-art solutions and challenges. We\nalso introduce a structured taxonomy and evaluation framework for PTI, focusing\non balancing resource efficiency with privacy and bridging the gap between\nhigh-performance inference and data privacy."
                },
                "authors": [
                    {
                        "name": "Yang Li"
                    },
                    {
                        "name": "Xinyu Zhou"
                    },
                    {
                        "name": "Yitong Wang"
                    },
                    {
                        "name": "Liangxin Qian"
                    },
                    {
                        "name": "Jun Zhao"
                    }
                ],
                "author_detail": {
                    "name": "Jun Zhao"
                },
                "author": "Jun Zhao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10315v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10315v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10309v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10309v1",
                "updated": "2025-05-15T13:55:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    55,
                    27,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:55:27Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    55,
                    27,
                    3,
                    135,
                    0
                ],
                "title": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Empirically evaluating commonsense intelligence in large language models\n  with large-scale human judgments"
                },
                "summary": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Commonsense intelligence in machines is often assessed by static benchmarks\nthat compare a model's output against human-prescribed correct labels. An\nimportant, albeit implicit, assumption of these labels is that they accurately\ncapture what any human would think, effectively treating human common sense as\nhomogeneous. However, recent empirical work has shown that humans vary\nenormously in what they consider commonsensical; thus what appears self-evident\nto one benchmark designer may not be so to another. Here, we propose a novel\nmethod for evaluating common sense in artificial intelligence (AI),\nspecifically in large language models (LLMs), that incorporates empirically\nobserved heterogeneity among humans by measuring the correspondence between a\nmodel's judgment and that of a human population. We first find that, when\ntreated as independent survey respondents, most LLMs remain below the human\nmedian in their individual commonsense competence. Second, when used as\nsimulators of a hypothetical population, LLMs correlate with real humans only\nmodestly in the extent to which they agree on the same set of statements. In\nboth cases, smaller, open-weight models are surprisingly more competitive than\nlarger, proprietary frontier models. Our evaluation framework, which ties\ncommonsense intelligence to its cultural basis, contributes to the growing call\nfor adapting AI models to human collectivities that possess different, often\nincompatible, social stocks of knowledge."
                },
                "authors": [
                    {
                        "name": "Tuan Dung Nguyen"
                    },
                    {
                        "name": "Duncan J. Watts"
                    },
                    {
                        "name": "Mark E. Whiting"
                    }
                ],
                "author_detail": {
                    "name": "Mark E. Whiting"
                },
                "author": "Mark E. Whiting",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10309v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10309v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.SI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10300v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10300v1",
                "updated": "2025-05-15T13:49:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    49,
                    2,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:49:02Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    49,
                    2,
                    3,
                    135,
                    0
                ],
                "title": "AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial\n  Responsible AI Practices during Early Design Stages",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI LEGO: Scaffolding Cross-Functional Collaboration in Industrial\n  Responsible AI Practices during Early Design Stages"
                },
                "summary": "Responsible AI (RAI) efforts increasingly emphasize the importance of\naddressing potential harms early in the AI development lifecycle through\nsocial-technical lenses. However, in cross-functional industry teams, this work\nis often stalled by a persistent knowledge handoff challenge: the difficulty of\ntransferring high-level, early-stage technical design rationales from technical\nexperts to non-technical or user-facing roles for ethical evaluation and harm\nidentification. Through literature review and a co-design study with 8\npractitioners, we unpack how this challenge manifests -- technical design\nchoices are rarely handed off in ways that support meaningful engagement by\nnon-technical roles; collaborative workflows lack shared, visual structures to\nsupport mutual understanding; and non-technical practitioners are left without\nscaffolds for systematic harm evaluation. Existing tools like JIRA or Google\nDocs, while useful for product tracking, are ill-suited for supporting joint\nharm identification across roles, often requiring significant extra effort to\nalign understanding. To address this, we developed AI LEGO, a web-based\nprototype that supports cross-functional AI practitioners in effectively\nfacilitating knowledge handoff and identifying harmful design choices in the\nearly design stages. Technical roles use interactive blocks to draft\ndevelopment plans, while non-technical roles engage with those blocks through\nstage-specific checklists and LLM-driven persona simulations to surface\npotential harms. In a study with 18 cross-functional practitioners, AI LEGO\nincreased the volume and likelihood of harms identified compared to baseline\nworksheets. Participants found that its modular structure and persona prompts\nmade harm identification more accessible, fostering clearer and more\ncollaborative RAI practices in early design.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Responsible AI (RAI) efforts increasingly emphasize the importance of\naddressing potential harms early in the AI development lifecycle through\nsocial-technical lenses. However, in cross-functional industry teams, this work\nis often stalled by a persistent knowledge handoff challenge: the difficulty of\ntransferring high-level, early-stage technical design rationales from technical\nexperts to non-technical or user-facing roles for ethical evaluation and harm\nidentification. Through literature review and a co-design study with 8\npractitioners, we unpack how this challenge manifests -- technical design\nchoices are rarely handed off in ways that support meaningful engagement by\nnon-technical roles; collaborative workflows lack shared, visual structures to\nsupport mutual understanding; and non-technical practitioners are left without\nscaffolds for systematic harm evaluation. Existing tools like JIRA or Google\nDocs, while useful for product tracking, are ill-suited for supporting joint\nharm identification across roles, often requiring significant extra effort to\nalign understanding. To address this, we developed AI LEGO, a web-based\nprototype that supports cross-functional AI practitioners in effectively\nfacilitating knowledge handoff and identifying harmful design choices in the\nearly design stages. Technical roles use interactive blocks to draft\ndevelopment plans, while non-technical roles engage with those blocks through\nstage-specific checklists and LLM-driven persona simulations to surface\npotential harms. In a study with 18 cross-functional practitioners, AI LEGO\nincreased the volume and likelihood of harms identified compared to baseline\nworksheets. Participants found that its modular structure and persona prompts\nmade harm identification more accessible, fostering clearer and more\ncollaborative RAI practices in early design."
                },
                "authors": [
                    {
                        "name": "Muzhe Wu"
                    },
                    {
                        "name": "Yanzhi Zhao"
                    },
                    {
                        "name": "Shuyi Han"
                    },
                    {
                        "name": "Michael Xieyang Liu"
                    },
                    {
                        "name": "Hong Shen"
                    }
                ],
                "author_detail": {
                    "name": "Hong Shen"
                },
                "author": "Hong Shen",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10300v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10300v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10297v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10297v1",
                "updated": "2025-05-15T13:44:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    44,
                    32,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:44:32Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    44,
                    32,
                    3,
                    135,
                    0
                ],
                "title": "Defending the Edge: Representative-Attention for Mitigating Backdoor\n  Attacks in Federated Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defending the Edge: Representative-Attention for Mitigating Backdoor\n  Attacks in Federated Learning"
                },
                "summary": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Federated learning (FL) enhances privacy and reduces communication cost for\nresource-constrained edge clients by supporting distributed model training at\nthe edge. However, the heterogeneous nature of such devices produces diverse,\nnon-independent, and identically distributed (non-IID) data, making the\ndetection of backdoor attacks more challenging. In this paper, we propose a\nnovel federated representative-attention-based defense mechanism, named FeRA,\nthat leverages cross-client attention over internal feature representations to\ndistinguish benign from malicious clients. FeRA computes an anomaly score based\non representation reconstruction errors, effectively identifying clients whose\ninternal activations significantly deviate from the group consensus. Our\nevaluation demonstrates FeRA's robustness across various FL scenarios,\nincluding challenging non-IID data distributions typical of edge devices.\nExperimental results show that it effectively reduces backdoor attack success\nrates while maintaining high accuracy on the main task. The method is\nmodel-agnostic, attack-agnostic, and does not require labeled reference data,\nmaking it well suited to heterogeneous and resource-limited edge deployments."
                },
                "authors": [
                    {
                        "name": "Chibueze Peace Obioma"
                    },
                    {
                        "name": "Youcheng Sun"
                    },
                    {
                        "name": "Mustafa A. Mustafa"
                    }
                ],
                "author_detail": {
                    "name": "Mustafa A. Mustafa"
                },
                "author": "Mustafa A. Mustafa",
                "arxiv_comment": "Submitted to ESORICS 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10297v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10297v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.13295v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.13295v2",
                "updated": "2025-05-15T13:42:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    42,
                    18,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-18T21:32:24Z",
                "published_parsed": [
                    2025,
                    2,
                    18,
                    21,
                    32,
                    24,
                    1,
                    49,
                    0
                ],
                "title": "Demonstrating specification gaming in reasoning models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Demonstrating specification gaming in reasoning models"
                },
                "summary": "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1\nwill often hack the benchmark by default, while language models like GPT-4o and\nClaude 3.5 Sonnet need to be told that normal play won't work to hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We demonstrate LLM agent specification gaming by instructing models to win\nagainst a chess engine. We find reasoning models like OpenAI o3 and DeepSeek R1\nwill often hack the benchmark by default, while language models like GPT-4o and\nClaude 3.5 Sonnet need to be told that normal play won't work to hack.\n  We improve upon prior work like (Hubinger et al., 2024; Meinke et al., 2024;\nWeij et al., 2024) by using realistic task prompts and avoiding excess nudging.\nOur results suggest reasoning models may resort to hacking to solve difficult\nproblems, as observed in OpenAI (2024)'s o1 Docker escape during cyber\ncapabilities testing."
                },
                "authors": [
                    {
                        "name": "Alexander Bondarenko"
                    },
                    {
                        "name": "Denis Volk"
                    },
                    {
                        "name": "Dmitrii Volkov"
                    },
                    {
                        "name": "Jeffrey Ladish"
                    }
                ],
                "author_detail": {
                    "name": "Jeffrey Ladish"
                },
                "author": "Jeffrey Ladish",
                "arxiv_comment": "Updated with o3 results",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.13295v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.13295v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10282v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10282v1",
                "updated": "2025-05-15T13:30:39Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    30,
                    39,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:30:39Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    30,
                    39,
                    3,
                    135,
                    0
                ],
                "title": "From Questions to Clinical Recommendations: Large Language Models\n  Driving Evidence-Based Clinical Decision Making",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Questions to Clinical Recommendations: Large Language Models\n  Driving Evidence-Based Clinical Decision Making"
                },
                "summary": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Clinical evidence, derived from rigorous research and data analysis, provides\nhealthcare professionals with reliable scientific foundations for informed\ndecision-making. Integrating clinical evidence into real-time practice is\nchallenging due to the enormous workload, complex professional processes, and\ntime constraints. This highlights the need for tools that automate evidence\nsynthesis to support more efficient and accurate decision making in clinical\nsettings. This study introduces Quicker, an evidence-based clinical decision\nsupport system powered by large language models (LLMs), designed to automate\nevidence synthesis and generate clinical recommendations modeled after standard\nclinical guideline development processes. Quicker implements a fully automated\nchain that covers all phases, from questions to clinical recommendations, and\nfurther enables customized decision-making through integrated tools and\ninteractive user interfaces. To evaluate Quicker's capabilities, we developed\nthe Q2CRBench-3 benchmark dataset, based on clinical guideline development\nrecords for three different diseases. Experimental results highlighted\nQuicker's strong performance, with fine-grained question decomposition tailored\nto user preferences, retrieval sensitivities comparable to human experts, and\nliterature screening performance approaching comprehensive inclusion of\nrelevant studies. In addition, Quicker-assisted evidence assessment effectively\nsupported human reviewers, while Quicker's recommendations were more\ncomprehensive and logically coherent than those of clinicians. In system-level\ntesting, collaboration between a single reviewer and Quicker reduced the time\nrequired for recommendation development to 20-40 minutes. In general, our\nfindings affirm the potential of Quicker to help physicians make quicker and\nmore reliable evidence-based clinical decisions."
                },
                "authors": [
                    {
                        "name": "Dubai Li"
                    },
                    {
                        "name": "Nan Jiang"
                    },
                    {
                        "name": "Kangping Huang"
                    },
                    {
                        "name": "Ruiqi Tu"
                    },
                    {
                        "name": "Shuyu Ouyang"
                    },
                    {
                        "name": "Huayu Yu"
                    },
                    {
                        "name": "Lin Qiao"
                    },
                    {
                        "name": "Chen Yu"
                    },
                    {
                        "name": "Tianshu Zhou"
                    },
                    {
                        "name": "Danyang Tong"
                    },
                    {
                        "name": "Qian Wang"
                    },
                    {
                        "name": "Mengtao Li"
                    },
                    {
                        "name": "Xiaofeng Zeng"
                    },
                    {
                        "name": "Yu Tian"
                    },
                    {
                        "name": "Xinping Tian"
                    },
                    {
                        "name": "Jingsong Li"
                    }
                ],
                "author_detail": {
                    "name": "Jingsong Li"
                },
                "author": "Jingsong Li",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10282v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10282v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.04877v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.04877v2",
                "updated": "2025-05-15T13:27:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    27,
                    26,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-07T09:41:04Z",
                "published_parsed": [
                    2025,
                    4,
                    7,
                    9,
                    41,
                    4,
                    0,
                    97,
                    0
                ],
                "title": "System Log Parsing with Large Language Models: A Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "System Log Parsing with Large Language Models: A Review"
                },
                "summary": "Log data provides crucial insights for tasks like monitoring, root cause\nanalysis, and anomaly detection. Due to the vast volume of logs, automated log\nparsing is essential to transform semi-structured log messages into structured\nrepresentations. Recent advances in large language models (LLMs) have\nintroduced the new research field of LLM-based log parsing. Despite promising\nresults, there is no structured overview of the approaches in this relatively\nnew research field with the earliest advances published in late 2023. This work\nsystematically reviews 29 LLM-based log parsing methods. We benchmark seven of\nthem on public datasets and critically assess their comparability and the\nreproducibility of their reported results. Our findings summarize the advances\nof this new research field, with insights on how to report results, which data\nsets, metrics and which terminology to use, and which inconsistencies to avoid,\nwith code and results made publicly available for transparency.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Log data provides crucial insights for tasks like monitoring, root cause\nanalysis, and anomaly detection. Due to the vast volume of logs, automated log\nparsing is essential to transform semi-structured log messages into structured\nrepresentations. Recent advances in large language models (LLMs) have\nintroduced the new research field of LLM-based log parsing. Despite promising\nresults, there is no structured overview of the approaches in this relatively\nnew research field with the earliest advances published in late 2023. This work\nsystematically reviews 29 LLM-based log parsing methods. We benchmark seven of\nthem on public datasets and critically assess their comparability and the\nreproducibility of their reported results. Our findings summarize the advances\nof this new research field, with insights on how to report results, which data\nsets, metrics and which terminology to use, and which inconsistencies to avoid,\nwith code and results made publicly available for transparency."
                },
                "authors": [
                    {
                        "name": "Viktor Beck"
                    },
                    {
                        "name": "Max Landauer"
                    },
                    {
                        "name": "Markus Wurzenberger"
                    },
                    {
                        "name": "Florian Skopik"
                    },
                    {
                        "name": "Andreas Rauber"
                    }
                ],
                "author_detail": {
                    "name": "Andreas Rauber"
                },
                "author": "Andreas Rauber",
                "arxiv_comment": "36 pages, 11 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2504.04877v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.04877v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2; I.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10278v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10278v1",
                "updated": "2025-05-15T13:27:18Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    27,
                    18,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:27:18Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    27,
                    18,
                    3,
                    135,
                    0
                ],
                "title": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MASS: Multi-Agent Simulation Scaling for Portfolio Construction"
                },
                "summary": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM-based multi-agent has gained significant attention for their potential in\nsimulation and enhancing performance. However, existing works are limited to\npure simulations or are constrained by predefined workflows, restricting their\napplicability and effectiveness. In this paper, we introduce the Multi-Agent\nScaling Simulation (MASS) for portfolio construction. MASS achieves stable and\ncontinuous excess returns by progressively increasing the number of agents for\nlarge-scale simulations to gain a superior understanding of the market and\noptimizing agent distribution end-to-end through a reverse optimization\nprocess, rather than relying on a fixed workflow. We demonstrate its\nsuperiority through performance experiments, ablation studies, backtesting\nexperiments, experiments on updated data and stock pools, scaling experiments,\nparameter sensitivity experiments, and visualization experiments, conducted in\ncomparison with 6 state-of-the-art baselines on 3 challenging A-share stock\npools. We expect the paradigm established by MASS to expand to other tasks with\nsimilar characteristics. The implementation of MASS has been open-sourced at\nhttps://github.com/gta0804/MASS."
                },
                "authors": [
                    {
                        "name": "Taian Guo"
                    },
                    {
                        "name": "Haiyang Shen"
                    },
                    {
                        "name": "Jinsheng Huang"
                    },
                    {
                        "name": "Zhengyang Mao"
                    },
                    {
                        "name": "Junyu Luo"
                    },
                    {
                        "name": "Zhuoru Chen"
                    },
                    {
                        "name": "Xuhui Liu"
                    },
                    {
                        "name": "Bingyu Xia"
                    },
                    {
                        "name": "Luchen Liu"
                    },
                    {
                        "name": "Yun Ma"
                    },
                    {
                        "name": "Ming Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Ming Zhang"
                },
                "author": "Ming Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10278v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10278v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.07714v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.07714v2",
                "updated": "2025-05-15T13:15:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    15,
                    56,
                    3,
                    135,
                    0
                ],
                "published": "2024-05-13T13:02:50Z",
                "published_parsed": [
                    2024,
                    5,
                    13,
                    13,
                    2,
                    50,
                    0,
                    134,
                    0
                ],
                "title": "Joint Robotic Aerial Base Station Deployment and Wireless Backhauling in\n  6G Multi-hop Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Joint Robotic Aerial Base Station Deployment and Wireless Backhauling in\n  6G Multi-hop Networks"
                },
                "summary": "Due to their ability to anchor into tall urban landforms, such as lampposts\nor street lights, robotic aerial base stations (RABSs) can create a\nhyper-flexible wireless multi-hop heterogeneous network to meet the forthcoming\ngreen, densified, and dynamic network deployment to support, inter alia, high\ndata rates. In this work, we propose a network infrastructure that can\nconcurrently support the wireless backhaul link capacity and access link\ntraffic demand in the millimeter-wave (mmWave) frequency band. The RABSs\ngrasping locations, resource blocks (RBs) assignment, and route flow control\nare simultaneously optimized to maximize the served traffic demands. Robotic\nbase stations capitalize on the fact that traffic distribution varies\nconsiderably across both time and space within a given geographical area.\nHence, they are able to relocate to suitable locations, i.e., 'follow' the\ntraffic demand as it unfolds to increase the overall network efficiency. To\ntackle the curse of dimensionality of the proposed mixed-integer linear\nproblem, we propose a greedy algorithm to obtain a competitive solution with\nlow computational complexity. Compared to baseline models, which are\nheterogeneous networks with randomly deployed fixed small cells and\npre-allocated RBs for wireless access and backhaul links, a wide set of\nnumerical investigations reveals that robotic base stations could improve the\nserved traffic demand. Specifically, the proposed mode serves at most 65\\% more\ntraffic demand compared to an equal number of deployed fixed small cells.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Due to their ability to anchor into tall urban landforms, such as lampposts\nor street lights, robotic aerial base stations (RABSs) can create a\nhyper-flexible wireless multi-hop heterogeneous network to meet the forthcoming\ngreen, densified, and dynamic network deployment to support, inter alia, high\ndata rates. In this work, we propose a network infrastructure that can\nconcurrently support the wireless backhaul link capacity and access link\ntraffic demand in the millimeter-wave (mmWave) frequency band. The RABSs\ngrasping locations, resource blocks (RBs) assignment, and route flow control\nare simultaneously optimized to maximize the served traffic demands. Robotic\nbase stations capitalize on the fact that traffic distribution varies\nconsiderably across both time and space within a given geographical area.\nHence, they are able to relocate to suitable locations, i.e., 'follow' the\ntraffic demand as it unfolds to increase the overall network efficiency. To\ntackle the curse of dimensionality of the proposed mixed-integer linear\nproblem, we propose a greedy algorithm to obtain a competitive solution with\nlow computational complexity. Compared to baseline models, which are\nheterogeneous networks with randomly deployed fixed small cells and\npre-allocated RBs for wireless access and backhaul links, a wide set of\nnumerical investigations reveals that robotic base stations could improve the\nserved traffic demand. Specifically, the proposed mode serves at most 65\\% more\ntraffic demand compared to an equal number of deployed fixed small cells."
                },
                "authors": [
                    {
                        "name": "Wen Shang"
                    },
                    {
                        "name": "Yuan Liao"
                    },
                    {
                        "name": "Vasilis Friderikos"
                    },
                    {
                        "name": "Halim Yanikomeroglu"
                    }
                ],
                "author_detail": {
                    "name": "Halim Yanikomeroglu"
                },
                "author": "Halim Yanikomeroglu",
                "arxiv_doi": "10.1109/WCNC61545.2025.10978194",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/WCNC61545.2025.10978194",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2405.07714v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.07714v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_comment": "This version reflects the conference publication in 2025 IEEE\n  Wireless Communications and Networking Conference (WCNC), with a minor\n  modified title and text content. The conference is:\"Integrated Robotic Aerial\n  Base Stations Deployment and Backhaul Design in 6G Multihop Networks,\" 2025\n  IEEE WCNC doi: 10.1109/WCNC61545.2025.10978194",
                "arxiv_primary_category": {
                    "term": "cs.NI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.NI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10261v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10261v1",
                "updated": "2025-05-15T13:11:14Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    11,
                    14,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:11:14Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    11,
                    14,
                    3,
                    135,
                    0
                ],
                "title": "The Evolving Landscape of Generative Large Language Models and\n  Traditional Natural Language Processing in Medicine",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Evolving Landscape of Generative Large Language Models and\n  Traditional Natural Language Processing in Medicine"
                },
                "summary": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language processing (NLP) has been traditionally applied to medicine,\nand generative large language models (LLMs) have become prominent recently.\nHowever, the differences between them across different medical tasks remain\nunderexplored. We analyzed 19,123 studies, finding that generative LLMs\ndemonstrate advantages in open-ended tasks, while traditional NLP dominates in\ninformation extraction and analysis tasks. As these technologies advance,\nethical use of them is essential to ensure their potential in medical\napplications."
                },
                "authors": [
                    {
                        "name": "Rui Yang"
                    },
                    {
                        "name": "Huitao Li"
                    },
                    {
                        "name": "Matthew Yu Heng Wong"
                    },
                    {
                        "name": "Yuhe Ke"
                    },
                    {
                        "name": "Xin Li"
                    },
                    {
                        "name": "Kunyu Yu"
                    },
                    {
                        "name": "Jingchi Liao"
                    },
                    {
                        "name": "Jonathan Chong Kai Liew"
                    },
                    {
                        "name": "Sabarinath Vinod Nair"
                    },
                    {
                        "name": "Jasmine Chiat Ling Ong"
                    },
                    {
                        "name": "Irene Li"
                    },
                    {
                        "name": "Douglas Teodoro"
                    },
                    {
                        "name": "Chuan Hong"
                    },
                    {
                        "name": "Daniel Shu Wei Ting"
                    },
                    {
                        "name": "Nan Liu"
                    }
                ],
                "author_detail": {
                    "name": "Nan Liu"
                },
                "author": "Nan Liu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10261v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10261v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10260v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10260v1",
                "updated": "2025-05-15T13:10:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    47,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:10:47Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    47,
                    3,
                    135,
                    0
                ],
                "title": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations\n  in Social Media Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Comparing LLM Text Annotation Skills: A Study on Human Rights Violations\n  in Social Media Data"
                },
                "summary": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the era of increasingly sophisticated natural language processing (NLP)\nsystems, large language models (LLMs) have demonstrated remarkable potential\nfor diverse applications, including tasks requiring nuanced textual\nunderstanding and contextual reasoning. This study investigates the\ncapabilities of multiple state-of-the-art LLMs - GPT-3.5, GPT-4, LLAMA3,\nMistral 7B, and Claude-2 - for zero-shot and few-shot annotation of a complex\ntextual dataset comprising social media posts in Russian and Ukrainian.\nSpecifically, the focus is on the binary classification task of identifying\nreferences to human rights violations within the dataset.\n  To evaluate the effectiveness of these models, their annotations are compared\nagainst a gold standard set of human double-annotated labels across 1000\nsamples. The analysis includes assessing annotation performance under different\nprompting conditions, with prompts provided in both English and Russian.\nAdditionally, the study explores the unique patterns of errors and\ndisagreements exhibited by each model, offering insights into their strengths,\nlimitations, and cross-linguistic adaptability.\n  By juxtaposing LLM outputs with human annotations, this research contributes\nto understanding the reliability and applicability of LLMs for sensitive,\ndomain-specific tasks in multilingual contexts. It also sheds light on how\nlanguage models handle inherently subjective and context-dependent judgments, a\ncritical consideration for their deployment in real-world scenarios."
                },
                "authors": [
                    {
                        "name": "Poli Apollinaire Nemkova"
                    },
                    {
                        "name": "Solomon Ubani"
                    },
                    {
                        "name": "Mark V. Albert"
                    }
                ],
                "author_detail": {
                    "name": "Mark V. Albert"
                },
                "author": "Mark V. Albert",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10260v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10260v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10259v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10259v1",
                "updated": "2025-05-15T13:10:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    31,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:10:31Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    10,
                    31,
                    3,
                    135,
                    0
                ],
                "title": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SpecOffload: Unlocking Latent GPU Capacity for LLM Inference on\n  Resource-Constrained Devices"
                },
                "summary": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload .",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Efficient LLM inference on resource-constrained devices presents significant\nchallenges in compute and memory utilization. Due to limited GPU memory,\nexisting systems offload model weights to CPU memory, incurring substantial I/O\noverhead between the CPU and GPU. This leads to two major inefficiencies: (1)\nGPU cores are underutilized, often remaining idle while waiting for data to be\nloaded; and (2) GPU memory has low impact on performance, as reducing its\ncapacity has minimal effect on overall throughput.In this paper, we propose\nSpecOffload, a high-throughput inference engine that embeds speculative\ndecoding into offloading. Our key idea is to unlock latent GPU resources for\nstoring and executing a draft model used for speculative decoding, thus\naccelerating inference at near-zero additional cost. To support this, we\ncarefully orchestrate the interleaved execution of target and draft models in\nspeculative decoding within the offloading pipeline, and propose a planner to\nmanage tensor placement and select optimal parameters. Compared to the best\nbaseline, SpecOffload improves GPU core utilization by 4.49x and boosts\ninference throughput by 2.54x. Our code is available at\nhttps://github.com/MobiSense/SpecOffload ."
                },
                "authors": [
                    {
                        "name": "Xiangwen Zhuge"
                    },
                    {
                        "name": "Xu Shen"
                    },
                    {
                        "name": "Zeyu Wang"
                    },
                    {
                        "name": "Fan Dang"
                    },
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Danyang Li"
                    },
                    {
                        "name": "Yahui Han"
                    },
                    {
                        "name": "Tianxiang Hao"
                    },
                    {
                        "name": "Zheng Yang"
                    }
                ],
                "author_detail": {
                    "name": "Zheng Yang"
                },
                "author": "Zheng Yang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10259v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10259v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06085v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06085v2",
                "updated": "2025-05-15T13:07:31Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    7,
                    31,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-09T14:29:37Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    14,
                    29,
                    37,
                    4,
                    129,
                    0
                ],
                "title": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Assessing Tenstorrent's RISC-V MatMul Acceleration Capabilities"
                },
                "summary": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The increasing demand for generative AI as Large Language Models (LLMs)\nservices has driven the need for specialized hardware architectures that\noptimize computational efficiency and energy consumption. This paper evaluates\nthe performance of the Tenstorrent Grayskull e75 RISC-V accelerator for basic\nlinear algebra kernels at reduced numerical precision, a fundamental operation\nin LLM computations. We present a detailed characterization of Grayskull's\nexecution model, gridsize, matrix dimensions, data formats, and numerical\nprecision impact computational efficiency. Furthermore, we compare Grayskull's\nperformance against state-of-the-art architectures with tensor acceleration,\nincluding Intel Sapphire Rapids processors and two NVIDIA GPUs (V100 and A100).\nWhilst NVIDIA GPUs dominate raw performance, Grayskull demonstrates a\ncompetitive trade-off between power consumption and computational throughput,\nreaching a peak of 1.55 TFLOPs/Watt with BF16."
                },
                "authors": [
                    {
                        "name": "Hiari Pizzini Cavagna"
                    },
                    {
                        "name": "Daniele Cesarini"
                    },
                    {
                        "name": "Andrea Bartolini"
                    }
                ],
                "author_detail": {
                    "name": "Andrea Bartolini"
                },
                "author": "Andrea Bartolini",
                "arxiv_comment": "Accepted to the Computational Aspects of Deep Learning Workshop at\n  ISC High Performance 2025. To appear in the ISC High Performance 2025\n  Workshop Proceedings",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06085v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06085v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.PF",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.PF",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10251v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10251v1",
                "updated": "2025-05-15T13:04:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    4,
                    53,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T13:04:53Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    13,
                    4,
                    53,
                    3,
                    135,
                    0
                ],
                "title": "SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SRT-H: A Hierarchical Framework for Autonomous Surgery via Language\n  Conditioned Imitation Learning"
                },
                "summary": "Research on autonomous robotic surgery has largely focused on simple task\nautomation in controlled environments. However, real-world surgical\napplications require dexterous manipulation over extended time scales while\ndemanding generalization across diverse variations in human tissue. These\nchallenges remain difficult to address using existing logic-based or\nconventional end-to-end learning strategies. To bridge this gap, we propose a\nhierarchical framework for dexterous, long-horizon surgical tasks. Our method\nemploys a high-level policy for task planning and a low-level policy for\ngenerating task-space controls for the surgical robot. The high-level planner\nplans tasks using language, producing task-specific or corrective instructions\nthat guide the robot at a coarse level. Leveraging language as a planning\nmodality offers an intuitive and generalizable interface, mirroring how\nexperienced surgeons instruct traineers during procedures. We validate our\nframework in ex-vivo experiments on a complex minimally invasive procedure,\ncholecystectomy, and conduct ablative studies to assess key design choices. Our\napproach achieves a 100% success rate across n=8 different ex-vivo\ngallbladders, operating fully autonomously without human intervention. The\nhierarchical approach greatly improves the policy's ability to recover from\nsuboptimal states that are inevitable in the highly dynamic environment of\nrealistic surgical applications. This work represents the first demonstration\nof step-level autonomy, marking a critical milestone toward autonomous surgical\nsystems for clinical studies. By advancing generalizable autonomy in surgical\nrobotics, our approach brings the field closer to real-world deployment.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Research on autonomous robotic surgery has largely focused on simple task\nautomation in controlled environments. However, real-world surgical\napplications require dexterous manipulation over extended time scales while\ndemanding generalization across diverse variations in human tissue. These\nchallenges remain difficult to address using existing logic-based or\nconventional end-to-end learning strategies. To bridge this gap, we propose a\nhierarchical framework for dexterous, long-horizon surgical tasks. Our method\nemploys a high-level policy for task planning and a low-level policy for\ngenerating task-space controls for the surgical robot. The high-level planner\nplans tasks using language, producing task-specific or corrective instructions\nthat guide the robot at a coarse level. Leveraging language as a planning\nmodality offers an intuitive and generalizable interface, mirroring how\nexperienced surgeons instruct traineers during procedures. We validate our\nframework in ex-vivo experiments on a complex minimally invasive procedure,\ncholecystectomy, and conduct ablative studies to assess key design choices. Our\napproach achieves a 100% success rate across n=8 different ex-vivo\ngallbladders, operating fully autonomously without human intervention. The\nhierarchical approach greatly improves the policy's ability to recover from\nsuboptimal states that are inevitable in the highly dynamic environment of\nrealistic surgical applications. This work represents the first demonstration\nof step-level autonomy, marking a critical milestone toward autonomous surgical\nsystems for clinical studies. By advancing generalizable autonomy in surgical\nrobotics, our approach brings the field closer to real-world deployment."
                },
                "authors": [
                    {
                        "name": "Ji Woong Kim"
                    },
                    {
                        "name": "Juo-Tung Chen"
                    },
                    {
                        "name": "Pascal Hansen"
                    },
                    {
                        "name": "Lucy X. Shi"
                    },
                    {
                        "name": "Antony Goldenberg"
                    },
                    {
                        "name": "Samuel Schmidgall"
                    },
                    {
                        "name": "Paul Maria Scheikl"
                    },
                    {
                        "name": "Anton Deguet"
                    },
                    {
                        "name": "Brandon M. White"
                    },
                    {
                        "name": "De Ru Tsai"
                    },
                    {
                        "name": "Richard Cha"
                    },
                    {
                        "name": "Jeffrey Jopling"
                    },
                    {
                        "name": "Chelsea Finn"
                    },
                    {
                        "name": "Axel Krieger"
                    }
                ],
                "author_detail": {
                    "name": "Axel Krieger"
                },
                "author": "Axel Krieger",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10251v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10251v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09178v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09178v2",
                "updated": "2025-05-15T12:49:27Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    49,
                    27,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T06:21:27Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    6,
                    21,
                    27,
                    2,
                    134,
                    0
                ],
                "title": "UniCAD: Efficient and Extendable Architecture for Multi-Task\n  Computer-Aided Diagnosis System",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniCAD: Efficient and Extendable Architecture for Multi-Task\n  Computer-Aided Diagnosis System"
                },
                "summary": "The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The growing complexity and scale of visual model pre-training have made\ndeveloping and deploying multi-task computer-aided diagnosis (CAD) systems\nincreasingly challenging and resource-intensive. Furthermore, the medical\nimaging community lacks an open-source CAD platform to enable the rapid\ncreation of efficient and extendable diagnostic models. To address these\nissues, we propose UniCAD, a unified architecture that leverages the robust\ncapabilities of pre-trained vision foundation models to seamlessly handle both\n2D and 3D medical images while requiring only minimal task-specific parameters.\nUniCAD introduces two key innovations: (1) Efficiency: A low-rank adaptation\nstrategy is employed to adapt a pre-trained visual model to the medical image\ndomain, achieving performance on par with fully fine-tuned counterparts while\nintroducing only 0.17% trainable parameters. (2) Plug-and-Play: A modular\narchitecture that combines a frozen foundation model with multiple\nplug-and-play experts, enabling diverse tasks and seamless functionality\nexpansion. Building on this unified CAD architecture, we establish an\nopen-source platform where researchers can share and access lightweight CAD\nexperts, fostering a more equitable and efficient research ecosystem.\nComprehensive experiments across 12 diverse medical datasets demonstrate that\nUniCAD consistently outperforms existing methods in both accuracy and\ndeployment efficiency. The source code and project page are available at\nhttps://mii-laboratory.github.io/UniCAD/."
                },
                "authors": [
                    {
                        "name": "Yitao Zhu"
                    },
                    {
                        "name": "Yuan Yin"
                    },
                    {
                        "name": "Zhenrong Shen"
                    },
                    {
                        "name": "Zihao Zhao"
                    },
                    {
                        "name": "Haiyu Song"
                    },
                    {
                        "name": "Sheng Wang"
                    },
                    {
                        "name": "Dinggang Shen"
                    },
                    {
                        "name": "Qian Wang"
                    }
                ],
                "author_detail": {
                    "name": "Qian Wang"
                },
                "author": "Qian Wang",
                "arxiv_comment": "14 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09178v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09178v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.15674v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.15674v2",
                "updated": "2025-05-15T12:42:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    42,
                    44,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-26T21:05:16Z",
                "published_parsed": [
                    2025,
                    1,
                    26,
                    21,
                    5,
                    16,
                    6,
                    26,
                    0
                ],
                "title": "TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and\n  Compression in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "TensorLLM: Tensorising Multi-Head Attention for Enhanced Reasoning and\n  Compression in LLMs"
                },
                "summary": "The reasoning abilities of Large Language Models (LLMs) can be improved by\nstructurally denoising their weights, yet existing techniques primarily focus\non denoising the feed-forward network (FFN) of the transformer block, and can\nnot efficiently utilise the Multi-head Attention (MHA) block, which is the core\nof transformer architectures. To address this issue, we propose a novel\nintuitive framework that, at its very core, performs MHA compression through a\nmulti-head tensorisation process and the Tucker decomposition. This enables\nboth higher-dimensional structured denoising and compression of the MHA\nweights, by enforcing a shared higher-dimensional subspace across the weights\nof the multiple attention heads. We demonstrate that this approach consistently\nenhances the reasoning capabilities of LLMs across multiple benchmark datasets,\nand for both encoder-only and decoder-only architectures, while achieving\ncompression rates of up to $\\sim 250$ times in the MHA weights, all without\nrequiring any additional data, training, or fine-tuning. Furthermore, we show\nthat the proposed method can be seamlessly combined with existing\nFFN-only-based denoising techniques to achieve further improvements in LLM\nreasoning performance.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The reasoning abilities of Large Language Models (LLMs) can be improved by\nstructurally denoising their weights, yet existing techniques primarily focus\non denoising the feed-forward network (FFN) of the transformer block, and can\nnot efficiently utilise the Multi-head Attention (MHA) block, which is the core\nof transformer architectures. To address this issue, we propose a novel\nintuitive framework that, at its very core, performs MHA compression through a\nmulti-head tensorisation process and the Tucker decomposition. This enables\nboth higher-dimensional structured denoising and compression of the MHA\nweights, by enforcing a shared higher-dimensional subspace across the weights\nof the multiple attention heads. We demonstrate that this approach consistently\nenhances the reasoning capabilities of LLMs across multiple benchmark datasets,\nand for both encoder-only and decoder-only architectures, while achieving\ncompression rates of up to $\\sim 250$ times in the MHA weights, all without\nrequiring any additional data, training, or fine-tuning. Furthermore, we show\nthat the proposed method can be seamlessly combined with existing\nFFN-only-based denoising techniques to achieve further improvements in LLM\nreasoning performance."
                },
                "authors": [
                    {
                        "name": "Yuxuan Gu"
                    },
                    {
                        "name": "Wuyang Zhou"
                    },
                    {
                        "name": "Giorgos Iacovides"
                    },
                    {
                        "name": "Danilo Mandic"
                    }
                ],
                "author_detail": {
                    "name": "Danilo Mandic"
                },
                "author": "Danilo Mandic",
                "arxiv_comment": "Accpeted for IEEE International Joint Conference on Neural Networks\n  (IJCNN 2025). The code is available at https://github.com/guyuxuan9/TensorLLM",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.15674v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.15674v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10219v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10219v1",
                "updated": "2025-05-15T12:22:21Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    22,
                    21,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:22:21Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    22,
                    21,
                    3,
                    135,
                    0
                ],
                "title": "Towards Safe Robot Foundation Models Using Inductive Biases",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Towards Safe Robot Foundation Models Using Inductive Biases"
                },
                "summary": "Safety is a critical requirement for the real-world deployment of robotic\nsystems. Unfortunately, while current robot foundation models show promising\ngeneralization capabilities across a wide variety of tasks, they fail to\naddress safety, an important aspect for ensuring long-term operation. Current\nrobot foundation models assume that safe behavior should emerge by learning\nfrom a sufficiently large dataset of demonstrations. However, this approach has\ntwo clear major drawbacks. Firstly, there are no formal safety guarantees for a\nbehavior cloning policy trained using supervised learning. Secondly, without\nexplicit knowledge of any safety constraints, the policy may require an\nunreasonable number of additional demonstrations to even approximate the\ndesired constrained behavior. To solve these key issues, we show how we can\ninstead combine robot foundation models with geometric inductive biases using\nATACOM, a safety layer placed after the foundation policy that ensures safe\nstate transitions by enforcing action constraints. With this approach, we can\nensure formal safety guarantees for generalist policies without providing\nextensive demonstrations of safe behavior, and without requiring any specific\nfine-tuning for safety. Our experiments show that our approach can be\nbeneficial both for classical manipulation tasks, where we avoid unwanted\ncollisions with irrelevant objects, and for dynamic tasks, such as the robot\nair hockey environment, where we can generate fast trajectories respecting\ncomplex tasks and joint space constraints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Safety is a critical requirement for the real-world deployment of robotic\nsystems. Unfortunately, while current robot foundation models show promising\ngeneralization capabilities across a wide variety of tasks, they fail to\naddress safety, an important aspect for ensuring long-term operation. Current\nrobot foundation models assume that safe behavior should emerge by learning\nfrom a sufficiently large dataset of demonstrations. However, this approach has\ntwo clear major drawbacks. Firstly, there are no formal safety guarantees for a\nbehavior cloning policy trained using supervised learning. Secondly, without\nexplicit knowledge of any safety constraints, the policy may require an\nunreasonable number of additional demonstrations to even approximate the\ndesired constrained behavior. To solve these key issues, we show how we can\ninstead combine robot foundation models with geometric inductive biases using\nATACOM, a safety layer placed after the foundation policy that ensures safe\nstate transitions by enforcing action constraints. With this approach, we can\nensure formal safety guarantees for generalist policies without providing\nextensive demonstrations of safe behavior, and without requiring any specific\nfine-tuning for safety. Our experiments show that our approach can be\nbeneficial both for classical manipulation tasks, where we avoid unwanted\ncollisions with irrelevant objects, and for dynamic tasks, such as the robot\nair hockey environment, where we can generate fast trajectories respecting\ncomplex tasks and joint space constraints."
                },
                "authors": [
                    {
                        "name": "Maximilian Tlle"
                    },
                    {
                        "name": "Theo Gruner"
                    },
                    {
                        "name": "Daniel Palenicek"
                    },
                    {
                        "name": "Tim Schneider"
                    },
                    {
                        "name": "Jonas Gnster"
                    },
                    {
                        "name": "Joe Watson"
                    },
                    {
                        "name": "Davide Tateo"
                    },
                    {
                        "name": "Puze Liu"
                    },
                    {
                        "name": "Jan Peters"
                    }
                ],
                "author_detail": {
                    "name": "Jan Peters"
                },
                "author": "Jan Peters",
                "arxiv_comment": "14 pages, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10219v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10219v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10218v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10218v1",
                "updated": "2025-05-15T12:22:10Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    22,
                    10,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:22:10Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    22,
                    10,
                    3,
                    135,
                    0
                ],
                "title": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable\n  Reward",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "RAIDEN-R1: Improving Role-awareness of LLMs via GRPO with Verifiable\n  Reward"
                },
                "summary": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Role-playing conversational agents (RPCAs) face persistent challenges in\nmaintaining role consistency. To address this, we propose RAIDEN-R1, a novel\nreinforcement learning framework that integrates Verifiable Role-Awareness\nReward (VRAR). The method introduces both singular and multi-term mining\nstrategies to generate quantifiable rewards by assessing role-specific keys.\nAdditionally, we construct a high-quality, role-aware Chain-of-Thought dataset\nthrough multi-LLM collaboration, and implement experiments to enhance reasoning\ncoherence. Experiments on the RAIDEN benchmark demonstrate RAIDEN-R1's\nsuperiority: our 14B-GRPO model achieves 88.04% and 88.65% accuracy on\nScript-Based Knowledge and Conversation Memory metrics, respectively,\noutperforming baseline models while maintaining robustness. Case analyses\nfurther reveal the model's enhanced ability to resolve conflicting contextual\ncues and sustain first-person narrative consistency. This work bridges the\nnon-quantifiability gap in RPCA training and provides insights into role-aware\nreasoning patterns, advancing the development of RPCAs."
                },
                "authors": [
                    {
                        "name": "Zongsheng Wang"
                    },
                    {
                        "name": "Kaili Sun"
                    },
                    {
                        "name": "Bowen Wu"
                    },
                    {
                        "name": "Qun Yu"
                    },
                    {
                        "name": "Ying Li"
                    },
                    {
                        "name": "Baoxun Wang"
                    }
                ],
                "author_detail": {
                    "name": "Baoxun Wang"
                },
                "author": "Baoxun Wang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10218v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10218v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10213v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10213v1",
                "updated": "2025-05-15T12:17:52Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    17,
                    52,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:17:52Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    17,
                    52,
                    3,
                    135,
                    0
                ],
                "title": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM\n  Performance on Time Series Forecasting",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Informed Forecasting: Leveraging Auxiliary Knowledge to Boost LLM\n  Performance on Time Series Forecasting"
                },
                "summary": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "With the widespread adoption of Large Language Models (LLMs), there is a\ngrowing need to establish best practices for leveraging their capabilities\nbeyond traditional natural language tasks. In this paper, a novel cross-domain\nknowledge transfer framework is proposed to enhance the performance of LLMs in\ntime series forecasting -- a task of increasing relevance in fields such as\nenergy systems, finance, and healthcare. The approach systematically infuses\nLLMs with structured temporal information to improve their forecasting\naccuracy. This study evaluates the proposed method on a real-world time series\ndataset and compares it to a naive baseline where the LLM receives no auxiliary\ninformation. Results show that knowledge-informed forecasting significantly\noutperforms the uninformed baseline in terms of predictive accuracy and\ngeneralization. These findings highlight the potential of knowledge transfer\nstrategies to bridge the gap between LLMs and domain-specific forecasting\ntasks."
                },
                "authors": [
                    {
                        "name": "Mohammadmahdi Ghasemloo"
                    },
                    {
                        "name": "Alireza Moradi"
                    }
                ],
                "author_detail": {
                    "name": "Alireza Moradi"
                },
                "author": "Alireza Moradi",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10213v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10213v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "stat.AP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10212v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10212v1",
                "updated": "2025-05-15T12:16:36Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    16,
                    36,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T12:16:36Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    16,
                    36,
                    3,
                    135,
                    0
                ],
                "title": "Do LLMs Memorize Recommendation Datasets? A Preliminary Study on\n  MovieLens-1M",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Do LLMs Memorize Recommendation Datasets? A Preliminary Study on\n  MovieLens-1M"
                },
                "summary": "Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have become increasingly central to\nrecommendation scenarios due to their remarkable natural language understanding\nand generation capabilities. Although significant research has explored the use\nof LLMs for various recommendation tasks, little effort has been dedicated to\nverifying whether they have memorized public recommendation dataset as part of\ntheir training data. This is undesirable because memorization reduces the\ngeneralizability of research findings, as benchmarking on memorized datasets\ndoes not guarantee generalization to unseen datasets. Furthermore, memorization\ncan amplify biases, for example, some popular items may be recommended more\nfrequently than others.\n  In this work, we investigate whether LLMs have memorized public\nrecommendation datasets. Specifically, we examine two model families (GPT and\nLlama) across multiple sizes, focusing on one of the most widely used dataset\nin recommender systems: MovieLens-1M. First, we define dataset memorization as\nthe extent to which item attributes, user profiles, and user-item interactions\ncan be retrieved by prompting the LLMs. Second, we analyze the impact of\nmemorization on recommendation performance. Lastly, we examine whether\nmemorization varies across model families and model sizes. Our results reveal\nthat all models exhibit some degree of memorization of MovieLens-1M, and that\nrecommendation performance is related to the extent of memorization. We have\nmade all the code publicly available at:\nhttps://github.com/sisinflab/LLM-MemoryInspector"
                },
                "authors": [
                    {
                        "name": "Dario Di Palma"
                    },
                    {
                        "name": "Felice Antonio Merra"
                    },
                    {
                        "name": "Maurizio Sfilio"
                    },
                    {
                        "name": "Vito Walter Anelli"
                    },
                    {
                        "name": "Fedelucio Narducci"
                    },
                    {
                        "name": "Tommaso Di Noia"
                    }
                ],
                "author_detail": {
                    "name": "Tommaso Di Noia"
                },
                "author": "Tommaso Di Noia",
                "arxiv_doi": "10.1145/3726302.3730178",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1145/3726302.3730178",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.10212v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10212v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.IR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.IR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2503.10325v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2503.10325v2",
                "updated": "2025-05-15T12:02:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    12,
                    2,
                    56,
                    3,
                    135,
                    0
                ],
                "published": "2025-03-13T13:03:38Z",
                "published_parsed": [
                    2025,
                    3,
                    13,
                    13,
                    3,
                    38,
                    3,
                    72,
                    0
                ],
                "title": "Collaborative Speculative Inference for Efficient LLM Inference Serving",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Collaborative Speculative Inference for Efficient LLM Inference Serving"
                },
                "summary": "Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Speculative inference is a promising paradigm employing small speculative\nmodels (SSMs) as drafters to generate draft tokens, which are subsequently\nverified in parallel by the target large language model (LLM). This approach\nenhances the efficiency of inference serving by reducing LLM inference latency\nand costs while preserving generation quality. However, existing speculative\nmethods face critical challenges, including inefficient resource utilization\nand limited draft acceptance, which constrain their scalability and overall\neffectiveness. To overcome these obstacles, we present CoSine, a novel\nspeculative inference system that decouples sequential speculative decoding\nfrom parallel verification, enabling efficient collaboration among multiple\nnodes. Specifically, CoSine routes inference requests to specialized drafters\nbased on their expertise and incorporates a confidence-based token fusion\nmechanism to synthesize outputs from cooperating drafters, ensuring\nhigh-quality draft generation. Additionally, CoSine dynamically orchestrates\nthe execution of speculative decoding and verification in a pipelined manner,\nemploying batch scheduling to selectively group requests and adaptive\nspeculation control to minimize idle periods. By optimizing parallel workflows\nthrough heterogeneous node collaboration, CoSine balances draft generation and\nverification throughput in real-time, thereby maximizing resource utilization.\nExperimental results demonstrate that CoSine achieves superior performance\ncompared to state-of-the-art speculative approaches. Notably, with equivalent\nresource costs, CoSine achieves up to a 23.2% decrease in latency and a 32.5%\nincrease in throughput compared to baseline methods."
                },
                "authors": [
                    {
                        "name": "Luyao Gao"
                    },
                    {
                        "name": "Jianchun Liu"
                    },
                    {
                        "name": "Hongli Xu"
                    },
                    {
                        "name": "Xichong Zhang"
                    },
                    {
                        "name": "Yunming Liao"
                    },
                    {
                        "name": "Liusheng Huang"
                    }
                ],
                "author_detail": {
                    "name": "Liusheng Huang"
                },
                "author": "Liusheng Huang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2503.10325v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2503.10325v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.DC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.DC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10202v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10202v1",
                "updated": "2025-05-15T11:58:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    58,
                    4,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T11:58:04Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    58,
                    4,
                    3,
                    135,
                    0
                ],
                "title": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models\n  via Vector Quantized Logits",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "VQ-Logits: Compressing the Output Bottleneck of Large Language Models\n  via Vector Quantized Logits"
                },
                "summary": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have achieved remarkable success but face\nsignificant computational and memory challenges, particularly due to their\nextensive output vocabularies. The final linear projection layer, mapping\nhidden states to vocabulary-sized logits, often constitutes a substantial\nportion of the model's parameters and computational cost during inference.\nExisting methods like adaptive softmax or hierarchical softmax introduce\nstructural complexities. In this paper, we propose VQ-Logits, a novel approach\nthat leverages Vector Quantization (VQ) to drastically reduce the parameter\ncount and computational load of the LLM output layer. VQ-Logits replaces the\nlarge V * dmodel output embedding matrix with a small, shared codebook of K\nembedding vectors (K << V ). Each token in the vocabulary is mapped to one of\nthese K codebook vectors. The LLM predicts logits over this compact codebook,\nwhich are then efficiently \"scattered\" to the full vocabulary space using the\nlearned or preassigned mapping. We demonstrate through extensive experiments on\nstandard language modeling benchmarks (e.g., WikiText-103, C4) that VQ-Logits\ncan achieve up to 99% parameter reduction in the output layer and 6x speedup in\nlogit computation, with only a marginal 4% increase in perplexity compared to\nfull softmax baselines. We further provide detailed ablation studies on\ncodebook size, initialization, and learning strategies, showcasing the\nrobustness and effectiveness of our approach."
                },
                "authors": [
                    {
                        "name": "Jintian Shao"
                    },
                    {
                        "name": "Hongyi Huang"
                    },
                    {
                        "name": "Jiayi Wu"
                    },
                    {
                        "name": "YiMing Cheng"
                    },
                    {
                        "name": "ZhiYu Wu"
                    },
                    {
                        "name": "You Shan"
                    },
                    {
                        "name": "MingKai Zheng"
                    }
                ],
                "author_detail": {
                    "name": "MingKai Zheng"
                },
                "author": "MingKai Zheng",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10202v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10202v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10192v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10192v1",
                "updated": "2025-05-15T11:50:02Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    50,
                    2,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T11:50:02Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    50,
                    2,
                    3,
                    135,
                    0
                ],
                "title": "Defect Detection in Photolithographic Patterns Using Deep Learning\n  Models Trained on Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Defect Detection in Photolithographic Patterns Using Deep Learning\n  Models Trained on Synthetic Data"
                },
                "summary": "In the photolithographic process vital to semiconductor manufacturing,\nvarious types of defects appear during EUV pattering. Due to ever-shrinking\npattern size, these defects are extremely small and cause false or missed\ndetection during inspection. Specifically, the lack of defect-annotated quality\ndata with good representation of smaller defects has prohibited deployment of\ndeep learning based defect detection models in fabrication lines. To resolve\nthe problem of data unavailability, we artificially generate scanning electron\nmicroscopy (SEM) images of line patterns with known distribution of defects and\nautonomously annotate them. We then employ state-of-the-art object detection\nmodels to investigate defect detection performance as a function of defect\nsize, much smaller than the pitch width. We find that the real-time object\ndetector YOLOv8 has the best mean average precision of 96% as compared to\nEfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We\nreport the smallest defect size that can be detected reliably. When tested on\nreal SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and\n78.3% of Break defects across all relevant instances. These promising results\nsuggest that synthetic data can be used as an alternative to real-world data in\norder to develop robust machine-learning models.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In the photolithographic process vital to semiconductor manufacturing,\nvarious types of defects appear during EUV pattering. Due to ever-shrinking\npattern size, these defects are extremely small and cause false or missed\ndetection during inspection. Specifically, the lack of defect-annotated quality\ndata with good representation of smaller defects has prohibited deployment of\ndeep learning based defect detection models in fabrication lines. To resolve\nthe problem of data unavailability, we artificially generate scanning electron\nmicroscopy (SEM) images of line patterns with known distribution of defects and\nautonomously annotate them. We then employ state-of-the-art object detection\nmodels to investigate defect detection performance as a function of defect\nsize, much smaller than the pitch width. We find that the real-time object\ndetector YOLOv8 has the best mean average precision of 96% as compared to\nEfficientNet, 83%, and SSD, 77%, with the ability to detect smaller defects. We\nreport the smallest defect size that can be detected reliably. When tested on\nreal SEM data, the YOLOv8 model correctly detected 84.6% of Bridge defects and\n78.3% of Break defects across all relevant instances. These promising results\nsuggest that synthetic data can be used as an alternative to real-world data in\norder to develop robust machine-learning models."
                },
                "authors": [
                    {
                        "name": "Prashant P. Shinde"
                    },
                    {
                        "name": "Priyadarshini P. Pai"
                    },
                    {
                        "name": "Shashishekar P. Adiga"
                    },
                    {
                        "name": "K. Subramanya Mayya"
                    },
                    {
                        "name": "Yongbeom Seo"
                    },
                    {
                        "name": "Myungsoo Hwang"
                    },
                    {
                        "name": "Heeyoung Go"
                    },
                    {
                        "name": "Changmin Park"
                    }
                ],
                "author_detail": {
                    "name": "Changmin Park"
                },
                "author": "Changmin Park",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10192v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10192v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10182v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10182v1",
                "updated": "2025-05-15T11:29:01Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    29,
                    1,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T11:29:01Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    29,
                    1,
                    3,
                    135,
                    0
                ],
                "title": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with\n  Synthetic Data for LLM Reasoning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Mining Hidden Thoughts from Texts: Evaluating Continual Pretraining with\n  Synthetic Data for LLM Reasoning"
                },
                "summary": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have demonstrated significant improvements in\nreasoning capabilities through supervised fine-tuning and reinforcement\nlearning. However, when training reasoning models, these approaches are\nprimarily applicable to specific domains such as mathematics and programming,\nwhich imposes fundamental constraints on the breadth and scalability of\ntraining data. In contrast, continual pretraining (CPT) offers the advantage of\nnot requiring task-specific signals. Nevertheless, how to effectively\nsynthesize training data for reasoning and how such data affect a wide range of\ndomains remain largely unexplored. This study provides a detailed evaluation of\nReasoning CPT, a form of CPT that uses synthetic data to reconstruct the hidden\nthought processes underlying texts, based on the premise that texts are the\nresult of the author's thinking process. Specifically, we apply Reasoning CPT\nto Gemma2-9B using synthetic data with hidden thoughts derived from STEM and\nLaw corpora, and compare it to standard CPT on the MMLU benchmark. Our analysis\nreveals that Reasoning CPT consistently improves performance across all\nevaluated domains. Notably, reasoning skills acquired in one domain transfer\neffectively to others; the performance gap with conventional methods widens as\nproblem difficulty increases, with gains of up to 8 points on the most\nchallenging problems. Furthermore, models trained with hidden thoughts learn to\nadjust the depth of their reasoning according to problem difficulty."
                },
                "authors": [
                    {
                        "name": "Yoichi Ishibashi"
                    },
                    {
                        "name": "Taro Yano"
                    },
                    {
                        "name": "Masafumi Oyamada"
                    }
                ],
                "author_detail": {
                    "name": "Masafumi Oyamada"
                },
                "author": "Masafumi Oyamada",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10182v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10182v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2312.01797v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2312.01797v3",
                "updated": "2025-05-15T11:25:13Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    25,
                    13,
                    3,
                    135,
                    0
                ],
                "published": "2023-12-04T10:37:58Z",
                "published_parsed": [
                    2023,
                    12,
                    4,
                    10,
                    37,
                    58,
                    0,
                    338,
                    0
                ],
                "title": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for\n  Robotics",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "LLM A*: Human in the Loop Large Language Models Enabled A* Search for\n  Robotics"
                },
                "summary": "This research focuses on how Large Language Models (LLMs) can help with\n(path) planning for mobile embodied agents such as robots, in a\nhuman-in-the-loop and interactive manner. A novel framework named LLM A*, aims\nto leverage the commonsense of LLMs, and the utility-optimal A* is proposed to\nfacilitate few-shot near-optimal path planning. Prompts are used for two main\npurposes: 1) to provide LLMs with essential information like environments,\ncosts, heuristics, etc.; 2) to communicate human feedback on intermediate\nplanning results to LLMs. This approach takes human feedback on board and\nrenders the entire planning process transparent (akin to a `white box') to\nhumans. Moreover, it facilitates code-free path planning, thereby fostering the\naccessibility and inclusiveness of artificial intelligence techniques to\ncommunities less proficient in coding. Comparative analysis against A* and RL\ndemonstrates that LLM A* exhibits greater efficiency in terms of search space\nand achieves paths comparable to A* while outperforming RL. The interactive\nnature of LLM A* also makes it a promising tool for deployment in collaborative\nhuman-robot tasks. Codes and Supplemental Materials can be found at GitHub:\nhttps://github.com/speedhawk/LLM-A-.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This research focuses on how Large Language Models (LLMs) can help with\n(path) planning for mobile embodied agents such as robots, in a\nhuman-in-the-loop and interactive manner. A novel framework named LLM A*, aims\nto leverage the commonsense of LLMs, and the utility-optimal A* is proposed to\nfacilitate few-shot near-optimal path planning. Prompts are used for two main\npurposes: 1) to provide LLMs with essential information like environments,\ncosts, heuristics, etc.; 2) to communicate human feedback on intermediate\nplanning results to LLMs. This approach takes human feedback on board and\nrenders the entire planning process transparent (akin to a `white box') to\nhumans. Moreover, it facilitates code-free path planning, thereby fostering the\naccessibility and inclusiveness of artificial intelligence techniques to\ncommunities less proficient in coding. Comparative analysis against A* and RL\ndemonstrates that LLM A* exhibits greater efficiency in terms of search space\nand achieves paths comparable to A* while outperforming RL. The interactive\nnature of LLM A* also makes it a promising tool for deployment in collaborative\nhuman-robot tasks. Codes and Supplemental Materials can be found at GitHub:\nhttps://github.com/speedhawk/LLM-A-."
                },
                "authors": [
                    {
                        "name": "Hengjia Xiao"
                    },
                    {
                        "name": "Peng Wang"
                    },
                    {
                        "name": "Mingzhe Yu"
                    },
                    {
                        "name": "Mattia Robbiani"
                    }
                ],
                "author_detail": {
                    "name": "Mattia Robbiani"
                },
                "author": "Mattia Robbiani",
                "arxiv_comment": "7 figures, 8 pages",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2312.01797v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2312.01797v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.07247v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.07247v2",
                "updated": "2025-05-15T11:01:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    11,
                    1,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-12T05:43:21Z",
                "published_parsed": [
                    2025,
                    5,
                    12,
                    5,
                    43,
                    21,
                    0,
                    132,
                    0
                ],
                "title": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring\n  with Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring\n  with Large Language Models"
                },
                "summary": "Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Subjective Answer Grading (SAG) plays a crucial role in education,\nstandardized testing, and automated assessment systems, particularly for\nevaluating short-form responses in Short Answer Scoring (SAS). However,\nexisting approaches often produce coarse-grained scores and lack detailed\nreasoning. Although large language models (LLMs) have demonstrated potential as\nzero-shot evaluators, they remain susceptible to bias, inconsistencies with\nhuman judgment, and limited transparency in scoring decisions. To overcome\nthese limitations, we introduce SAS-Bench, a benchmark specifically designed\nfor LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring,\nexpert-annotated error categories, and a diverse range of question types\nderived from real-world subject-specific exams. This benchmark facilitates\ndetailed evaluation of model reasoning processes and explainability. We also\nrelease an open-source dataset containing 1,030 questions and 4,109 student\nresponses, each annotated by domain experts. Furthermore, we conduct\ncomprehensive experiments with various LLMs, identifying major challenges in\nscoring science-related questions and highlighting the effectiveness of\nfew-shot prompting in improving scoring accuracy. Our work offers valuable\ninsights into the development of more robust, fair, and educationally\nmeaningful LLM-based evaluation systems."
                },
                "authors": [
                    {
                        "name": "Peichao Lai"
                    },
                    {
                        "name": "Kexuan Zhang"
                    },
                    {
                        "name": "Yi Lin"
                    },
                    {
                        "name": "Linyihan Zhang"
                    },
                    {
                        "name": "Feiyang Ye"
                    },
                    {
                        "name": "Jinhao Yan"
                    },
                    {
                        "name": "Yanwei Xu"
                    },
                    {
                        "name": "Conghui He"
                    },
                    {
                        "name": "Yilei Wang"
                    },
                    {
                        "name": "Wentao Zhang"
                    },
                    {
                        "name": "Bin Cui"
                    }
                ],
                "author_detail": {
                    "name": "Bin Cui"
                },
                "author": "Bin Cui",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.07247v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.07247v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.04363v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.04363v3",
                "updated": "2025-05-15T10:57:51Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    57,
                    51,
                    3,
                    135,
                    0
                ],
                "published": "2024-07-05T09:06:47Z",
                "published_parsed": [
                    2024,
                    7,
                    5,
                    9,
                    6,
                    47,
                    4,
                    187,
                    0
                ],
                "title": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AriGraph: Learning Knowledge Graph World Models with Episodic Memory for\n  LLM Agents"
                },
                "summary": "Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in the capabilities of Large Language Models (LLMs) have created\na promising foundation for developing autonomous agents. With the right tools,\nthese agents could learn to solve tasks in new environments by accumulating and\nupdating their knowledge. Current LLM-based agents process past experiences\nusing a full history of observations, summarization, retrieval augmentation.\nHowever, these unstructured memory representations do not facilitate the\nreasoning and planning essential for complex decision-making. In our study, we\nintroduce AriGraph, a novel method wherein the agent constructs and updates a\nmemory graph that integrates semantic and episodic memories while exploring the\nenvironment. We demonstrate that our Ariadne LLM agent, consisting of the\nproposed memory architecture augmented with planning and decision-making,\neffectively handles complex tasks within interactive text game environments\ndifficult even for human players. Results show that our approach markedly\noutperforms other established memory methods and strong RL baselines in a range\nof problems of varying complexity. Additionally, AriGraph demonstrates\ncompetitive performance compared to dedicated knowledge graph-based methods in\nstatic multi-hop question-answering."
                },
                "authors": [
                    {
                        "name": "Petr Anokhin"
                    },
                    {
                        "name": "Nikita Semenov"
                    },
                    {
                        "name": "Artyom Sorokin"
                    },
                    {
                        "name": "Dmitry Evseev"
                    },
                    {
                        "name": "Andrey Kravchenko"
                    },
                    {
                        "name": "Mikhail Burtsev"
                    },
                    {
                        "name": "Evgeny Burnaev"
                    }
                ],
                "author_detail": {
                    "name": "Evgeny Burnaev"
                },
                "author": "Evgeny Burnaev",
                "arxiv_comment": "Code for this work is avaliable at\n  https://github.com/AIRI-Institute/AriGraph",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.04363v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.04363v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2501.13018v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2501.13018v2",
                "updated": "2025-05-15T10:49:09Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    49,
                    9,
                    3,
                    135,
                    0
                ],
                "published": "2025-01-22T17:05:38Z",
                "published_parsed": [
                    2025,
                    1,
                    22,
                    17,
                    5,
                    38,
                    2,
                    22,
                    0
                ],
                "title": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Multi-Objective Hyperparameter Selection via Hypothesis Testing on\n  Reliability Graphs"
                },
                "summary": "The selection of hyperparameters, such as prompt templates in large language\nmodels (LLMs), must often strike a balance between reliability and cost. In\nmany cases, structural relationships between the expected reliability levels of\nthe hyperparameters can be inferred from prior information and held-out data --\ne.g., longer prompt templates may be more detailed and thus more reliable.\nHowever, existing hyperparameter selection methods either do not provide formal\nreliability guarantees or are unable to incorporate structured knowledge in the\nhyperparameter space. This paper introduces reliability graph-based Pareto\ntesting (RG-PT), a novel multi-objective hyperparameter selection framework\nthat maintains formal reliability guarantees in terms of false discovery rate\n(FDR), while accounting for known relationships among hyperparameters via a\ndirected acyclic graph. Edges in the graph reflect expected reliability and\ncost trade-offs among hyperparameters, which are inferred via the Bradley-Terry\n(BT) ranking model from prior information and held-out data. Experimental\nevaluations demonstrate that RG-PT significantly outperforms existing methods\nsuch as learn-then-test (LTT) and Pareto testing (PT) through a more efficient\nexploration of the hyperparameter space.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The selection of hyperparameters, such as prompt templates in large language\nmodels (LLMs), must often strike a balance between reliability and cost. In\nmany cases, structural relationships between the expected reliability levels of\nthe hyperparameters can be inferred from prior information and held-out data --\ne.g., longer prompt templates may be more detailed and thus more reliable.\nHowever, existing hyperparameter selection methods either do not provide formal\nreliability guarantees or are unable to incorporate structured knowledge in the\nhyperparameter space. This paper introduces reliability graph-based Pareto\ntesting (RG-PT), a novel multi-objective hyperparameter selection framework\nthat maintains formal reliability guarantees in terms of false discovery rate\n(FDR), while accounting for known relationships among hyperparameters via a\ndirected acyclic graph. Edges in the graph reflect expected reliability and\ncost trade-offs among hyperparameters, which are inferred via the Bradley-Terry\n(BT) ranking model from prior information and held-out data. Experimental\nevaluations demonstrate that RG-PT significantly outperforms existing methods\nsuch as learn-then-test (LTT) and Pareto testing (PT) through a more efficient\nexploration of the hyperparameter space."
                },
                "authors": [
                    {
                        "name": "Amirmohammad Farzaneh"
                    },
                    {
                        "name": "Osvaldo Simeone"
                    }
                ],
                "author_detail": {
                    "name": "Osvaldo Simeone"
                },
                "author": "Osvaldo Simeone",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2501.13018v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2501.13018v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "math.IT",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.06111v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.06111v2",
                "updated": "2025-05-15T10:31:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    31,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-09T15:11:13Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    15,
                    11,
                    13,
                    4,
                    129,
                    0
                ],
                "title": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "UniVLA: Learning to Act Anywhere with Task-centric Latent Actions"
                },
                "summary": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A generalist robot should perform effectively across various environments.\nHowever, most existing approaches heavily rely on scaling action-annotated data\nto enhance their capabilities. Consequently, they are often limited to single\nphysical specification and struggle to learn transferable knowledge across\ndifferent embodiments and environments. To confront these limitations, we\npropose UniVLA, a new framework for learning cross-embodiment\nvision-language-action (VLA) policies. Our key innovation is to derive\ntask-centric action representations from videos with a latent action model.\nThis enables us to exploit extensive data across a wide spectrum of embodiments\nand perspectives. To mitigate the effect of task-irrelevant dynamics, we\nincorporate language instructions and establish a latent action model within\nthe DINO feature space. Learned from internet-scale videos, the generalist\npolicy can be deployed to various robots through efficient latent action\ndecoding. We obtain state-of-the-art results across multiple manipulation and\nnavigation benchmarks, as well as real-robot deployments. UniVLA achieves\nsuperior performance over OpenVLA with less than 1/20 of pretraining compute\nand 1/10 of downstream data. Continuous performance improvements are observed\nas heterogeneous data, even including human videos, are incorporated into the\ntraining pipeline. The results underscore UniVLA's potential to facilitate\nscalable and efficient robot policy learning."
                },
                "authors": [
                    {
                        "name": "Qingwen Bu"
                    },
                    {
                        "name": "Yanting Yang"
                    },
                    {
                        "name": "Jisong Cai"
                    },
                    {
                        "name": "Shenyuan Gao"
                    },
                    {
                        "name": "Guanghui Ren"
                    },
                    {
                        "name": "Maoqing Yao"
                    },
                    {
                        "name": "Ping Luo"
                    },
                    {
                        "name": "Hongyang Li"
                    }
                ],
                "author_detail": {
                    "name": "Hongyang Li"
                },
                "author": "Hongyang Li",
                "arxiv_comment": "Accepted to RSS 2025. Code is available at\n  https://github.com/OpenDriveLab/UniVLA",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.06111v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.06111v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.RO",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.15523v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.15523v2",
                "updated": "2025-05-15T10:18:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    18,
                    58,
                    3,
                    135,
                    0
                ],
                "published": "2024-05-24T13:05:05Z",
                "published_parsed": [
                    2024,
                    5,
                    24,
                    13,
                    5,
                    5,
                    4,
                    145,
                    0
                ],
                "title": "The Mosaic Memory of Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The Mosaic Memory of Large Language Models"
                },
                "summary": "As Large Language Models (LLMs) become widely adopted, understanding how they\nlearn from, and memorize, training data becomes crucial. Memorization in LLMs\nis widely assumed to only occur as a result of sequences being repeated in the\ntraining data. Instead, we show that LLMs memorize by assembling information\nfrom similar sequences, a phenomena we call mosaic memory. We show major LLMs\nto exhibit mosaic memory, with fuzzy duplicates contributing to memorization as\nmuch as 0.8 of an exact duplicate and even heavily modified sequences\ncontributing substantially to memorization. Despite models display reasoning\ncapabilities, we somewhat surprisingly show memorization to be predominantly\nsyntactic rather than semantic. We finally show fuzzy duplicates to be\nubiquitous in real-world data, untouched by deduplication techniques. Taken\ntogether, our results challenge widely held beliefs and show memorization to be\na more complex, mosaic process, with real-world implications for privacy,\nconfidentiality, model utility and evaluation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) become widely adopted, understanding how they\nlearn from, and memorize, training data becomes crucial. Memorization in LLMs\nis widely assumed to only occur as a result of sequences being repeated in the\ntraining data. Instead, we show that LLMs memorize by assembling information\nfrom similar sequences, a phenomena we call mosaic memory. We show major LLMs\nto exhibit mosaic memory, with fuzzy duplicates contributing to memorization as\nmuch as 0.8 of an exact duplicate and even heavily modified sequences\ncontributing substantially to memorization. Despite models display reasoning\ncapabilities, we somewhat surprisingly show memorization to be predominantly\nsyntactic rather than semantic. We finally show fuzzy duplicates to be\nubiquitous in real-world data, untouched by deduplication techniques. Taken\ntogether, our results challenge widely held beliefs and show memorization to be\na more complex, mosaic process, with real-world implications for privacy,\nconfidentiality, model utility and evaluation."
                },
                "authors": [
                    {
                        "name": "Igor Shilov"
                    },
                    {
                        "name": "Matthieu Meeus"
                    },
                    {
                        "name": "Yves-Alexandre de Montjoye"
                    }
                ],
                "author_detail": {
                    "name": "Yves-Alexandre de Montjoye"
                },
                "author": "Yves-Alexandre de Montjoye",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.15523v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.15523v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10143v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10143v1",
                "updated": "2025-05-15T10:17:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    17,
                    35,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T10:17:35Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    17,
                    35,
                    3,
                    135,
                    0
                ],
                "title": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response\n  Generation of LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "GE-Chat: A Graph Enhanced RAG Framework for Evidential Response\n  Generation of LLMs"
                },
                "summary": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models are now key assistants in human decision-making\nprocesses. However, a common note always seems to follow: \"LLMs can make\nmistakes. Be careful with important info.\" This points to the reality that not\nall outputs from LLMs are dependable, and users must evaluate them manually.\nThe challenge deepens as hallucinated responses, often presented with seemingly\nplausible explanations, create complications and raise trust issues among\nusers. To tackle such issue, this paper proposes GE-Chat, a knowledge Graph\nenhanced retrieval-augmented generation framework to provide Evidence-based\nresponse generation. Specifically, when the user uploads a material document, a\nknowledge graph will be created, which helps construct a retrieval-augmented\nagent, enhancing the agent's responses with additional knowledge beyond its\ntraining corpus. Then we leverage Chain-of-Thought (CoT) logic generation,\nn-hop sub-graph searching, and entailment-based sentence generation to realize\naccurate evidence retrieval. We demonstrate that our method improves the\nexisting models' performance in terms of identifying the exact evidence in a\nfree-form context, providing a reliable way to examine the resources of LLM's\nconclusion and help with the judgment of the trustworthiness."
                },
                "authors": [
                    {
                        "name": "Longchao Da"
                    },
                    {
                        "name": "Parth Mitesh Shah"
                    },
                    {
                        "name": "Kuan-Ru Liou"
                    },
                    {
                        "name": "Jiaxing Zhang"
                    },
                    {
                        "name": "Hua Wei"
                    }
                ],
                "author_detail": {
                    "name": "Hua Wei"
                },
                "author": "Hua Wei",
                "arxiv_comment": "5 pages, 4 figures, accepted to IJCAI2025 demo track",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10143v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10143v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T30",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7; I.2.4; H.3.3",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10134v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10134v1",
                "updated": "2025-05-15T10:04:44Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    4,
                    44,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T10:04:44Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    10,
                    4,
                    44,
                    3,
                    135,
                    0
                ],
                "title": "Large Wireless Localization Model (LWLM): A Foundation Model for\n  Positioning in 6G Networks",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Wireless Localization Model (LWLM): A Foundation Model for\n  Positioning in 6G Networks"
                },
                "summary": "Accurate and robust localization is a critical enabler for emerging 5G and 6G\napplications, including autonomous driving, extended reality (XR), and smart\nmanufacturing. While data-driven approaches have shown promise, most existing\nmodels require large amounts of labeled data and struggle to generalize across\ndeployment scenarios and wireless configurations. To address these limitations,\nwe propose a foundation-model-based solution tailored for wireless\nlocalization. We first analyze how different self-supervised learning (SSL)\ntasks acquire general-purpose and task-specific semantic features based on\ninformation bottleneck (IB) theory. Building on this foundation, we design a\npretraining methodology for the proposed Large Wireless Localization Model\n(LWLM). Specifically, we propose an SSL framework that jointly optimizes three\ncomplementary objectives: (i) spatial-frequency masked channel modeling\n(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)\nposition-invariant contrastive learning (PICL). These objectives jointly\ncapture the underlying semantics of wireless channel from multiple\nperspectives. We further design lightweight decoders for key downstream tasks,\nincluding time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,\nsingle base station (BS) localization, and multiple BS localization.\nComprehensive experimental results confirm that LWLM consistently surpasses\nboth model-based and supervised learning baselines across all localization\ntasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer\nmodels without pretraining, and exhibits strong generalization under\nlabel-limited fine-tuning and unseen BS configurations, confirming its\npotential as a foundation model for wireless localization.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Accurate and robust localization is a critical enabler for emerging 5G and 6G\napplications, including autonomous driving, extended reality (XR), and smart\nmanufacturing. While data-driven approaches have shown promise, most existing\nmodels require large amounts of labeled data and struggle to generalize across\ndeployment scenarios and wireless configurations. To address these limitations,\nwe propose a foundation-model-based solution tailored for wireless\nlocalization. We first analyze how different self-supervised learning (SSL)\ntasks acquire general-purpose and task-specific semantic features based on\ninformation bottleneck (IB) theory. Building on this foundation, we design a\npretraining methodology for the proposed Large Wireless Localization Model\n(LWLM). Specifically, we propose an SSL framework that jointly optimizes three\ncomplementary objectives: (i) spatial-frequency masked channel modeling\n(SF-MCM), (ii) domain-transformation invariance (DTI), and (iii)\nposition-invariant contrastive learning (PICL). These objectives jointly\ncapture the underlying semantics of wireless channel from multiple\nperspectives. We further design lightweight decoders for key downstream tasks,\nincluding time-of-arrival (ToA) estimation, angle-of-arrival (AoA) estimation,\nsingle base station (BS) localization, and multiple BS localization.\nComprehensive experimental results confirm that LWLM consistently surpasses\nboth model-based and supervised learning baselines across all localization\ntasks. In particular, LWLM achieves 26.0%--87.5% improvement over transformer\nmodels without pretraining, and exhibits strong generalization under\nlabel-limited fine-tuning and unseen BS configurations, confirming its\npotential as a foundation model for wireless localization."
                },
                "authors": [
                    {
                        "name": "Guangjin Pan"
                    },
                    {
                        "name": "Kaixuan Huang"
                    },
                    {
                        "name": "Hui Chen"
                    },
                    {
                        "name": "Shunqing Zhang"
                    },
                    {
                        "name": "Christian Hger"
                    },
                    {
                        "name": "Henk Wymeersch"
                    }
                ],
                "author_detail": {
                    "name": "Henk Wymeersch"
                },
                "author": "Henk Wymeersch",
                "arxiv_comment": "13 pages,16 figures.This work has been submitted to the IEEE for\n  possible publication",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10134v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10134v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10117v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10117v1",
                "updated": "2025-05-15T09:42:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    42,
                    11,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T09:42:11Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    42,
                    11,
                    3,
                    135,
                    0
                ],
                "title": "Learning Virtual Machine Scheduling in Cloud Computing through Language\n  Agents",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Learning Virtual Machine Scheduling in Cloud Computing through Language\n  Agents"
                },
                "summary": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In cloud services, virtual machine (VM) scheduling is a typical Online\nDynamic Multidimensional Bin Packing (ODMBP) problem, characterized by\nlarge-scale complexity and fluctuating demands. Traditional optimization\nmethods struggle to adapt to real-time changes, domain-expert-designed\nheuristic approaches suffer from rigid strategies, and existing learning-based\nmethods often lack generalizability and interpretability. To address these\nlimitations, this paper proposes a hierarchical language agent framework named\nMiCo, which provides a large language model (LLM)-driven heuristic design\nparadigm for solving ODMBP. Specifically, ODMBP is formulated as a Semi-Markov\nDecision Process with Options (SMDP-Option), enabling dynamic scheduling\nthrough a two-stage architecture, i.e., Option Miner and Option Composer.\nOption Miner utilizes LLMs to discover diverse and useful non-context-aware\nstrategies by interacting with constructed environments. Option Composer\nemploys LLMs to discover a composing strategy that integrates the\nnon-context-aware strategies with the contextual ones. Extensive experiments on\nreal-world enterprise datasets demonstrate that MiCo achieves a 96.9\\%\ncompetitive ratio in large-scale scenarios involving more than 10,000 virtual\nmachines. It maintains high performance even under nonstationary request flows\nand diverse configurations, thus validating its effectiveness in complex and\nlarge-scale cloud environments."
                },
                "authors": [
                    {
                        "name": "JieHao Wu"
                    },
                    {
                        "name": "Ziwei Wang"
                    },
                    {
                        "name": "Junjie Sheng"
                    },
                    {
                        "name": "Wenhao Li"
                    },
                    {
                        "name": "Xiangfei Wang"
                    },
                    {
                        "name": "Jun Luo"
                    }
                ],
                "author_detail": {
                    "name": "Jun Luo"
                },
                "author": "Jun Luo",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10117v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10117v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10113v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10113v1",
                "updated": "2025-05-15T09:35:26Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    35,
                    26,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T09:35:26Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    35,
                    26,
                    3,
                    135,
                    0
                ],
                "title": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical\n  Specialty Data in Medical LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "What Does Neuro Mean to Cardio? Investigating the Role of Clinical\n  Specialty Data in Medical LLMs"
                },
                "summary": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "In this paper, we introduce S-MedQA, an English medical question-answering\n(QA) dataset for benchmarking large language models in fine-grained clinical\nspecialties. We use S-MedQA to check the applicability of a popular hypothesis\nrelated to knowledge injection in the knowledge-intense scenario of medical QA,\nand show that: 1) training on data from a speciality does not necessarily lead\nto best performance on that specialty and 2) regardless of the specialty\nfine-tuned on, token probabilities of clinically relevant terms for all\nspecialties increase consistently. Thus, we believe improvement gains come\nmostly from domain shifting (e.g., general to medical) rather than knowledge\ninjection and suggest rethinking the role of fine-tuning data in the medical\ndomain. We release S-MedQA and all code needed to reproduce all our experiments\nto the research community."
                },
                "authors": [
                    {
                        "name": "Xinlan Yan"
                    },
                    {
                        "name": "Di Wu"
                    },
                    {
                        "name": "Yibin Lei"
                    },
                    {
                        "name": "Christof Monz"
                    },
                    {
                        "name": "Iacer Calixto"
                    }
                ],
                "author_detail": {
                    "name": "Iacer Calixto"
                },
                "author": "Iacer Calixto",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10113v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10113v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.05922v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.05922v2",
                "updated": "2025-05-15T09:31:11Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    9,
                    31,
                    11,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-09T09:54:07Z",
                "published_parsed": [
                    2025,
                    5,
                    9,
                    9,
                    54,
                    7,
                    4,
                    129,
                    0
                ],
                "title": "Cape: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Cape: Context-Aware Prompt Perturbation Mechanism with Differential\n  Privacy"
                },
                "summary": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) have gained significant popularity due to their\nremarkable capabilities in text understanding and generation. However, despite\ntheir widespread deployment in inference services such as ChatGPT, concerns\nabout the potential leakage of sensitive user data have arisen. Existing\nsolutions primarily rely on privacy-enhancing technologies to mitigate such\nrisks, facing the trade-off among efficiency, privacy, and utility. To narrow\nthis gap, we propose Cape, a context-aware prompt perturbation mechanism based\non differential privacy, to enable efficient inference with an improved\nprivacy-utility trade-off. Concretely, we introduce a hybrid utility function\nthat better captures the token similarity. Additionally, we propose a\nbucketized sampling mechanism to handle large sampling space, which might lead\nto long-tail phenomenons. Extensive experiments across multiple datasets, along\nwith ablation studies, demonstrate that Cape achieves a better privacy-utility\ntrade-off compared to prior state-of-the-art works."
                },
                "authors": [
                    {
                        "name": "Haoqi Wu"
                    },
                    {
                        "name": "Wei Dai"
                    },
                    {
                        "name": "Li Wang"
                    },
                    {
                        "name": "Qiang Yan"
                    }
                ],
                "author_detail": {
                    "name": "Qiang Yan"
                },
                "author": "Qiang Yan",
                "arxiv_comment": "to be published in ICML 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.05922v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.05922v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10093v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10093v1",
                "updated": "2025-05-15T08:51:53Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    51,
                    53,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T08:51:53Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    51,
                    53,
                    3,
                    135,
                    0
                ],
                "title": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based\n  China Studies Using Generative AI",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "From Text to Network: Constructing a Knowledge Graph of Taiwan-Based\n  China Studies Using Generative AI"
                },
                "summary": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Taiwanese China Studies (CS) has developed into a rich, interdisciplinary\nresearch field shaped by the unique geopolitical position and long standing\nacademic engagement with Mainland China. This study responds to the growing\nneed to systematically revisit and reorganize decades of Taiwan based CS\nscholarship by proposing an AI assisted approach that transforms unstructured\nacademic texts into structured, interactive knowledge representations. We apply\ngenerative AI (GAI) techniques and large language models (LLMs) to extract and\nstandardize entity relation triples from 1,367 peer reviewed CS articles\npublished between 1996 and 2019. These triples are then visualized through a\nlightweight D3.js based system, forming the foundation of a domain specific\nknowledge graph and vector database for the field. This infrastructure allows\nusers to explore conceptual nodes and semantic relationships across the corpus,\nrevealing previously uncharted intellectual trajectories, thematic clusters,\nand research gaps. By decomposing textual content into graph structured\nknowledge units, our system enables a paradigm shift from linear text\nconsumption to network based knowledge navigation. In doing so, it enhances\nscholarly access to CS literature while offering a scalable, data driven\nalternative to traditional ontology construction. This work not only\ndemonstrates how generative AI can augment area studies and digital humanities\nbut also highlights its potential to support a reimagined scholarly\ninfrastructure for regional knowledge systems."
                },
                "authors": [
                    {
                        "name": "Hsuan-Lei Shao"
                    }
                ],
                "author_detail": {
                    "name": "Hsuan-Lei Shao"
                },
                "author": "Hsuan-Lei Shao",
                "arxiv_comment": "4 pages, 4 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10093v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10093v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.4; H.3.3; J.5",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2404.12636v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2404.12636v3",
                "updated": "2025-05-15T08:49:58Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    49,
                    58,
                    3,
                    135,
                    0
                ],
                "published": "2024-04-19T05:36:21Z",
                "published_parsed": [
                    2024,
                    4,
                    19,
                    5,
                    36,
                    21,
                    4,
                    110,
                    0
                ],
                "title": "MORepair: Teaching LLMs to Repair Code via Multi-Objective Fine-tuning",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "MORepair: Teaching LLMs to Repair Code via Multi-Objective Fine-tuning"
                },
                "summary": "Within the realm of software engineering, specialized tasks on code, such as\nprogram repair, present unique challenges, necessitating fine-tuning Large\nlanguage models~(LLMs) to unlock state-of-the-art performance. Fine-tuning\napproaches proposed in the literature for LLMs on program repair tasks\ngenerally overlook the need to reason about the logic behind code changes,\nbeyond syntactic patterns in the data. High-performing fine-tuning experiments\nalso usually come at very high computational costs. With MORepair, we propose a\nnovel perspective on the learning focus of LLM fine-tuning for program repair:\nwe not only adapt the LLM parameters to the syntactic nuances of the task of\ncode transformation (objective 1), but we also specifically fine-tune the LLM\nwith respect to the logical reason behind the code change in the training data\n(objective 2). Such a multi-objective fine-tuning will instruct LLMs to\ngenerate high-quality patches. We apply MORepair to fine-tune four open-source\nLLMs with different sizes and architectures. Experimental results on\nfunction-level and repository-level repair benchmarks show that the implemented\nfine-tuning effectively boosts LLM repair performance by 11.4% to 56.0%. We\nfurther show that our fine-tuning strategy yields superior performance compared\nto the state-of-the-art approaches, including standard fine-tuning,\nFine-tune-CoT, and RepairLLaMA.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Within the realm of software engineering, specialized tasks on code, such as\nprogram repair, present unique challenges, necessitating fine-tuning Large\nlanguage models~(LLMs) to unlock state-of-the-art performance. Fine-tuning\napproaches proposed in the literature for LLMs on program repair tasks\ngenerally overlook the need to reason about the logic behind code changes,\nbeyond syntactic patterns in the data. High-performing fine-tuning experiments\nalso usually come at very high computational costs. With MORepair, we propose a\nnovel perspective on the learning focus of LLM fine-tuning for program repair:\nwe not only adapt the LLM parameters to the syntactic nuances of the task of\ncode transformation (objective 1), but we also specifically fine-tune the LLM\nwith respect to the logical reason behind the code change in the training data\n(objective 2). Such a multi-objective fine-tuning will instruct LLMs to\ngenerate high-quality patches. We apply MORepair to fine-tune four open-source\nLLMs with different sizes and architectures. Experimental results on\nfunction-level and repository-level repair benchmarks show that the implemented\nfine-tuning effectively boosts LLM repair performance by 11.4% to 56.0%. We\nfurther show that our fine-tuning strategy yields superior performance compared\nto the state-of-the-art approaches, including standard fine-tuning,\nFine-tune-CoT, and RepairLLaMA."
                },
                "authors": [
                    {
                        "name": "Boyang Yang"
                    },
                    {
                        "name": "Haoye Tian"
                    },
                    {
                        "name": "Jiadong Ren"
                    },
                    {
                        "name": "Hongyu Zhang"
                    },
                    {
                        "name": "Jacques Klein"
                    },
                    {
                        "name": "Tegawend F. Bissyand"
                    },
                    {
                        "name": "Claire Le Goues"
                    },
                    {
                        "name": "Shunfu Jin"
                    }
                ],
                "author_detail": {
                    "name": "Shunfu Jin"
                },
                "author": "Shunfu Jin",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2404.12636v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2404.12636v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10089v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10089v1",
                "updated": "2025-05-15T08:47:55Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    47,
                    55,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T08:47:55Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    47,
                    55,
                    3,
                    135,
                    0
                ],
                "title": "XRAG: Cross-lingual Retrieval-Augmented Generation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "XRAG: Cross-lingual Retrieval-Augmented Generation"
                },
                "summary": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "We propose XRAG, a novel benchmark designed to evaluate the generation\nabilities of LLMs in cross-lingual Retrieval-Augmented Generation (RAG)\nsettings where the user language does not match the retrieval results. XRAG is\nconstructed from recent news articles to ensure that its questions require\nexternal knowledge to be answered. It covers the real-world scenarios of\nmonolingual and multilingual retrieval, and provides relevancy annotations for\neach retrieved document. Our novel dataset construction pipeline results in\nquestions that require complex reasoning, as evidenced by the significant gap\nbetween human and LLM performance. Consequently, XRAG serves as a valuable\nbenchmark for studying LLM reasoning abilities, even before considering the\nadditional cross-lingual complexity. Experimental results on five LLMs uncover\ntwo previously unreported challenges in cross-lingual RAG: 1) in the\nmonolingual retrieval setting, all evaluated models struggle with response\nlanguage correctness; 2) in the multilingual retrieval setting, the main\nchallenge lies in reasoning over retrieved information across languages rather\nthan generation of non-English text."
                },
                "authors": [
                    {
                        "name": "Wei Liu"
                    },
                    {
                        "name": "Sony Trenous"
                    },
                    {
                        "name": "Leonardo F. R. Ribeiro"
                    },
                    {
                        "name": "Bill Byrne"
                    },
                    {
                        "name": "Felix Hieber"
                    }
                ],
                "author_detail": {
                    "name": "Felix Hieber"
                },
                "author": "Felix Hieber",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10089v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10089v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10083v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10083v1",
                "updated": "2025-05-15T08:37:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    37,
                    23,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T08:37:23Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    37,
                    23,
                    3,
                    135,
                    0
                ],
                "title": "ChronoSteer: Bridging Large Language Model and Time Series Foundation\n  Model via Synthetic Data",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ChronoSteer: Bridging Large Language Model and Time Series Foundation\n  Model via Synthetic Data"
                },
                "summary": "Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Conventional forecasting methods rely on unimodal time series data, limiting\ntheir ability to exploit rich textual information. Recently, large language\nmodels (LLMs) and time series foundation models (TSFMs) have demonstrated\npowerful capability in textual reasoning and temporal modeling, respectively.\nIntegrating the strengths of both to construct a multimodal model that\nconcurrently leverages both temporal and textual information for future\ninference has emerged as a critical research challenge. To address the scarcity\nof event-series paired data, we propose a decoupled framework: an LLM is\nemployed to transform textual events into revision instructions, which are then\nused to steer the output of TSFM. To implement this framework, we introduce\nChronoSteer, a multimodal TSFM that can be steered through textual revision\ninstructions, effectively bridging LLM and TSFM. Moreover, to mitigate the\nshortage of cross-modal instruction-series paired data, we devise a two-stage\ntraining strategy based on synthetic data. In addition, we also construct a\nhigh-quality multimodal time series forecasting benchmark to address the\ninformation leakage concerns during evaluation. After integrating with an LLM,\nChronoSteer, which is trained exclusively on synthetic data, achieves a 25.7%\nimprovement in prediction accuracy compared to the unimodal backbone and a\n22.5% gain over the previous state-of-the-art multimodal method."
                },
                "authors": [
                    {
                        "name": "Chengsen Wang"
                    },
                    {
                        "name": "Qi Qi"
                    },
                    {
                        "name": "Zhongwen Rao"
                    },
                    {
                        "name": "Lujia Pan"
                    },
                    {
                        "name": "Jingyu Wang"
                    },
                    {
                        "name": "Jianxin Liao"
                    }
                ],
                "author_detail": {
                    "name": "Jianxin Liao"
                },
                "author": "Jianxin Liao",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10083v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10083v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2504.13016v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2504.13016v2",
                "updated": "2025-05-15T08:28:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    28,
                    23,
                    3,
                    135,
                    0
                ],
                "published": "2025-04-17T15:24:25Z",
                "published_parsed": [
                    2025,
                    4,
                    17,
                    15,
                    24,
                    25,
                    3,
                    107,
                    0
                ],
                "title": "ORIS allocation to minimize the outage probability in a multi-user VLC\n  scenario",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ORIS allocation to minimize the outage probability in a multi-user VLC\n  scenario"
                },
                "summary": "Visible Light Communication (VLC) is a promising solution to address the\ngrowing demand for wireless data, leveraging the widespread use of\nlight-emitting diodes (LEDs) as transmitters. However, its deployment is\nchallenged by link blockages that cause connectivity outages. Optical\nreconfigurable intelligent surfaces (ORISs) have recently emerged as a solution\nto mitigate these disruptions. This work considers a multi-user VLC system and\ninvestigates the optimal association of ORISs to LEDs and users to minimize the\noutage probability while limiting the number of ORISs used. Numerical results\nfrom our proposed optimization algorithm demonstrate that using ORISs can\nreduce the outage probability by up to 85% compared to a no-ORIS scenario.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visible Light Communication (VLC) is a promising solution to address the\ngrowing demand for wireless data, leveraging the widespread use of\nlight-emitting diodes (LEDs) as transmitters. However, its deployment is\nchallenged by link blockages that cause connectivity outages. Optical\nreconfigurable intelligent surfaces (ORISs) have recently emerged as a solution\nto mitigate these disruptions. This work considers a multi-user VLC system and\ninvestigates the optimal association of ORISs to LEDs and users to minimize the\noutage probability while limiting the number of ORISs used. Numerical results\nfrom our proposed optimization algorithm demonstrate that using ORISs can\nreduce the outage probability by up to 85% compared to a no-ORIS scenario."
                },
                "authors": [
                    {
                        "name": "Borja Genoves Guzman"
                    },
                    {
                        "name": "Mat Brandt-Pearce"
                    }
                ],
                "author_detail": {
                    "name": "Mat Brandt-Pearce"
                },
                "author": "Mat Brandt-Pearce",
                "arxiv_doi": "10.1109/LPT.2025.3566979",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/LPT.2025.3566979",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2504.13016v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2504.13016v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "B. G. Guzman and M. Brandt-Pearce, \"ORIS allocation to minimize\n  the outage probability in a multi-user VLC scenario,\" in IEEE Photonics\n  Technology Letters, 2025",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10074v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10074v1",
                "updated": "2025-05-15T08:24:47Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    24,
                    47,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T08:24:47Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    24,
                    47,
                    3,
                    135,
                    0
                ],
                "title": "Leveraging Graph Retrieval-Augmented Generation to Support Learners'\n  Understanding of Knowledge Concepts in MOOCs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Leveraging Graph Retrieval-Augmented Generation to Support Learners'\n  Understanding of Knowledge Concepts in MOOCs"
                },
                "summary": "Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Massive Open Online Courses (MOOCs) lack direct interaction between learners\nand instructors, making it challenging for learners to understand new knowledge\nconcepts. Recently, learners have increasingly used Large Language Models\n(LLMs) to support them in acquiring new knowledge. However, LLMs are prone to\nhallucinations which limits their reliability. Retrieval-Augmented Generation\n(RAG) addresses this issue by retrieving relevant documents before generating a\nresponse. However, the application of RAG across different MOOCs is limited by\nunstructured learning material. Furthermore, current RAG systems do not\nactively guide learners toward their learning needs. To address these\nchallenges, we propose a Graph RAG pipeline that leverages Educational\nKnowledge Graphs (EduKGs) and Personal Knowledge Graphs (PKGs) to guide\nlearners to understand knowledge concepts in the MOOC platform CourseMapper.\nSpecifically, we implement (1) a PKG-based Question Generation method to\nrecommend personalized questions for learners in context, and (2) an\nEduKG-based Question Answering method that leverages the relationships between\nknowledge concepts in the EduKG to answer learner selected questions. To\nevaluate both methods, we conducted a study with 3 expert instructors on 3\ndifferent MOOCs in the MOOC platform CourseMapper. The results of the\nevaluation show the potential of Graph RAG to empower learners to understand\nnew knowledge concepts in a personalized learning experience."
                },
                "authors": [
                    {
                        "name": "Mohamed Abdelmagied"
                    },
                    {
                        "name": "Mohamed Amine Chatti"
                    },
                    {
                        "name": "Shoeb Joarder"
                    },
                    {
                        "name": "Qurat Ul Ain"
                    },
                    {
                        "name": "Rawaa Alatrash"
                    }
                ],
                "author_detail": {
                    "name": "Rawaa Alatrash"
                },
                "author": "Rawaa Alatrash",
                "arxiv_comment": "Accepted at EMOOCs 2025",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10074v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10074v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2407.12393v5",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2407.12393v5",
                "updated": "2025-05-15T08:22:08Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    22,
                    8,
                    3,
                    135,
                    0
                ],
                "published": "2024-07-17T08:13:22Z",
                "published_parsed": [
                    2024,
                    7,
                    17,
                    8,
                    13,
                    22,
                    2,
                    199,
                    0
                ],
                "title": "PersLLM: A Personified Training Approach for Large Language Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "PersLLM: A Personified Training Approach for Large Language Models"
                },
                "summary": "Large language models (LLMs) exhibit human-like intelligence, enabling them\nto simulate human behavior and support various applications that require both\nhumanized communication and extensive knowledge reserves. Efforts are made to\npersonify LLMs with special training data or hand-crafted prompts, while\ncorrespondingly faced with challenges such as insufficient data usage or rigid\nbehavior patterns. Consequently, personified LLMs fail to capture personified\nknowledge or express persistent opinion. To fully unlock the potential of LLM\npersonification, we propose PersLLM, a framework for better data construction\nand model tuning. For insufficient data usage, we incorporate strategies such\nas Chain-of-Thought prompting and anti-induction, improving the quality of data\nconstruction and capturing the personality experiences, knowledge, and thoughts\nmore comprehensively. For rigid behavior patterns, we design the tuning process\nand introduce automated DPO to enhance the specificity and dynamism of the\nmodels' personalities, which leads to a more natural opinion communication.\nBoth automated metrics and expert human evaluations demonstrate the\neffectiveness of our approach. Case studies in human-machine interactions and\nmulti-agent systems further suggest potential application scenarios and future\ndirections for LLM personification.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large language models (LLMs) exhibit human-like intelligence, enabling them\nto simulate human behavior and support various applications that require both\nhumanized communication and extensive knowledge reserves. Efforts are made to\npersonify LLMs with special training data or hand-crafted prompts, while\ncorrespondingly faced with challenges such as insufficient data usage or rigid\nbehavior patterns. Consequently, personified LLMs fail to capture personified\nknowledge or express persistent opinion. To fully unlock the potential of LLM\npersonification, we propose PersLLM, a framework for better data construction\nand model tuning. For insufficient data usage, we incorporate strategies such\nas Chain-of-Thought prompting and anti-induction, improving the quality of data\nconstruction and capturing the personality experiences, knowledge, and thoughts\nmore comprehensively. For rigid behavior patterns, we design the tuning process\nand introduce automated DPO to enhance the specificity and dynamism of the\nmodels' personalities, which leads to a more natural opinion communication.\nBoth automated metrics and expert human evaluations demonstrate the\neffectiveness of our approach. Case studies in human-machine interactions and\nmulti-agent systems further suggest potential application scenarios and future\ndirections for LLM personification."
                },
                "authors": [
                    {
                        "name": "Zheni Zeng"
                    },
                    {
                        "name": "Jiayi Chen"
                    },
                    {
                        "name": "Huimin Chen"
                    },
                    {
                        "name": "Yukun Yan"
                    },
                    {
                        "name": "Yuxuan Chen"
                    },
                    {
                        "name": "Zhenghao Liu"
                    },
                    {
                        "name": "Zhiyuan Liu"
                    },
                    {
                        "name": "Maosong Sun"
                    }
                ],
                "author_detail": {
                    "name": "Maosong Sun"
                },
                "author": "Maosong Sun",
                "arxiv_comment": "8 pages for main text, 5 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2407.12393v5",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2407.12393v5",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2401.16627v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2401.16627v2",
                "updated": "2025-05-15T08:21:56Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    21,
                    56,
                    3,
                    135,
                    0
                ],
                "published": "2024-01-29T23:56:02Z",
                "published_parsed": [
                    2024,
                    1,
                    29,
                    23,
                    56,
                    2,
                    0,
                    29,
                    0
                ],
                "title": "Resource allocation exploiting reflective surfaces to minimize the\n  outage probability in VLC",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Resource allocation exploiting reflective surfaces to minimize the\n  outage probability in VLC"
                },
                "summary": "Visible light communication (VLC) is a technology that complements radio\nfrequency (RF) to fulfill the ever-increasing demand for wireless data traffic.\nThe ubiquity of light-emitting diodes (LEDs), exploited as transmitters,\nincreases the VLC market penetration and positions it as one of the most\npromising technologies to alleviate the spectrum scarcity of RF. However, VLC\ndeployment is hindered by blockage causing connectivity outages in the presence\nof obstacles. Recently, optical reconfigurable intelligent surfaces (ORISs)\nhave been considered to mitigate this problem. While prior works exploit ORISs\nfor data or secrecy rate maximization, this paper studies the optimal placement\nof mirrors and ORISs, and the LED power allocation, for jointly minimizing the\noutage probability while keeping the lighting standards. We describe an optimal\noutage minimization framework and present solvable heuristics. We provide\nextensive numerical results and show that the use of ORISs may reduce the\noutage probability by up to 67% with respect to a no-mirror scenario and\nprovide a gain of hundreds of kbit/J in optical energy efficiency with respect\nto the presented benchmark.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Visible light communication (VLC) is a technology that complements radio\nfrequency (RF) to fulfill the ever-increasing demand for wireless data traffic.\nThe ubiquity of light-emitting diodes (LEDs), exploited as transmitters,\nincreases the VLC market penetration and positions it as one of the most\npromising technologies to alleviate the spectrum scarcity of RF. However, VLC\ndeployment is hindered by blockage causing connectivity outages in the presence\nof obstacles. Recently, optical reconfigurable intelligent surfaces (ORISs)\nhave been considered to mitigate this problem. While prior works exploit ORISs\nfor data or secrecy rate maximization, this paper studies the optimal placement\nof mirrors and ORISs, and the LED power allocation, for jointly minimizing the\noutage probability while keeping the lighting standards. We describe an optimal\noutage minimization framework and present solvable heuristics. We provide\nextensive numerical results and show that the use of ORISs may reduce the\noutage probability by up to 67% with respect to a no-mirror scenario and\nprovide a gain of hundreds of kbit/J in optical energy efficiency with respect\nto the presented benchmark."
                },
                "authors": [
                    {
                        "name": "Borja Genoves Guzman"
                    },
                    {
                        "name": "Maximo Morales Cespedes"
                    },
                    {
                        "name": "Victor P. Gil Jimenez"
                    },
                    {
                        "name": "Ana Garcia Armada"
                    },
                    {
                        "name": "Maite Brandt-Pearce"
                    }
                ],
                "author_detail": {
                    "name": "Maite Brandt-Pearce"
                },
                "author": "Maite Brandt-Pearce",
                "arxiv_doi": "10.1109/TWC.2025.3547648",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/10.1109/TWC.2025.3547648",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2401.16627v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2401.16627v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "B. G. Guzman, M. M. Cespedes, V. P. Gil Jimenez, A. G. Armada and\n  M. Brandt-Pearce, \"Resource allocation exploiting reflective surfaces to\n  minimize the outage probability in VLC,\" in IEEE Transactions on Wireless\n  Communications, 2025",
                "arxiv_primary_category": {
                    "term": "eess.SP",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10066v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10066v1",
                "updated": "2025-05-15T08:07:04Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    7,
                    4,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T08:07:04Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    7,
                    4,
                    3,
                    135,
                    0
                ],
                "title": "Dark LLMs: The Growing Threat of Unaligned AI Models",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Dark LLMs: The Growing Threat of Unaligned AI Models"
                },
                "summary": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) rapidly reshape modern life, advancing fields\nfrom healthcare to education and beyond. However, alongside their remarkable\ncapabilities lies a significant threat: the susceptibility of these models to\njailbreaking. The fundamental vulnerability of LLMs to jailbreak attacks stems\nfrom the very data they learn from. As long as this training data includes\nunfiltered, problematic, or 'dark' content, the models can inherently learn\nundesirable patterns or weaknesses that allow users to circumvent their\nintended safety controls. Our research identifies the growing threat posed by\ndark LLMs models deliberately designed without ethical guardrails or modified\nthrough jailbreak techniques. In our research, we uncovered a universal\njailbreak attack that effectively compromises multiple state-of-the-art models,\nenabling them to answer almost any question and produce harmful outputs upon\nrequest. The main idea of our attack was published online over seven months\nago. However, many of the tested LLMs were still vulnerable to this attack.\nDespite our responsible disclosure efforts, responses from major LLM providers\nwere often inadequate, highlighting a concerning gap in industry practices\nregarding AI safety. As model training becomes more accessible and cheaper, and\nas open-source LLMs proliferate, the risk of widespread misuse escalates.\nWithout decisive intervention, LLMs may continue democratizing access to\ndangerous knowledge, posing greater risks than anticipated."
                },
                "authors": [
                    {
                        "name": "Michael Fire"
                    },
                    {
                        "name": "Yitzhak Elbazis"
                    },
                    {
                        "name": "Adi Wasenstein"
                    },
                    {
                        "name": "Lior Rokach"
                    }
                ],
                "author_detail": {
                    "name": "Lior Rokach"
                },
                "author": "Lior Rokach",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10066v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10066v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "68T50, 68T05, 68P25",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "I.2.7",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10063v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10063v1",
                "updated": "2025-05-15T08:05:12Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    5,
                    12,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T08:05:12Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    8,
                    5,
                    12,
                    3,
                    135,
                    0
                ],
                "title": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance\n  Multi-Document QA Capability",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "CAFE: Retrieval Head-based Coarse-to-Fine Information Seeking to Enhance\n  Multi-Document QA Capability"
                },
                "summary": "Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Advancements in Large Language Models (LLMs) have extended their input\ncontext length, yet they still struggle with retrieval and reasoning in\nlong-context inputs. Existing methods propose to utilize the prompt strategy\nand retrieval head to alleviate this limitation. However, they still face\nchallenges in balancing retrieval precision and recall, impacting their\nefficacy in answering questions. To address this, we introduce $\\textbf{CAFE}$,\na two-stage coarse-to-fine method to enhance multi-document question-answering\ncapacities. By gradually eliminating the negative impacts of background and\ndistracting documents, CAFE makes the responses more reliant on the evidence\ndocuments. Initially, a coarse-grained filtering method leverages retrieval\nheads to identify and rank relevant documents. Then, a fine-grained steering\nmethod guides attention to the most relevant content. Experiments across\nbenchmarks show CAFE outperforms baselines, achieving up to 22.1% and 13.7%\nSubEM improvement over SFT and RAG methods on the Mistral model, respectively."
                },
                "authors": [
                    {
                        "name": "Han Peng"
                    },
                    {
                        "name": "Jinhao Jiang"
                    },
                    {
                        "name": "Zican Dong"
                    },
                    {
                        "name": "Wayne Xin Zhao"
                    },
                    {
                        "name": "Lei Fang"
                    }
                ],
                "author_detail": {
                    "name": "Lei Fang"
                },
                "author": "Lei Fang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10063v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10063v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10047v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10047v1",
                "updated": "2025-05-15T07:45:45Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    45,
                    45,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T07:45:45Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    45,
                    45,
                    3,
                    135,
                    0
                ],
                "title": "Enhancing performance in bolt torque tightening using a connected torque\n  wrench and augmented reality",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Enhancing performance in bolt torque tightening using a connected torque\n  wrench and augmented reality"
                },
                "summary": "Modern production rates and the increasing complexity of mechanical systems\nrequire efficient and effective manufacturing and assembly processes. The\ntransition to Industry 4.0, supported by the deployment of innovative tools\nsuch as Augmented Reality (AR), equips the industry to tackle future\nchallenges. Among critical processes, the assembly and tightening of bolted\njoints stand out due to their significant safety and economic implications\nacross various industrial sectors. This study proposes an innovative tightening\nmethod designed to enhance the reliability of bolted assembly tightening\nthrough the use of Augmented Reality and connected tools. A\n6-Degrees-of-Freedom (6-DoF) tracked connected torque wrench assists the\noperator during tightening, ensuring each screw is tightened to the correct\ntorque. The effectiveness of this method is compared with the conventional\ntightening method using paper instructions. Participants in the study carried\nout tightening sequences on two simple parts with multiple screws. The study\nevaluates the impact of the proposed method on task performance and its\nacceptability to operators. The tracked connected torque wrench provides\nconsiderable assistance to the operators, including wrench control and\nautomatic generation of tightening reports. The results suggest that the\nAR-based method has the potential to ensure reliable torque tightening of\nbolted joints.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Modern production rates and the increasing complexity of mechanical systems\nrequire efficient and effective manufacturing and assembly processes. The\ntransition to Industry 4.0, supported by the deployment of innovative tools\nsuch as Augmented Reality (AR), equips the industry to tackle future\nchallenges. Among critical processes, the assembly and tightening of bolted\njoints stand out due to their significant safety and economic implications\nacross various industrial sectors. This study proposes an innovative tightening\nmethod designed to enhance the reliability of bolted assembly tightening\nthrough the use of Augmented Reality and connected tools. A\n6-Degrees-of-Freedom (6-DoF) tracked connected torque wrench assists the\noperator during tightening, ensuring each screw is tightened to the correct\ntorque. The effectiveness of this method is compared with the conventional\ntightening method using paper instructions. Participants in the study carried\nout tightening sequences on two simple parts with multiple screws. The study\nevaluates the impact of the proposed method on task performance and its\nacceptability to operators. The tracked connected torque wrench provides\nconsiderable assistance to the operators, including wrench control and\nautomatic generation of tightening reports. The results suggest that the\nAR-based method has the potential to ensure reliable torque tightening of\nbolted joints."
                },
                "authors": [
                    {
                        "name": "Adeline Fau"
                    },
                    {
                        "name": "Mina Ghobrial"
                    },
                    {
                        "name": "Philippe Seitier"
                    },
                    {
                        "name": "Pierre Lagarrigue"
                    },
                    {
                        "name": "Michel Galaup"
                    },
                    {
                        "name": "Alain Daidi"
                    },
                    {
                        "name": "Patrick Gilles"
                    }
                ],
                "author_detail": {
                    "name": "Patrick Gilles"
                },
                "arxiv_affiliation": "INSA Toulouse",
                "author": "Patrick Gilles",
                "arxiv_doi": "27E8;10.21741/9781644903599-213",
                "links": [
                    {
                        "title": "doi",
                        "href": "http://dx.doi.org/27E8;10.21741/9781644903599-213",
                        "rel": "related",
                        "type": "text/html"
                    },
                    {
                        "href": "http://arxiv.org/abs/2505.10047v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10047v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_journal_ref": "Material Forming - ESAFORM2025, May 2025, Paestum, Italy.\n  pp.1982-1991",
                "arxiv_primary_category": {
                    "term": "cs.HC",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "physics.class-ph",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10046v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10046v1",
                "updated": "2025-05-15T07:43:23Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    43,
                    23,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T07:43:23Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    43,
                    23,
                    3,
                    135,
                    0
                ],
                "title": "Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Exploring the Deep Fusion of Large Language Models and Diffusion\n  Transformers for Text-to-Image Synthesis"
                },
                "summary": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "This paper does not describe a new method; instead, it provides a thorough\nexploration of an important yet understudied design space related to recent\nadvances in text-to-image synthesis -- specifically, the deep fusion of large\nlanguage models (LLMs) and diffusion transformers (DiTs) for multi-modal\ngeneration. Previous studies mainly focused on overall system performance\nrather than detailed comparisons with alternative methods, and key design\ndetails and training recipes were often left undisclosed. These gaps create\nuncertainty about the real potential of this approach. To fill these gaps, we\nconduct an empirical study on text-to-image generation, performing controlled\ncomparisons with established baselines, analyzing important design choices, and\nproviding a clear, reproducible recipe for training at scale. We hope this work\noffers meaningful data points and practical guidelines for future research in\nmulti-modal generation."
                },
                "authors": [
                    {
                        "name": "Bingda Tang"
                    },
                    {
                        "name": "Boyang Zheng"
                    },
                    {
                        "name": "Xichen Pan"
                    },
                    {
                        "name": "Sayak Paul"
                    },
                    {
                        "name": "Saining Xie"
                    }
                ],
                "author_detail": {
                    "name": "Saining Xie"
                },
                "author": "Saining Xie",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10046v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10046v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10044v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10044v1",
                "updated": "2025-05-15T07:41:40Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    41,
                    40,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T07:41:40Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    41,
                    40,
                    3,
                    135,
                    0
                ],
                "title": "To what extent can current French mobile network support agricultural\n  robots?",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "To what extent can current French mobile network support agricultural\n  robots?"
                },
                "summary": "The large-scale integration of robots in agriculture offers many promises for\nenhancing sustainability and increasing food production. The numerous\napplications of agricultural robots rely on the transmission of data via mobile\nnetwork, with the amount of data depending on the services offered by the\nrobots and the level of on-board technology. Nevertheless, infrastructure\nrequired to deploy these robots, as well as the related energy and\nenvironmental consequences, appear overlooked in the digital agriculture\nliterature. In this study, we propose a method for assessing the additional\nenergy consumption and carbon footprint induced by a large-scale deployment of\nagricultural robots. Our method also estimates the share of agricultural area\nthat can be managed by the deployed robots with respect to network\ninfrastructure constraints. We have applied this method to metropolitan France\nmobile network and agricultural parcels for five different robotic scenarios.\nOur results show that increasing the robot's bitrate needs leads to significant\nadditional impacts, which increase at a pace that is poorly captured by\nclassical linear extrapolation methods. When constraining the network to the\nexisting sites, increased bitrate needs also comes with a rapidly decreasing\nmanageable agricultural area.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The large-scale integration of robots in agriculture offers many promises for\nenhancing sustainability and increasing food production. The numerous\napplications of agricultural robots rely on the transmission of data via mobile\nnetwork, with the amount of data depending on the services offered by the\nrobots and the level of on-board technology. Nevertheless, infrastructure\nrequired to deploy these robots, as well as the related energy and\nenvironmental consequences, appear overlooked in the digital agriculture\nliterature. In this study, we propose a method for assessing the additional\nenergy consumption and carbon footprint induced by a large-scale deployment of\nagricultural robots. Our method also estimates the share of agricultural area\nthat can be managed by the deployed robots with respect to network\ninfrastructure constraints. We have applied this method to metropolitan France\nmobile network and agricultural parcels for five different robotic scenarios.\nOur results show that increasing the robot's bitrate needs leads to significant\nadditional impacts, which increase at a pace that is poorly captured by\nclassical linear extrapolation methods. When constraining the network to the\nexisting sites, increased bitrate needs also comes with a rapidly decreasing\nmanageable agricultural area."
                },
                "authors": [
                    {
                        "name": "Pierre La Rocca"
                    },
                    {
                        "name": "Gal Guennebaud"
                    },
                    {
                        "name": "Aurlie Bugeau"
                    }
                ],
                "author_detail": {
                    "name": "Aurlie Bugeau"
                },
                "arxiv_affiliation": "IUF, LaBRI, UB",
                "author": "Aurlie Bugeau",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10044v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10044v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CY",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CY",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2405.04760v4",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2405.04760v4",
                "updated": "2025-05-15T07:33:07Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    33,
                    7,
                    3,
                    135,
                    0
                ],
                "published": "2024-05-08T02:09:17Z",
                "published_parsed": [
                    2024,
                    5,
                    8,
                    2,
                    9,
                    17,
                    2,
                    129,
                    0
                ],
                "title": "Large Language Models for Cyber Security: A Systematic Literature Review",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models for Cyber Security: A Systematic Literature Review"
                },
                "summary": "The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in various domains,\nincluding cybersecurity. As the volume and sophistication of cyber threats\ncontinue to grow, there is an increasing need for intelligent systems that can\nautomatically detect vulnerabilities, analyze malware, and respond to attacks.\nIn this survey, we conduct a comprehensive review of the literature on the\napplication of LLMs in cybersecurity (LLM4Security). By comprehensively\ncollecting over 30K relevant papers and systematically analyzing 127 papers\nfrom top security and software engineering venues, we aim to provide a holistic\nview of how LLMs are being used to solve diverse problems across the\ncybersecurity domain. Through our analysis, we identify several key findings.\nFirst, we observe that LLMs are being applied to a wide range of cybersecurity\ntasks, including vulnerability detection, malware analysis, network intrusion\ndetection, and phishing detection. Second, we find that the datasets used for\ntraining and evaluating LLMs in these tasks are often limited in size and\ndiversity, highlighting the need for more comprehensive and representative\ndatasets. Third, we identify several promising techniques for adapting LLMs to\nspecific cybersecurity domains, such as fine-tuning, transfer learning, and\ndomain-specific pre-training. Finally, we discuss the main challenges and\nopportunities for future research in LLM4Security, including the need for more\ninterpretable and explainable models, the importance of addressing data privacy\nand security concerns, and the potential for leveraging LLMs for proactive\ndefense and threat hunting. Overall, our survey provides a comprehensive\noverview of the current state-of-the-art in LLM4Security and identifies several\npromising directions for future research.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "The rapid advancement of Large Language Models (LLMs) has opened up new\nopportunities for leveraging artificial intelligence in various domains,\nincluding cybersecurity. As the volume and sophistication of cyber threats\ncontinue to grow, there is an increasing need for intelligent systems that can\nautomatically detect vulnerabilities, analyze malware, and respond to attacks.\nIn this survey, we conduct a comprehensive review of the literature on the\napplication of LLMs in cybersecurity (LLM4Security). By comprehensively\ncollecting over 30K relevant papers and systematically analyzing 127 papers\nfrom top security and software engineering venues, we aim to provide a holistic\nview of how LLMs are being used to solve diverse problems across the\ncybersecurity domain. Through our analysis, we identify several key findings.\nFirst, we observe that LLMs are being applied to a wide range of cybersecurity\ntasks, including vulnerability detection, malware analysis, network intrusion\ndetection, and phishing detection. Second, we find that the datasets used for\ntraining and evaluating LLMs in these tasks are often limited in size and\ndiversity, highlighting the need for more comprehensive and representative\ndatasets. Third, we identify several promising techniques for adapting LLMs to\nspecific cybersecurity domains, such as fine-tuning, transfer learning, and\ndomain-specific pre-training. Finally, we discuss the main challenges and\nopportunities for future research in LLM4Security, including the need for more\ninterpretable and explainable models, the importance of addressing data privacy\nand security concerns, and the potential for leveraging LLMs for proactive\ndefense and threat hunting. Overall, our survey provides a comprehensive\noverview of the current state-of-the-art in LLM4Security and identifies several\npromising directions for future research."
                },
                "authors": [
                    {
                        "name": "Hanxiang Xu"
                    },
                    {
                        "name": "Shenao Wang"
                    },
                    {
                        "name": "Ningke Li"
                    },
                    {
                        "name": "Kailong Wang"
                    },
                    {
                        "name": "Yanjie Zhao"
                    },
                    {
                        "name": "Kai Chen"
                    },
                    {
                        "name": "Ting Yu"
                    },
                    {
                        "name": "Yang Liu"
                    },
                    {
                        "name": "Haoyu Wang"
                    }
                ],
                "author_detail": {
                    "name": "Haoyu Wang"
                },
                "author": "Haoyu Wang",
                "arxiv_comment": "56 pages,6 figures",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2405.04760v4",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2405.04760v4",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CR",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CR",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.09427v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.09427v2",
                "updated": "2025-05-15T07:22:20Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    22,
                    20,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-14T14:28:24Z",
                "published_parsed": [
                    2025,
                    5,
                    14,
                    14,
                    28,
                    24,
                    2,
                    134,
                    0
                ],
                "title": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SafePath: Conformal Prediction for Safe LLM-Based Autonomous Navigation"
                },
                "summary": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Large Language Models (LLMs) show growing promise in autonomous driving by\nreasoning over complex traffic scenarios to generate path plans. However, their\ntendencies toward overconfidence, and hallucinations raise critical safety\nconcerns. We introduce SafePath, a modular framework that augments LLM-based\npath planning with formal safety guarantees using conformal prediction.\nSafePath operates in three stages. In the first stage, we use an LLM that\ngenerates a set of diverse candidate paths, exploring possible trajectories\nbased on agent behaviors and environmental cues. In the second stage, SafePath\nfilters out high-risk trajectories while guaranteeing that at least one safe\noption is included with a user-defined probability, through a multiple-choice\nquestion-answering formulation that integrates conformal prediction. In the\nfinal stage, our approach selects the path with the lowest expected collision\nrisk when uncertainty is low or delegates control to a human when uncertainty\nis high. We theoretically prove that SafePath guarantees a safe trajectory with\na user-defined probability, and we show how its human delegation rate can be\ntuned to balance autonomy and safety. Extensive experiments on nuScenes and\nHighway-env show that SafePath reduces planning uncertainty by 77\\% and\ncollision rates by up to 70\\%, demonstrating effectiveness in making LLM-driven\npath planning more safer."
                },
                "authors": [
                    {
                        "name": "Achref Doula"
                    },
                    {
                        "name": "Max Mhlhuser"
                    },
                    {
                        "name": "Alejandro Sanchez Guinea"
                    }
                ],
                "author_detail": {
                    "name": "Alejandro Sanchez Guinea"
                },
                "author": "Alejandro Sanchez Guinea",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.09427v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.09427v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.RO",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.19159v3",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.19159v3",
                "updated": "2025-05-15T07:04:35Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    7,
                    4,
                    35,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-26T14:15:24Z",
                "published_parsed": [
                    2025,
                    2,
                    26,
                    14,
                    15,
                    24,
                    2,
                    57,
                    0
                ],
                "title": "A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs"
                },
                "summary": "Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. However, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the ``Patch-like'' feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe propose a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35% pruning on the\nVicuna-7B model, our method achieved a 1.654% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Compared to width-wise pruning, depth-wise pruning can significantly\naccelerate inference in resource-constrained scenarios. However, treating the\nentire Transformer layer as the minimum pruning unit may degrade model\nperformance by indiscriminately discarding the entire information of the layer.\nThis paper reveals the ``Patch-like'' feature relationship between layers in\nlarge language models by analyzing the correlation of the outputs of different\nlayers in the reproducing kernel Hilbert space. Building on this observation,\nwe propose a sliding layer merging method that dynamically selects and fuses\nconsecutive layers from top to bottom according to a pre-defined similarity\nthreshold, thereby simplifying the model structure while maintaining its\nperformance. Extensive experiments on LLMs with various architectures and\ndifferent parameter scales show that our method outperforms existing pruning\ntechniques in both zero-shot inference performance and retraining recovery\nquality after pruning. In particular, in the experiment with 35% pruning on the\nVicuna-7B model, our method achieved a 1.654% improvement in average\nperformance on zero-shot tasks compared to the existing method. Moreover, we\nfurther reveal the potential of combining depth pruning with width pruning to\nenhance the pruning effect. Our codes are available at\nhttps://github.com/920927/SLM-a-sliding-layer-merging-method."
                },
                "authors": [
                    {
                        "name": "Xuan Ding"
                    },
                    {
                        "name": "Rui Sun"
                    },
                    {
                        "name": "Yunjian Zhang"
                    },
                    {
                        "name": "Xiu Yan"
                    },
                    {
                        "name": "Yueqi Zhou"
                    },
                    {
                        "name": "Kaihao Huang"
                    },
                    {
                        "name": "Suzhong Fu"
                    },
                    {
                        "name": "Angelica I Aviles-Rivero"
                    },
                    {
                        "name": "Chuanlong Xie"
                    },
                    {
                        "name": "Yao Zhu"
                    }
                ],
                "author_detail": {
                    "name": "Yao Zhu"
                },
                "author": "Yao Zhu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.19159v3",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.19159v3",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CV",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CV",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10013v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10013v1",
                "updated": "2025-05-15T06:53:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    53,
                    37,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T06:53:37Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    53,
                    37,
                    3,
                    135,
                    0
                ],
                "title": "DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "DIF: A Framework for Benchmarking and Verifying Implicit Bias in LLMs"
                },
                "summary": "As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "As Large Language Models (LLMs) have risen in prominence over the past few\nyears, there has been concern over the potential biases in LLMs inherited from\nthe training data. Previous studies have examined how LLMs exhibit implicit\nbias, such as when response generation changes when different social contexts\nare introduced. We argue that this implicit bias is not only an ethical, but\nalso a technical issue, as it reveals an inability of LLMs to accommodate\nextraneous information. However, unlike other measures of LLM intelligence,\nthere are no standard methods to benchmark this specific subset of LLM bias. To\nbridge this gap, we developed a method for calculating an easily interpretable\nbenchmark, DIF (Demographic Implicit Fairness), by evaluating preexisting LLM\nlogic and math problem datasets with sociodemographic personas. We demonstrate\nthat this method can statistically validate the presence of implicit bias in\nLLM behavior and find an inverse trend between question answering accuracy and\nimplicit bias, supporting our argument."
                },
                "authors": [
                    {
                        "name": "Lake Yin"
                    },
                    {
                        "name": "Fan Huang"
                    }
                ],
                "author_detail": {
                    "name": "Fan Huang"
                },
                "author": "Fan Huang",
                "arxiv_comment": "7 pages, 1 figure",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10013v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10013v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.CL",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.CL",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10010v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10010v1",
                "updated": "2025-05-15T06:45:37Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    45,
                    37,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T06:45:37Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    45,
                    37,
                    3,
                    135,
                    0
                ],
                "title": "ImagineBench: Evaluating Reinforcement Learning with Large Language\n  Model Rollouts",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "ImagineBench: Evaluating Reinforcement Learning with Large Language\n  Model Rollouts"
                },
                "summary": "A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "A central challenge in reinforcement learning (RL) is its dependence on\nextensive real-world interaction data to learn task-specific policies. While\nrecent work demonstrates that large language models (LLMs) can mitigate this\nlimitation by generating synthetic experience (noted as imaginary rollouts) for\nmastering novel tasks, progress in this emerging field is hindered due to the\nlack of a standard benchmark. To bridge this gap, we introduce ImagineBench,\nthe first comprehensive benchmark for evaluating offline RL algorithms that\nleverage both real rollouts and LLM-imaginary rollouts. The key features of\nImagineBench include: (1) datasets comprising environment-collected and\nLLM-imaginary rollouts; (2) diverse domains of environments covering\nlocomotion, robotic manipulation, and navigation tasks; and (3) natural\nlanguage task instructions with varying complexity levels to facilitate\nlanguage-conditioned policy learning. Through systematic evaluation of\nstate-of-the-art offline RL algorithms, we observe that simply applying\nexisting offline RL algorithms leads to suboptimal performance on unseen tasks,\nachieving 35.44% success rate in hard tasks in contrast to 64.37% of method\ntraining on real rollouts for hard tasks. This result highlights the need for\nalgorithm advancements to better leverage LLM-imaginary rollouts. Additionally,\nwe identify key opportunities for future research: including better utilization\nof imaginary rollouts, fast online adaptation and continual learning, and\nextension to multi-modal tasks. Our code is publicly available at\nhttps://github.com/LAMDA-RL/ImagineBench."
                },
                "authors": [
                    {
                        "name": "Jing-Cheng Pang"
                    },
                    {
                        "name": "Kaiyuan Li"
                    },
                    {
                        "name": "Yidi Wang"
                    },
                    {
                        "name": "Si-Hang Yang"
                    },
                    {
                        "name": "Shengyi Jiang"
                    },
                    {
                        "name": "Yang Yu"
                    }
                ],
                "author_detail": {
                    "name": "Yang Yu"
                },
                "author": "Yang Yu",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10010v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10010v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10008v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10008v1",
                "updated": "2025-05-15T06:43:32Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    43,
                    32,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T06:43:32Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    43,
                    32,
                    3,
                    135,
                    0
                ],
                "title": "SVA-ICL: Improving LLM-based Software Vulnerability Assessment via\n  In-Context Learning and Information Fusion",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SVA-ICL: Improving LLM-based Software Vulnerability Assessment via\n  In-Context Learning and Information Fusion"
                },
                "summary": "Context: Software vulnerability assessment (SVA) is critical for identifying,\nevaluating, and prioritizing security weaknesses in software applications.\nObjective: Despite the increasing application of large language models (LLMs)\nin various software engineering tasks, their effectiveness in SVA remains\nunderexplored. Method: To address this gap, we introduce a novel approach\nSVA-ICL, which leverages in-context learning (ICL) to enhance LLM performance.\nOur approach involves the selection of high-quality demonstrations for ICL\nthrough information fusion, incorporating both source code and vulnerability\ndescriptions. For source code, we consider semantic, lexical, and syntactic\nsimilarities, while for vulnerability descriptions, we focus on textual\nsimilarity. Based on the selected demonstrations, we construct context prompts\nand consider DeepSeek-V2 as the LLM for SVA-ICL. Results: We evaluate the\neffectiveness of SVA-ICL using a large-scale dataset comprising 12,071 C/C++\nvulnerabilities. Experimental results demonstrate that SVA-ICL outperforms\nstate-of-the-art SVA baselines in terms of Accuracy, F1-score, and MCC\nmeasures. Furthermore, ablation studies highlight the significance of component\ncustomization in SVA-ICL, such as the number of demonstrations, the\ndemonstration ordering strategy, and the optimal fusion ratio of different\nmodalities. Conclusion: Our findings suggest that leveraging ICL with\ninformation fusion can effectively improve the effectiveness of LLM-based SVA,\nwarranting further research in this direction.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Context: Software vulnerability assessment (SVA) is critical for identifying,\nevaluating, and prioritizing security weaknesses in software applications.\nObjective: Despite the increasing application of large language models (LLMs)\nin various software engineering tasks, their effectiveness in SVA remains\nunderexplored. Method: To address this gap, we introduce a novel approach\nSVA-ICL, which leverages in-context learning (ICL) to enhance LLM performance.\nOur approach involves the selection of high-quality demonstrations for ICL\nthrough information fusion, incorporating both source code and vulnerability\ndescriptions. For source code, we consider semantic, lexical, and syntactic\nsimilarities, while for vulnerability descriptions, we focus on textual\nsimilarity. Based on the selected demonstrations, we construct context prompts\nand consider DeepSeek-V2 as the LLM for SVA-ICL. Results: We evaluate the\neffectiveness of SVA-ICL using a large-scale dataset comprising 12,071 C/C++\nvulnerabilities. Experimental results demonstrate that SVA-ICL outperforms\nstate-of-the-art SVA baselines in terms of Accuracy, F1-score, and MCC\nmeasures. Furthermore, ablation studies highlight the significance of component\ncustomization in SVA-ICL, such as the number of demonstrations, the\ndemonstration ordering strategy, and the optimal fusion ratio of different\nmodalities. Conclusion: Our findings suggest that leveraging ICL with\ninformation fusion can effectively improve the effectiveness of LLM-based SVA,\nwarranting further research in this direction."
                },
                "authors": [
                    {
                        "name": "Chaoyang Gao"
                    },
                    {
                        "name": "Xiang Chen"
                    },
                    {
                        "name": "Guangbei Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Guangbei Zhang"
                },
                "author": "Guangbei Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10008v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10008v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.SE",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.SE",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2505.10003v1",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2505.10003v1",
                "updated": "2025-05-15T06:32:59Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    32,
                    59,
                    3,
                    135,
                    0
                ],
                "published": "2025-05-15T06:32:59Z",
                "published_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    32,
                    59,
                    3,
                    135,
                    0
                ],
                "title": "AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom\n  Domain Large Model",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "AI2MMUM: AI-AI Oriented Multi-Modal Universal Model Leveraging Telecom\n  Domain Large Model"
                },
                "summary": "Designing a 6G-oriented universal model capable of processing multi-modal\ndata and executing diverse air interface tasks has emerged as a common goal in\nfuture wireless systems. Building on our prior work in communication\nmulti-modal alignment and telecom large language model (LLM), we propose a\nscalable, task-aware artificial intelligence-air interface multi-modal\nuniversal model (AI2MMUM), which flexibility and effectively perform various\nphysical layer tasks according to subtle task instructions. The LLM backbone\nprovides robust contextual comprehension and generalization capabilities, while\na fine-tuning approach is adopted to incorporate domain-specific knowledge. To\nenhance task adaptability, task instructions consist of fixed task keywords and\nlearnable, implicit prefix prompts. Frozen radio modality encoders extract\nuniversal representations and adapter layers subsequently bridge radio and\nlanguage modalities. Moreover, lightweight task-specific heads are designed to\ndirectly output task objectives. Comprehensive evaluations demonstrate that\nAI2MMUM achieves SOTA performance across five representative physical\nenvironment/wireless channel-based downstream tasks using the WAIR-D and\nDeepMIMO datasets.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Designing a 6G-oriented universal model capable of processing multi-modal\ndata and executing diverse air interface tasks has emerged as a common goal in\nfuture wireless systems. Building on our prior work in communication\nmulti-modal alignment and telecom large language model (LLM), we propose a\nscalable, task-aware artificial intelligence-air interface multi-modal\nuniversal model (AI2MMUM), which flexibility and effectively perform various\nphysical layer tasks according to subtle task instructions. The LLM backbone\nprovides robust contextual comprehension and generalization capabilities, while\na fine-tuning approach is adopted to incorporate domain-specific knowledge. To\nenhance task adaptability, task instructions consist of fixed task keywords and\nlearnable, implicit prefix prompts. Frozen radio modality encoders extract\nuniversal representations and adapter layers subsequently bridge radio and\nlanguage modalities. Moreover, lightweight task-specific heads are designed to\ndirectly output task objectives. Comprehensive evaluations demonstrate that\nAI2MMUM achieves SOTA performance across five representative physical\nenvironment/wireless channel-based downstream tasks using the WAIR-D and\nDeepMIMO datasets."
                },
                "authors": [
                    {
                        "name": "Tianyu Jiao"
                    },
                    {
                        "name": "Zhuoran Xiao"
                    },
                    {
                        "name": "Yihang Huang"
                    },
                    {
                        "name": "Chenhui Ye"
                    },
                    {
                        "name": "Yijia Feng"
                    },
                    {
                        "name": "Liyu Cai"
                    },
                    {
                        "name": "Jiang Chang"
                    },
                    {
                        "name": "Fangkun Liu"
                    },
                    {
                        "name": "Yin Xu"
                    },
                    {
                        "name": "Dazhi He"
                    },
                    {
                        "name": "Yunfeng Guan"
                    },
                    {
                        "name": "Wenjun Zhang"
                    }
                ],
                "author_detail": {
                    "name": "Wenjun Zhang"
                },
                "author": "Wenjun Zhang",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2505.10003v1",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2505.10003v1",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.LG",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.LG",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "eess.SP",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            },
            {
                "id": "http://arxiv.org/abs/2502.02883v2",
                "guidislink": true,
                "link": "http://arxiv.org/abs/2502.02883v2",
                "updated": "2025-05-15T06:31:41Z",
                "updated_parsed": [
                    2025,
                    5,
                    15,
                    6,
                    31,
                    41,
                    3,
                    135,
                    0
                ],
                "published": "2025-02-05T04:41:59Z",
                "published_parsed": [
                    2025,
                    2,
                    5,
                    4,
                    41,
                    59,
                    2,
                    36,
                    0
                ],
                "title": "SensorChat: Answering Qualitative and Quantitative Questions during\n  Long-Term Multimodal Sensor Interactions",
                "title_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "SensorChat: Answering Qualitative and Quantitative Questions during\n  Long-Term Multimodal Sensor Interactions"
                },
                "summary": "Natural language interaction with sensing systems is crucial for addressing\nusers' personal concerns and providing health-related insights into their daily\nlives. When a user asks a question, the system automatically analyzes the full\nhistory of sensor data, extracts relevant information, and generates an\nappropriate response. However, existing systems are limited to short-duration\n(e.g., one minute) or low-frequency (e.g., daily step count) sensor data. In\naddition, they struggle with quantitative questions that require precise\nnumerical answers. In this work, we introduce SensorChat, the first end-to-end\nQA system designed for daily life monitoring using long-duration,\nhigh-frequency time series data. Given raw sensor signals spanning multiple\ndays and a user-defined natural language question, SensorChat generates\nsemantically meaningful responses that directly address user concerns.\nSensorChat effectively handles both quantitative questions that require\nnumerical precision and qualitative questions that require high-level reasoning\nto infer subjective insights. To achieve this, SensorChat uses an innovative\nthree-stage pipeline including question decomposition, sensor data query, and\nanswer assembly. The first and third stages leverage Large Language Models\n(LLMs) to interpret human queries and generate responses. The intermediate\nquerying stage extracts relevant information from the complete sensor data\nhistory. Real-world implementation demonstrate SensorChat's capability for\nreal-time interactions on a cloud server while also being able to run entirely\non edge platforms after quantization. Comprehensive QA evaluations show that\nSensorChat achieves up to 93% higher answer accuracy than state-of-the-art\nsystems on quantitative questions. Additionally, a user study with eight\nvolunteers highlights SensorChat's effectiveness in answering qualitative and\nopen-ended questions.",
                "summary_detail": {
                    "type": "text/plain",
                    "language": null,
                    "base": "",
                    "value": "Natural language interaction with sensing systems is crucial for addressing\nusers' personal concerns and providing health-related insights into their daily\nlives. When a user asks a question, the system automatically analyzes the full\nhistory of sensor data, extracts relevant information, and generates an\nappropriate response. However, existing systems are limited to short-duration\n(e.g., one minute) or low-frequency (e.g., daily step count) sensor data. In\naddition, they struggle with quantitative questions that require precise\nnumerical answers. In this work, we introduce SensorChat, the first end-to-end\nQA system designed for daily life monitoring using long-duration,\nhigh-frequency time series data. Given raw sensor signals spanning multiple\ndays and a user-defined natural language question, SensorChat generates\nsemantically meaningful responses that directly address user concerns.\nSensorChat effectively handles both quantitative questions that require\nnumerical precision and qualitative questions that require high-level reasoning\nto infer subjective insights. To achieve this, SensorChat uses an innovative\nthree-stage pipeline including question decomposition, sensor data query, and\nanswer assembly. The first and third stages leverage Large Language Models\n(LLMs) to interpret human queries and generate responses. The intermediate\nquerying stage extracts relevant information from the complete sensor data\nhistory. Real-world implementation demonstrate SensorChat's capability for\nreal-time interactions on a cloud server while also being able to run entirely\non edge platforms after quantization. Comprehensive QA evaluations show that\nSensorChat achieves up to 93% higher answer accuracy than state-of-the-art\nsystems on quantitative questions. Additionally, a user study with eight\nvolunteers highlights SensorChat's effectiveness in answering qualitative and\nopen-ended questions."
                },
                "authors": [
                    {
                        "name": "Xiaofan Yu"
                    },
                    {
                        "name": "Lanxiang Hu"
                    },
                    {
                        "name": "Benjamin Reichman"
                    },
                    {
                        "name": "Dylan Chu"
                    },
                    {
                        "name": "Rushil Chandrupatla"
                    },
                    {
                        "name": "Xiyuan Zhang"
                    },
                    {
                        "name": "Larry Heck"
                    },
                    {
                        "name": "Tajana Rosing"
                    }
                ],
                "author_detail": {
                    "name": "Tajana Rosing"
                },
                "author": "Tajana Rosing",
                "arxiv_comment": "Under review",
                "links": [
                    {
                        "href": "http://arxiv.org/abs/2502.02883v2",
                        "rel": "alternate",
                        "type": "text/html"
                    },
                    {
                        "title": "pdf",
                        "href": "http://arxiv.org/pdf/2502.02883v2",
                        "rel": "related",
                        "type": "application/pdf"
                    }
                ],
                "arxiv_primary_category": {
                    "term": "cs.AI",
                    "scheme": "http://arxiv.org/schemas/atom"
                },
                "tags": [
                    {
                        "term": "cs.AI",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    },
                    {
                        "term": "cs.HC",
                        "scheme": "http://arxiv.org/schemas/atom",
                        "label": null
                    }
                ]
            }
        ]
    }
]